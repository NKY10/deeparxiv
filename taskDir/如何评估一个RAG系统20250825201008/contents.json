{"RAGXplain: From Explainable Evaluation to Actionable Guidance of RAG Pipelines": {"Introduction": "Retrieval-Augmented Generation (RAG) systems\nintegrate large language models (LLMs) with ex-\nternal knowledge retrieval to produce more fac-\ntual, relevant, and contextually grounded outputs.\nThis paradigm\u2019s utility spans a wide spectrum,\nfrom enhancing the currency and relevance of\nmodern web search (Wang et al., 2023; Nakano\net al., 2022) to proving crucial in high-stakes ap-\nplications (e.g., finance, law, biomedicine) that de-\nmand faithful and transparent answers (Kim et al.,\n2025; Li et al., 2024b; Xiong et al., 2024). De-\nspite RAG\u2019s promise, rigorous evaluation remainschallenging, particularly for non-technical users\nwho require more than abstract numerical metrics\n(Saad-Falcon et al., 2024; Es et al., 2023).\nIn this paper, we introduce RAGXplain , an\nevaluation framework designed for adaptability.\nIt computes diverse quantitative metrics for RAG\nsystems\u2014allowing users to customize the met-\nric suite\u2014and translates these scores into human-\nreadable explanations and actionable recommen-\ndations. By harnessing the reasoning capabilities\nof various LLMs, RAGXplain addresses the dis-\nconnect between opaque numeric evaluations and\nthe practical diagnostic feedback needed for sys-\ntem improvement\u2014a gap noted in prior work (Es\net al., 2023; Saad-Falcon et al., 2024; Friel et al.,\n2024).\nA key contribution of RAGXplain is its de-\nsign to promote transparency and provide action-\nable diagnostics, thereby fostering user trust while\nalso addressing technical gaps in complex RAG\npipeline development. Unlike opaque \u201cblack-box\u201d\nmodels (Roy et al., 2024; Muller et al., 2025),\nRAGXplain uses LLMs to create a transparency\nlayer. By providing human-readable explanations\nof system reasoning, it aims to improve system in-\nterpretability, which can help mitigate the \u201ctrust\ngap\u201d\u2014a significant barrier to AI adoption. Users\ncan understand not only system outputs but also\ntheir underlying rationale, transforming the RAG\npipeline into a more interpretable process and fa-\ncilitating human-AI collaboration.\nRAGXplain employs a multi-stage explanation\nmodule to analyze performance at retrieval and\ngeneration stages. While raw metric scores of-\nfer basic comparison, they often lack actionable\ninsights for diverse users. RAGXplain addresses\nthis by mapping scores to natural language expla-\nnations that clarify causes of performance short-\ncomings. Drawing on explainable AI methods (Xu\net al., 2023; Liu et al., 2024a; Ru et al., 2024;\nLi et al., 2024a; Liu et al., 2025), it formulatestargeted recommendations for issues like poor re-\ntrieval coverage or generation misalignment, en-\nabling stakeholders to diagnose and improve their\nsystems.\nOur primary contributions are as follows:\n\u2022Explainable Evaluation Metrics: We demon-\nstrate how standard quantitative measures can\nbe augmented with LLM-generated narratives,\ntransforming abstract scores into detailed, user-\nfriendly explanations that illuminate specific ar-\neas of weakness.\n\u2022Actionable System Recommendations: By\nisolating critical failure modes\u2014whether in\nthe retrieval phase or the answer generation\nphase\u2014RAGXplain provides prioritized, practi-\ncal recommendations for immediate system en-\nhancement.\n\u2022Building Trust Through Transparency: By\nmaking AI systems more explainable and ac-\ncountable, RAGXplain helps address the trust\ndeficit limiting AI adoption in high-stakes do-\nmains, fostering more informed user interaction\nand reliance on AI.\n\u2022Empirical Validation: Through comprehen-\nsive experiments on popular public benchmarks,\nwe show that RAGXplain not only correlates\nclosely with human judgment but also drives\nmeasurable improvements in RAG performance,\nbuilding upon and extending the evaluation\nparadigms introduced in works like (Friel et al.,\n2024; Liu et al., 2024a).\n2", "Related Work": "Prior work in evaluating and explaining AI sys-\ntems, particularly for RAG pipelines, can be\nbroadly categorized into three areas: RAG-\nspecific evaluation frameworks, general explain-\nable AI metrics, and hybrid approaches combining\nboth evaluation and explanation.\n2.1", "RAG Evaluation Frameworks": "The emergence of RAGhas sparked development\nof specialized evaluation tools. RAGAS (Es et al.,\n2023) introduced reference-free metrics focusing\non context relevance and faithfulness, while ARES\n(Saad-Falcon et al., 2024) proposed automated\nevaluation through fine-tuned LLM judges. Both\nframeworks provide robust numerical assessments\nbut lack detailed explanations for their verdicts.\nRAGBench & TRACe (Friel et al., 2024) of-\nfered comprehensive dataset-level and pipeline-specific evaluations, while CoFE-RAG (Liu et al.,\n2024a) emphasized granular assessment of in-\ndividual pipeline stages. Though robust, these\nframeworks often target technical users, provid-\ning less guidance for non-experts. Similarly, more\nrecent specialized tools like UAEval4RAG (Peng\net al., 2025) and Eval-RAG (Maklad et al., 2025)\naddress specific challenges such as unanswerable\nqueries and factual correctness, respectively, but\ntypically offer narrow insights rather than broader\nsystem-level recommendations. A concise com-\nparison of RAGXplain with other leading RAG\nevaluation approaches is provided in Appendix Ta-\nble 3.\n2.2", "Explainable AI Metrics": "Traditional evaluation metrics like BLEU (Pap-\nineni et al., 2002) and ROUGE (Lin, 2004) provide\nonly numerical scores without explanation. Re-\ncent work has attempted to address this limitation\nthrough various approaches to metric explainabil-\nity.\nInstructScore (Xu et al., 2023) advanced ex-\nplainable metrics by providing diagnostic reports\nwith scores, though it is tailored for general text\ngeneration and focuses on individual examples\nrather than RAG-specific pipeline improvements.\nSimilarly, tools like RAGChecker (Ru et al., 2024)\nand vRAG-Eval (Wang et al., 2024) incorporate\nbasic explanations for RAG evaluation, but this\nfeedback tends to be instance-specific, thereby\nlacking the aggregate analysis crucial for systemic\noptimization.\n2.3 Hybrid Approaches and RAGXplain\u2019s\nPosition\nRAGXplain synthesizes robust evaluation with\nmeaningful, RAG-specific explanations. While\nprior works offer numerical assessments (e.g.,\nRAGAS (Es et al., 2023), ARES (Saad-Falcon\net al., 2024)) or general NLG explanations (e.g.,\nInstructScore (Xu et al., 2023)), RAGXplain pro-\nvides multi-level (instance and dataset) analysis\nleading to actionable recommendations. It builds\non concepts like Databricks\u2019 \u201dGrading Note\u201d (Liu\net al., 2024b) by not only enhancing interpretabil-\nity but also generating prioritized improvement\nsteps. Key differentiators include its compatibil-\nity with various LLMs and its focus on tailor-\ning explanations and recommendations for com-\nmon RAG failure modes, bridging technical as-\nsessment and practical system improvement for di-Algorithm 1 RAGXplain Pipeline\n1:Input: Dataset D, Metric Definitions Mdef\n2:Output: Insights I, Recommendations R\n3:E\u2190 \u2205\n4:foreach record d\u2208 D do\n5: c\u2190RetrieveContext (d)\n6: a\u2190GenerateAnswer (d, c)\n7: foreach metric definition m\u2208Mdefdo\n8: (s, e)\u2190EvalMetric (m, d, c, a )\n9: Add (m, d, s, e )toE\n10: end for\n11:end for\n12:Magg\u2190AggregateMetrics (E)\n13:I\u2190GenerateInsights (Magg,D)\n14:R\u2190RecommendActions (I,D,Magg)\n15:return (I, R )\nverse users.\n3 RAGXplain Framework\nRAGXplain is a three-stage framework (Fig-\nure 1) providing quantitative metrics, qualitative\ninsights, and actionable recommendations to eval-\nuate and refine RAG pipelines. Its generic ar-\nchitecture is agnostic to the specific LLMs em-\nployed for judging or reasoning and allows for a\ncustomizable suite of evaluation metrics. To make\nthese outputs accessible, it integrates natural lan-\nguage explanations, enabling non-expert users to\nreadily identify and address system weaknesses.\nAlgorithm 1 outlines the RAGXplain pipeline,\nwhich encompasses record-by-record dataset pro-\ncessing, LLM-based individual metric computa-\ntion, dataset-level score aggregation, and the syn-\nthesis of insights and actionable recommenda-\ntions. This overall process, also visualized in\nFigure 1, is structured into three primary stages.\nFirst, metric calculation (Sections 3.1 and 3.3)\nquantifies system performance and generates ini-\ntial explanations. Second, insight generation\n(Section 3.4) synthesizes these individual assess-\nments to identify broader patterns and potential\nroot causes. Finally, action item recommenda-\ntions (Section 3.5) provide targeted advice for sys-\ntem improvement. The subsequent subsections\nelaborate on each of these core stages.\n3.1 Metrics and the \u201cMetric Diamond\u201d\nRAGXplain supports a flexible set of evaluation\nmetrics. We prioritize LLM-based metrics overtraditional mathematical ones due to their superior\nsemantic understanding and, crucially, their ability\nto generate human-readable explanations along-\nside scores (Section 3.3). This explanatory power\nis fundamental to RAGXplain\u2019s capacity for ac-\ntionable diagnostics, which simpler metrics can-\nnot provide. While users can define and integrate\ncustom metrics, we present a core suite of six, vi-\nsualized as the metric diamond (Figure 2). This\nsix-facet structure, building on concepts like the\n\u201cRAG Triad\u201d (Madzou, 2024), offers a compre-\nhensive assessment well-suited for thorough RAG\nevaluation due to its broad coverage.\n\u2022Context Relevancy (User Input \u2192Context):\nCaptures how closely the retrieved context\naligns with the user\u2019s question. Even if the con-\ntent is generally on topic, it might not include all\nkey details required in the user\u2019s question \u2014re-\nsulting in an incomplete or only partially useful\ncontext.\n\u2022Context Adherence (Context \u2192Generated An-\nswer): Evaluates whether the model\u2019s final an-\nswer faithfully uses the retrieved data, as op-\nposed to hallucinating or relying on internal\nmemorized knowledge.\n\u2022Answer Relevancy (User Input \u2192Generated\nAnswer): Checks if the generated answer ade-\nquately addresses the original question, ensuring\ndirectness and completeness.\n\u2022Context Recall (Context \u2192Ground Truth An-\nswer): Determines if the necessary information\nto form the correct (ground truth) answer is fully\npresent in the retrieved context. This metric is\ncrucial in diagnosing whether the retriever stage\nmissed key facts.\n\u2022Factuality (Ground Truth Answer \u2192Generated\nAnswer): Examines the generated answer\u2019s fac-\ntual correctness, comparing it to a ground truth\nreference. This helps detect subtle inaccuracies\nor omitted details.\n\u2022Grading Note (User Input \u2192Generated An-\nswer): Introduced by Databricks(Liu et al.,\n2024b), this metric checks whether the gener-\nated answer follows the expected structural for-\nmat and style. It operates in two steps: first, an\nLLM call produces a blueprint of the ideal an-\nswer structure based on the user\u2019s query; then, aFigure 1: RAGXplain\u2019s evaluation pipeline. The system processes input through three main stages: (1) Metric\ncalculation using LLM-as-judge approach, (2) Insight generation synthesizing patterns across metrics, and (3)\nAction item recommendation for improvement.\nsecond LLM call evaluates how closely the ac-\ntual answer adheres to that blueprint.\nWhen combined, these metrics provide a full-\nchain perspective similar to CoFE-RAG (Liu et al.,\n2024a) but with built-in pathways for explana-\ntion\u2014a design that is necessary to provide rele-\nvant and useful suggestions for users to analyze\nand improve the RAG system. For instance, a sys-\ntem might retrieve excellent context (Context Rel-\nevancy) but score low on Context Adherence if the\nmodel does not actually use that context.\n3.2 Human Labeling for Metric Validation\nFor human validation, two trained annotators as-\nsessed 100 records across the 6 metrics (600 data\npoints). Using the same instructions and scor-\ning rubrics as the LLM judges (see Appendix A),\nthey provided a 0\u20131 score and textual justifica-\ntion for each assessment. Our methodological ap-\nproach for this validation, particularly the use of\nKendall\u2019s \u03c4to compare LLM outputs with human\njudgments, follows that of ARES (Saad-Falcon\net al., 2024).\nResults (Table 1) indicate excellent alignment\nbetween LLM-based scores and human judg-\nments, as measured by Kendall\u2019s \u03c4correlation.\nThe specific correlation values obtained are alsocomparable to those reported in ARES for similar\nassessments. For \u2019Answer Relevancy,\u2019 Kendall\u2019s \u03c4\nwas 0.6470 ( p < 0.001). While this statistically\nsignificant correlation is lower than for other met-\nrics, the Mean Absolute Error (MAE) is small\n(0.08). This combination suggests the LLM ex-\nhibits greater variance in replicating the human-\nperceived rank order for this dimension. This\nstrong overall alignment is further consistent with\nthe broader body of research on leveraging LLMs\nas evaluators in RAG settings (Li et al., 2024a; Liu\net al., 2025; Asai et al., 2023).\n3.3 Metric Calculation\nStage 1: Score and Explanation Generation.\nFor each record, an LLM generates a numeric\nscore (0\u20131) and a brief textual rationale for each\nmetric. The specific prompts used to elicit these\nscores and explanations for our core metrics are\nprovided in Appendix A (Sections A.1\u2013A.7). This\nLLM judge provides robust metric assessments\nand initial explanations. The Grading Note met-\nric, in particular, additionally requires a secondary\nLLM call that refines the structural completeness\nevaluation.\nStage 2: Aggregation. After processing all\nrecords, RAGXplain computes dataset-level statis-Figure 2: RAGXplain\u2019s \u201cmetric diamond\u201d. Each edge\nrepresents a relationship between two components of\nthe RAG pipeline, measured by a dedicated metric.\ntics (mean, standard deviation) for each metric and\nidentifies representative outliers. This aggregated\nview highlights which metrics are frequently prob-\nlematic, thereby guiding subsequent interpretative\nsteps.\n3.4 Insight Generation\nAfter computing the metric scores and generat-\ning explanations for each record, our framework\nsynthesizes high-level insights across the entire\ndataset.\nProcessing Details: For each metric, a special-\nized reasoning LLM receives a structured input\nsummarizing the metric\u2019s name, description, av-\nerage score, and illustrative examples (see Ap-\npendix A.8 for an example of this structured input\nand the resulting output). The full prompt guid-\ning this LLM for insight generation is provided in\nAppendix A.8.\nReasoning Process: Upon receiving the struc-\ntured input, the specialized reasoning model an-\nalyzes both the quantitative scores and the qualita-\ntive explanations from the selected examples. Its\nreasoning capability is crucial for:\n1. Converting raw metrics into an aggregate nar-\nrative that summarizes the overall dataset per-\nformance.\n2. Hypothesizing the underlying causes of per-\nformance issues (e.g., incomplete retrieval or\nmisalignment between retrieved context and\ngenerated answer).\n3. Identifying recurring trends or anomalies\nacross user queries.\nThe output is a coherent, 1\u20132 paragraph free-text\nnarrative containing:\u2022 An overview of the dataset\u2019s performance for the\ngiven metric.\n\u2022 Key trends and issues that emerged from the ex-\namples.\n\u2022 Actionable suggestions and improvements pre-\nsented in non-technical language, ranging from\nchanges in retrieval parameters and adjustments\nto the generation prompt to adding a reranking\nstep, among other targeted modifications.\nThe prompt used to instruct the subsequent\nmodel for generating action items, along with de-\ntailed input/output examples, can be found in Ap-\npendix A.9.\n3.5 Action Item Recommendation\nThe final stage of our framework transforms\ndataset-level insights into structured, actionable\nrecommendations that guide improvements to the\nRAG system. Whereas previous stages focus\non identifying performance trends and potential\nweaknesses, this stage provides a clear, prioritized\nroadmap with recommendations intended to im-\nprove performance by systematically addressing\nthese issues.\nTo generate recommendations, we employ a\nspecialized reasoning model (e.g., GPT-o3-mini or\nDeepseek R1) that is designed not only to summa-\nrize performance data but also to diagnose under-\nlying causes by analyzing both metric trends and\nindividual examples. In our approach, the reason-\ning model processes aggregated insights that in-\nclude overall performance summaries, key trends,\nand representative high and low-scoring examples.\nThis structured input allows the model to reason\nabout recurring issues and to map them to known\nRAG pipeline challenges. The recommendation\nprocess unfolds in several steps: first, the model\nanalyzes the dataset-level insights by considering\nfactors such as average and extreme scores, as well\nas the corresponding explanations from selected\nrecords. For instance, it reviews:\n\u2022 The overall performance for each metric, in-\ncluding trends and anomalies.\n\u2022 Specific examples where scores are notably\nlow or high, along with their associated\nquery-answer pairs.\n\u2022 A predefined list of common issues and\nknown fixes in RAG pipelines.MetricAnswer\nRelevancyContext\nRelevancyContext\nAdherenceFactualityContext\nRecallGrading\nNotes\nKendall\u2013 \u03c4 0.6470* 0.7111* 0.8831* 0.8576* 0.9513* 0.7029*\nMAE 0.08 0.04 0.01 0.04 0.01 0.09\nTable 1: Comparing LLM metrics to human p", "RAGXplain Framework": "RAGXplain is a three-stage framework (Fig-\nure 1) providing quantitative metrics, qualitative\ninsights, and actionable recommendations to eval-\nuate and refine RAG pipelines. Its generic ar-\nchitecture is agnostic to the specific LLMs em-\nployed for judging or reasoning and allows for a\ncustomizable suite of evaluation metrics. To make\nthese outputs accessible, it integrates natural lan-\nguage explanations, enabling non-expert users to\nreadily identify and address system weaknesses.\nAlgorithm 1 outlines the RAGXplain pipeline,\nwhich encompasses record-by-record dataset pro-\ncessing, LLM-based individual metric computa-\ntion, dataset-level score aggregation, and the syn-\nthesis of insights and actionable recommenda-\ntions. This overall process, also visualized in\nFigure 1, is structured into three primary stages.\nFirst, metric calculation (Sections 3.1 and 3.3)\nquantifies system performance and generates ini-\ntial explanations. Second, insight generation\n(Section 3.4) synthesizes these individual assess-\nments to identify broader patterns and potential\nroot causes. Finally, action item recommenda-\ntions (Section 3.5) provide targeted advice for sys-\ntem improvement. The subsequent subsections\nelaborate on each of these core stages.\n3.1 Metrics and the \u201cMetric Diamond\u201d\nRAGXplain supports a flexible set of evaluation\nmetrics. We prioritize LLM-based metrics overtraditional mathematical ones due to their superior\nsemantic understanding and, crucially, their ability\nto generate human-readable explanations along-\nside scores (Section 3.3). This explanatory power\nis fundamental to RAGXplain\u2019s capacity for ac-\ntionable diagnostics, which simpler metrics can-\nnot provide. While users can define and integrate\ncustom metrics, we present a core suite of six, vi-\nsualized as the metric diamond (Figure 2). This\nsix-facet structure, building on concepts like the\n\u201cRAG Triad\u201d (Madzou, 2024), offers a compre-\nhensive assessment well-suited for thorough RAG\nevaluation due to its broad coverage.\n\u2022Context Relevancy (User Input \u2192Context):\nCaptures how closely the retrieved context\naligns with the user\u2019s question. Even if the con-\ntent is generally on topic, it might not include all\nkey details required in the user\u2019s question \u2014re-\nsulting in an incomplete or only partially useful\ncontext.\n\u2022Context Adherence (Context \u2192Generated An-\nswer): Evaluates whether the model\u2019s final an-\nswer faithfully uses the retrieved data, as op-\nposed to hallucinating or relying on internal\nmemorized knowledge.\n\u2022Answer Relevancy (User Input \u2192Generated\nAnswer): Checks if the generated answer ade-\nquately addresses the original question, ensuring\ndirectness and completeness.\n\u2022Context Recall (Context \u2192Ground Truth An-\nswer): Determines if the necessary information\nto form the correct (ground truth) answer is fully\npresent in the retrieved context. This metric is\ncrucial in diagnosing whether the retriever stage\nmissed key facts.\n\u2022Factuality (Ground Truth Answer \u2192Generated\nAnswer): Examines the generated answer\u2019s fac-\ntual correctness, comparing it to a ground truth\nreference. This helps detect subtle inaccuracies\nor omitted details.\n\u2022Grading Note (User Input \u2192Generated An-\nswer): Introduced by Databricks(Liu et al.,\n2024b), this metric checks whether the gener-\nated answer follows the expected structural for-\nmat and style. It operates in two steps: first, an\nLLM call produces a blueprint of the ideal an-\nswer structure based on the user\u2019s query; then, aFigure 1: RAGXplain\u2019s evaluation pipeline. The system processes input through three main stages: (1) Metric\ncalculation using LLM-as-judge approach, (2) Insight generation synthesizing patterns across metrics, and (3)\nAction item recommendation for improvement.\nsecond LLM call evaluates how closely the ac-\ntual answer adheres to that blueprint.\nWhen combined, these metrics provide a full-\nchain perspective similar to CoFE-RAG (Liu et al.,\n2024a) but with built-in pathways for explana-\ntion\u2014a design that is necessary to provide rele-\nvant and useful suggestions for users to analyze\nand improve the RAG system. For instance, a sys-\ntem might retrieve excellent context (Context Rel-\nevancy) but score low on Context Adherence if the\nmodel does not actually use that context.\n3.2 Human Labeling for Metric Validation\nFor human validation, two trained annotators as-\nsessed 100 records across the 6 metrics (600 data\npoints). Using the same instructions and scor-\ning rubrics as the LLM judges (see Appendix A),\nthey provided a 0\u20131 score and textual justifica-\ntion for each assessment. Our methodological ap-\nproach for this validation, particularly the use of\nKendall\u2019s \u03c4to compare LLM outputs with human\njudgments, follows that of ARES (Saad-Falcon\net al., 2024).\nResults (Table 1) indicate excellent alignment\nbetween LLM-based scores and human judg-\nments, as measured by Kendall\u2019s \u03c4correlation.\nThe specific correlation values obtained are alsocomparable to those reported in ARES for similar\nassessments. For \u2019Answer Relevancy,\u2019 Kendall\u2019s \u03c4\nwas 0.6470 ( p < 0.001). While this statistically\nsignificant correlation is lower than for other met-\nrics, the Mean Absolute Error (MAE) is small\n(0.08). This combination suggests the LLM ex-\nhibits greater variance in replicating the human-\nperceived rank order for this dimension. This\nstrong overall alignment is further consistent with\nthe broader body of research on leveraging LLMs\nas evaluators in RAG settings (Li et al., 2024a; Liu\net al., 2025; Asai et al., 2023).\n3.3 Metric Calculation\nStage 1: Score and Explanation Generation.\nFor each record, an LLM generates a numeric\nscore (0\u20131) and a brief textual rationale for each\nmetric. The specific prompts used to elicit these\nscores and explanations for our core metrics are\nprovided in Appendix A (Sections A.1\u2013A.7). This\nLLM judge provides robust metric assessments\nand initial explanations. The Grading Note met-\nric, in particular, additionally requires a secondary\nLLM call that refines the structural completeness\nevaluation.\nStage 2: Aggregation. After processing all\nrecords, RAGXplain computes dataset-level statis-Figure 2: RAGXplain\u2019s \u201cmetric diamond\u201d. Each edge\nrepresents a relationship between two components of\nthe RAG pipeline, measured by a dedicated metric.\ntics (mean, standard deviation) for each metric and\nidentifies representative outliers. This aggregated\nview highlights which metrics are frequently prob-\nlematic, thereby guiding subsequent interpretative\nsteps.\n3.4 Insight Generation\nAfter computing the metric scores and generat-\ning explanations for each record, our framework\nsynthesizes high-level insights across the entire\ndataset.\nProcessing Details: For each metric, a special-\nized reasoning LLM receives a structured input\nsummarizing the metric\u2019s name, description, av-\nerage score, and illustrative examples (see Ap-\npendix A.8 for an example of this structured input\nand the resulting output). The full prompt guid-\ning this LLM for insight generation is provided in\nAppendix A.8.\nReasoning Process: Upon receiving the struc-\ntured input, the specialized reasoning model an-\nalyzes both the quantitative scores and the qualita-\ntive explanations from the selected examples. Its\nreasoning capability is crucial for:\n1. Converting raw metrics into an aggregate nar-\nrative that summarizes the overall dataset per-\nformance.\n2. Hypothesizing the underlying causes of per-\nformance issues (e.g., incomplete retrieval or\nmisalignment between retrieved context and\ngenerated answer).\n3. Identifying recurring trends or anomalies\nacross user queries.\nThe output is a coherent, 1\u20132 paragraph free-text\nnarrative containing:\u2022 An overview of the dataset\u2019s performance for the\ngiven metric.\n\u2022 Key trends and issues that emerged from the ex-\namples.\n\u2022 Actionable suggestions and improvements pre-\nsented in non-technical language, ranging from\nchanges in retrieval parameters and adjustments\nto the generation prompt to adding a reranking\nstep, among other targeted modifications.\nThe prompt used to instruct the subsequent\nmodel for generating action items, along with de-\ntailed input/output examples, can be found in Ap-\npendix A.9.\n3.5 Action Item Recommendation\nThe final stage of our framework transforms\ndataset-level insights into structured, actionable\nrecommendations that guide improvements to the\nRAG system. Whereas previous stages focus\non identifying performance trends and potential\nweaknesses, this stage provides a clear, prioritized\nroadmap with recommendations intended to im-\nprove performance by systematically addressing\nthese issues.\nTo generate recommendations, we employ a\nspecialized reasoning model (e.g., GPT-o3-mini or\nDeepseek R1) that is designed not only to summa-\nrize performance data but also to diagnose under-\nlying causes by analyzing both metric trends and\nindividual examples. In our approach, the reason-\ning model processes aggregated insights that in-\nclude overall performance summaries, key trends,\nand representative high and low-scoring examples.\nThis structured input allows the model to reason\nabout recurring issues and to map them to known\nRAG pipeline challenges. The recommendation\nprocess unfolds in several steps: first, the model\nanalyzes the dataset-level insights by considering\nfactors such as average and extreme scores, as well\nas the corresponding explanations from selected\nrecords. For instance, it reviews:\n\u2022 The overall performance for each metric, in-\ncluding trends and anomalies.\n\u2022 Specific examples where scores are notably\nlow or high, along with their associated\nquery-answer pairs.\n\u2022 A predefined list of common issues and\nknown fixes in RAG pipelines.MetricAnswer\nRelevancyContext\nRelevancyContext\nAdherenceFactualityContext\nRecallGrading\nNotes\nKendall\u2013 \u03c4 0.6470* 0.7111* 0.8831* 0.8576* 0.9513* 0.7029*\nMAE 0.08 0.04 0.01 0.04 0.01 0.09\nTable 1: Comparing LLM metrics to human p", "Human Labeling for Metric Validation": "For human validation, two trained annotators as-\nsessed 100 records across the 6 metrics (600 data\npoints). Using the same instructions and scor-\ning rubrics as the LLM judges (see Appendix A),\nthey provided a 0\u20131 score and textual justifica-\ntion for each assessment. Our methodological ap-\nproach for this validation, particularly the use of\nKendall\u2019s \u03c4to compare LLM outputs with human\njudgments, follows that of ARES (Saad-Falcon\net al., 2024).\nResults (Table 1) indicate excellent alignment\nbetween LLM-based scores and human judg-\nments, as measured by Kendall\u2019s \u03c4correlation.\nThe specific correlation values obtained are alsocomparable to those reported in ARES for similar\nassessments. For \u2019Answer Relevancy,\u2019 Kendall\u2019s \u03c4\nwas 0.6470 ( p < 0.001). While this statistically\nsignificant correlation is lower than for other met-\nrics, the Mean Absolute Error (MAE) is small\n(0.08). This combination suggests the LLM ex-\nhibits greater variance in replicating the human-\nperceived rank order for this dimension. This\nstrong overall alignment is further consistent with\nthe broader body of research on leveraging LLMs\nas evaluators in RAG settings (Li et al., 2024a; Liu\net al., 2025; Asai et al., 2023).\n3.3", "Metric Calculation": "Stage 1: Score and Explanation Generation.\nFor each record, an LLM generates a numeric\nscore (0\u20131) and a brief textual rationale for each\nmetric. The specific prompts used to elicit these\nscores and explanations for our core metrics are\nprovided in Appendix A (Sections A.1\u2013A.7). This\nLLM judge provides robust metric assessments\nand initial explanations. The Grading Note met-\nric, in particular, additionally requires a secondary\nLLM call that refines the structural completeness\nevaluation.\nStage 2: Aggregation. After processing all\nrecords, RAGXplain computes dataset-level statis-Figure 2: RAGXplain\u2019s \u201cmetric diamond\u201d. Each edge\nrepresents a relationship between two components of\nthe RAG pipeline, measured by a dedicated metric.\ntics (mean, standard deviation) for each metric and\nidentifies representative outliers. This aggregated\nview highlights which metrics are frequently prob-\nlematic, thereby guiding subsequent interpretative\nsteps.\n3.4", "Insight Generation": "After computing the metric scores and generat-\ning explanations for each record, our framework\nsynthesizes high-level insights across the entire\ndataset.\nProcessing Details: For each metric, a special-\nized reasoning LLM receives a structured input\nsummarizing the metric\u2019s name, description, av-\nerage score, and illustrative examples (see Ap-\npendix A.8 for an example of this structured input\nand the resulting output). The full prompt guid-\ning this LLM for insight generation is provided in\nAppendix A.8.\nReasoning Process: Upon receiving the struc-\ntured input, the specialized reasoning model an-\nalyzes both the quantitative scores and the qualita-\ntive explanations from the selected examples. Its\nreasoning capability is crucial for:\n1. Converting raw metrics into an aggregate nar-\nrative that summarizes the overall dataset per-\nformance.\n2. Hypothesizing the underlying causes of per-\nformance issues (e.g., incomplete retrieval or\nmisalignment between retrieved context and\ngenerated answer).\n3. Identifying recurring trends or anomalies\nacross user queries.\nThe output is a coherent, 1\u20132 paragraph free-text\nnarrative containing:\u2022 An overview of the dataset\u2019s performance for the\ngiven metric.\n\u2022 Key trends and issues that emerged from the ex-\namples.\n\u2022 Actionable suggestions and improvements pre-\nsented in non-technical language, ranging from\nchanges in retrieval parameters and adjustments\nto the generation prompt to adding a reranking\nstep, among other targeted modifications.\nThe prompt used to instruct the subsequent\nmodel for generating action items, along with de-\ntailed input/output examples, can be found in Ap-\npendix A.9.\n3.5", "Action Item Recommendation": "The final stage of our framework transforms\ndataset-level insights into structured, actionable\nrecommendations that guide improvements to the\nRAG system. Whereas previous stages focus\non identifying performance trends and potential\nweaknesses, this stage provides a clear, prioritized\nroadmap with recommendations intended to im-\nprove performance by systematically addressing\nthese issues.\nTo generate recommendations, we employ a\nspecialized reasoning model (e.g., GPT-o3-mini or\nDeepseek R1) that is designed not only to summa-\nrize performance data but also to diagnose under-\nlying causes by analyzing both metric trends and\nindividual examples. In our approach, the reason-\ning model processes aggregated insights that in-\nclude overall performance summaries, key trends,\nand representative high and low-scoring examples.\nThis structured input allows the model to reason\nabout recurring issues and to map them to known\nRAG pipeline challenges. The recommendation\nprocess unfolds in several steps: first, the model\nanalyzes the dataset-level insights by considering\nfactors such as average and extreme scores, as well\nas the corresponding explanations from selected\nrecords. For instance, it reviews:\n\u2022 The overall performance for each metric, in-\ncluding trends and anomalies.\n\u2022 Specific examples where scores are notably\nlow or high, along with their associated\nquery-answer pairs.\n\u2022 A predefined list of common issues and\nknown fixes in RAG pipelines.MetricAnswer\nRelevancyContext\nRelevancyContext\nAdherenceFactualityContext\nRecallGrading\nNotes\nKendall\u2013 \u03c4 0.6470* 0.7111* 0.8831* 0.8576* 0.9513* 0.7029*\nMAE 0.08 0.04 0.01 0.04 0.01 0.09\nTable 1: Comparing LLM metrics to human p", "Computational Efficiency": "RAGXplain achieves practical efficiency: eval-\nuating 200 records costs under $1.00 (metrics:\n\u223c$0.84 with GPT-4o-mini; synthesis: \u223c$0.07\nwith GPT-o3-mini) and completes in \u223c38s (P90).\nThis latency includes \u223c5s for parallelized per-record metric calculations (which scale minimally\nwith dataset size), \u223c10s for insight generation,\nand\u223c23s for recommendations.\n4", "Experiments": "We rigorously evaluate RAGXplain\u2019s efficacy in\ndiagnosing and improving RAG system perfor-\nmance. Our evaluation procedure is designed to\nassess both traditional quality metrics and the im-\npact of incorporating RAGXplain\u2019s natural lan-\nguage feedback and actionable recommendations.\n4.1", "Datasets and Baseline Setup": "We conducted experiments on five question an-\nswering datasets. Our selection includes:\n\u2022Regular QA: NaturalQuestions\n(NQ)(Kwiatkowski et al., 2019) and\nPopQA(Mallen et al., 2022).\n\u2022Long-Form QA: WikiPassageQA (Cohen et al.,\n2018), ASQA(Stelmakh et al., 2023), and\nWixQA-ExpertWritten (Cohen et al., 2025).\nFor our baseline, we utilized the \u201dNaive RAG\u201d\nimplementation from FlashRAG (Jin et al., 2024),\nan open-source toolkit for RAG that has been\nadopted in several recent research works (Sun\net al., 2025; Li et al., 2024c). While FlashRAG\noffers various advanced RAG configurations, we\nspecifically employed their basic implementa-\ntion to represent a standard RAG pipeline with-\nout additional optimizations. This baseline re-\ntrieves context passages and generates answers in\na straightforward manner, thereby serving as a rep-\nresentative \u201dvanilla\u201d RAG system against which\nwe can evaluate our proposed methods. Details of\nthe parameter settings are provided in Appendix B.\nAcross all datasets, we randomly sampled 200\nexamples per benchmark to capture a representa-\ntive performance profile. Each example was pro-\ncessed by FlashRAG to generate an answer, after\nwhich our evaluation metrics were computed.Metric PopQA NQ WikiQA ASQA WixQA-EW\nNaive RAGX Naive RAGX Naive RAGX Naive RAGX Naive RAGX\nF1 Score 0.41 0.49 0.58 0.48 0.27 0.30 0.34 0.37 0.38 0.42\nBLEU 0.03 0.08 0.08 0.06 0.04 0.07 0.13 0.18 0.09 0.12\nROUGE-1 0.34 0.42 0.52 0.42 0.22 0.25 0.31 0.36 0.30 0.35\nAnswer Relevancy 0.67 0.71 0.76 0.76 0.95 0.96 0.88 0.79 0.99 0.97\nContext Relevancy 0.49 0.72 0.66 0.70 0.58 0.72 0.60 0.72 0.75 0.80\nContext Adherence 0.40 0.47 0.46 0.58 0.69 0.95 0.65 0.91 0.76 0.84\nGrading Note 0.72 0.75 0.57 0.62 0.91 0.94 0.85 0.83 0.84 0.82\nFactuality 0.50 0.59 0.73 0.72 0.57 0.60 0.61 0.57 0.70 0.73\nTable 2: Performance comparison of the Naive approach versus the enhanced Naive implementation incorporating\nRAGXplain (RAGX) suggestions across various datasets: PopQA (Mallen et al., 2022), NQ (Kwiatkowski et al.,\n2019), WikiPassageQA (Cohen et al., 2018), ASQA (Stelmakh et al., 2023), and WixQA-ExpertWritten (Cohen\net al., 2025).\n4.2", "Quantitative Analysis": "This subsection evaluates explainability : we\nask whether RAGXplain\u2019s natural-language diag-\nnoses, when acted upon, deliver measurable im-\nprovements in a RAG pipeline. Accordingly, we\nmeasure (i) the accuracy of the core quantitative\nscores and (ii) the practical utility of the accompa-\nnying explanations.\nThe evaluation procedure unfolds in two stages.\nIn the initial phase, the baseline FlashRAG outputs\nare assessed using both traditional quality metrics\n(e.g., F1, BLEU, ROUGE-1, as presented in Ta-\nble 2) and the suite of RAGXplain-specific metrics\n(detailed in Section 3.1). For each record, the sys-\ntem computes a numeric score alongside a brief\ntextual explanation, highlighting any deficiencies\ndetected within the retrieval or generation stages.\nThese per-sample evaluations are then aggregated\ninto dataset-level statistics, yielding an overall pic-\nture of system performance and identifying com-\nmon areas of concern.\nIn the second phase, we rigorously evaluate\nRAGXplain\u2019s diagnostic power by automatically\napplying a finite set of explanation-induced mod-\nifications to the baseline \u201cNaive RAG\u201d system,\nmirroring realistic developer actions while elim-\ninating manual tuning bias. These modifications\nwere directly guided by the general categories\nof issues and solution types (e.g., insufficient\ndocument retrieval, poor context adherence) that\nRAGXplain\u2019s recommendations typically address.\nThe specific programmatic interventions, detailed\nin Appendix C, represent concrete instantiations ofthese broader recommendation themes, chosen to\nensure experimental consistency and reproducibil-\nity. This protocol resembles the controlled stud-\nies of ARES (Saad-Falcon et al., 2024), yet dif-\nfers in that each intervention originates from the\nexplanation itself rather than from a hand-crafted\nhyper-parameter sweep, making it a direct test of\nexplanatory utility. For example, if RAGXplain\nidentified a recurring issue such as \u201cretrieved con-\ntext is insufficient\u201d (evidenced by low Context Re-\ncall scores and corresponding explanations), a tar-\ngeted programmatic adjustment, like increasing\nthe retrieval parameter kby a set value (e.g., 5),\nwas automatically applied. This automated appli-\ncation of RAGXplain-indicated interventions was\nchosen to ensure experimental consistency and re-\nproducibility, allowing for a clear measure of the\nrecommendations\u2019 direct effect. While RAGX-\nplain is ultimately intended to empower users with\ninsights for manual, nuanced system tuning, this\nexperimental design isolates and quantifies the ef-\nficacy of its core diagnostic and prescriptive capa-\nbilities. The revised system is re-run on the same\n200-sample benchmark, and the resulting outputs\nare re-evaluated to quantify improvements across\nall metrics.\nBy uniting conventional quantitative metrics\nwith detailed, actionable feedback derived from\nour metric diamond, this evaluation procedure\nnot only benchmarks overall performance but also\nprovides clear guidance on targeted improvements\nfor the RAG pipeline.4.3", "Quantitative Results": "Table 2 demonstrates that improvements guided\nby RAGXplain\u2019s recommendations generally en-\nhanced performance across diverse datasets,\nfrom general corpora (PopQA, NQ, WikiQA,\nASQA) to an enterprise knowledge base (WixQA-\nExpertWritten). Average metric scores generally\nincreased, highlighting the framework\u2019s diagnos-\ntic efficacy.\nThe most significant gains were in faithfulness:\nContext Adherence rose substantially across\ndatasets like WikiQA (+0.26), ASQA (+0.26),\nNQ (+0.12), and WixQA-ExpertWritten (+0.08).\nThis increased reliance on provided context also\ngenerally improved Context Relevancy and, on\nseveral datasets including PopQA and WixQA-\nExpertWritten, Factuality .\nTraditional lexical metrics (F1, BLEU,\nROUGE-1) often improved, as seen on PopQA,\nWikiQA, ASQA, and WixQA-ExpertWritten\n(e.g., WixQA-EW F1 +0.04). However, on NQ,\nthese lexical scores decreased despite improved\nContext Adherence. This counterintuitive result\nis likely attributable to NQ\u2019s predominantly short\nreference answers; when a system generates more\nverbose, contextually grounded responses (as\nimproved Context Adherence suggests occurred\nhere), n-gram-based metrics that inherently\nfavor brevity against such short", "Qualitative User Feedback": "To further assess practical utility, we interviewed\nfour non-expert RAG users who applied RAGX-\nplain to their own datasets. This qualitative feed-\nback, complementing the quantitative results (Sec-\ntion 4.2) and metric validation (Section 3.2), re-\nvealed that users found the explanations clear and\nhelpful for understanding system behavior. They\nreported that the framework shows promise for en-hancing user confidence and facilitating system di-\nagnosis, particularly for those less familiar with\nLLM intricacies. This preliminary user validation\nunderscores the value of such transparency for sys-\ntem dependability.\n4.5", "Discussion": "The results show that RAGXplain\u2019s explanation\nmodule not only clarifies why a RAG system un-\nderperforms but, uniquely, demonstrates that act-\ning on these explanations yields consistent metric\nimprovements\u2014an outcome not evaluated in simi-\nlar works such as ARES (Saad-Falcon et al., 2024)\nor RAGBench (Friel et al., 2024). By explicitly\nlinking poor scores to pipeline stages\u2014and enu-\nmerating potential fixes\u2014we align with the call\nfor \u201cactionable metrics\u201d in (Friel et al., 2024).\nFurthermore, RAGXplain\u2019s approach to trans-\nparent evaluation aims to improve how users re-\nlate to AI systems. The proposed metric diamond\n(Fig. 2) enhances process visibility, while gener-\nated insights attribute failures to specific pipeline\nstages. Natural language explanations can act as\nconfidence signals, and actionable recommenda-\ntions enable collaborative remediation. This tran-\nsition from opaque, black-box models towards\nmore transparent, explainable systems facilitates\nimproved human-AI interaction. As demonstrated\nby our qualitative user feedback (Section 4.4),\nthis transparency can indeed foster informed trust\nbased on clear reasoning chains and transparent\nevaluation criteria, moving beyond the blind trust\noften required by previous ML systems.\n5", "Conclusion": "We introduced RAGXplain , an evaluation frame-\nwork that enhances RAG explainability by trans-\nlating quantitative scores into human-readable ex-\nplanations and actionable recommendations us-\ning LLMs. Our experiments validate its capac-\nity to empower diverse users by offering a struc-\ntured procedure for diagnosing issues and system-\natically improving complex RAG systems. This\ntransparency fosters user trust. Building on prior\nwork (Liu et al., 2024a; Xu et al., 2023), RAGX-\nplain offers a full-chain evaluation. Future work\nincludes refining domain-specific modules and\nfurther validation. Ultimately, RAGXplain ad-\nvances the development of more robust, under-\nstandable, and user-centric RAG systems.Acknowledgments\nWe extend our sincere gratitude to our colleagues\nin the Wix Data Science and Wix Labeling\nteams for their support and insightful discussions\nthroughout this research. We would especially\nlike to thank Kseniia Nalyvaiko for her diligent\nand precise work on dataset labeling, which was\ninstrumental to the validation of our framework.\nThis research was supported by Wix.com.", "Answer Relevancy Metric Prompt": "The following prompt is used to instruct the LLM\nin generating the answer relevance metric score for\neach generated answer.\nROLE You are a Response Quality\nAssessor, responsible for evaluating\nhow relevant AI responses are to user\nqueries. Your role is to analyze the\nextent to which the answers address\nthe specific needs and requirements\nof the user\u2019s question.\nTASK DESCRIPTION You will be\ngiven a user\u2019s question along with\nan AI response. Your task is to\nevaluate the relevance of the AI\u2019s\nanswer and select the rating category\nthat best reflects how well the\nresponse addresses the user\u2019s query.\nEVALUATION CRITERIA\n\u20221: Highly Relevant: Answer\nfully addresses the question and\ndemonstrates clear understanding\nof user\u2019s situation.\n\u20220.8: Relevant: Answer addresses\nmain points effectively but may\nmiss minor details of the user\u2019s\nintention and needs.\n\u20220.6: Partially Relevant: Answer\naddresses some aspects but misses\nimportant parts of the user\u2019s\nintention and needs.\n\u20220.4: Mostly Irrelevant: Shows\nbasic understanding but provides\ninappropriate or incomplete\nsolutions; fails to address core\nneeds.\n\u20220.2: Not Relevant: Answer is\ncompletely off-topic or fails\nto address the question in any\nmeaningful way.\nINSTRUCTIONS\n\u2022Review the user query to\nunderstand the query requirements\n\u2022Analyze the AI response against\nthese query requirements\n\u2022Assign a rating (0-1) based on\nhow relevant the answer is to the\nuser\u2019s query\n\u2022Provide the rating in the\nspecified output format\n\u2022Provide a brief explanation\nfocusing on answer relevancyA.2", "Context Relevancy Metric Prompt": "The following prompt is used to instruct the LLM\nin generating the context relevance metric score\nfor each retrieved context.\nROLE You are a Context Relevance\nEvaluator, specialized in assessing\nhow well retrieved information\nmatches and answers user queries.\nTASK DESCRIPTION Evaluate how\nrelevant and helpful the provided\ncontext is for answering the user\nquery by generating a relevance score\nand brief explanation.\nIn case that the the retrieved\ncontext is split mid-sentence or\nmid-paragraph, consider it in the\nscore and in the explanation.\nEVALUATION CRITERIA\n\u20221: Highly relevant with complete\ninformation to answer the query,\nand sentences and paragraphs that\nare not split (including clear\ninformation about impossibility\nif applicable, or not specific\ninformation when it\u2019s not\npossible).\n\u20220.8: Relevant with most of the\ninformation needed to answer the\nquery, without split content.\n\u20220.6: Somewhat relevant with\npartial information to answer\nthe query, or with some split\ncontent.\n\u20220.4: Marginally relevant but\ndoesn\u2019t provide information to\nanswer the query, or with split\ncontent.\n\u20220.2: No relevant information\nto the query or data is mostly\nsplit.\nINSTRUCTIONS\n\u2022Assess if the context contains\ninformation that directly\naddresses the user query\n\u2022Evaluate how completely the\ncontext can help answer the query\n\u2022Check if the context contains\nthe specific information the user\nneeds\n\u2022Consider if the context clearly\nindicates something is not\npossible - treat this as valid\ninformation if it directly\naddresses the query\u2022Consider if additional\ninformation would be needed to\nfully answer the query\n\u2022Provide a brief explanation\nfocusing on answer relevancy\nA.3", "Context Adherence Metric Prompt": "The following prompt is used to instruct the LLM\nin generating the context adherence metric score\nfor each generated answer and retrieved context.\nROLE You are a Context Adherence\nEvaluator. Your job is to determine\nif an answer is strictly derived from\na given context without introducing\nexternal information or assumptions.\nTASK DESCRIPTION Evaluate if\nthe AI response strictly based\non the provided context, without\nintroducing any external information\nor assumptions.\nEVALUATION CRITERIA\n\u20221: Excellent Adherence:\n\u2013The answer is fully derived\nfrom the context.\n\u2013No external information or\nassumptions.\n\u20220.8: Good Adherence:\n\u2013The answer is mostly\ngrounded in the context.\n\u2013Only minor assumptions are\nintroduced.\n\u20220.6: Moderate Adherence:\n\u2013The answer is partially\nsupported by the context,\nbut there are a few\nunsupported or assumed\npoints.\n\u20220.4: Poor Adherence:\n\u2013The answer contains some\ncontext-supported elements\nbut mostly relies on\nexternal information or\nassumptions.\n\u20220.2: Very Poor Adherence:\n\u2013The answer is almost\nentirely unsupported by the\ncontext.\n\u2013Multiple external facts or\nassumptions are introduced.INSTRUCTIONS\n\u2022Read the provided context\ncarefully\n\u2022Analyze the AI response\n\u2022Check if every piece of\ninformation in the answer is\nsupported by the context\n\u2022Assign a rating (0-1) based on\nhow strictly the answer adheres\nto the context (use the rating\ncriteria above)\n\u2022Provide a brief explanation for\nyour decision\nA.4", "Factuality Metric Prompt": "The following prompt is used to instruct the LLM\nin generating the factuality metric score for each\ngenerated answer.\nROLE You are a Factual Alignment\nExpert. Your job is to evaluate\nhow well an AI response includes the\nessential information from a ground\ntruth answer (GT answer) according to\na given user query.\nNote that the Ground Truth (GT\nAnswer), is the \"Correct\" answer\ngenerated by an expert, and was\ncreated to evaluate the model, and\nis NOT part of the AI response or the\ncontext.\nTASK DESCRIPTION You will be\npresented with three elements: a\nquestion, a GT answer, and an AI\nresponse. Determine how well the\nAI response includes the essential\ninformation from the GT answer that\nhelps to solve the user\u2019s query.\nIn case of any additional or\nextra information present in the\nAI response, only make sure it\u2019s not\npreventing the user from solving his\nquery.\nEVALUATION CRITERIA\n\u20221: Complete Match: All\nessential information from GT\nanswer appears in AI response,\nproviding complete solution to\nthe query.\n\u20220.8: Strong Match: Most\nessential information is present,\nwith only minor details missing\nthat don\u2019t impact the solution\nsignificantly.\u20220.6: Partial Match: Core\ninformation is present but\nmissing some important details\nthat would help better solve the\nquery.\n\u20220.4: Limited Match: Only basic\nor partial information present,\nmissing several essential\nelements needed for the solution.\n\u20220.2: Poor Match: Missing most\nessential information or contains\nincorrect information that could\nmislead the user.\nINSTRUCTIONS\n\u2022Read the question carefully and\nanalyze the ground truth answer\nto identify all key information\nelements that help solve the\nquery\n\u2022Compare the AI response\n(candidate answer) against\nthe ground truth, focusing on\npresence of important information\n\u2022Evaluate the completeness and\naccuracy of the information\ntransfer\n\u2022Assign a rating (0-1) based on\nhow well important information is\npreserved\n\u2022Provide a brief explanation\nfocusing on factuality\nA.5", "Context Recall Metric Prompt": "The following prompt is used to instruct the LLM\nin generating the context recall metric score for\neach retrieved context.\nROLE You are a Context Evaluation\nExpert. Your job is to assess how\nwell a retrieved context contains the\nessential information present in a\nground truth answer (GT answer).\nTASK DESCRIPTION You will be\npresented with three pieces of\ninformation: a user query, its\nground truth answer , and a retrieved\ncontext (that will be used to create\nan AI response from). Determine\nhow well the essential information\nfrom the GT answer appears in the\nretrieved context. Additional\ninformation in the retrieved context\nshould not affect the scoring.\nEVALUATION CRITERIA\u20221: Complete Match: All\nessential information from the\nGT answer is present in the\nretrieved context. The context\nfully enables answering the\nuser\u2019s question.\n\u20220.8: Strong Match: All\nessential information is present,\nbut some minor details are\nmissing. The context still\neffectively answers the user\u2019s\nquestion.\n\u20220.6: Partial Match: Most\nessential information is present,\nbut some important details are\nmissing. The context partially\nanswers the user\u2019s question.\n\u20220.4: Weak Match: Only basic or\nlimited essential information is\npresent. The context provides\ninsufficient information to\nproperly answer the user\u2019s\nquestion.\n\u20220.2: No Match: Essential\ninformation is missing or\nincorrect. The context cannot\nbe used to answer the user\u2019s\nquestion.\nINSTRUCTIONS\n\u2022Read the question to understand\nthe idea of what the user asks\nfor\n\u2022Break down the GT answer into\nessential information (key facts,\nmain concepts, direct answers)\n\u2022Check if these information pieces\nappear in the retrieved context\n\u2022Focus only on finding the ground\ntruth information in the context\n- ignore any additional or\nextra information present in\nthe retrieved context\n\u2022Assign a rating (0-1) based\non information coverage and\nrelevance\nA.6", "Grading Notes Generation Prompt": "The following prompt is used to instruct the LLM\nin generating the grading note for each generated\nanswer.\nROLE You are a Technical Education\nExpert specializing in creating\nevaluation criteria. Your job is togenerate concise text that highlights\nthe key elements of an ideal response\nto technical queries. Focus on the\nexpected structure of the answer, and\nnot on the content.\nTASK DESCRIPTION Create brief\nbut precise grading notes that\noutline the essential 1-2 components\nfor evaluating responses to the given\nuser query. The notes should help\nevaluators assess the quality and\ncompleteness of answers.\nINSTRUCTIONS\n\u2022Focus on critical structure\nrequirements\n\u2022Include essential elements only\n\u2022Keep notes concise, SHORT, and\nclear\n\u2022Focus on the basic structure and\nnot on the content\n\u2022Only mention the top 1-2 most\ncrucial elements that MUST appear\nin the answer\n\u2022Ignore issues with the content\nand focus on the structure\nrequirements (for example, \"the\nanswer should contain steps that\nsolve the user issue\") without\nmentioning the content itself\nA.7", "Grading Note Metric Prompt": "The following prompt is used to instruct the LLM\nin generating the grading note metric for each gen-\nerated answer.\nROLE You are an Expert Answer\nQuality Evaluator. Your job is to\nassess the quality of AI responses\nbased on provided grading note\ncriteria.\nTASK DESCRIPTION Evaluate\nthe quality of an AI response by\ncomparing it against the grading note\ncriteria and assign an appropriate\nscore.\nEVALUATION CRITERIA\n\u20221: Excellent: Fully satisfies\nall grading note aspects of the\ngrading note.\n\u20220.8: Good: Addresses most\ngrading note aspects with minor\nomissions.\u20220.6: Fair: Covers some grading\nnote aspects but misses key\nelements.\n\u20220.4: Poor: Addresses few\ngrading note aspects with\nsignificant omissions.\n\u20220.2: Very Poor: Does not\naddress the grading note\ncriteria.\nINSTRUCTIONS\n\u2022Read the user query to understand\nthe context\n\u2022Review the grading note\nrequirements carefully\n\u2022Analyze the AI response against\nthese requirements\n\u2022Assign a score based on the\nscoring scale\n\u2022Provide a brief explanation\njustifying the score\nA.8", "Insight Generation Prompt": "The following prompt is used to instruct the LLM\nin generating insights from metric evaluations.\nBACKGROUND CONTEXT\nRetrieval-Augmented Generation\n(RAG) is a hybrid approach combining\nretrieval-based and generative models\nto enhance text generation with\nexternal knowledge. It consists\nof two main components: retrieval\nand generation.\nIn the retrieval stage, a retriever\n(typically a dense vector model or\na sparse method like BM25) searches\na knowledge base (e.g., documents,\nembeddings) using a query derived\nfrom the input. The retrieved\ndocuments or passages are then passed\nto the generative model (often a\ntransformer-based LLM with a specific\nprompt) as additional context.\nThe generation stage uses this\ncontext to produce a more informed\nand relevant response, typically via\nattention mechanisms that incorporate\nboth the input query and the\nretrieved content. This architecture\nallows RAG to generate accurate,\nup-to-date, and knowledge-rich\nresponses beyond what the model was\npre trained on.\nINSTRUCTIONS You are an\nanalytical model tasked with\nproviding insights for a given RAGmetric evaluation. Follow these\nsteps carefully to analyze the data\nand draw meaningful observations for\nthe model\u2019s builders to improve their\nmodel:\nParse the contents of Insights\nwhich includes:\n\u2022\"metric name\": The name of the\nmetric being evaluated.\n\u2022\"metric description\": A short\ndescription of the metric being\nmeasured.\n\u2022\"avg score\": The average numeric\nscore for this metric.\n\u2022\"random examples\": A list of\nevaluation samples, where each\nobject provides:\n\u2013\"score\": The numeric score\nfor that sample.\n\u2013\"explanation\": The\nreasoning behind the score.\n\u2013\"question\": The user\u2019s\nquestion.\n\u2013\"candidate answer\": The\nmodel\u2019s generated answer\nbeing evaluated.\n\u2013\"gtanswer\" (optional):\nThe ground truth/reference\nanswer, if applicable. The\nground truth answer is given\nonly for evaluation needs,\nand the RAG system does not\nexpose to it at all.\nEvaluate and provide insights based\non the following steps:\n\u2022Comment generally on the overall\nperformance of the metric based\non \"avg score\" (e.g., is the\nperformance strong, moderate, or\npoor?).\n\u2022If the overall performance of the\nmetric is relatively low:\n\u2013Suggest practical\nimprovements that can be\nimplemented to boost the\nperformance of this metric\nin the future.\n\u2013When possible, provide an\nexact idea, for example - in\nthe generation parts of the\nRAG - what exact sentence\nto include in the prompt,\nin order to get the desired\noutput.\n\u2013Pay attention - Don\u2019t\nsuggest improvement in the\nprompt if the issue is in\nthe retrieval component.\nUpdating the prompt will not\nwork for the retrieval parts\nin the RAG process.\u2022If the overall performance of\nthe metric is relatively high (at\nleast 0.95):\n\u2013Explain that the overall\nperformance of the metric is\ngreat, and that changing it\nwon\u2019t lead to a major uplift\nin the performance.\nFinal Output Provide a concise\nanalysis (3{5 sentences) as plain\ntext. Avoid unnecessary formatting\nand keep your language clear and\ndirect. Make an insight that ensures\nthat the model\u2019s creator knows how\nto process the input data, provide\nmeaningful qualitative feedback, and\noffer concise actionable insights for\nimprovement.\nNote\n\u2022Based on the description,\nunderstand which components\nare relevant to each metric\nand use only those in your\nrecommendations.\n\u2022We do not provide examples of\ncontext as it is too long.\nA.9", "Action Item Generation Prompt": "The following prompt is used to instruct the LLM\nin generating actionable recommendations based\non aggregated insights.\nBACKGROUND CONTEXT\nRetrieval-Augmented Generation (RAG)\nis a hybrid approach combining\nretrieval-based and generative\nmodels to enhance text generation\nwith external knowledge. It consists\nof two main components: retrieval\nand generation. In the retrieval\nstage, a retriever (typically a\ndense vector model or a sparse method\nlike BM25) searches a knowledge base\n(e.g., documents, embeddings) using\na query derived from the input. The\nretrieved documents or passages are\nthen passed to the generative model\n(often a transformer-based LLM with\na specific prompt) as additional\ncontext. The generation stage uses\nthis context to produce a more\ninformed and relevant response,\ntypically via attention mechanisms\nthat incorporate both the input\nquery and the retrieved content.\nThis architecture allows RAG to\ngenerate accurate, up-to-date, and\nknowledge-rich responses beyond what\nthe model was pre-trained on.Note The Ground Truth (GT Answer),\nis the \"Correct\" answer generated\nby an expert, and was created to\nevaluate the model, and is NOT part\nof the answer or the context.\nYOUR TASK\nYou are tasked with analyzing\nevaluation results from a\nRetrieval-Augmented Generation (RAG)\npipeline. The analysis should be\nbased on a comprehensive metrics\ndictionary containing detailed\nperformance insights.\nINPUT FORMAT\nThe input format is a JSON object\nstructured as follows:\nListing 1: Example input structure for action item\ngeneration (within prompt description).\ninsights = {\n\"metric_name_1\": {\n\"metric_description\": \"<description>\",\n\"metric_insights\": \"<insight>\",\n\"avg_score\": <average_score>,\n// ... other fields as in your example ...\n\"max_score_example_question_ground_truth_answer\":\n\"<ground_truth_answer_for_max_score_example>\"\n},\n// ... more metrics or examples ...\n\"example_34\": {\n\"question\": \"<question>\",\n// ... other fields as in your example ...\n\"metric_name_2 - Explanation\": \"<explanation>\"\n}\n// ...\n}\n(Please see Appendix A.9 for a fully\nrendered example of the input JSON.)\nGUIDELINES\n1. In your recommendations,\nelaborate shortly on the hints\n(one or more) that lead you to\nprovide the recommendation.\n2. Use the provided insights,\nscores, and data examples\nto formulate up to four\nrecommendations that will\nmaximize the % uplift and deliver\nthe most beneficial suggestions\n(for example: improvement from\n97% to 99% is not meaningful).\n3. Support each recommendation\nwith specific examples from the\nmetrics. Note: The examples\nprovided are randomly sampled\nwith the user\u2019s data.\n4. Maintain a clear connection\nbetween metrics insights and\nproposed improvements.\n5. Focus on actionable improvements\nthat directly address identified\nissues.\n6. When mentioning examples, don\u2019t\nuse the example number andinstead use the example\u2019s user\nquestion and any other relevant\nfields.\n7. When possible, provide an exact\nidea, for example - what exact\nsentence to include in the\nprompt, in order to get the\ndesired output.\n8. Write simple, short and concise\nexplanations and suggestions.\n9. Prioritize the suggestions\naccording to their potential\nto increase the model\u2019s score per\nmetric - ( High Impact & Priority\n/Medium Impact & Priority /Low\nImpact & Priority ), and to their\nimplementation feasibility. Rely\non the extent of the issue, and\nexplain to the user how much\nhe can improve the score of the\nspecific metric, by implying the\nsuggestion.\n10. Don\u2019t refer to specific scores in\nyour recommendations.\n11. Provide the output in a markdown\nformat. Don\u2019t write anything\nexcept the markdown code.\nOUTPUT STRUCTURE EXAMPLE\nThe following illustrates the desired\nMarkdown output structure: Title\nrecommendation (High Impact &\nPriority) **What\u2019s wrong?: **Short\nexplanation **Why did this happen?: **\nShort explanation **Example: **Clear\nand concise example with a short\nexplanation **How to fix it?: **\nClear and concise explanation, with\nspecific prompt examples if necessary\nTitle recommendation (Medium Impact\n& Priority) **What\u2019s wrong?: **Short\nexplanation **Why did this happen?: **\nShort explanation **Example: **Clear\nand concise example with a short\nexplanation **How to fix it?: **\nClear and concise explanation, with\nspecific prompt examples if necessary\nB", "FlashRAG Configuration Parameters": "This section outlines the essential FlashRAG pa-\nrameters that influence retrieval and generation\nperformance. Parameters related to file paths\nand other environment-specific settings have been\nomitted for clarity.\nB.1", "Dataset": "D, Metric Definitions Mdef\n2:Output: Insights I, Recommendations R\n3:E\u2190 \u2205\n4:foreach record d\u2208 D do\n5: c\u2190RetrieveContext (d)\n6: a\u2190GenerateAnswer (d, c)\n7: foreach metric definition m\u2208Mdefdo\n8: (s, e)\u2190EvalMetric (m, d, c, a )\n9: Add (m, d, s, e )toE\n10: end for\n11:end for\n12:Magg\u2190AggregateMetrics (E)\n13:I\u2190GenerateInsights (Magg,D)\n14:R\u2190RecommendActions (I,D,Magg)\n15:return (I, R )\nverse users.\n3 RAGXplain Framework\nRAGXplain is a three-stage framework (Fig-\nure 1) providing quantitative metrics, qualitative\ninsights, and actionable recommendations to eval-\nuate and refine RAG pipelines. Its generic ar-\nchitecture is agnostic to the specific LLMs em-\nployed for judging or reasoning and allows for a\ncustomizable suite of evaluation metrics. To make\nthese outputs accessible, it integrates natural lan-\nguage explanations, enabling non-expert users to\nreadily identify and address system weaknesses.\nAlgorithm 1 outlines the RAGXplain pipeline,\nwhich encompasses record-by-record dataset pro-\ncessing, LLM-based individual metric computa-\ntion, dataset-level score aggregation, and the syn-\nthesis of insights and actionable recommenda-\ntions. This overall process, also visualized in\nFigure 1, is structured into three primary stages.\nFirst, metric calculation (Sections 3.1 and 3.3)\nquantifies system performance and generates ini-\ntial explanations. Second, insight generation\n(Section 3.4) synthesizes these individual assess-\nments to identify broader patterns and potential\nroot causes. Finally, action item recommenda-\ntions (Section 3.5) provide targeted advice for sys-\ntem improvement. The subsequent subsections\nelaborate on each of these core stages.\n3.1 Metrics and the \u201cMetric Diamond\u201d\nRAGXplain supports a flexible set of evaluation\nmetrics. We prioritize LLM-based metrics overtraditional mathematical ones due to their superior\nsemantic understanding and, crucially, their ability\nto generate human-readable explanations along-\nside scores (Section 3.3). This explanatory power\nis fundamental to RAGXplain\u2019s capacity for ac-\ntionable diagnostics, which simpler metrics can-\nnot provide. While users can define and integrate\ncustom metrics, we present a core suite of six, vi-\nsualized as the metric diamond (Figure 2). This\nsix-facet structure, building on concepts like the\n\u201cRAG Triad\u201d (Madzou, 2024), offers a compre-\nhensive assessment well-suited for thorough RAG\nevaluation due to its broad coverage.\n\u2022Context Relevancy (User Input \u2192Context):\nCaptures how closely the retrieved context\naligns with the user\u2019s question. Even if the con-\ntent is generally on topic, it might not include all\nkey details required in the user\u2019s question \u2014re-\nsulting in an incomplete or only partially useful\ncontext.\n\u2022Context Adherence (Context \u2192Generated An-\nswer): Evaluates whether the model\u2019s final an-\nswer faithfully uses the retrieved data, as op-\nposed to hallucinating or relying on internal\nmemorized knowledge.\n\u2022Answer Relevancy (User Input \u2192Generated\nAnswer): Checks if the generated answer ade-\nquately addresses the original question, ensuring\ndirectness and completeness.\n\u2022Context Recall (Context \u2192Ground Truth An-\nswer): Determines if the necessary information\nto form the correct (ground truth) answer is fully\npresent in the retrieved context. This metric is\ncrucial in diagnosing whether the retriever stage\nmissed key facts.\n\u2022Factuality (Ground Truth Answer \u2192Generated\nAnswer): Examines the generated answer\u2019s fac-\ntual correctness, comparing it to a ground truth\nreference. This helps detect subtle inaccuracies\nor omitted details.\n\u2022Grading Note (User Input \u2192Generated An-\nswer): Introduced by Databricks(Liu et al.,\n2024b), this metric checks whether the gener-\nated answer follows the expected structural for-\nmat and style. It operates in two steps: first, an\nLLM call produces a blueprint of the ideal an-\nswer structure based on the user\u2019s query; then, aFigure 1: RAGXplain\u2019s evaluation pipeline. The system processes input through three main stages: (1) Metric\ncalculation using LLM-as-judge approach, (2) Insight generation synthesizing patterns across metrics, and (3)\nAction item recommendation for improvement.\nsecond LLM call evaluates how closely the ac-\ntual answer adheres to that blueprint.\nWhen combined, these metrics provide a full-\nchain perspective similar to CoFE-RAG (Liu et al.,\n2024a) but with built-in pathways for explana-\ntion\u2014a design that is necessary to provide rele-\nvant and useful suggestions for users to analyze\nand improve the RAG system. For instance, a sys-\ntem might retrieve excellent context (Context Rel-\nevancy) but score low on Context Adherence if the\nmodel does not actually use that context.\n3.2 Human Labeling for Metric Validation\nFor human validation, two trained annotators as-\nsessed 100 records across the 6 metrics (600 data\npoints). Using the same instructions and scor-\ning rubrics as the LLM judges (see Appendix A),\nthey provided a 0\u20131 score and textual justifica-\ntion for each assessment. Our methodological ap-\nproach for this validation, particularly the use of\nKendall\u2019s \u03c4to compare LLM outputs with human\njudgments, follows that of ARES (Saad-Falcon\net al., 2024).\nResults (Table 1) indicate excellent alignment\nbetween LLM-based scores and human judg-\nments, as measured by Kendall\u2019s \u03c4correlation.\nThe specific correlation values obtained are alsocomparable to those reported in ARES for similar\nassessments. For \u2019Answer Relevancy,\u2019 Kendall\u2019s \u03c4\nwas 0.6470 ( p < 0.001). While this statistically\nsignificant correlation is lower than for other met-\nrics, the Mean Absolute Error (MAE) is small\n(0.08). This combination suggests the LLM ex-\nhibits greater variance in replicating the human-\nperceived rank order for this dimension. This\nstrong overall alignment is further consistent with\nthe broader body of research on leveraging LLMs\nas evaluators in RAG settings (Li et al., 2024a; Liu\net al., 2025; Asai et al., 2023).\n3.3 Metric Calculation\nStage 1: Score and Explanation Generation.\nFor each record, an LLM generates a numeric\nscore (0\u20131) and a brief textual rationale for each\nmetric. The specific prompts used to elicit these\nscores and explanations for our core metrics are\nprovided in Appendix A (Sections A.1\u2013A.7). This\nLLM judge provides robust metric assessments\nand initial explanations. The Grading Note met-\nric, in particular, additionally requires a secondary\nLLM call that refines the structural completeness\nevaluation.\nStage 2: Aggregation. After processing all\nrecords, RAGXplain computes dataset-level statis-Figure 2: RAGXplain\u2019s \u201cmetric diamond\u201d. Each edge\nrepresents a relationship between two components of\nthe RAG pipeline, measured by a dedicated metric.\ntics (mean, standard deviation) for each metric and\nidentifies representative outliers. This aggregated\nview highlights which metrics are frequently prob-\nlematic, thereby guiding subsequent interpretative\nsteps.\n3.4 Insight Generation\nAfter computing the metric scores and generat-\ning explanations for each record, our framework\nsynthesizes high-level insights across the entire\ndataset.\nProcessing Details: For each metric, a special-\nized reasoning LLM receives a structured input\nsummarizing the metric\u2019s name, description, av-\nerage score, and illustrative examples (see Ap-\npendix A.8 for an example of this structured input\nand the resulting output). The full prompt guid-\ning this LLM for insight generation is provided in\nAppendix A.8.\nReasoning Process: Upon receiving the struc-\ntured input, the specialized reasoning model an-\nalyzes both the quantitative scores and the qualita-\ntive explanations from the selected examples. Its\nreasoning capability is crucial for:\n1. Converting raw metrics into an aggregate nar-\nrative that summarizes the overall dataset per-\nformance.\n2. Hypothesizing the underlying causes of per-\nformance issues (e.g., incomplete retrieval or\nmisalignment between retrieved context and\ngenerated answer).\n3. Identifying recurring trends or anomalies\nacross user queries.\nThe output is a coherent, 1\u20132 paragraph free-text\nnarrative containing:\u2022 An overview of the dataset\u2019s performance for the\ngiven metric.\n\u2022 Key trends and issues that emerged from the ex-\namples.\n\u2022 Actionable suggestions and improvements pre-\nsented in non-technical language, ranging from\nchanges in retrieval parameters and adjustments\nto the generation prompt to adding a reranking\nstep, among other targeted modifications.\nThe prompt used to instruct the subsequent\nmodel for generating action items, along with de-\ntailed input/output examples, can be found in Ap-\npendix A.9.\n3.5 Action Item Recommendation\nThe final stage of our framework transforms\ndataset-level insights into structured, actionable\nrecommendations that guide improvements to the\nRAG system. Whereas previous stages focus\non identifying performance trends and potential\nweaknesses, this stage provides a clear, prioritized\nroadmap with recommendations intended to im-\nprove performance by systematically addressing\nthese issues.\nTo generate recommendations, we employ a\nspecialized reasoning model (e.g., GPT-o3-mini or\nDeepseek R1) that is designed not only to summa-\nrize performance data but also to diagnose under-\nlying causes by analyzing both metric trends and\nindividual examples. In our approach, the reason-\ning model processes aggregated insights that in-\nclude overall performance summaries, key trends,\nand representative high and low-scoring examples.\nThis structured input allows the model to reason\nabout recurring issues and to map them to known\nRAG pipeline challenges. The recommendation\nprocess unfolds in several steps: first, the model\nanalyzes the dataset-level insights by considering\nfactors such as average and extreme scores, as well\nas the corresponding explanations from selected\nrecords. For instance, it reviews:\n\u2022 The overall performance for each metric, in-\ncluding trends and anomalies.\n\u2022 Specific examples where scores are notably\nlow or high, along with their associated\nquery-answer pairs.\n\u2022 A predefined list of common issues and\nknown fixes in RAG pipelines.MetricAnswer\nRelevancyContext\nRelevancyContext\nAdherenceFactualityContext\nRecallGrading\nNotes\nKendall\u2013 \u03c4 0.6470* 0.7111* 0.8831* 0.8576* 0.9513* 0.7029*\nMAE 0.08 0.04 0.01 0.04 0.01 0.09\nTable 1: Comparing LLM metrics to human p", "Generation Prompt": "The following prompt is used to instruct the LLM\nin generating the grading note for each generated\nanswer.\nROLE You are a Technical Education\nExpert specializing in creating\nevaluation criteria. Your job is togenerate concise text that highlights\nthe key elements of an ideal response\nto technical queries. Focus on the\nexpected structure of the answer, and\nnot on the content.\nTASK DESCRIPTION Create brief\nbut precise grading notes that\noutline the essential 1-2 components\nfor evaluating responses to the given\nuser query. The notes should help\nevaluators assess the quality and\ncompleteness of answers.\nINSTRUCTIONS\n\u2022Focus on critical structure\nrequirements\n\u2022Include essential elements only\n\u2022Keep notes concise, SHORT, and\nclear\n\u2022Focus on the basic structure and\nnot on the content\n\u2022Only mention the top 1-2 most\ncrucial elements that MUST appear\nin the answer\n\u2022Ignore issues with the content\nand focus on the structure\nrequirements (for example, \"the\nanswer should contain steps that\nsolve the user issue\") without\nmentioning the content itself\nA.7 Grading Note Metric Prompt\nThe following prompt is used to instruct the LLM\nin generating the grading note metric for each gen-\nerated answer.\nROLE You are an Expert Answer\nQuality Evaluator. Your job is to\nassess the quality of AI responses\nbased on provided grading note\ncriteria.\nTASK DESCRIPTION Evaluate\nthe quality of an AI response by\ncomparing it against the grading note\ncriteria and assign an appropriate\nscore.\nEVALUATION CRITERIA\n\u20221: Excellent: Fully satisfies\nall grading note aspects of the\ngrading note.\n\u20220.8: Good: Addresses most\ngrading note aspects with minor\nomissions.\u20220.6: Fair: Covers some grading\nnote aspects but misses key\nelements.\n\u20220.4: Poor: Addresses few\ngrading note aspects with\nsignificant omissions.\n\u20220.2: Very Poor: Does not\naddress the grading note\ncriteria.\nINSTRUCTIONS\n\u2022Read the user query to understand\nthe context\n\u2022Review the grading note\nrequirements carefully\n\u2022Analyze the AI response against\nthese requirements\n\u2022Assign a score based on the\nscoring scale\n\u2022Provide a brief explanation\njustifying the score\nA.8 Insight Generation Prompt\nThe following prompt is used to instruct the LLM\nin generating insights from metric evaluations.\nBACKGROUND CONTEXT\nRetrieval-Augmented Generation\n(RAG) is a hybrid approach combining\nretrieval-based and generative models\nto enhance text generation with\nexternal knowledge. It consists\nof two main components: retrieval\nand generation.\nIn the retrieval stage, a retriever\n(typically a dense vector model or\na sparse method like BM25) searches\na knowledge base (e.g., documents,\nembeddings) using a query derived\nfrom the input. The retrieved\ndocuments or passages are then passed\nto the generative model (often a\ntransformer-based LLM with a specific\nprompt) as additional context.\nThe generation stage uses this\ncontext to produce a more informed\nand relevant response, typically via\nattention mechanisms that incorporate\nboth the input query and the\nretrieved content. This architecture\nallows RAG to generate accurate,\nup-to-date, and knowledge-rich\nresponses beyond what the model was\npre trained on.\nINSTRUCTIONS You are an\nanalytical model tasked with\nproviding insights for a given RAGmetric evaluation. Follow these\nsteps carefully to analyze the data\nand draw meaningful observations for\nthe model\u2019s builders to improve their\nmodel:\nParse the contents of Insights\nwhich includes:\n\u2022\"metric name\": The name of the\nmetric being evaluated.\n\u2022\"metric description\": A short\ndescription of the metric being\nmeasured.\n\u2022\"avg score\": The average numeric\nscore for this metric.\n\u2022\"random examples\": A list of\nevaluation samples, where each\nobject provides:\n\u2013\"score\": The numeric score\nfor that sample.\n\u2013\"explanation\": The\nreasoning behind the score.\n\u2013\"question\": The user\u2019s\nquestion.\n\u2013\"candidate answer\": The\nmodel\u2019s generated answer\nbeing evaluated.\n\u2013\"gtanswer\" (optional):\nThe ground truth/reference\nanswer, if applicable. The\nground truth answer is given\nonly for evaluation needs,\nand the RAG system does not\nexpose to it at all.\nEvaluate and provide insights based\non the following steps:\n\u2022Comment generally on the overall\nperformance of the metric based\non \"avg score\" (e.g., is the\nperformance strong, moderate, or\npoor?).\n\u2022If the overall performance of the\nmetric is relatively low:\n\u2013Suggest practical\nimprovements that can be\nimplemented to boost the\nperformance of this metric\nin the future.\n\u2013When possible, provide an\nexact idea, for example - in\nthe generation parts of the\nRAG - what exact sentence\nto include in the prompt,\nin order to get the desired\noutput.\n\u2013Pay attention - Don\u2019t\nsuggest improvement in the\nprompt if the issue is in\nthe retrieval component.\nUpdating the prompt will not\nwork for the retrieval parts\nin the RAG process.\u2022If the overall performance of\nthe metric is relatively high (at\nleast 0.95):\n\u2013Explain that the overall\nperformance of the metric is\ngreat, and that changing it\nwon\u2019t lead to a major uplift\nin the performance.\nFinal Output Provide a concise\nanalysis (3{5 sentences) as plain\ntext. Avoid unnecessary formatting\nand keep your language clear and\ndirect. Make an insight that ensures\nthat the model\u2019s creator knows how\nto process the input data, provide\nmeaningful qualitative feedback, and\noffer concise actionable insights for\nimprovement.\nNote\n\u2022Based on the description,\nunderstand which components\nare relevant to each metric\nand use only those in your\nrecommendations.\n\u2022We do not provide examples of\ncontext as it is too long.\nA.9 Action Item Generation Prompt\nThe following prompt is used to instruct the LLM\nin generating actionable recommendations based\non aggregated insights.\nBACKGROUND CONTEXT\nRetrieval-Augmented Generation (RAG)\nis a hybrid approach combining\nretrieval-based and generative\nmodels to enhance text generation\nwith external knowledge. It consists\nof two main components: retrieval\nand generation. In the retrieval\nstage, a retriever (typically a\ndense vector model or a sparse method\nlike BM25) searches a knowledge base\n(e.g., documents, embeddings) using\na query derived from the input. The\nretrieved documents or passages are\nthen passed to the generative model\n(often a transformer-based LLM with\na specific prompt) as additional\ncontext. The generation stage uses\nthis context to produce a more\ninformed and relevant response,\ntypically via attention mechanisms\nthat incorporate both the input\nquery and the retrieved content.\nThis architecture allows RAG to\ngenerate accurate, up-to-date, and\nknowledge-rich responses beyond what\nthe model was pre-trained on.Note The Ground Truth (GT Answer),\nis the \"Correct\" answer generated\nby an expert, and was created to\nevaluate the model, and is NOT part\nof the answer or the context.\nYOUR TASK\nYou are tasked with analyzing\nevaluation results from a\nRetrieval-Augmented Generation (RAG)\npipeline. The analysis should be\nbased on a comprehensive metrics\ndictionary containing detailed\nperformance insights.\nINPUT FORMAT\nThe input format is a JSON object\nstructured as follows:\nListing 1: Example input structure for action item\ngeneration (within prompt description).\ninsights = {\n\"metric_name_1\": {\n\"metric_description\": \"<description>\",\n\"metric_insights\": \"<insight>\",\n\"avg_score\": <average_score>,\n// ... other fields as in your example ...\n\"max_score_example_question_ground_truth_answer\":\n\"<ground_truth_answer_for_max_score_example>\"\n},\n// ... more metrics or examples ...\n\"example_34\": {\n\"question\": \"<question>\",\n// ... other fields as in your example ...\n\"metric_name_2 - Explanation\": \"<explanation>\"\n}\n// ...\n}\n(Please see Appendix A.9 for a fully\nrendered example of the input JSON.)\nGUIDELINES\n1. In your recommendations,\nelaborate shortly on the hints\n(one or more) that lead you to\nprovide the recommendation.\n2. Use the provided insights,\nscores, and data examples\nto formulate up to four\nrecommendations that will\nmaximize the % uplift and deliver\nthe most beneficial suggestions\n(for example: improvement from\n97% to 99% is not meaningful).\n3. Support each recommendation\nwith specific examples from the\nmetrics. Note: The examples\nprovided are randomly sampled\nwith the user\u2019s data.\n4. Maintain a clear connection\nbetween metrics insights and\nproposed improvements.\n5. Focus on actionable improvements\nthat directly address identified\nissues.\n6. When mentioning examples, don\u2019t\nuse the example number andinstead use the example\u2019s user\nquestion and any other relevant\nfields.\n7. When possible, provide an exact\nidea, for example - what exact\nsentence to include in the\nprompt, in order to get the\ndesired output.\n8. Write simple, short and concise\nexplanations and suggestions.\n9. Prioritize the suggestions\naccording to their potential\nto increase the model\u2019s score per\nmetric - ( High Impact & Priority\n/Medium Impact & Priority /Low\nImpact & Priority ), and to their\nimplementation feasibility. Rely\non the extent of the issue, and\nexplain to the user how much\nhe can improve the score of the\nspecific metric, by implying the\nsuggestion.\n10. Don\u2019t refer to specific scores in\nyour recommendations.\n11. Provide the output in a markdown\nformat. Don\u2019t write anything\nexcept the markdown code.\nOUTPUT STRUCTURE EXAMPLE\nThe following illustrates the desired\nMarkdown output structure: Title\nrecommendation (High Impact &\nPriority) **What\u2019s wrong?: **Short\nexplanation **Why did this happen?: **\nShort explanation **Example: **Clear\nand concise example with a short\nexplanation **How to fix it?: **\nClear and concise explanation, with\nspecific prompt examples if necessary\nTitle recommendation (Medium Impact\n& Priority) **What\u2019s wrong?: **Short\nexplanation **Why did this happen?: **\nShort explanation **Example: **Clear\nand concise example with a short\nexplanation **How to fix it?: **\nClear and concise explanation, with\nspecific prompt examples if necessary\nB FlashRAG Configuration Parameters\nThis section outlines the essential FlashRAG pa-\nrameters that influence retrieval and generation\nperformance. Parameters related to file paths\nand other environment-specific settings have been\nomitted for clarity.\nB.1 Dataset\n\u2022Dataset :wiki18 fulldoc\n\u2022framework :openaiMethod Description Explainability\nRAGXplain (Ours) Multi-metric evaluation with\nNatural Language explanations\nand actionable recommenda-\ntions.Detailed, user-friendly.\nRAGAS (Es et al., 2023) Reference-free metrics (context\nrelevance, faithfulness).None.\nARES (Saad-Falcon et al.,\n2024)Automated evaluation via fine-\ntuned LLM judges.Minimal.\nvRAG-Eval (Wang et al., 2024) LLM-based grading with score\nand brief rationale.Basic rationale.\nCoFE-RAG (Liu et al., 2024a) Fine-grained evaluation of\npipeline stages (numeric out-\nput).None.\nTRACe (Friel et al., 2024) LLM-based and algorithmic\nbased grading with scoreNone.\nUAEval4RAG (Peng et al.,\n2025)Evaluates unanswerability via\nspecific metrics.Limited.\nEval-RAG (Maklad et al.,\n2025)Retrieval validation for factual\ncorrectness.None.\nRAGChecker (Ru et al., 2024) Diagnostic tool for debugging\nRAG pipelines.Minimal (developer-oriented).\nInstructScore (Xu et al., 2023) General NLG metric with diag-\nnostic feedback.Present for individual example.\nnot RAG-specific.\nTable 3: Comparison of RAG evaluation methods with a focus on explainability.\n\u2022generator model :openai\n\u2022generator batch size: 25\n\u2022generator max input len: 50,000\n\u2022retrieval method :bm25\n\u2022retrieval topk : 5\n\u2022retrieval query max length : 10,000\n\u2022retrieval batch size: 1024\n\u2022testsample num : 200\n\u2022random sample :True\n\u2022seed: 2024\n\u2022max tokens : 1024\n\u2022temperature : 0B.2 Generation Prompt\nThe prompt is taken directly from the FlashRAG\nimplementation:\n\u201cYou are a friendly AI Assistant. Instructions:\nRespond to the input as a friendly AI assistant,\ngenerating human-like text, and follow the in-\nstructions in the input if applicable. The fol-\nlowing are provided"}, "RAGBench: Explainable Benchmark for Retrieval-Augmented Generation Systems": {"Introduction": "Despite remarkable reasoning and conversational abilities, out-of-the-box pre-trained Large Language\nModels (LLMs) struggle to reason about out-of-domain, knowledge-intensive queries [ 20,13]. In\nresponse, Retriever-Augmented Generation (RAG) systems [ 20,19] are becoming increasingly\npopular in user-facing dialogue applications [ 34]. Generally, RAG systems comprise a retriever\ncomponent that queries relevant documents from an in-domain corpus and a downstream LLM\ngenerator model that incorporates the retrieved documents along with the original user query to output\nan informed response. The additional context helps ground the LLM in factual information and has\nbeen shown to boost performance on knowledge-intensive tasks [20].\nStill, when used in production settings, RAG systems are prone to hallucinations as the generator\nmodel struggles to retrieve relevant information from the context [ 1,30,7]. In the absence of a\none-fits-all approach, application-specific RAG systems must be fine-tuned for optimal performance\non domain-specific tasks. However, the choice of retriever and generator models for each application\nis complex and has serious implications on overall system quality and costs. With numerous\n*Equal Contributions\nPreprint. Under review.arXiv:2407.11005v2  [cs.CL]  16 Jan 2025commercial and open-source generative LLMs readily available1and many variable parameters in the\nRAG system design (Figure 1), tuning an optimal system for a particular RAG application involves\niterative evaluation of multiple configurations. This motivates the need for automated RAG evaluation\nsolutions.\nIn response, automated RAG evaluation systems like RAGAS [ 9] and TruLens [ 36] have emerged.\nThese systems adopt a zero-shot LLM prompt-based approach to predict a set of curated RAG\nevaluation metrics. However, the lack of unified RAG benchmarks makes it difficult to compare\napproaches against each other. Each new study designs a new dataset, often employing LLMs as\ngenerators and labelers [ 9,32,4], which renders them irreproducible. A few benchmarks like RGB\n[4], AttributionBench [ 22] and RAGTruth [ 40] have been proposed recently, but they are small in\nsize and target a disjoint set of labels. The exact RAG evaluation criteria also vary from study to\nstudy. ARES [ 32] and RAGAS [ 9] define a context relevance metric to evaluate the quality of the\nretrieved documents, along with answer relevance andfaithfulness to evaluate the quality of the\ngenerative model. However, others have explored other metrics like correctness [1]noise rejection\nandrobustness [4], to name a few. Finally, most studies evaluate on small in-domain evaluation\ndatasets that are specific to each new application [ 32,33,9,1,4], leaving cross-domain generalization\nan open question.\nIn this work we propose RAGBench: a comprehensive dataset for training and benchmarking\nRAG evaluation models. RAGBench comprises data sourced from multiple domains along with\na comprehensive suite of evaluation metrics. Specifically, we adopt existing metric definitions for\ncontext relevance ,answer faithfulness [9,32] and introduce two new metrics: context utilization and\nanswer completeness . We argue that this new suite of metrics better describes the overall RAG system\nperformance, with the potential to provide granular, actionable insights to the RAG practitioner.\nWe evaluate state-of-the art LLMs and existing RAG evaluation systems on RAGBench. We find that\na 400M-parameter DeBERTa-large model that was fine-tuned on RAGBench outperforms few-shot\nLLM judges across numerous domains and task types. We highlight this result to motivate future\nwork aimed at leveraging these data for advancing RAG evaluation models and improving on the\nproposed benchmark.\n2", "Related Work": "We differentiate our work from existing ground-truth RAG datasets like ChatRAGBench [ 23], CRAG\n[41], DomainRAG [ 38], and ALCE [ 10]. These datasets contain RAG samples with ground-truth\nresponses and are used for end-to-end evaluation of RAG systems via response-level metrics like\nexact match or ROUGE scores. In contrast, we design RAGBench to enable development of more\nmature evaluation systems that effectively evaluate different parts of the RAG system along multiple\ndimensions like retriever relevance, adherence and completeness of the response.\nVarious RAG benchmarks focus specifically on hallucination detection [ 40,33,21,5]. FELM [ 5]\nis a multi-domain and task dataset with factuality labels for open domain QA. RAGTruth [ 40],\nDelucionQA [ 33], and HaluEval [ 21] are RAG-specific datasets with both synthetic as well as human-\nannotated labels for hallucinations in LLM reseponses. While these are appropriate benchmarks for\nhallucination detection models, they do not offer the level of granularity we offer with RAGBench\nthat is necessary to understand the RAG system as a whole.\nRAG evaluation Recently, several parallel efforts have proposed approaches to automated RAG\nevaluation. In RAGAS [ 9], the authors query an LLM-judge (GPT-3.5) with a curated prompt to\nevaluate context relevance ,answer relevance andfaithfulness of a RAG response. Next, Saad-Falcon\net al. [32] propose ARES, a framework for fine-tuning smaller NLI models to predict the same\nmetrics. In parallel, Chen et al. [4]develop a heuristic system to probe LLM\u2019s robustness to noisy and\nirrelevant context documents, and Adlakha et al. [1]explore heuristic algorithms to estimate RAG\ncorrectness andfaithfulness . The lack of established RAG benchmarks makes it difficult to compare\nthese approaches against each other. We aim to address this limitation by introducing RAGBench.\nFinetuned RAG evaluation models Fine-tuned LLM judges are another a common way to ap-\nproach the LLM evaluation task [ 16,44,40]. A number of studies also leverage small, fine-tuned\n1https://huggingface.co/spaces/lmsys/chatbot-arena-leaderboard\n2Figure 1: RAG system workflow, with highlighted variable parameters: (1) Context format and length,\n(2) retriever model, (3) number of retrieved documents, and (4) generation model.\nNatural Language Inference (NLI) models for RAG hallucination detection [ 2,22,32]. NLI models\nmeasure the degree of entailment between a premise and a hypothesis, which has been successfully\nrepurposed for evaluating LLM response attribution in RAG setting. In this work, we train and evalu-\nate an NLI model for RAG evaluation using RAGBench. The fine-tuned model not only outperforms\nLLM judges in hallucination/attribution detection but also excels on the new RAG evaluation metrics\nwe propose.\n3", "RAGBench Construction": "3.1", "Component Datasets": "RAGBench is a collection of real-world datasets that span different domains and RAG task types.\nWe source data from open-book Question-Answer (QA) datasets (CovidQA [ 26], PubmedQA [ 14],\nHotpotQA [ 42], MS Marco [ 28], CUAD [ 12], EManual [ 27], TechQA [ 3], FinQA [ 6], TAT-QA [ 47],\nExpertQA [ 25], HAGRID [ 15]), as well one that was specifically adapted for RAG (DelucionQA [ 33]).\nWe transform all 12 component datasets to a standardized RAG format with consistent annotations.\nTo best represent real-world RAG scenarios, we vary a number parameters to construct the benchmark:\nthe source domain, number of context documents, context token length, and the response generator\nmodel Figure 1 illustrates where these variable parameters fall in the RAG pipeline.\nSource Domains RAGBench comprises five distinct domains: bio-medical research (PubmedQA,\nCovidQA), general knowledge (HotpotQA, MS Marco, HAGRID, ExperQA), legal contracts (CuAD),\ncustomer support (DelucionQA, EManual, TechQA), and finance (FinBench, TAT-QA). We select\nthese specific domains based on availability of data, and applicability to real-world RAG applications\nacross different industry verticals. For detailed descriptions of each component data source, refer to\nAppendix 7.2.\nContext Token Length Context token length in RAGBench ranges from 100 to 11k tokens, which\nwe report in Table 1. Notably, CUAD documents feature long contexts of up to 11k tokens each,\ncompared to the relatively short context in PubMedQA.\nTask Types We curate RAGBench to inlcude a variety of difficult RAG task types. Customer\nsupport datasets simulate a common application of RAG in industry settings. FinQA and TAT-QA\nrequire numerical reasoning over hybrid tabular and text data. HotpotQA, CovidQA, and PubMedQA\nnecessitate retrieval and reasoning over multiple context docs. The CUAD dataset is a challenging\naddition to RAGBench for several reasons: (i) it represents a difficult and highly-specialized real-\nworld domain in which of-the-shelf pre-trained LLM models struggle to perform well [ 24], and (ii) it\nis equally challenging in RAG context due to very long context lengths of legal contract documents.\n3Table 1: RAGBench component datasets.\nDataset DomainDocument\nSourceQuestion\nSource#docsdoc\nlength#Train #Dev #Test\nPubMedQAbiomedical\nresearchresearch\nabstractsautomated\nheuristics4 99 19.5k 2.5k 2.5k\nCovidQA-RAGbiomedical\nresearchresearch\npapersexpert 4 122 2.5k 534 492\nHotpotQAgeneral\nknowledgewikipediacrowd-\nsourced4 126 3.7k 847 776\nMS Marcogeneral\nknowledgeweb pagesuser\nweb queries10 94 3.7k 790 839\nHAGRIDgeneral knowl-\nedgewikipedia expert 3 153 2.0k 322 1.3k\nExpertQAgeneral knowl-\nedgegoogle search expert 3 548 1.6k 202 203\nCUAD legallegal\ncontractsexpert 1 11k 1.5k 506 508\nDelucionQAcustomer\nsupportJeep manual LLM 3 296 1.5k 177 182\nEManualcustomer\nsupportTV manual annotator 3 165 1k 132 132\nTechQAcustomer\nsupportTechnotes tech forums 5 1.8k 1.2k 302 310\nFinQA financeearning\nreportsexpert 3 310 12k 1.7k 2.2k\nTAT-QA financefinancial\nreportsexpert 5 96 26k 3.2k 3.2k\nTotal 78k 12k 11k\nQuestion Sources All component datasets include domain-specific questions that represent real-\nworld user queries about various topics. Questions for DelucionQA, HotpotQA, and EManual are\ncrowd-sourced; questions for CovidQA, CUAD, HAGRID, ExpertQA, and FinQA are composed by\ndomain experts; MS Marco is sourced from real-world user web search queries; likewise, TechQA\nquestions are user queries posted on IBM technical forums; PubMedQA is the only dataset with\nautomatically-generated questions from research article titles.\nResponse Generation For each component dataset we generate responses with LLMs. Exceptions\nto this are HAGRID and ExpertQA datasets, which contain LLM-generated responses in the original\ndata. To introduce variability into the dataset, we generate two responses per input with different\nmodes: GPT-3.5 (gpt-3.5-0125) and Claude 3 Haiku. Both are proprietary models that are offered at\na reasonable price point2, which we believe make them suitable candidates for generating real-world\nRAG responses. For CUAD we only generate responses with Claude 3 Haiku due to prohibitively\nlong context lengths that exceed the GPT-3.5 16k token limit. To encourage a diverse distribution\nof labels in RAGBench, we use a basic prompt (Appendix 7.3) that does not explicitly require the\nmodel to stick to the provided context when generating the response. We set the temperature to 1.0\nfor generation.\nData Splits We split each component dataset into train, validation, and test sets, ensuring there is\nno overlap in queries across splits from the same data source. RAGBench totals 100k samples, split\nacross train, validation, and test sets. Component dataset statistics are reported in Table 1.\n3.2", "TRACe Evaluation Framework": "We propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the\nresponse generator components of RAG. An optimal RAG system must balance accuracy and\nefficiency. The retriever should precisely return all the necessary information to address the user\n2https://openai.com/api/pricing/, https://www.anthropic.com/api\n4Figure 2: Example of RAG Question, Context, and Response. Relevant context spans are highlighted,\nand utilized spans are underlined .\nquery, avoiding any superfluous data. The generator must effectively utilize the retrieved information,\nensuring the response is strictly based on the provided context without introducing any hallucinations\nin the output.\nTowards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evalua-\ntion framework to measure u Tilization, Relevance, Adherence, and Completeness of a RAG system.\nUtilization, Adherence, and Completeness measure the quality of the generator. Adherence here is\nsynonymous with previously proposed answer faithfullness ,groundednes , and attribution , all terms\nused in literature to measure how well an LLM output adheres to a source of factual information.\nRelevance measures the quality of the retriever output with respect to the query. Below we formalize\nthe definition of each metric.\nDefinitions LetDbe a set of context documents {d1...dn}retrieved for a RAG input query. We\ndefine a set of relevant tokens in diasRi={t1, ...tr}.Riencodes information in context document\ndithat is useful for answering the query. Similarly, we define Ui={t1, ...tu}as the set of utilized\ntokens in document di, which reflect information that the generation model is using to produce\na response. Refer to Figure 2 for a visual representation of relevant andutilized spans. Len (x)\nmeasures the length of strings in x, which can be interpreted as character length, token length, or\nsentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best\nwith our annotation schema (Section 3.4). However, token or character length may also be suitable\nfor other use cases.\nContext Relevance Context Relevance is defined in [ 9,32] as the fraction of the retrieved context\nthat is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess\ninformation to the generation model. Long context inputs into the generator may accrue unnecessary\ncosts, as well as compromise the quality of the generated output. We measure relevance of context\ndocument dias:\ndocument relevance =Len (Ri)\nLen (di)(1)\nExample-level relevance can be aggregated over all context documents in the example as:\nexample relevance =P|D|\ni=1Len (Ri)\nP|D|\ni=1Len (di)(2)\nContext Utilization Context Utilization is a new metric introduced in TRACe. We aim to measure\nthe the fraction of the retrieved context that is used by the generator to produce the response. Low\nUtilization in combination with low Relevance points to a greedy retriever, while low Utilization\nalone points to a weak generator that fails to leverage the provided context efficiently. Document-level\nand example-level Utilization are defined as:\ndocument utilization =Len (Ui)\nLen (di)example utilization =P|D|\ni=1Len (Ui)\nP|D|\ni=1Len (di)(3)\nCompleteness Completeness is another new metrics we introduce to measure how well the response\nincorporates all the relevant information in the context. Note that this is different from Utilization; it\nis possible to have high Relevance and high Utilization, but low Completeness when the generator\n5Figure 3: Distributions of relevance, utilization, and completeness labels in RAGBench. Y-axis is\nnormalized to visualize densities.\nutilizes irrelevant information in the context to produce a low quality response. Completeness for\ndocument diis calculated as the fraction of utilized substrings among all relevant substrings:\ncompleteness =Len (Ri\u2229Ui)\nLen (Ri)(4)\nAnd can be extended to example-level by considering all relevant and utilized substrings across all\ncontext documents.\nAdherence Adherence is designed to detect hallucinations in RAG responses. Our definition of\nAdherence is synonymous with answer faithfullness [ 9,32], groundednes [ 36], and attribution [ 31].\nFor alignment with existing hallucination detection approaches, we define example-level adherence\nas a boolean indicating whether or not all parts of the response are grounded in the context. However,\nin our annotation schema (Section 3.4) we also define Ai={t1, ...ta}as the set of response tokens\nthat are supported by the context to enable granular Adherence evaluation.\n3.3", "RAGBench Statistics": "RAGBench component datasets contain between 1% - 20% hallucinations. ExpertQA, CovidQA, and\nMS Marco contain the highest fraction of hallucinated responses (12%, 16%, and 13%, respectively),\nwhile Cuad, FinQA, and TAT-QA contain the least (about 1% for each). We visualize distributions of\nrelevance, utilization, and completeness scores in Figure 3.\n3.4", "LLM annotator": "We prompt GPT-4 ( gpt-4-0125-preview ) to produce ground truth Adherence, Relevance, and\nUtilization labels for input ( documents ,query ,response ) tuples in RAGBench. Completeness is\neasily derived from span-level Relevance and Utilization annotations, thus we don\u2019t request explicit\nannotations for it.\nFor high quality labels, we use proven techniques like chain of thought [ 39] that have been shown\nto maximize the correlation between GPT-4 and human judgements [ 43,46]. For relevance and\nutilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the\ninput documents. For adherence, we instruct the LLM to identify which response sentences, if any,\nare supported by the provided context. We can then derive an example-level boolean adherence label\nby checking if all response sentences are supported. The exact prompt used for annotation is provided\nin Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our\nGPT-labeler, which we outline in Appendix 7.5.\nAlignment with Human Judgements We validate our metric formulations and labeling approach\non a human annotated benchmark. DelucionQA [ 33] is a curated collection of user queries on the\noperation of Jeep\u2019s 2023 Gladiator model. Natural language queries are first generated by an LLM,\nthen reviewed and filtered by human annotators. Context documents are sourced from Jeep\u2019s Gladiator\nUser Manual, and responses are generated by various LLMs. Combined with example-level and\nspan-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world\nuser queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example-\nand span-level agreement with human judgements on the DelucionQA test split (Table 2).\n6Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and\nAccuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations\nare evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated\nagainst DelucionQA(40) : a subset of 40 examples randomly sampled from the DelucionQA test set\nand annotated by the authors.\nTest Set Metric F1 Accuracy\nDelucionQA - example level Adherence 0.96 0.93\nDelucionQA - span level Adherence 0.97 0.95\nDelucionQA(40) - span level Utilization 0.92 0.94\nDelucionQA(40) - span level Relevance 0.76 0.78\n(a) Retriever vs. Relevance\n (b) LLM vs. Hallucination\n(c) Prompt vs. Utilization\n (d) Prompt vs. Completeness\nFigure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and\nconfiguration of RAG retriever component affects the average relevance of the retrieved context. In\nthis example, a dense retriever with a low number of documents per query (k=2) yields the highest\naverage context relevance. (b) The choice of LLM and generation prompt affect how well the RAG\nsystem utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt\nleads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates.\nTo validate relevance and utilization annotations, we also annotate a small subset of DelucionQA\nwith granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table\n2. Similar to adherence, we observe high correlation between relevance and utilization judgements\nfrom GPT-4 and humans. Details of the annotation process and additional validations are found in\nAppendix 7.6.\n73.5", "RAG Case Study": "We design a case study to further motivate and validate the proposed TRACe framework. We sample\n100 realistic world knowledge queries from the RAGTruth [ 40] QA training set, along with 2,512\nunique context document chunks from the same dataset to use as inputs into our mock RAG systems.\nWe simulate RAG setups of varying quality by controlling four configurable parameters: the retriever\n(sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-\nturbo vs. GPT-4o), and 4 versions of the generation prompt template . For illustrative purposes,\nwe evaluate one prompt template comprising of only the question (no context) vs. three RAG-style\ntemplates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought\n(CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the\n100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation\nprompt to evaluate the TRACe metrics.\nFigure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example,\nwe confirm that the choice of the generative LLM model affects the amount of hallucinations (or\nnon-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-\nthought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model\nand less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with\nCoT, which may point to limitations in the model\u2019s ability to reason about complex concepts 4b.\nOverall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to\nhigher utilization of the provided context and more complete responses (4c, 4d). Finally, we show\nthat the choice of the retriever affects average relevance of the retrieved context documents per query\n4a.\n4 Experiments\n4.1 LLM Judge\nWe benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query\nGPT-3.5 with our annotation prompt, (2) RAGAS [ 9], and (3) TruLens [ 36]. RAGAS employs a\nseries of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context\nRelevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness\n(Adherence) and Context Relevance.\n4.2 Fine-tuned Judge\nWe fine-tune a DeBERTa-v3-Large [ 11] NLI checkpoint3from Laurer et al. [18] with one key\narchitecture modification: we add a shallow prediction head for each of the output RAG metrics,\nwhich allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective\nand enables transfer learning from head to head through back-propagation down to the shared base\nlayers. Each prediction head is a single layer feed-forward net that acts on the token-level output of\nthe last DeBERTa layer.\nWe attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and\nanother head on the response tokens to estimate Adherence. For training, we broadcast sentence-level\nannotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and\nAdherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized\nspans and Adherent spans and calculate TRACe metrics using equations 2, 3, and 4. For comparison\nwith existing hallucination detection approaches, we also aggregate Adherence probabilities across\nthe entire response to produce an example-level response adherence label. For details about training\nand hyperparameters, refer to Appendix 7.8.\n4.3 Evaluation\nOur granular annotation schema allows for various evaluation setups. For example, we could\nevaluate either span-level or example/response-level predictions. For easy comparison with existing\nRAG evaluation approaches that are less granular, we report area under the receiver-operator curve\n3https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n8Table 3: Benchmark evaluation on test splits. Reporting AUROC for predicting hallucinated responses\n(Hal), RMSE for predicting Context Relevance (Rel) and utilization (Util).\u2217indicates statistical\nsignificance at 95% confidence intervals, measured by bootstrap comparing the top and second-best\nresults. RAGAS and Trulens do not evaluate Utilization.\nGPT-3.5 RAGAS TruLens DeBERTA\nDataset Hal\u2191Rel\u2193Util\u2193 Hal\u2191Rel\u2193Util\u2193Hal\u2191Rel\u2193Util\u2193Hal\u2191Rel\u2193Util\u2193\nPubMedQA 0.51 0.21\u22170.16 0.54 0.37 - 0.62 0.45 - 0.80\u22170.26 0.17\nCovidQA-RAG 0.57 0.18 0.11 0.58 0.17 - 0.62 0.58 - 0.77\u22170.19 0.11\nHotpotQA 0.59 0.11 0.08 0.62 0.14 - 0.64 0.73 - 0.85\u22170.11 0.08\nMS Marco 0.65 0.23 0.11 0.63 0.25 - 0.62 0.61 - 0.70 0.22 0.10\nHAGRID 0.58 0.22 0.15 0.62 0.22 - 0.67 0.69 - 0.81\u22170.20\u22170.13\nExpertQA 0.55 0.31 0.23 0.57 0.28 - 0.70 0.60 - 0.87\u22170.18\u22170.11\u2217\nDelucionQA 0.57 0.18 0.10 0.70\u22170.22 - 0.55 0.64 - 0.64 0.15\u22170.10\nEManual 0.54 0.17 0.11\u22170.57 0.27 - 0.61 0.64 - 0.76\u22170.13\u22170.13\nTechQA 0.51 0.10 0.05 0.52 0.12 - 0.57 0.70 - 0.86\u22170.08\u22170.04\u2217\nFinQA 0.57 0.10 0.13 0.57 0.06\u2217- 0.53 0.79 - 0.81\u22170.10 0.10\nTAT-QA 0.52 0.20 0.17\u22170.63 0.18\u2217- 0.59 0.72 - 0.83\u22170.27 0.23\nCUAD 0.51 0.27 0.11 0.66 0.19\u2217- 0.40 0.66 - 0.80\u22170.24 0.10\n(AUROC) on the response-level hallucination detection task, and root mean squared error (RMSE)\nfor example-level context Relevance and Utilization predictions.\n5 Results\nTable 3 reports results on test splits of each RAGBench component dataset. We compare baseline\nLLM methods with a finetunes DeBERTA encoder that trained on the full RAGBench train split.\nWe observe that the finetuned DeBERTa model trained on RAGBench achieves competitive perfor-\nmance with billion-parameter LLM judges across numerous domain subsets of the RAGBench test\nset. On the hallucination detection task, DeBERTA AUROC scores range from 0.64 to 0.86. While\nRMSE for relevance and utilization range from 0.04 to 0.26, depending on the domain and task.\nEstimating Context Relevance is Difficult As shown in Table 3, Relevance RMSE scores are\ngenerally higher than those for Utilization, indicating a greater difficulty in the relevance prediction\ntask. Utilization can be assessed through a straightforward semantic comparison between the context\nand the response. In contrast, relevance is a more intricate metric. Due to the nature of RAG, the\nmajority of retrieved documents are semantically related to the query. However, mere semantic\nsimilarity is insufficient. The model must ascertain whether the provided context includes specific\ninformation necessary to accurately answer the question. Thus, the task inherently involves deriving\nthe correct answer, followed by assessing what information in the context may be used to arrive at\nthat answer.\n6 Conclusion\nIn this paper we introduce RAGBench, a large-scale dataset composed of real-world RAG examples\nintended for training and benchmarking powerful RAG evaluation models . To this end, we formulate\nTRACe, a RAG evaluation framework comprising four metrics: u Tilization, Relevance, Adherence,\nandCompleteness. TRACe standardizes the evaluation process, offering a consistent and system-\natic approach to measuring RAG system performance across various dimensions. We propose an\nautomated approach to generate TRACe labels for RAGBench with an LLM and demonstrate high\ncorrelation between our approach and human judgements.\nWe benchmark existing RAG evaluation frameworks on RAGBench and demonstrate that a 400M\nparameter DeBERTa model finetuned on RAGBench data performs competitively with billion-\n9parameter LLM Judges and commercial RAG evaluation systems. Though, the gap between the\nbest-performing RAG evaluator and ground truth is still large. We motivate future work to leverage\nRAGBench toward fine-tuning more powerful evaluation models to explore the potential for narrowing\nthe performance gap between these models and the ground truth.\nOur contributions address the need for standardized benchmarks and methodologies, enabling more\nprecise and actionable insights into the strengths and weaknesses of different RAG systems. This,\nin turn, will facilitate iterative improvement of RAG models, driving forward the capabilities of\nretrieval-augmented generation in real-world applications.", "Experiments": "4.1", "LLM Judge": "We benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query\nGPT-3.5 with our annotation prompt, (2) RAGAS [ 9], and (3) TruLens [ 36]. RAGAS employs a\nseries of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context\nRelevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness\n(Adherence) and Context Relevance.\n4.2", "Fine-tuned Judge": "We fine-tune a DeBERTa-v3-Large [ 11] NLI checkpoint3from Laurer et al. [18] with one key\narchitecture modification: we add a shallow prediction head for each of the output RAG metrics,\nwhich allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective\nand enables transfer learning from head to head through back-propagation down to the shared base\nlayers. Each prediction head is a single layer feed-forward net that acts on the token-level output of\nthe last DeBERTa layer.\nWe attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and\nanother head on the response tokens to estimate Adherence. For training, we broadcast sentence-level\nannotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and\nAdherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized\nspans and Adherent spans and calculate TRACe metrics using equations 2, 3, and 4. For comparison\nwith existing hallucination detection approaches, we also aggregate Adherence probabilities across\nthe entire response to produce an example-level response adherence label. For details about training\nand hyperparameters, refer to Appendix 7.8.\n4.3", "Evaluation": "Framework\nWe propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the\nresponse generator components of RAG. An optimal RAG system must balance accuracy and\nefficiency. The retriever should precisely return all the necessary information to address the user\n2https://openai.com/api/pricing/, https://www.anthropic.com/api\n4Figure 2: Example of RAG Question, Context, and Response. Relevant context spans are highlighted,\nand utilized spans are underlined .\nquery, avoiding any superfluous data. The generator must effectively utilize the retrieved information,\nensuring the response is strictly based on the provided context without introducing any hallucinations\nin the output.\nTowards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evalua-\ntion framework to measure u Tilization, Relevance, Adherence, and Completeness of a RAG system.\nUtilization, Adherence, and Completeness measure the quality of the generator. Adherence here is\nsynonymous with previously proposed answer faithfullness ,groundednes , and attribution , all terms\nused in literature to measure how well an LLM output adheres to a source of factual information.\nRelevance measures the quality of the retriever output with respect to the query. Below we formalize\nthe definition of each metric.\nDefinitions LetDbe a set of context documents {d1...dn}retrieved for a RAG input query. We\ndefine a set of relevant tokens in diasRi={t1, ...tr}.Riencodes information in context document\ndithat is useful for answering the query. Similarly, we define Ui={t1, ...tu}as the set of utilized\ntokens in document di, which reflect information that the generation model is using to produce\na response. Refer to Figure 2 for a visual representation of relevant andutilized spans. Len (x)\nmeasures the length of strings in x, which can be interpreted as character length, token length, or\nsentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best\nwith our annotation schema (Section 3.4). However, token or character length may also be suitable\nfor other use cases.\nContext Relevance Context Relevance is defined in [ 9,32] as the fraction of the retrieved context\nthat is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess\ninformation to the generation model. Long context inputs into the generator may accrue unnecessary\ncosts, as well as compromise the quality of the generated output. We measure relevance of context\ndocument dias:\ndocument relevance =Len (Ri)\nLen (di)(1)\nExample-level relevance can be aggregated over all context documents in the example as:\nexample relevance =P|D|\ni=1Len (Ri)\nP|D|\ni=1Len (di)(2)\nContext Utilization Context Utilization is a new metric introduced in TRACe. We aim to measure\nthe the fraction of the retrieved context that is used by the generator to produce the response. Low\nUtilization in combination with low Relevance points to a greedy retriever, while low Utilization\nalone points to a weak generator that fails to leverage the provided context efficiently. Document-level\nand example-level Utilization are defined as:\ndocument utilization =Len (Ui)\nLen (di)example utilization =P|D|\ni=1Len (Ui)\nP|D|\ni=1Len (di)(3)\nCompleteness Completeness is another new metrics we introduce to measure how well the response\nincorporates all the relevant information in the context. Note that this is different from Utilization; it\nis possible to have high Relevance and high Utilization, but low Completeness when the generator\n5Figure 3: Distributions of relevance, utilization, and completeness labels in RAGBench. Y-axis is\nnormalized to visualize densities.\nutilizes irrelevant information in the context to produce a low quality response. Completeness for\ndocument diis calculated as the fraction of utilized substrings among all relevant substrings:\ncompleteness =Len (Ri\u2229Ui)\nLen (Ri)(4)\nAnd can be extended to example-level by considering all relevant and utilized substrings across all\ncontext documents.\nAdherence Adherence is designed to detect hallucinations in RAG responses. Our definition of\nAdherence is synonymous with answer faithfullness [ 9,32], groundednes [ 36], and attribution [ 31].\nFor alignment with existing hallucination detection approaches, we define example-level adherence\nas a boolean indicating whether or not all parts of the response are grounded in the context. However,\nin our annotation schema (Section 3.4) we also define Ai={t1, ...ta}as the set of response tokens\nthat are supported by the context to enable granular Adherence evaluation.\n3.3 RAGBench Statistics\nRAGBench component datasets contain between 1% - 20% hallucinations. ExpertQA, CovidQA, and\nMS Marco contain the highest fraction of hallucinated responses (12%, 16%, and 13%, respectively),\nwhile Cuad, FinQA, and TAT-QA contain the least (about 1% for each). We visualize distributions of\nrelevance, utilization, and completeness scores in Figure 3.\n3.4 LLM annotator\nWe prompt GPT-4 ( gpt-4-0125-preview ) to produce ground truth Adherence, Relevance, and\nUtilization labels for input ( documents ,query ,response ) tuples in RAGBench. Completeness is\neasily derived from span-level Relevance and Utilization annotations, thus we don\u2019t request explicit\nannotations for it.\nFor high quality labels, we use proven techniques like chain of thought [ 39] that have been shown\nto maximize the correlation between GPT-4 and human judgements [ 43,46]. For relevance and\nutilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the\ninput documents. For adherence, we instruct the LLM to identify which response sentences, if any,\nare supported by the provided context. We can then derive an example-level boolean adherence label\nby checking if all response sentences are supported. The exact prompt used for annotation is provided\nin Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our\nGPT-labeler, which we outline in Appendix 7.5.\nAlignment with Human Judgements We validate our metric formulations and labeling approach\non a human annotated benchmark. DelucionQA [ 33] is a curated collection of user queries on the\noperation of Jeep\u2019s 2023 Gladiator model. Natural language queries are first generated by an LLM,\nthen reviewed and filtered by human annotators. Context documents are sourced from Jeep\u2019s Gladiator\nUser Manual, and responses are generated by various LLMs. Combined with example-level and\nspan-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world\nuser queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example-\nand span-level agreement with human judgements on the DelucionQA test split (Table 2).\n6Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and\nAccuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations\nare evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated\nagainst DelucionQA(40) : a subset of 40 examples randomly sampled from the DelucionQA test set\nand annotated by the authors.\nTest Set Metric F1 Accuracy\nDelucionQA - example level Adherence 0.96 0.93\nDelucionQA - span level Adherence 0.97 0.95\nDelucionQA(40) - span level Utilization 0.92 0.94\nDelucionQA(40) - span level Relevance 0.76 0.78\n(a) Retriever vs. Relevance\n (b) LLM vs. Hallucination\n(c) Prompt vs. Utilization\n (d) Prompt vs. Completeness\nFigure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and\nconfiguration of RAG retriever component affects the average relevance of the retrieved context. In\nthis example, a dense retriever with a low number of documents per query (k=2) yields the highest\naverage context relevance. (b) The choice of LLM and generation prompt affect how well the RAG\nsystem utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt\nleads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates.\nTo validate relevance and utilization annotations, we also annotate a small subset of DelucionQA\nwith granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table\n2. Similar to adherence, we observe high correlation between relevance and utilization judgements\nfrom GPT-4 and humans. Details of the annotation process and additional validations are found in\nAppendix 7.6.\n73.5 RAG Case Study\nWe design a case study to further motivate and validate the proposed TRACe framework. We sample\n100 realistic world knowledge queries from the RAGTruth [ 40] QA training set, along with 2,512\nunique context document chunks from the same dataset to use as inputs into our mock RAG systems.\nWe simulate RAG setups of varying quality by controlling four configurable parameters: the retriever\n(sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-\nturbo vs. GPT-4o), and 4 versions of the generation prompt template . For illustrative purposes,\nwe evaluate one prompt template comprising of only the question (no context) vs. three RAG-style\ntemplates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought\n(CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the\n100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation\nprompt to evaluate the TRACe metrics.\nFigure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example,\nwe confirm that the choice of the generative LLM model affects the amount of hallucinations (or\nnon-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-\nthought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model\nand less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with\nCoT, which may point to limitations in the model\u2019s ability to reason about complex concepts 4b.\nOverall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to\nhigher utilization of the provided context and more complete responses (4c, 4d). Finally, we show\nthat the choice of the retriever affects average relevance of the retrieved context documents per query\n4a.\n4 Experiments\n4.1 LLM Judge\nWe benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query\nGPT-3.5 with our annotation prompt, (2) RAGAS [ 9], and (3) TruLens [ 36]. RAGAS employs a\nseries of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context\nRelevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness\n(Adherence) and Context Relevance.\n4.2 Fine-tuned Judge\nWe fine-tune a DeBERTa-v3-Large [ 11] NLI checkpoint3from Laurer et al. [18] with one key\narchitecture modification: we add a shallow prediction head for each of the output RAG metrics,\nwhich allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective\nand enables transfer learning from head to head through back-propagation down to the shared base\nlayers. Each prediction head is a single layer feed-forward net that acts on the token-level output of\nthe last DeBERTa layer.\nWe attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and\nanother head on the response tokens to estimate Adherence. For training, we broadcast sentence-level\nannotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and\nAdherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized\nspans and Adherent spans and calculate TRACe metrics using equations 2, 3, and 4. For comparison\nwith existing hallucination detection approaches, we also aggregate Adherence probabilities across\nthe entire response to produce an example-level response adherence label. For details about training\nand hyperparameters, refer to Appendix 7.8.\n4.3 Evaluation\nOur granular annotation schema allows for various evaluation setups. For example, we could\nevaluate either span-level or example/response-level predictions. For easy comparison with existing\nRAG evaluation approaches that are less granular, we report area under the receiver-operator curve\n3https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n8Table 3: Benchmark evaluation on test splits. Reporting AUROC for predicting hallucinated responses\n(Hal), RMSE for predicting Context Relevance (Rel) and utilization (Util).\u2217indicates statistical\nsignificance at 95% confidence intervals, measured by bootstrap comparing the top and second-best\nresults. RAGAS and Trulens do not evaluate Utilization.\nGPT-3.5 RAGAS TruLens DeBERTA\nDataset Hal\u2191Rel\u2193Util\u2193 Hal\u2191Rel\u2193Util\u2193Hal\u2191Rel\u2193Util\u2193Hal\u2191Rel\u2193Util\u2193\nPubMedQA 0.51 0.21\u22170.16 0.54 0.37 - 0.62 0.45 - 0.80\u22170.26 0.17\nCovidQA-RAG 0.57 0.18 0.11 0.58 0.17 - 0.62 0.58 - 0.77\u22170.19 0.11\nHotpotQA 0.59 0.11 0.08 0.62 0.14 - 0.64 0.73 - 0.85\u22170.11 0.08\nMS Marco 0.65 0.23 0.11 0.63 0.25 - 0.62 0.61 - 0.70 0.22 0.10\nHAGRID 0.58 0.22 0.15 0.62 0.22 - 0.67 0.69 - 0.81\u22170.20\u22170.13\nExpertQA 0.55 0.31 0.23 0.57 0.28 - 0.70 0.60 - 0.87\u22170.18\u22170.11\u2217\nDelucionQA 0.57 0.18 0.10 0.70\u22170.22 - 0.55 0.64 - 0.64 0.15\u22170.10\nEManual 0.54 0.17 0.11\u22170.57 0.27 - 0.61 0.64 - 0.76\u22170.13\u22170.13\nTechQA 0.51 0.10 0.05 0.52 0.12 - 0.57 0.70 - 0.86\u22170.08\u22170.04\u2217\nFinQA 0.57 0.10 0.13 0.57 0.06\u2217- 0.53 0.79 - 0.81\u22170.10 0.10\nTAT-QA 0.52 0.20 0.17\u22170.63 0.18\u2217- 0.59 0.72 - 0.83\u22170.27 0.23\nCUAD 0.51 0.27 0.11 0.66 0.19\u2217- 0.40 0.66 - 0.80\u22170.24 0.10\n(AUROC) on the response-level hallucination detection task, and root mean squared error (RMSE)\nfor example-level context Relevance and Utilization predictions.\n5", "Results": "Table 3 reports results on test splits of each RAGBench component dataset. We compare baseline\nLLM methods with a finetunes DeBERTA encoder that trained on the full RAGBench train split.\nWe observe that the finetuned DeBERTa model trained on RAGBench achieves competitive perfor-\nmance with billion-parameter LLM judges across numerous domain subsets of the RAGBench test\nset. On the hallucination detection task, DeBERTA AUROC scores range from 0.64 to 0.86. While\nRMSE for relevance and utilization range from 0.04 to 0.26, depending on the domain and task.\nEstimating Context Relevance is Difficult As shown in Table 3, Relevance RMSE scores are\ngenerally higher than those for Utilization, indicating a greater difficulty in the relevance prediction\ntask. Utilization can be assessed through a straightforward semantic comparison between the context\nand the response. In contrast, relevance is a more intricate metric. Due to the nature of RAG, the\nmajority of retrieved documents are semantically related to the query. However, mere semantic\nsimilarity is insufficient. The model must ascertain whether the provided context includes specific\ninformation necessary to accurately answer the question. Thus, the task inherently involves deriving\nthe correct answer, followed by assessing what information in the context may be used to arrive at\nthat answer.\n6", "Conclusion": "In this paper we introduce RAGBench, a large-scale dataset composed of real-world RAG examples\nintended for training and benchmarking powerful RAG evaluation models . To this end, we formulate\nTRACe, a RAG evaluation framework comprising four metrics: u Tilization, Relevance, Adherence,\nandCompleteness. TRACe standardizes the evaluation process, offering a consistent and system-\natic approach to measuring RAG system performance across various dimensions. We propose an\nautomated approach to generate TRACe labels for RAGBench with an LLM and demonstrate high\ncorrelation between our approach and human judgements.\nWe benchmark existing RAG evaluation frameworks on RAGBench and demonstrate that a 400M\nparameter DeBERTa model finetuned on RAGBench data performs competitively with billion-\n9parameter LLM Judges and commercial RAG evaluation systems. Though, the gap between the\nbest-performing RAG evaluator and ground truth is still large. We motivate future work to leverage\nRAGBench toward fine-tuning more powerful evaluation models to explore the potential for narrowing\nthe performance gap between these models and the ground truth.\nOur contributions address the need for standardized benchmarks and methodologies, enabling more\nprecise and actionable insights into the strengths and weaknesses of different RAG systems. This,\nin turn, will facilitate iterative improvement of RAG models, driving forward the capabilities of\nretrieval-augmented generation in real-world applications.", "Appendix": "7.2.\nContext Token Length Context token length in RAGBench ranges from 100 to 11k tokens, which\nwe report in Table 1. Notably, CUAD documents feature long contexts of up to 11k tokens each,\ncompared to the relatively short context in PubMedQA.\nTask Types We curate RAGBench to inlcude a variety of difficult RAG task types. Customer\nsupport datasets simulate a common application of RAG in industry settings. FinQA and TAT-QA\nrequire numerical reasoning over hybrid tabular and text data. HotpotQA, CovidQA, and PubMedQA\nnecessitate retrieval and reasoning over multiple context docs. The CUAD dataset is a challenging\naddition to RAGBench for several reasons: (i) it represents a difficult and highly-specialized real-\nworld domain in which of-the-shelf pre-trained LLM models struggle to perform well [ 24], and (ii) it\nis equally challenging in RAG context due to very long context lengths of legal contract documents.\n3Table 1: RAGBench component datasets.\nDataset DomainDocument\nSourceQuestion\nSource#docsdoc\nlength#Train #Dev #Test\nPubMedQAbiomedical\nresearchresearch\nabstractsautomated\nheuristics4 99 19.5k 2.5k 2.5k\nCovidQA-RAGbiomedical\nresearchresearch\npapersexpert 4 122 2.5k 534 492\nHotpotQAgeneral\nknowledgewikipediacrowd-\nsourced4 126 3.7k 847 776\nMS Marcogeneral\nknowledgeweb pagesuser\nweb queries10 94 3.7k 790 839\nHAGRIDgeneral knowl-\nedgewikipedia expert 3 153 2.0k 322 1.3k\nExpertQAgeneral knowl-\nedgegoogle search expert 3 548 1.6k 202 203\nCUAD legallegal\ncontractsexpert 1 11k 1.5k 506 508\nDelucionQAcustomer\nsupportJeep manual LLM 3 296 1.5k 177 182\nEManualcustomer\nsupportTV manual annotator 3 165 1k 132 132\nTechQAcustomer\nsupportTechnotes tech forums 5 1.8k 1.2k 302 310\nFinQA financeearning\nreportsexpert 3 310 12k 1.7k 2.2k\nTAT-QA financefinancial\nreportsexpert 5 96 26k 3.2k 3.2k\nTotal 78k 12k 11k\nQuestion Sources All component datasets include domain-specific questions that represent real-\nworld user queries about various topics. Questions for DelucionQA, HotpotQA, and EManual are\ncrowd-sourced; questions for CovidQA, CUAD, HAGRID, ExpertQA, and FinQA are composed by\ndomain experts; MS Marco is sourced from real-world user web search queries; likewise, TechQA\nquestions are user queries posted on IBM technical forums; PubMedQA is the only dataset with\nautomatically-generated questions from research article titles.\nResponse Generation For each component dataset we generate responses with LLMs. Exceptions\nto this are HAGRID and ExpertQA datasets, which contain LLM-generated responses in the original\ndata. To introduce variability into the dataset, we generate two responses per input with different\nmodes: GPT-3.5 (gpt-3.5-0125) and Claude 3 Haiku. Both are proprietary models that are offered at\na reasonable price point2, which we believe make them suitable candidates for generating real-world\nRAG responses. For CUAD we only generate responses with Claude 3 Haiku due to prohibitively\nlong context lengths that exceed the GPT-3.5 16k token limit. To encourage a diverse distribution\nof labels in RAGBench, we use a basic prompt (Appendix 7.3) that does not explicitly require the\nmodel to stick to the provided context when generating the response. We set the temperature to 1.0\nfor generation.\nData Splits We split each component dataset into train, validation, and test sets, ensuring there is\nno overlap in queries across splits from the same data source. RAGBench totals 100k samples, split\nacross train, validation, and test sets. Component dataset statistics are reported in Table 1.\n3.2 TRACe Evaluation Framework\nWe propose a suite of four comprehensive metrics to evaluate the quality of the retriever and the\nresponse generator components of RAG. An optimal RAG system must balance accuracy and\nefficiency. The retriever should precisely return all the necessary information to address the user\n2https://openai.com/api/pricing/, https://www.anthropic.com/api\n4Figure 2: Example of RAG Question, Context, and Response. Relevant context spans are highlighted,\nand utilized spans are underlined .\nquery, avoiding any superfluous data. The generator must effectively utilize the retrieved information,\nensuring the response is strictly based on the provided context without introducing any hallucinations\nin the output.\nTowards comprehensive evaluation of the abovementioned criteria, we introduce the TRACe evalua-\ntion framework to measure u Tilization, Relevance, Adherence, and Completeness of a RAG system.\nUtilization, Adherence, and Completeness measure the quality of the generator. Adherence here is\nsynonymous with previously proposed answer faithfullness ,groundednes , and attribution , all terms\nused in literature to measure how well an LLM output adheres to a source of factual information.\nRelevance measures the quality of the retriever output with respect to the query. Below we formalize\nthe definition of each metric.\nDefinitions LetDbe a set of context documents {d1...dn}retrieved for a RAG input query. We\ndefine a set of relevant tokens in diasRi={t1, ...tr}.Riencodes information in context document\ndithat is useful for answering the query. Similarly, we define Ui={t1, ...tu}as the set of utilized\ntokens in document di, which reflect information that the generation model is using to produce\na response. Refer to Figure 2 for a visual representation of relevant andutilized spans. Len (x)\nmeasures the length of strings in x, which can be interpreted as character length, token length, or\nsentence length. For calculating ground-truth metrics, we employ sentence-length, since it aligns best\nwith our annotation schema (Section 3.4). However, token or character length may also be suitable\nfor other use cases.\nContext Relevance Context Relevance is defined in [ 9,32] as the fraction of the retrieved context\nthat is relevant to the input query. Low relevance points to an inefficient retriever that supplies excess\ninformation to the generation model. Long context inputs into the generator may accrue unnecessary\ncosts, as well as compromise the quality of the generated output. We measure relevance of context\ndocument dias:\ndocument relevance =Len (Ri)\nLen (di)(1)\nExample-level relevance can be aggregated over all context documents in the example as:\nexample relevance =P|D|\ni=1Len (Ri)\nP|D|\ni=1Len (di)(2)\nContext Utilization Context Utilization is a new metric introduced in TRACe. We aim to measure\nthe the fraction of the retrieved context that is used by the generator to produce the response. Low\nUtilization in combination with low Relevance points to a greedy retriever, while low Utilization\nalone points to a weak generator that fails to leverage the provided context efficiently. Document-level\nand example-level Utilization are defined as:\ndocument utilization =Len (Ui)\nLen (di)example utilization =P|D|\ni=1Len (Ui)\nP|D|\ni=1Len (di)(3)\nCompleteness Completeness is another new metrics we introduce to measure how well the response\nincorporates all the relevant information in the context. Note that this is different from Utilization; it\nis possible to have high Relevance and high Utilization, but low Completeness when the generator\n5Figure 3: Distributions of relevance, utilization, and completeness labels in RAGBench. Y-axis is\nnormalized to visualize densities.\nutilizes irrelevant information in the context to produce a low quality response. Completeness for\ndocument diis calculated as the fraction of utilized substrings among all relevant substrings:\ncompleteness =Len (Ri\u2229Ui)\nLen (Ri)(4)\nAnd can be extended to example-level by considering all relevant and utilized substrings across all\ncontext documents.\nAdherence Adherence is designed to detect hallucinations in RAG responses. Our definition of\nAdherence is synonymous with answer faithfullness [ 9,32], groundednes [ 36], and attribution [ 31].\nFor alignment with existing hallucination detection approaches, we define example-level adherence\nas a boolean indicating whether or not all parts of the response are grounded in the context. However,\nin our annotation schema (Section 3.4) we also define Ai={t1, ...ta}as the set of response tokens\nthat are supported by the context to enable granular Adherence evaluation.\n3.3 RAGBench Statistics\nRAGBench component datasets contain between 1% - 20% hallucinations. ExpertQA, CovidQA, and\nMS Marco contain the highest fraction of hallucinated responses (12%, 16%, and 13%, respectively),\nwhile Cuad, FinQA, and TAT-QA contain the least (about 1% for each). We visualize distributions of\nrelevance, utilization, and completeness scores in Figure 3.\n3.4 LLM annotator\nWe prompt GPT-4 ( gpt-4-0125-preview ) to produce ground truth Adherence, Relevance, and\nUtilization labels for input ( documents ,query ,response ) tuples in RAGBench. Completeness is\neasily derived from span-level Relevance and Utilization annotations, thus we don\u2019t request explicit\nannotations for it.\nFor high quality labels, we use proven techniques like chain of thought [ 39] that have been shown\nto maximize the correlation between GPT-4 and human judgements [ 43,46]. For relevance and\nutilization we request the LLM-annotator to directly identify relevant and utilized sub-strings in the\ninput documents. For adherence, we instruct the LLM to identify which response sentences, if any,\nare supported by the provided context. We can then derive an example-level boolean adherence label\nby checking if all response sentences are supported. The exact prompt used for annotation is provided\nin Appendix 7.4. We apply post-processing steps to ensure high quality, reliable annotations from our\nGPT-labeler, which we outline in Appendix 7.5.\nAlignment with Human Judgements We validate our metric formulations and labeling approach\non a human annotated benchmark. DelucionQA [ 33] is a curated collection of user queries on the\noperation of Jeep\u2019s 2023 Gladiator model. Natural language queries are first generated by an LLM,\nthen reviewed and filtered by human annotators. Context documents are sourced from Jeep\u2019s Gladiator\nUser Manual, and responses are generated by various LLMs. Combined with example-level and\nspan-level annotations for hallucination, DelucionQA represents a realistic distribution of real-world\nuser queries and RAG responses. We find that our GPT annotator achieves 93% and 95% example-\nand span-level agreement with human judgements on the DelucionQA test split (Table 2).\n6Table 2: GPT-4 annotator achieves high alignment with human judgements. We report F1 and\nAccuracy alignment metrics on human annotated subsets of DelucionQA. Adherence annotations\nare evaluated against DelucionQA human-annotated labels. Relevance and Utilization are evaluated\nagainst DelucionQA(40) : a subset of 40 examples randomly sampled from the DelucionQA test set\nand annotated by the authors.\nTest Set Metric F1 Accuracy\nDelucionQA - example level Adherence 0.96 0.93\nDelucionQA - span level Adherence 0.97 0.95\nDelucionQA(40) - span level Utilization 0.92 0.94\nDelucionQA(40) - span level Relevance 0.76 0.78\n(a) Retriever vs. Relevance\n (b) LLM vs. Hallucination\n(c) Prompt vs. Utilization\n (d) Prompt vs. Completeness\nFigure 4: Relationship between RAG system configuration and TRACe metrics. (a) The choice and\nconfiguration of RAG retriever component affects the average relevance of the retrieved context. In\nthis example, a dense retriever with a low number of documents per query (k=2) yields the highest\naverage context relevance. (b) The choice of LLM and generation prompt affect how well the RAG\nsystem utilizes the provided context. Prompting the LLM with a detailed chain-of-thought prompt\nleads to reduced hallucinations, as well as higher response utilization (c) and completeness (d) rates.\nTo validate relevance and utilization annotations, we also annotate a small subset of DelucionQA\nwith granular relevance and utilization labels. We refer to this subset as DelucionQA(40) in Table\n2. Similar to adherence, we observe high correlation between relevance and utilization judgements\nfrom GPT-4 and humans. Details of the annotation process and additional validations are found in\nAppendix 7.6.\n73.5 RAG Case Study\nWe design a case study to further motivate and validate the proposed TRACe framework. We sample\n100 realistic world knowledge queries from the RAGTruth [ 40] QA training set, along with 2,512\nunique context document chunks from the same dataset to use as inputs into our mock RAG systems.\nWe simulate RAG setups of varying quality by controlling four configurable parameters: the retriever\n(sparse TF-IDF vs. dense), number of retrieved documents (2 vs. 4), generation model (GPT-3.5-\nturbo vs. GPT-4o), and 4 versions of the generation prompt template . For illustrative purposes,\nwe evaluate one prompt template comprising of only the question (no context) vs. three RAG-style\ntemplates that encourage the LLM to utilize the provided context and/or utilize chain-of-thought\n(CoT). The full generation templates are provided in Appendix 7.7. We generate responses for the\n100 RAGTruth queries with each of the 32 resulting RAG systems and use our LLM annotation\nprompt to evaluate the TRACe metrics.\nFigure 4 demonstrates how the different RAG configurations influence TRACe metrics. For example,\nwe confirm that the choice of the generative LLM model affects the amount of hallucinations (or\nnon-adherent responses) in the generated text. Not surprisingly, GPT-4o combined with a chain-of-\nthought prompt yields the lowest amount of hallucination compared to the weaker GPT-3.5 model\nand less detailed prompts. Curiously, we find that GPT-3.5 hallucinates more when prompted with\nCoT, which may point to limitations in the model\u2019s ability to reason about complex concepts 4b.\nOverall, we find that prompting the LLM to think step-by-step and explain its reasoning leads to\nhigher utilization of the provided context and more complete responses (4c, 4d). Finally, we show\nthat the choice of the retriever affects average relevance of the retrieved context documents per query\n4a.\n4 Experiments\n4.1 LLM Judge\nWe benchmarks a few LLM evaluators on RAGBench: (1) zero-shot GPT-3.5-judge, where we query\nGPT-3.5 with our annotation prompt, (2) RAGAS [ 9], and (3) TruLens [ 36]. RAGAS employs a\nseries of few-shot prompts to GPT-3.5 to measure answer groundedness (Adherence) and Context\nRelevance metrics. Trulens is another zero-shot prompting approach that measures answer faithfulness\n(Adherence) and Context Relevance.\n4.2 Fine-tuned Judge\nWe fine-tune a DeBERTa-v3-Large [ 11] NLI checkpoint3from Laurer et al. [18] with one key\narchitecture modification: we add a shallow prediction head for each of the output RAG metrics,\nwhich allows us to compute all TRACe metrics in a single forward pass. This is both cost-effective\nand enables transfer learning from head to head through back-propagation down to the shared base\nlayers. Each prediction head is a single layer feed-forward net that acts on the token-level output of\nthe last DeBERTa layer.\nWe attach two heads on the context tokens to estimate Relevance and Utilization probabilities, and\nanother head on the response tokens to estimate Adherence. For training, we broadcast sentence-level\nannotations to tokens, and tune to maximize token-level probabilities of Relevant, Utilized, and\nAdherent spans. At inference, we impose a probability threshold=0.5 to predict Relevant and Utilized\nspans and Adherent spans and calculate TRACe metrics using equations 2, 3, and 4. For comparison\nwith existing hallucination detection approaches, we also aggregate Adherence probabilities across\nthe entire response to produce an example-level response adherence label. For details about training\nand hyperparameters, refer to Appendix 7.8.\n4.3 Evaluation\nOur granular annotation schema allows for various evaluation setups. For example, we could\nevaluate either span-level or example/response-level predictions. For easy comparison with existing\nRAG evaluation approaches that are less granular, we report area under the receiver-operator curve\n3https://huggingface.co/MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\n8Table 3: Benchmark evaluation on test splits. Reporting AUROC for predicting hallucinated responses\n(Hal), RMSE for predicting Context Relevance (Rel) and utilization (Util).\u2217indicates statistical\nsignificance at 95% confidence intervals, measured by bootstrap comparing the top and second-best\nresults. RAGAS and Trulens do not evaluate Utilization.\nGPT-3.5 RAGAS TruLens DeBERTA\nDataset Hal\u2191Rel\u2193Util\u2193 Hal\u2191Rel\u2193Util\u2193Hal\u2191Rel\u2193Util\u2193Hal\u2191Rel\u2193Util\u2193\nPubMedQA 0.51 0.21\u22170.16 0.54 0.37 - 0.62 0.45 - 0.80\u22170.26 0.17\nCovidQA-RAG 0.57 0.18 0.11 0.58 0.17 - 0.62 0.58 - 0.77\u22170.19 0.11\nHotpotQA 0.59 0.11 0.08 0.62 0.14 - 0.64 0.73 - 0.85\u22170.11 0.08\nMS Marco 0.65 0.23 0.11 0.63 0.25 - 0.62 0.61 - 0.70 0.22 0.10\nHAGRID 0.58 0.22 0.15 0.62 0.22 - 0.67 0.69 - 0.81\u22170.20\u22170.13\nExpertQA 0.55 0.31 0.23 0.57 0.28 - 0.70 0.60 - 0.87\u22170.18\u22170.11\u2217\nDelucionQA 0.57 0.18 0.10 0.70\u22170.22 - 0.55 0.64 - 0.64 0.15\u22170.10\nEManual 0.54 0.17 0.11\u22170.57 0.27 - 0.61 0.64 - 0.76\u22170.13\u22170.13\nTechQA 0.51 0.10 0.05 0.52 0.12 - 0.57 0.70 - 0.86\u22170.08\u22170.04\u2217\nFinQA 0.57 0.10 0.13 0.57 0.06\u2217- 0.53 0.79 - 0.81\u22170.10 0.10\nTAT-QA 0.52 0.20 0.17\u22170.63 0.18\u2217- 0.59 0.72 - 0.83\u22170.27 0.23\nCUAD 0.51 0.27 0.11 0.66 0.19\u2217- 0.40 0.66 - 0.80\u22170.24 0.10\n(AUROC) on the response-level hallucination detection task, and root mean squared error (RMSE)\nfor example-level context Relevance and Utilization predictions.\n5 Results\nTable 3 reports results on test splits of each RAGBench component dataset. We compare baseline\nLLM methods with a finetunes DeBERTA encoder that trained on the full RAGBench train split.\nWe observe that the finetuned DeBERTa model trained on RAGBench achieves competitive perfor-\nmance with billion-parameter LLM judges across numerous domain subsets of the RAGBench test\nset. On the hallucination detection task, DeBERTA AUROC scores range from 0.64 to 0.86. While\nRMSE for relevance and utilization range from 0.04 to 0.26, depending on the domain and task.\nEstimating Context Relevance is Difficult As shown in Table 3, Relevance RMSE scores are\ngenerally higher than those for Utilization, indicating a greater difficulty in the relevance prediction\ntask. Utilization can be assessed through a straightforward semantic comparison between the context\nand the response. In contrast, relevance is a more intricate metric. Due to the nature of RAG, the\nmajority of retrieved documents are semantically related to the query. However, mere semantic\nsimilarity is insufficient. The model must ascertain whether the provided context includes specific\ninformation necessary to accurately answer the question. Thus, the task inherently involves deriving\nthe correct answer, followed by assessing what information in the context may be used to arrive at\nthat answer.\n6 Conclusion\nIn this paper we introduce RAGBench, a large-scale dataset composed of real-world RAG examples\nintended for training and benchmarking powerful RAG evaluation models . To this end, we formulate\nTRACe, a RAG evaluation framework comprising four metrics: u Tilization, Relevance, Adherence,\nandCompleteness. TRACe standardizes the evaluation process, offering a consistent and system-\natic approach to measuring RAG system performance across various dimensions. We propose an\nautomated approach to generate TRACe labels for RAGBench with an LLM and demonstrate high\ncorrelation between our approach and human judgements.\nWe benchmark existing RAG evaluation frameworks on RAGBench and demonstrate that a 400M\nparameter DeBERTa model finetuned on RAGBench data performs competitively with billion-\n9parameter LLM Judges and commercial RAG evaluation systems. Though, the gap between the\nbest-performing RAG evaluator and ground truth is still large. We motivate future work to leverage\nRAGBench toward fine-tuning more powerful evaluation models to explore the potential for narrowing\nthe performance gap between these models and the ground truth.\nOur contributions address the need for standardized benchmarks and methodologies, enabling more\nprecise and actionable insights into the strengths and weaknesses of different RAG systems. This,\nin turn, will facilitate iterative improvement of RAG models, driving forward the capabilities of\nretrieval-augmented generation in real-world applications.", "RAGBench Code and Data": "We release RAGBench data on Hugginggface: https://huggingface.co/datasets/\nrungalileo/ragbench . Refer to model card and documentation there.\nWe publish our inferfence and evaluation code on Gihub: https://github.com/rungalileo/\nragbench/tree/main/ragbench .\n7.2", "RAGBench Dataset Details": "RAGBench is sourced from publicly released acadmic and industry datasets. As far as we know, none\nof the component datasets contain personally identifiable information or offensive content.\nPubMedQA [ 14]PubMedQA is a collection of PubMed research abstracts with corresponding\nyes/no/maybe questions paired with each abstract. The original dataset comprises 3 subsets: PQA-L,\nPQA-U, and PQA-A, with 1k, 60k, and 210k abstracts, respectively. For all subsets, the question\nis derived from the title of the PubMed article using rule-based heuristics. Long answers are\nautomatically derived from the last sentence of the abstract for PQA-L and PQA-U, and QA-L\nanswers are further reviewed by expert annotators and annotated as yes/no/maybe. PQA-A comprises\nexclusively automatically generated questions and short answers.\nFor RAGBench we utilize the PQA-U subset and re-frame it from QA into a RAG task. To simulate\nRAG, we leverage already segmented PQA-U abstracts context chunks and we encode them into a\nvector DB with OpenAI embeddings. The size of the resulting DB is 200k. We retrieve 4 chunks for\neach PQA-U question using FAISS with eucledian distance as the similarity function. We ignore the\nresponses and labels in the original dataset and generate new responses with an LLM.\nCovidQA-RAG CovidQA-RAG is a combination of 2k expert-annotated questions sourced from\nCOVID-QA [ 26] and a vector database of 250,000 100-word passages built by Siriwardhana et al.\n[34]. Both questions and answers are sourced from CORD-19 [ 37] collection of research articles\nabout COVID-19.\nWe embed the questions and database passages with OpenAI embeddings and retrieve up to N\npassages for each COVID-QA question from the vector database using FAISS with eucledian distance\nas the similarity function and max_distance=0.25. We generate responses for each resulting RAG\n(context, question) instance with an LLM.\nHotpotQA [ 42]HotpotQA comprises 113K crowd-sourced question-answer pairs sourced from\nWikipedia. Each pair is associated with a set of related context passages from one or multiple\nWikipedia pages. The dataset is constructed in a way that requires multi-hop reasoning over multiple\ncontext documents to arrive at the answer, which renders it a valuable candidate for our benchmark.\nWe sample data from the dev-distractor split, which contains up to 8 distractor context documents\nper sample. We downsample the context documents to 4 per example, making sure to include the\ndocument containing the response. We treat the context passages in HotpotQA as RAG context\ndocuments, and generate responses for each (context, question) instance with an LLM.\nMS Marco [ 28]MS Marco is an open-domain question answering dataset sourced from Bing\nsearch engine user query logs. Each question is associated with 10 context passages retrieved via\nBing web search. Human annotators compose a response based on the provided context documents,\nand label the documents utilized in the response as relevant. We sample data from the original version\nof the dataset, comprising 80k train, 10k validation, and 10k test samples. As with other datasets, we\nignore the human annotated answers and generate responses with an LLM in RAG setting.\nCUAD [ 12]CUAD is a collection of commercial legal contracts with expert annotated questions\nand responses. The contracts are sourced from a public legal contract library(EDGAR) and range\nfrom 1-100 pages in length. Experts in the legal domain compose multiple questions per contract\nand label the relevant parts of the contract that are useful for answering the questions. There are\n21k questions pertaining to 510 documents in total. The questions are very specific to each contract,\n14thus we don\u2019t perform additional retrieval over the contract corpus, and form RAG examples with 1\ncontext contract each for our benchmark. Due to high anntoation costs associated with long-context\nRAG, we sample 5 question per doc. As with other datasets, we generate responses with an LLM in\nRAG setting.\nDelucionQA [ 33]DelucionQA is a domain-specific RAG dataset leveraging Jeep\u2019s 2023 Gladiator\nmodel manual as the source of knowledge. The questions and answers are automatically generated by\nlarge language models. RAG context passages are retrieved from the Jeep car manual via both sparse\nand dense retrieval methods to add variance in the sample distribution. Further, MTurk workers\nannotate whether or not responses are supported by the context.\nUpon closer inspection, we found only 1 relevant passage associated with each question in the\nDelucionQA dataset. To make the dataset more challenging for RAGBench, we build a vector\ndatabase from the 1,046 context passages in DelucionQA and and retrieve up to 3 context documents\nper question from it. We use text-embedding-ada-002 embeddings from OpenAI to build the\ndatabase. There are 913 unique questions in DelucionQA. For each resulting (context, question)\nsample, we generate responses with an LLM.\nEManual [ 27]EManual is a question answer dataset comprising consumer electronic device\nmanuals and realistic questions about them composed by human annotators. The subset made\navailable at the time of writing amounts to 659 unique questions about the Samsung Smart TV/remote\nand the accompanying user manual, segmented into 261 chunks. To form a RAG dataset, we embed\nthe manual segments into a vector database with OpenAI embedding and retrieve up to 3 context\ndocuments per question from it. For each resulting (context, question) sample, we generate responses\nwith an LLM.\nTechQA [ 3]TechQA is a collection of real-world user questions posted on IBMDeveloper and\nDeveloperWorks forums, along with 50 technical support documents relating to each question. The\ndocuments are sourced from database of 800k technical documents that support accepted answers\non the tech forums. The authors release 1.4k questions, split between train, validation, and test sets.\nThe data are curated such that fractions on the each split unanswerable given the information in the\nlinked documents, which makes it a good candidate for RAGBench. To reduce annotation costs,\nwe sub-sample the data down to 10 documents per question, making sure to include the document\ncontaining the answer, when applicable. We use the provided splits with (context document, question)\nexamples and generate responses for each with an LLM.\nFinQA [ 6]FinQA is a QA dataset of financial report passages and associated questions. Questions\nare curated such that numerical reasoning over multiple unstructured and tabular inputs is required to\narrive at the answer. FinQA totals 8,281 financial QA pairs, split between train, validation, and test\nsplits. We retain the original splits and generate 2 LLM responses per each context-query example in\nFinQA.\nTAT-QA [ 47]TAT-QA is another financial QA dataset that requires numerical reasoning over\ntables and text. The data are sourced from 500 financial reports released on https://www.\nannualreports.com/ . Expert annotators with background in finance annotate question-answer\npairs based on the available documents. We leverage the full dataset (13k train, 1.6k validation and\ntest) but generate new responses with LLMs for RAGBench.\nHAGRID [ 15]HAGRID is a QA dataset built on top of MIRACL [ 45], a multi-lingual information-\nretrieval dataset. HAGRID passes questions and relevant context documents from MIRACLE through\nan LLM to produce a response for each example in the dataset. Annotors then rate the response\non informativeness and attribution dimensions. The original context documents are sourced from\nWikipedia and associated questions are generated by expert annotators. Since HAGRID already\ncontains LLM-generated responses, we directly use them and don\u2019t generate additional responses for\nRAGBench.\nExpertQA [ 25]ExpertQA is a collection of curated questions from domain-experts in various\nfields of sicence, arts, and law. The dataset also contains expert curated passsages relevant to each\n15question, alongside LLM-generated responses. As with HAGRID, we leverage the LLM-generated\nresponses in ExpertQA directly for our RAG dataset.\n7.3", "Response Generation Prompt": "We use the following prompt template to generate LLM responses for each sample in RAGBench.\nContext documents, separated by line breaks, along with the question are slotted in for each generation\nsample.\nUse the following pieces of context to answer the question.\n{documents}\nQuestion: {question}\n7.4", "GPT Labeling Prompt": "We use the following prompt template to generate annotations with GPT-4\nI asked someone to answer a question based on one or more documents.\nYour task is to review their response and assess whether or not each sentence\nin that response is supported by text in the documents. And if so, which\nsentences in the documents provide that support. You will also tell me which\nof the documents contain useful information for answering the question, and\nwhich of the documents the answer was sourced from.\nHere are the documents, each of which is split into sentences. Alongside each\nsentence is associated key, such as \u20190a.\u2019 or \u20190b.\u2019 that you can use to refer\nto it:\n\u2018\u2018\u2018\n{documents}\n\u2018\u2018\u2018\nThe question was:\n\u2018\u2018\u2018\n{question}\n\u2018\u2018\u2018\nHere is their response, split into sentences. Alongside each sentence is\nassociated key, such as \u2019a.\u2019 or \u2019b.\u2019 that you can use to refer to it. Note\nthat these keys are unique to the response, and are not related to the keys\nin the documents:\n\u2018\u2018\u2018\n{answer}\n\u2018\u2018\u2018\nYou must respond with a JSON object matching this schema:\n\u2018\u2018\u2018\n{{\n\"relevance_explanation\": string,\n\"all_relevant_sentence_keys\": [string],\n\"overall_supported_explanation\": string,\n\"overall_supported\": boolean,\n\"sentence_support_information\": [\n{{\n\"response_sentence_key\": string,\n\"explanation\": string,\n16\"supporting_sentence_keys\": [string],\n\"fully_supported\": boolean\n}},\n],\n\"all_utilized_sentence_keys\": [string]\n}}\n\u2018\u2018\u2018\nThe relevance_explanation field is a string explaining which documents\ncontain useful information for answering the question. Provide a step-by-step\nbreakdown of information provided in the documents and how it is useful for\nanswering the question.\nThe all_relevant_sentence_keys field is a list of all document sentences keys\n(e.g. \u20190a\u2019) that are revant to the question. Include every sentence that is\nuseful and relevant to the question, even if it was not used in the response,\nor if only parts of the sentence are useful. Ignore the provided response when\nmaking this judgement and base your judgement solely on the provided documents\nand question. Omit sentences that, if removed from the document, would not\nimpact someone\u2019s ability to answer the question.\nThe overall_supported_explanation field is a string explaining why the response\n*as a whole* is or is not supported by the documents. In this field, provide a\nstep-by-step breakdown of the claims made in the response and the support (or\nlack thereof) for those claims in the documents. Begin by assessing each claim\nseparately, one by one; don\u2019t make any remarks about the response as a whole\nuntil you have assessed all the claims in isolation.\nThe overall_supported field is a boolean indicating whether the response as a\nwhole is supported by the documents. This value should reflect the conclusion\nyou drew at the end of your step-by-step breakdown in overall_supported_explanation.\nIn the sentence_support_information field, provide information about the support\n*for each sentence* in the response.\nThe sentence_support_information field is a list of objects, one for each sentence\nin the response. Each object MUST have the following fields:\n- response_sentence_key: a string identifying the sentence in the response.\nThis key is the same as the one used in the response above.\n- explanation: a string explaining why the sentence is or is not supported by the\ndocuments.\n- supporting_sentence_keys: keys (e.g. \u20190a\u2019) of sentences from the documents that\nsupport the response sentence. If the sentence is not supported, this list MUST\nbe empty. If the sentence is supported, this list MUST contain one or more keys.\nIn special cases where the sentence is supported, but not by any specific sentence,\nyou can use the string \"supported_without_sentence\" to indicate that the sentence\nis generally supported by the documents. Consider cases where the sentence is\nexpressing inability to answer the question due to lack of relevant information in\nthe provided contex as \"supported_without_sentence\". In cases where the sentence\nis making a general statement (e.g. outlining the steps to produce an answer, or\nsummarizing previously stated sentences, or a transition sentence), use the\nsting \"general\".In cases where the sentence is correctly stating a well-known fact,\nlike a mathematical formula, use the string \"well_known_fact\". In cases where the\nsentence is performing numerical reasoning (e.g. addition, multiplication), use\nthe string \"numerical_reasoning\".\n- fully_supported: a boolean indicating whether the sentence is fully supported by\nthe documents.\n- This value should reflect the conclusion you drew at the end of your step-by-step\nbreakdown in explanation.\n- If supporting_sentence_keys is an empty list, then fully_supported must be false.\n17- Otherwise, use fully_supported to clarify whether everything in the response\nsentence is fully supported by the document text indicated in supporting_sentence_keys\n(fully_supported = true), or whether the sentence is only partially or incompletely\nsupported by that document text (fully_supported = false).\nThe all_utilized_sentence_keys field is a list of all sentences keys (e.g. \u20190a\u2019) that\nwere used to construct the answer. Include every sentence that either directly supported\nthe answer, or was implicitly used to construct the answer, even if it was not used\nin its entirety. Omit sentences that were not used, and could have been removed from\nthe documents without affecting the answer.\nYou must respond with a valid JSON string. Use escapes for quotes, e.g. \u2018\\\\\"\u2018, and\nnewlines, e.g. \u2018\\\\n\u2018. Do not write anything before or after the JSON string. Do not\nwrap the JSON string in backticks like \u2018\u2018\u2018 or \u2018\u2018\u2018json.\nAs a reminder: your task is to review the response and assess which documents contain\nuseful information pertaining to the question, and how each sentence in the response\nis supported by the text in the documents.\\\n7.5", "Annotation Post-Processing Steps": "As shown in Appendix 7.4, we request very detailed annotations with explanations from GPT-4-turbo.\nWe pivot on chain-of-thought [ 39] and redundancy to encourage high quality labels from the annotator\nmodel.\nFor Adherence, we request both response-level and sentence-level annotations that we compare in\npost-processing to identify inconsistencies where GPT-4 disagrees with its own judgements. For\nexample, if GPT-4 claims a response as supported by the context as a whole, but identifies no\nsupporting information for one or more claims in the response, we send the example for re-annotation.\nWe re-annotate all data up to 3 times, after which a fraction (<2%) of the data are still conflicting.\nAfter manual inspection, we find that the majority of the conflicts arise from partially hallucinated\nsentences that are somewhat, but not fully, grounded in the context. We leverage a sentence-level\n\"fully_supported\" boolean annotation to identify and resolve such cases. According to our annotation\nschema, we treat all partially supported sentences as hallucinations.\nSince all TRACe metrics are related, we qualitatively observe that taking the extra measures for\nAdherence also positively impacts the quality and stability of the relevance and utilization labels.\nIn the final post-processing step, we remove any off-schema keys that GPT-4-turbo sometimes\ninjects into the response. For example, it will occasionally misspell \"supporting_sentence_keys\"\nas \"support ed_sentence_keys\" and/or introduce completely new fields into the output json. We\nalgorithmically find and remove/replace such annotation errors.\n7.6", "Annotation Alignment with Human Judgements": "7.6.1", "Adherence Alignment with DelucionQA": "We validate our metric formulations and labeling approach on a human annotated benchmark.\nDelucionQA [ 33] is a curated collection of user queries on the operation of Jeep\u2019s 2023 Gladiator\nmodel. Natural language queries are first generated by an LLM, then reviewed and filtered by human\nannotators. Context documents are sourced from Jeep\u2019s Gladiator User Manual, and responses are\ngenerated by various LLMs. Human annotators label each response sentence as \"Supported\" by the\ncontext documents, \"Conflicted\", or \"Neither\". Example-level labels are derived from the span-level\nannotation as follows: if at least one response sentence is annotated as \"Conflicted\" or \"Neither\", the\nwhole response receives a Hallucinated label.\nIn our initial investigation, we found that sentences that DelucionQA labels as \"Neither\" often fall into\none of two categories: (1) general filler statements (e.g. \"Here are the steps:\"), (2) claims of missing\ninformation (e.g. \"There is no mention of any problem with engine start-up in freezing weather\nrelated to DEF.\"). According to our annotation schema, these types of statements are generally\ngrounded in the context and not hallucinations. Thus, we remove examples with any \"Neither\"\n18Table 4: Annotation Alignment with DelucionQA. We report F1 and Accuracy metrics on human\nannotated subsets of DelucionQA. DelucionQA(40) is a subset of 40 examples randomly sampled\nfrom the DelucionQA test set and annotated for Relevance and Utilization by the authors.\nTest Set Metric F1 Accuracy\nDelucionQA - example level Adherence 0.83 0.76\nDelucionQA - example level - remove \"Neither\" Adherence 0.96 0.93\nDelucionQA - span level - remove \"Neither\" Adherence 0.97 0.95\nDelucionQA(40) - sapen level Utilization 0.92 0.94\nDelucionQA(40) - span level Relevance 0.76 0.78\nTable 5: Ranking of Simulated RAG Systems. We evaluate GPT-4-turbo annotations on simulated\nRAG datasets from Saad-Falcon et al. [32]. The data from each source are synthetically augmented\nto create sets with increasing degrees of context relevance (Rel) and answer adherence (Adh). We\nannotate 500 samples from each set and rank them according to the average context relevance and\nanswer adherence metrics. We report Kendall\u2019s tau to evaluate the agreement between GPT-4-turbo\nrankings and ground truth (higher is better).\nNQ HotpotQA WoW FEVER\nRel Adh Rel Adh Rel Adh Rel Adh\nKendall\u2019s Tau binary 1.0 0.83 0.87 1.0 1.0 0.89 1.0 0.78\nKendall\u2019s Tau continuous 0.94 - 0.73 - 1.0 - 0.77 -\nsentence annotations for our analysis. We annotate the remaining 421 examples with our LLM\nannotator and report alignment with human annotations in Table 4.\n7.6.2", "Relevance and Utilization Alignment with DelucionQA": "To validate Relevance and Utilization annotations, the authors annotate a small set of 40 randomly\nsamples examples from the DelucionQA test set. We follow the same instructions as in out annotation\nprompt 7.4 to label relevant and utilized context sentences, given the context, query, and response.\nWe report sentence-level F1 and overall alignment (Accuracy) scores in Table 4.\n7.6.3", "Rank-based Alignment for Adherence and Relevance": "We use mock RAG datasets generated by Saad-Falcon et al. [32] for this analysis. Their RAG\nvalidation set is sampled from KILT [ 29], including Natural Questions (NQ)[ 17], HotpotQA[ 42],\nFEVER[ 35], and Wizards of Wikipedia (WoW) [ 8] datasets. The authors synthetically generate\nsystems of varying quality by adjusting the ratio of relevant documents and responses in the data. We\nsample 500 examples from each simulated RAG dataset and annotated them as described in section\n3.4. Next, we calculate average annotated context relevance and adherence scores for each dataset\nand use those to rank the mock systems. We compare our rankings to ground truth with the Kendall\nrank correlation (Kendall\u2019s \u03c4) metric, which evaluates the agreement between two sets of ranks on a\nscale from 0 (no agreement) to 1 (perfect agreement).\nAs shown in Table 5, the GPT-4 annotations achieve high Kendall\u2019s \u03c4ranging from 0.78 to 1. For a\nfair comparison with the ground truth labels, we derive binary context relevance and labels from the\nGPT-4 annotations by thresholding the example Relevance score (equation 2) at 0. For comparison,\nwe also report ranking results with our more granular example-level Relevance scores that range\nfrom 0-1. We find that these metrics produce a different ranking (see lower Kendall\u2019s \u03c4in Table 5),\nwhich we attribute to the metrics capturing differences in retrieved context length across the different\nexamples.\n7.7", "DeBERTa model training": "We train the model on a Google Cloud Platform A-100 GPU instance for 3 epochs with initial learning\nrate 5\u22126for the base model layers and 2\u22125for the heads, with warmup and a linear decay rate.\n20Table 6: RAG Case Study Results.\nretriever k prompt version generation model ave relevance ave utilization ave completeness pc hallucinated\ntfidf 2 no context gpt-3.5-turbo-0125 0.65 0.30 0.53 0.94\ntfidf 2 no context gpt-4o 0.66 0.45 0.70 0.95\ntfidf 2 short gpt-3.5-turbo-0125 0.62 0.43 0.72 0.19\ntfidf 2 short gpt-4o 0.66 0.54 0.81 0.29\ntfidf 2 long gpt-3.5-turbo-0125 0.62 0.43 0.71 0.04\ntfidf 2 long gpt-4o 0.66 0.35 0.57 0.08\ntfidf 2 long + CoT gpt-3.5-turbo-0125 0.65 0.53 0.82 0.13\ntfidf 2 long + CoT gpt-4o 0.63 0.50 0.78 0.04\ntfidf 4 no context gpt-3.5-turbo-0125 0.46 0.22 0.53 0.86\ntfidf 4 no context gpt-4o 0.45 0.29 0.65 0.86\ntfidf 4 short gpt-3.5-turbo-0125 0.43 0.29 0.67 0.18\ntfidf 4 short gpt-4o 0.47 0.38 0.78 0.13\ntfidf 4 long gpt-3.5-turbo-0125 0.45 0.28 0.64 0.05\ntfidf 4 long gpt-4o 0.48 0.28 0.60 0.05\ntfidf 4 long + CoT gpt-3.5-turbo-0125 0.44 0.33 0.72 0.10\ntfidf 4 long + CoT gpt-4o 0.49 0.36 0.73 0.02\nfaiss 2 no context gpt-3.5-turbo-0125 0.81 0.40 0.50 0.94\nfaiss 2 no context gpt-4o 0.81 0.53 0.66 0.87\nfaiss 2 short gpt-3.5-turbo-0125 0.75 0.55 0.72 0.07\nfaiss 2 short gpt-4o 0.82 0.72 0.86 0.11\nfaiss 2 long gpt-3.5-turbo-0125 0.78 0.58 0.71 0.04\nfaiss 2 long gpt-4o 0.82 0.52 0.62 0.10\nfaiss 2 long + CoT gpt-3.5-turbo-0125 0.81 0.64 0.76 0.07\nfaiss 2 long + CoT gpt-4o 0.83 0.69 0.82 0.04\nfaiss 4 no context gpt-3.5-turbo-0125 0.58 0.27 0.47 0.88\nfaiss 4 no context gpt-4o 0.58 0.36 0.60 0.79\nfaiss 4 short gpt-3.5-turbo-0125 0.53 0.31 0.59 0.14\nfaiss 4 short gpt-4o 0.58 0.47 0.78 0.07\nfaiss 4 long gpt-3.5-turbo-0125 0.54 0.33 0.61 0.08\nfaiss 4 long gpt-4o 0.61 0.32 0.52 0.09\nfaiss 4 long + CoT gpt-3.5-turbo-0125 0.58 0.42 0.71 0.09\nfaiss 4 long + CoT gpt-4o 0.60 0.47 0.76 0.05\n21"}, "MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation": {"Introduction": "Retrieval augmented generation (RAG) is emerging\nas a popular application of large language models\n(LLMs) and a powerful paradigm to improve LLMs\nfactuality (Gao et al., 2024). A RAG pipeline\nfirst retrieves relevant documents from an index\nbased on a query and then composes a response\n*Work done by first author during an internship at Amazon.\nFirst two authors contributed equally to this work. Correspond-\ning author: Jayasimha Talur email talurj@amazon.com.\n1We release https://github.com/amazon-science/\nMEMERAG our benchmark to support the community developing\naccurate evaluation methods for multilingual RAG systems.using an LLM. Grounding the LLM response on\nretrieved knowledge helps mitigate outdated knowl-\nedge, lack of domain expertise and reduce halluci-\nnations (Lewis et al., 2020; Gao et al., 2024). Col-\nlecting benchmarking data for RAG is challenging\ndue to the complexity of the pipeline that includes\ninformation retrieval andtext generation . Text gen-\neration, our focus in this paper, has, in general, two\nmodes of automatic evaluation : reference-based\nand reference-free, which differ in the availability\nof human-generated gold", "Related Work": "Due to the pipeline approach of RAG systems, the\nevaluation can be split into 3 main components:\n1) retriever metrics to identify relevant chunks of\ninformation to the input typically measured with\nrecall/precision@K (Manning et al., 2008); 2) gen-\nerator metrics to identify the \u201cusefulness\u201d of the\ngenerated answers in relation to the input, this is\ndone across fine grained dimensions such as faith-\nfulness and relevance, either leveraging", "Dataset Construction": "Meta-evaluation datasets enable the development\nof reliable automatic evaluators. In a RAG setup,\nthe input to the evaluator is composed of a question\nq(user input), a context c(set of passages auto-\nmatically retrieved to the question) and an answer\nagenerated by a language model to answer the\nquestion based on the context. The end-to-end eval-\nuator then needs to judge the quality of the answer\nagiven the context and the question (c, q). Fol-\nlowing previous work (Saad-Falcon et al., 2024; Es\net al., 2024) we focus on two quality dimensions:\nFaithfulness Is the answer grounded on the con-\ntext, regardless of your world knowledge?\nRelevance Is the answer relevant to the question,\nregardless of the context?\nWe build a multilingual end-to-end meta-\nevaluation RAG (MEMERAG) dataset by extend-\ning the MIRACL dataset (Zhang et al., 2023) to in-\nclude model-generated answers and human-based\nquality judgements . More precisely, we select rel-\nevant question-context pairs, generate answers us-\ning various language models and gather expert\nhuman annotations on the quality of those an-swers. Our dataset encompasses 5 languages: En-\nglish (EN), German (DE), Spanish (ES), French\n(FR), and Hindi (HI), which represent multiple lan-\nguage families and both high- and low-resource lan-\nguages. Figure 1 shows examples from the dataset,\nwith LLM-generated answers and coarse- to fine-\ngrained human-assigned labels for the faithfulness\nand relevance dimensions.\nThe MIRACL dataset is composed of questions\nwritten by humans in their native languages, one\nor more passages automatically retrieved from the\nWikipedia, and human annotations about the rele-\nvance of each passage. Building a dataset starting\nfrom native questions in each language allows to\nevaluate RAG pipelines without resorting to (ma-\nchine) translations, thus avoiding limitations and\nbiases associated with translation. Note, however,\nthat as questions were elicited from native speak-\ners independently across different languages, the\nresulting data set is not parallel.\n3.1", "Question Selection": "The questions in the MIRACL dataset were gener-\nated by humans based on prompts. This leads to\nquestions that may be answerable by the prompt but\nmay have ambiguity outside of that context. In par-\nticular, we identified as problematic the questionsfor which the right answer can change over time.\nFor example \u201c Who is the president of Spain? \u201d or\n\u201cHow old is Drake Hogestyn? \u201d. In a RAG setting,\ndifferent passages may have been written at differ-\nent times and provide conflicting context. Addi-\ntionally, the time of reference is usually not explicit\nin the question. To remove those complications we\nautomatically filtered out time-dependent questions\nacross all languages4. We combined the train and\ndev splits of the MIRACL dataset, which corre-\nsponds to a total of 3,662, 305, 2,810, 1,486 and\n1,519 questions respectively for EN, DE, ES, FR\nand HI. Of those, 244, 13, 120, 64 and 86 were\nidentified as time-dependent and filtered out.\n3.2", "Context Selection": "The MIRACL dataset has an average of 10.3 pas-\nsages per question, which corresponds to an aver-\nage of 1,218 words of context in the English train\nand dev splits, with similar numbers in other lan-\nguages. To reduce the cognitive load on human\nannotators, we limit the context per query to 5 pas-\nsages.\nThe source dataset provides human-annotated\nbinary relevance labels for passages. To more\naccurately simulate an automated retrieval pro-\ncess, we rank the passages for each question using\nBM25 (Sch\u00fctze et al., 2008), as implemented in\n(L\u00f9, 2024). We then select the top-5 ranked pas-\nsages for each question. If these top-5 passages\ndo not contain any human-annotated relevant pas-\nsages, we replace the lowest-ranked passage with\nthe highest-ranked relevant passage from the full\nset. This approach ensures that each question has\nat least one relevant passage in its context, avoid-\ning scenarios where annotators would evaluate re-\nsponses without any relevant information.\nIt is worth noting that simulating scenarios\nwhere no relevant passages exist is straightforward\n(e.g., by including only irrelevant passages). In\nsuch cases, for faithfulness evaluation, we would\nexpect responses like \"The provided documents do\nnot contain a relevant answer.\" Our method focuses\non faithfulness while efficiently utilizing human\nannotation efforts by ensuring that each evaluated\ncase has at least some relevant context.\n3.3", "Answer Generation": "After question and passage selection, we generate\nan answer for each question-context pair and each\n4See Appendix F for all prompts used during dataset con-\nstruction.of five state-of-the-art LLMs. We generated an-\nswers with Claude 3 Sonnet, Llama3 70B, Llama3\n8B, Mistral 7B, and GPT-4o mini. Those LLMs\nwere selected to cover a range of model sizes, open\nweight and proprietary models. We prompted all\nthe models in English5, asking to answer the ques-\ntion based only on the given context, and requesting\nthe answer to be provided in the same language as\nthe context and question. For all models, we set\nthe temperature to 0.1, and maximum number of\noutput tokens to 1000.\nWe thus produced answers for more than 1000\nquestions per language, except for German for\nwhich MIRACL only contains 305 questions. As\nour focus is on long-form answers, we further fil-\ntered out questions for which any of the 5 models\ngenerated an answer shorter than 10 words.\n3.4", "Annotation Guidelines": "The task of annotating answers with faithfulness\nis challenging due to several factors. First, it in-\nvolves some subjectivity which might impact Inter-\nAnnotator Agreement (IAA) (Kryscinski et al.,\n2020; Tang et al., 2024b). Then, its label space\nis not precisely defined in the literature (Tang et al.,\n2024b; Laban et al., 2023; Malaviya et al., 2024).\nFinally, although faithfulness should be ideally\nevaluated for atomic facts, it is generally evalu-\nated at the sentence or even document level, due to\nannotation costs.\nStarting with the factuality error taxonomy in-\ntroduced in Tang et al. (2024b), we ran a number\nof annotation pilots to refine the label space and\nguidelines.\nFinally, we converged to three coarse-grained la-\nbels ( Supported ,Not supported ,Challenging to de-\ntermine ), explained through 10 fine-grained labels.\nTo increase the consistency of the annotation (IAA),\nwe guide the annotation process through a flow\nchart (documented in Figure 3 in Appendix A). For\nrelevance, which is significantly less ambiguous\nto evaluate, we device a simple annotation process\nwith three labels: Directly answers the question ,\nAdds context to the answer , and Unrelated to the\nquestion . Note that the first two labels can be used\nto describe \"relevant\" sentences, while the last label\nidentifies \"irrelevant\" sentences. (See Appendix A\n5There is evidence in the literature for better model accu-\nracy when the models are prompted to process in English (Lai\net al., 2023; Liu et al., 2024), though under particular scenar-\nios other languages could perform better (Behzad et al., 2024).\nWe leave additional prompting experiments for future work.Lang #QAnswer Context\n#S Avg. #W Avg. #W\nEN 250 400 30.3 613.5\nDE 250 468 27.3 455.0\nES 250 563 52.1 522.3\nFR 250 540 48.7 478.3\nHI 250 351 23.8 571.5\nTotal 1,250 2,322\nTable 1: General statistics of the MEMERAG dataset.\n#Q: number of questions, #S: number of sentences, #W:\nnumber of words. Each answer is annotated at the sen-\ntence level, leading to 2,322 total sentences annotated\nby experts for faithfulness and relevance.\nfor more details.)\n3.5", "Annotation Process": "From the question-context-answer triplets obtained\nin Section 3.3, we randomly sampled 250 questions\nper language (50 per answer-generating model,\nwithout overlapping questions for diversity). We\nemployed a professional vendor with native anno-\ntators6to gather annotations for each sentence7of\nthe 250 answers per language. The statistics of the\nannotated dataset are presented in Table 1. Among\nthe 250 answers per language, a random subset of\n10 were assigned to 3 annotators for computing the\nIAA and the rest to a single annotator (see Table 2).\nThe annotations were gathered via a web-based\ntool that implemented the flow chart of the anno-\ntation guidelines (see Appendix A for details). To\nfurther enhance IAA, we drew upon the findings\nof Krishna et al. (2023), which demonstrated that\nhighlighting relevant information aids annotators in\nperforming tasks and reaching consensus. Thus, we\nutilized the Llama 3 70B LLM to identify sentences\nwithin the retrieved passages that could potentially\nserve as supporting information to the answer sen-\ntences (see Appendix F).\nThe English annotations required approximately\n25 hours of total annotation time, averaging 5.5\nminutes per question. This covered 250 questions,\nincluding 10 that were annotated by three different\nannotators for quality control. Similar time invest-\nments were observed for the other four languages.\nTable 2 summarizes the IAA per language for\nfaithfulness and relevance labels assigned by 3 an-\n6Annotators were compensated with a competitive hourly\nrate that is benchmarked against similar roles in their country\nof residence.\n7Sentences were segmented using the pySBD(Sadvilkar\nand Neumann, 2020) package.notators. We report IAA using Gwet\u2019s AC1 (Gwet,\n2008) and Fleiss Kappa (Fleiss, 1971). We observe\nhigh agreement for faithfulness (0.84-0.93 Gwet\u2019s\nAC1 and 0.70-0.88 Fleiss Kappa) and even higher\nagreement for relevance (0.95-1.0 Gwet\u2019s AC1 and\n0.63-1.0 Fleiss Kappa). This shows that the annota-\ntors are aligned and indicates a high quality of the\nannotations. Comparing to previous work, Tang\net al. (2024b) report a Fleiss Kappa of 0.34-0.42 on\nfaithfulness labels which they deem fair to moder-\nate agreement. Note that a direct comparison with\nthis work is not possible as they deal with different\ntasks, nevertheless the high IAA we are report-\ning is a testament of the effectiveness of our flow\nchart-based annotation design. Further details on\nIAA with fine-grained explanatory labels, and an\nextended version of the dataset (MEMERAG-Ext)\nwith five annotators for 150 answers per language\ncan be found in Appendix B.\nLang Faithfulness Relevance\nGwet\u2019s Fleiss Gwet\u2019s Fleiss\nAC1 Kappa AC1 Kappa\nEN 0.93 0.77 1.00 1.00\nDE 0.84 0.81 0.95 0.73\nES 0.91 0.76 1.00 1.00\nFR 0.89 0.88 0.93 0.63\nHI 0.89 0.70 1.00 1.00\nTable 2: Inter-annotator agreement (IAA) on the faith-\nfulness and relevance dimensions with 3 annotators.\nAnnotations are at the answer sentence level.\n4", "Annotation Results": "We present in this section the results of the hu-\nman annotations for the 2,322 sentences of the\nMEMERAG dataset.\nTable 3 shows the distribution of faithfulness and\nrelevance labels across the five languages. The dis-\ntribution of labels is consistent across all languages,\nwith a few exceptions. On faithfulness, German\nand Hindi show higher percentages of Supported\nanswers. On relevance, Spanish presents a signif-\nicantly higher percentage of labels Adds context\nto the answer compared to other languages while\nEnglish had a very low share of labels Unrelated to\nthe question . As a partial explanation for this, we\nnote that Spanish questions generated the largest\nnumbers of output sentences with 52.1 words vs.,\nfor example, 30.3 words for English (see Table 1).Lang Faithfulness Relevance\n\u2714 \u2715 ?\u2714 \u2713 \u2715\nEN 65.2 31.5 3.2 65.2 32.5 2.2\nDE 71.2 26.7 2.1 61.3 26.5 12.2\nES 65.7 32.9 1.4 48.8 43.9 7.3\nFR 62.0 37.8 0.2 63.3 29.3 7.4\nHI 73.8 25.6 0.6 68.9 21.4 9.7\nTable 3: Label distribution in the benchmark. Percent-\nage of sentences labelled as supported ( \u2714), not sup-\nported ( \u2715), challenging to determine ( ?) for faithful-\nness, and as directly answers the question ( \u2714), adds\ncontext to the answer ( \u2713), unrelated to the question ( \u2715)\nfor relevance, per language.\nLabel EN DE ES FR HI\nDirect paraphrase 8.5 41.7 25.0 33.7 40.7\nLogical conclusion 41.2 28.2 30.4 28.0 5.7\nOther 15.5 1.3 10.3 0.4 27.4\nAdds new info 7.0 9.6 16.0 15.0 14.8\nContradiction 4.5 11.3 8.3 5.9 7.1\nMis-referencing 1.5 3.0 2.3 3.7 0.3\nNuance shift 6.8 0.6 4.3 5.6 1.7\nOpinion as fact 0.5 0.6 0.2 2.2 0.3\nWrong reasoning 10.0 0.6 1.4 1.9 0.3\nOther 1.2 0.9 0.4 3.5 1.1\nChalleng. to determ. 3.2 2.1 1.4 0.2 0.6\nTable 4: Fine-grained faithfulness label distribution\nin the benchmark. Percentage of sentences with\neach label per language. The three sections corre-\nspond to the coarse-grained labels Supported/Not sup-\nported/Challenging to determine .\nHence, we expect that the relevance statistics re-\nflect the tendency of Spanish answers to be more\nverbose.\nFor a more granular insights into the annotations,\nwe show in Table 4 the distribution of the explana-\ntory fine-grained faithfulness labels for each lan-\nguage. As label names are quite self-explanatory,\nwe refer for their precise meaning to Figure 3 in Ap-\npendix A. Table 4 is split into three blocks, respec-\ntively, addressing fine-grained labels for Supported\n(top), Not supported (middle), and Challenging to\ndetermine (bottom) answers. We observe remark-\nable differences across languages. For example,\nsupported answers for English are prominently un-\nder form of a logical conclusion from the context\n(41.2%), for German and Hindi from direct para-\nphrasing of information in the context (41.7% and\n40.7%), while for French and Spanish from a more\nbalanced combination of the two reasons. On theside of unsupported answers, the main mistake type\nin English is Wrong reasoning (10%), i.e. answers\nare non logical conclusions from the context, while\nthis type or error is significantly rarer (under 2%)\nfor all other languages. The rate of Adds new in-\nformation errors, a.k.a. hallucinations, ranges from\n7% for English to 16% for Spanish. The observed\ncross-linguistic variations in the distribution of la-\nbels can be attributed to the different nature of the\nquestions and accuracy of the models across the 5\nlanguages.\n5", "Dataset Applications": "The MEMERAG dataset is designed to support\nthe development of reliable automatic evaluation\nmethods. For that purpose, we describe in this\nsection how our dataset can be used as a bench-\nmark to enable various meta-evaluation use cases.\nWe focus on two applications: 1) Prompt selec-\ntion: The ability of our benchmark to effectively\nselect prompts for automatic evaluation, 2) Model\nselection : The effectiveness of our benchmark to\ncompare and select models. By concentrating on\nthese aspects, we can evaluate the benchmark\u2019s util-\nity as a comprehensive tool for multilingual model\nassessment. While we provide reference baselines\nfor each application, our focus is on showcasing\nthe effectiveness of the benchmark rather than the\nunderlying capabilities of the LLMs.\n5.1", "Experimental Setup": "Benchmark Tasks Our benchmark is composed\nof multiple tasks defined by the annotation dimen-\nsion and subset considered. On the annotation di-\nmension, we focus our experiments on the coarse-\ngrained faithfulness dimension. This dimension\nis more challenging than relevance as highlighted\nby the lower IAA, while retaining a high-enough\nIAA to make for a trustworthy benchmark. We\ninvite benchmark users to also experiment on the\nother dimensions provided by the dataset depend-\ning on their use case. Note that for this experi-\nment, we remove the sentences labelled as Chal-\nlenging to determine by human annotators. On\nthe annotation subset, we first distinguish the mul-\ntilingual task which uses the full dataset and the\nmonolingual task, which only considers a single\nlanguage. Those tasks can then be further broken\ndown at the fine-grained level by considering the\nsubset of sentences with a certain fine-grained label.\nPerformance is evaluated with Balanced Accuracy(BAcc), with equal weights on each coarse-grained\nlabel and language. We conduct significance test-\ning using permutation tests (Good, 2013), with fur-\nther details in Appendix E.\nReference Prompts To demonstrate how the\nbenchmark can be used to select the appropri-\nate prompt, we experiment with multiple prompt-\ning strategies from simple to advanced techniques,\nstarting with zero-shot prompting, where LLMs\ndirectly classify statements as Supported orNot\nsupported . We then implement chain-of-thought\n(COT) prompting (Wei et al., 2022), which incorpo-\nrates an intermediate reasoning step. While these\nbasic prompting strategies provide a good start-\ning point for evaluation, our initial experiments\nrevealed limitations in their ability to capture the\nnuanced requirements of our specific task. With-\nout explicit guidelines in the prompt, automatic\nevaluators rely on their \u201cworld knowledge\u201d, which\nmay not align with the specific requirements of\nthe evaluation task. To overcome this, we add in-\nstructions from the annotation guidelines (AG) in\nthe prompt. Adding annotation guidelines provides\nclear criteria for what constitutes Supported versus\nNot Supported sentences, reducing ambiguity in\nthe evaluation process. The various prompts are\npresented in Appendix G.\nReference Models We experiment with four\nLLMs with varying model sizes and capabilities:\nGPT-4o mini, Qwen 2.5 32B, and two versions\nof Llama 3.2 (11B and 90B)8. In case an LLM\ndoes not produce one of the required labels, we\nrepeatedly prompt the LLM up to five times with\ntemperature and top_p equal to 0.1 to get a valid\nlabel. If the LLM fails to generate a label after five\nretries, we treat the datapoint as an error (wrong\nlabel).\n5.2", "Experimental Results": "Our benchmark enables systematic evaluation of\ndifferent approaches to automated faithfulness eval-\nuation. To illustrate this, we examine how the\nbenchmark can surface the effectiveness of vari-\nous automatic evaluation models and prompting\nstrategies.\n8In comparison to the LLMs selected for answer genera-\ntion, see Section 3.3, we upgraded Llama from 3 to 3.2 as the\ncontext length of 8K tokens was not sufficient for all prompts.\nWe picked GPT-4o mini as representative of proprietary mod-\nels. Additionally, Mistral was excluded as it performed poorly\nin initial experiments.PromptGPT-4o\nminiQwen\n2.5 32BLlama\n3.2 90BLlama\n3.2 11B\nZS 59.7 66.7 58.0 55.4\nCOT 61.4 68.8 59.9 62.5\nAG 71.6\u202072.6 62.8\u202057.9\nAG+COT 71.7 71.8\u202064.4 61.6\u2020\nTable 5: Reference baselines for the multilingual task on\ncoarse-grained faithfulness. Balanced accuracy (BAcc)\nof the automatic evaluators averaged across the 5 lan-\nguages (EN, DE, ES, FR, HI) using zero-shot (ZS),\nchain-of-thought (COT), annotation guidelines (AG)\nand AG+COT prompting strategies. Bold indicates best\nperformance for the column, \u2020indicates results not sta-\ntistically different from the best (p > 0.05). Additional\nresults on monolingual tasks and standard errors can be\nfound in Appendix E, Tables 10-12.\nTable 5 demonstrates the benchmark\u2019s ability to\ncompare different prompting approaches across lan-\nguages. The benchmark reveals consistent patterns,\nshowing how different prompt designs impact eval-\nuation quality. As expected, adding a reasoning\nstep (COT) improves over zero-shot prompting. In\naddition, adding annotation guidelines (AG) helps\nalign automated evaluators with human judgments\nacross all languages. Comparing the two best mod-\nels GPT-4o mini and Qwen 2.5 32B, Qwen 2.5 32B\nexcels in the zero-shot and COT setups, showcas-\ning higher \u201cout-of-the-box\u201d alignment with human\njudgements. GPT-4o mini achieves similar perfor-\nmance once the annotation guidelines are added to\nthe prompt.\nFigure 2, shows the performance per language\nof various automatic evaluators with a fixed prompt\n(AG + COT), which allows us to select the best\nmodel for each language. We observe that GPT-4o\nmini performs best in English. For the rest of the\nlanguages Qwen 2.5 32B performs the best how-\never, the results are not statistically different from\nGPT-4o mini. Our benchmark also provides users\nwith the capability to conduct detailed, fine-grained\nanalyses of model performance across various di-\nmensions of faithfulness. The breakdown of au-\ntomatic evaluation performance by error type is\nshown in Appendix I.\n6", "Conclusions": "We introduced a high-quality and challenging mul-\ntilingual end-to-end meta-evaluation benchmark\nfor RAG (MEMERAG). Our carefully designed\nflow-chart-based annotation achieved a high inter-Llama 3.2 11B Qwen 2.5 32B Llama 3.2 90B GPT-4o mini405060708090100% BAcc60.076.8\n63.273.7% BAcc Scores for DE\nBest performance\nNot statistically different from best\nLlama 3.2 11B Qwen 2.5 32B Llama 3.2 90B GPT-4o mini405060708090100% BAcc 60.262.5 62.668.4% BAcc Scores for EN\nLlama 3.2 11B Qwen 2.5 32B Llama 3.2 90B GPT-4o mini405060708090100% BAcc59.171.1\n63.469.9% BAcc Scores for ES\nLlama 3.2 11B Qwen 2.5 32B Llama 3.2 90B GPT-4o mini405060708090100% BAcc65.074.4\n63.273.7% BAcc Scores for FR\nLlama 3.2 11B Qwen 2.5 32B Llama 3.2 90B GPT-4o mini405060708090100% BAcc65.275.5 75.1 74.2% BAcc Scores for HIFigure 2: Reference baselines for the monolingual task on coarse-grained faithfulness. Balanced Accuracy (BAcc)\nof the automatic evaluators using various LLMs across five languages: EN, DE, ES, FR and HI. Each plot compares\nthe performance of four models: Llama 3.2 11B, Qwen 2.5 32B, Llama 3.2 90B, and GPT-4o mini using AG + COT\nprompt. The best-performing model for each language is highlighted with a darker blue bar. Bars with diagonal\nhatching indicate results not statistically different from the best (p > 0.05).\nannotator agreement rate supporting the reliability\nof the benchmark. The introduced MEMERAG\ndataset opens the door for multiple application\nscenarios, including but not limited to the demon-\nstrated cases, i.e. prompt selection and model se-\nlection.\nFor the meta-evaluation setup, we develop and\ncompare various LLMs-as-a-judge and observe that\nautomatic evaluators performance varies across the\nlanguages, influenced by language characteristics,\nnative-question complexity and LLM generation\nnuances. These variations underscore the impor-\ntance of our testbed, which demonstrated consis-\ntent results when comparing the prompting tech-\nniques (COT+guidelines > COT > zero-shot) and\nprovides a foundation for developing better multi-\nlingual evaluators.\n7", "Limitations": "Due to time and cost constraints, our annotations\nand experiments are limited in terms of promptingtechniques, LLMs we experimented with and lan-\nguages we annotated. Nevertheless, we diversified\nour LLMs across size and \u201copenness\u201d while the\nlanguages represent two families and low and high\nresource ones. In addition, there exists in the liter-\nature fine-tuned factuality evaluators for English,\nthough we expect those to not work as well on non-\nEnglish languages. Another method is to approxi-\nmate factuality through entailment tasks (i.e. XNLI\ndataset) though such methods were shown (for En-\nglish) to be inferior to multi-task training and dis-\ntillation and data augmentation from LLMs (Tang\net al., 2024a). Fine-tuning multilingual evaluators\nand examining transfer learning across languages\nis interesting but is left for future work that can\nleverage our dataset for this purpose.\nAs we advocate for a native testing approach,\nthe questions across the languages are not paral-\nlel, which could introduce a dimension of differ-\nent questions and LLM generations complexities\nacross the different language test data. The datawe collected presents different challenges which\nare captured according to our fine grained error\nlabels (Table 4). Future work could balance the\nchallenges and complexities by collecting data for\nspecific challenging phenomena. Note that this bal-\nancing is not straightforward, it can be done on the\nquestion side though this is insufficient as it does\nnot control for the answer complexity. Controlling\nfor the answer complexity is a challenging prob-\nlem as the answer side is model generated (one\nmethod is to generate many answers and select for\ncertain phenomena with human in the loop which\nis costly).", "Human Annotation Guidance": "Before conducting large-scale annotations, we con-\nducted a pilot with 10 English RAG outputs and 3\nannotators. We asked annotators to evaluate faith-\nfulness at the sentence level either as Supported\nor with a subset of the factuality mistakes typol-\nogy in Tang et al. (2024b) (developed for sum-\nmarization): Contradiction ,Hallucination ,Mis-\nreferencing ,Nuance meaning shift ,Opinion stated\nas fact ,Wrong reasoning , to which was an Other\nmistake label was added to account for unforeseen\nmistakes in the RAG setting. As this led to very\nlow IAA, we conducted another round reducing\nthe non-supported labels to Stating opinion as fact ,\nDrawing wrong conclusions ,Other mistake but this\nstill resulted in low IAA (Gwet\u2019s AC1 0.45).\nUpon careful analysis of the annotator disagree-\nments in the pilot and further rounds of calibration,\nwe developed the annotation workflow shown in\nFigure 3. The key improvements were: (i) add a\nChallenging to determine label, (ii) have two levels\nof labels, a coarse-grained level ( Supported ,Not\nsupported ,Challenging to determine ) and a fine-\ngrained level for precision on the mistakes, (iii)\nenforce annotators to follow a specific reasoning\nwith a flow chart, (iv) use numbers for the fine-\ngrained level rather than labels which could be\nmisinterpreted. Those guidelines allowed to reach\nsignificantly higher IAA on faithfulness (Gwet\u2019s\nAC1 0.81 coarse-grained). Likewise we iterated\non the relevance labels, starting from Must have ,\nNice to have , orIrrelevant and converging to the\nmore explicit Directly answers the question ,Adds\ncontext to the answer , orUnrelated to the ques-\ntion. This also increased the IAA significantly. The\nscreenshot of the user interface used by human\nannotators is shown in Figure 4.\nB", "IAA and Extended Dataset": "As explained in Section 3.5, the IAA on the\nMEMERAG dataset was measured by triple an-\nnotation on a random subset of 10 questions per\nlanguage. Detailed IAA metrics, at both coarse-\nand fine-grained levels, can be found in Table 6.\nWe observe that IAA is high for faithfulness at\ncoarse-grained level, and for relevance at both lev-\nels. This prompted us to 1) focus the experiments\ndescribed in this paper to those dimensions with\nhigh IAA, and 2) investigate the low IAA for fine-\ngrained faithfulness.\nThe disagreements highlighted by low IAA areMEMERAG\nLang #SIAA (Gwet\u2019s AC1)\nFaithfulness Relevance\nCoarse Fine Coarse Fine\nEN 13 0.93 0.63 1 0.92\nDE 31 0.84 0.47 0.95 0.91\nES 20 0.91 0.3 1 0.93\nFR 23 0.89 0.58 0.93 0.92\nHI 17 0.89 0.18 1 1\nTable 6: Inter-annotator agreement (IAA) on the faith-\nfulness and relevance dimensions with 3 annotators\n(MEMERAG subset), for coarse-grained and fine-\ngrained levels. Annotations are at the answer sentence\nlevel (#S number of answer sentences is provided).\nMEMERAG-Ext\nLang #SIAA (Gwet\u2019s AC1)\nFaithfulness Relevance\nCoarse Fine Coarse Fine\nEN 226 0.83 0.41 1.00 0.9\nDE 272 0.75 0.47 0.92 0.79\nES 276 0.91 0.39 1.00 0.97\nFR 370 0.72 0.53 0.99 0.8\nHI 208 0.91 0.33 0.98 0.92\nTable 7: Inter-annotator agreement (IAA) on the faith-\nfulness and relevance dimensions with 5 annotators\n(MEMERAG-Ext), for coarse-grained and fine-grained\nlevels. Annotations are at the answer sentence level (#S\nnumber of answer sentences is provided).\ndue to ambiguity in the annotation guidelines. The\nfact that such ambiguity remained after significant\nwork on refining the guidelines and guiding an-\nnotators with UI elements hints at the irreducible\nsubjectivity of the task. Subjectivity manifests it-\nself as a non-trivial probability distribution of the\nannotation label obtained by drawing a random hu-\nman annotator for a given question. It would be\nof interest in the development of automated anno-\ntators to compare this distribution to that of the\nannotations of a stochastic LLM-based annotator.\nTo investigate those aspects, we created an\nextended dataset, called MEMERAG-Ext. The\ndataset is composed of 150 question-context pairs\nper language, extracted similarly to MEMERAG\nwith the process described in Section 3. The ques-\ntions are disjoint between the two datasets, except\nfor German where there is some overlap due to\nthe limited number of questions in MIRACL. The\nannotation process then differed in two ways: 1)\nannotations were obtained for all question from 5Figure 3: Annotation guideline for faithfulness labelling by human\nFigure 4: User interface used by human annotators for labelling faithfulness and relevance.\nexpert annotators, and 2) The Challenging to deter-\nmine label was not an option for annotators to force\nmeaningful labels even at the cost of disagreements.\nIAA metrics for the MEMERAG-Ext dataset can\nbe found in Table 7. Those results are in line with\nthe IAA observed in MEMERAG, which confirms\nits quality. Detailed investigation of the disagree-\nments between human annotators, and comparison\nto automated annotators are left as future work.We release this extendended dataset along with the\nmain dataset to promote related work by the re-\nsearch community.\nC Detailed numbers on the MEMERAG\ndataset\nIn this section, we look at the detailed statics\nof faithfulness and relevance in the MEMERAGLang #QAnswer Context\n#S Avg. #W Avg. #W\nEN 150 226 27.5 591.4\nDE 150 272 30.0 456.8\nES 150 276 39.1 490.8\nFR 150 370 48.1 453.2\nHI 150 208 23.0 576.6\nTotal 750 1352\nTable 8: General statistics of the MEMERAG-Ext\ndataset. #Q: number of questions, #S: number of sen-\ntences, #W: number of words. Each answer is annotated\nat the sentence level, leading to 1,352 total sentences\nannotated by experts for faithfulness and relevance.\ndataset itself. Overall, the faithfulness and rele-\nvance in the dataset variate a lot from one language\nto another language and from one generator model\nto another. There is usually a range of 10-20 per-\ncentage points between the language with lowest\npercentage and the language with highest percent-\nage of supported sentences. This supports our as-\nsumption of variable behaviour/performance across\ndifferent languages.\nGiven the large variation, the percentage of sup-\nported answers is larger than that of unsuported\nanswers. Similarly, the ratio of relevant answers is\nin general larger than that of other relevance buck-\nets (except for ES with LLM-D; EN, ES and FR\nwith LLM-A). Interestingly, when analysing the\nvairous results per model and language, we did no\nfind general pattern. This is counter intuitive to our\nassumption of having more supported sentences\nfor English language. Surprisingly, even though\nHindi is a low-resource language the models are\nproducing 70% supported sentences, except for\nLLM-D that produced only around 62% supported\nsentences.\nD Fine grained automatic annotation\nanalysis\nFigure 5 shows a heatmap representing the failure\nmodes of automatic evaluators. We observe that\nimproving the prompting strategy from ZS to AG +\nCOT, reduces automatic evaluation errors across all\nerror categories, except for Llama 3.2 11B, where\nthe model makes more errors in the \u201cLogical con-\nclusion category\u201d. We also observe that \u201cWrong\nreasoning\u201d, \u201cNuance shift\u201d and \u201cLogical conclu-\nsion\u201d are the top error categories for all the models\ntested. Future work could explore prompting or\nfine-tuning techniques designed to handle specificerror types.\nE Statistical Significance Test Details\nIn Table 5 and Figure 2, we aim to determine\nwhether the scores (in terms of BAcc) for the best\nprompt and best model was significantly different\nfrom the other scores in the table. To test statistical\nsignificance, we use permutation test (Good, 2013),\na non-parametric method for comparing two related\nsamples. The null hypothesis for this test suggests\nthat there is no significant difference between the\nperformance of the best-performing prompt/models\nand the other prompts/models, while the alternative\nhypothesis suggests a significant difference exists.\nWe consider \u03b1= 0.05as significance level. Con-\nsequently, when p > 0.05, we are not able to reject\nthe null hypothesis, indicating that prompts and\nmodels have similar performance.\nF Prompts used for dataset construction\nIn section we describe the various prompts used in\nour pipeline. Out prompts are written as Jinja29\ntemplates.\nF.1 Time-dependent answer filtering\nDuring internal pilots, we identified answers that\nrelied on current date / current affairs that could\nbe challenging to determine their faithfulness. For\nexample, to the question \"How old is Yann Le-\nCun?\" is the answer correct if the LLM uses the\ntime when it was trained? To avoid such cases we\nused Llama3 70B to filter out those cases using the\nprompt shown below:\nTime-dependent Answer filtering\nYou are an NLP assistant that helps\nto identify if a question requires to\nknow when is today (day, or month, or\nyear), current affairs, or up-to-date\ninformation. Give the step by step of\nhow to answer the question in between\nthe labels <rationale></rationale>. Then\nverify if the steps included to know any\ninformation about the current time and\ngive your answer in between the tags:\n<label></label>. Please only use \u2019yes\u2019 or\n\u2019no\u2019 in your final answer. You will be given\nthe question in {{language}}.\nProvide your rationale and label in\nEnglish.\n9https://jinja.palletsprojects.com/en/stable/Model EN DE ES FR HI\nLLM-A (615)\nFaithfulness 52.7 / 43.8 / 3.5 74.3 / 17.1 / 8.6 58.6 / 41.4 / 0 59.6 / 40.4 / 0 73.2 / 26.8 / 0\nRelevance 42.5 / 51.7 / 5.8 60.9 / 37.4 / 1.7 42.3 / 54.5 / 3.2 48 / 50.3 / 1.7 46.8 / 41.3 / 11.9\nLLM-B (315)\nFaithfulness 76.1 / 17.9 / 6 88.4 / 11.6 / 0 88.2 / 11.8 / 0 58.6 / 41.4 / 0 84.9 / 15.1 / 0\nRelevance 67.1 / 31.5 / 1.4 72.7 / 22.1 / 5.2 75 / 22.2 / 2.8 87.1 / 12.9 / 0 80.7 / 10.5 / 8.8\nLLM-C (315)\nFaithfulness 61.4 / 35.1 / 3.5 69.8 / 30.2 / 0 68.1 / 31.9 / 0 80.3 / 19.7 / 0 80 / 20 / 0\nRelevance 86.9 / 11.5 / 1.6 78.3 / 20.3 / 1.4 69.3 / 28 / 2.7 87.1 / 10 / 2.9 85.9 / 9.4 / 4.7\nLLM-D (743)\nFaithfulness 67.3 / 30.8 / 1.9 61.4 / 38 / 0.6 61 / 35.7 / 3.3 59.8 / 40.2 / 0 63 / 34.6 / 2.5\nRelevance 70.3 / 29.7 / 0 41.5 / 33 / 25.5 33 / 52.3 / 14.7 54.8 / 26.1 / 19.1 57.1 / 24.2 / 18.7\nLLM-E (334)\nFaithfulness 77.2 / 21.1 / 1.7 73.9 / 26.1 / 0 69.7 / 29 / 1.3 59.2 / 39.5 / 1.3 73.3 / 26.7 / 0\nRelevance 80.3 / 19.7 / 0 75.4 / 17.4 / 7.3 63.4 / 36.6 / 0 73.8 / 25 / 1.2 90.6 / 9.4 / 0\nTable 9: Percentage of sentences labelled as Supported/Not Supported/Challenging to determine for the faithfulness,\nand as Directly answers the question/Adds context to the answer/Unrelated to the question for the relevance, per\nlanguage and model. The total number of sentences generated by the model across all languages is given in\nparenthesis besides the model name.\nGPT4o mini Llama 3.2 11B Llama 3.2 90B Qwen 2.5 32B\nAnnotator ModelWrong reasoning\nNuance shift\nAdds new information\nLogical conclusion\nContradiction\nMis-referencing\nOther mistake\nOther\nDirect paraphrase\nOpinion as factFine-grained Label28 33 30 24\n25 26 26 25\n19 24 20 19\n11 26 8 15\n10 13 13 12\n6 6 6 4\n4 5 4 4\n4 2 1 5\n2 2 1 2\n1 2 2 2Zero Shot\nEnglish\nGPT4o mini Llama 3.2 11B Llama 3.2 90B Qwen 2.5 32B\nAnnotator ModelWrong reasoning\nNuance shift\nAdds new information\nLogical conclusion\nContradiction\nMis-referencing\nOther mistake\nOther\nOpinion as fact\nDirect paraphraseFine-grained Label27 24 30 25\n25 22 25 26\n16 17 18 19\n10 31 15 9\n12 9 9 10\n5 4 6 5\n4 4 4 4\n1 5 2 1\n2 2 2 2\n0 3 2 0Chain of Thought\nEnglish\nGPT4o mini Llama 3.2 11B Llama 3.2 90B Qwen 2.5 32B\nAnnotator ModelLogical conclusion\nWrong reasoning\nNuance shift\nAdds new information\nContradiction\nOther\nMis-referencing\nDirect paraphrase\nOther mistake\nOpinion as factFine-grained Label30 24 8 28\n16 33 26 14\n17 24 25 21\n10 22 21 11\n4 14 14 4\n8 2 2 8\n3 6 5 4\n7 2 1 6\n3 4 4 4\n1 2 2 1Annotation Guidelines (AG)\nEnglish\nGPT4o mini Llama 3.2 11B Llama 3.2 90B Qwen 2.5 32B\nAnnotator ModelLogical conclusion\nNuance shift\nWrong reasoning\nAdds new information\nContradiction\nOther\nMis-referencing\nDirect paraphrase\nOther mistake\nOpinion as factFine-grained Label18 51 11 15\n22 17 26 24\n19 21 23 21\n11 12 16 16\n6 8 10 11\n5 16 3 5\n5 3 5 5\n3 6 3 4\n2 3 4 4\n2 1 2 2Annotation Guidelines (AG) + COT\nEnglish51015202530\n051015202530\n51015202530\n1020304050\nDisagreement between Ground Truth and Automatic Annotator for English\nFigure 5: Fine-grained faithfulness errors when automatic evaluator disagrees with ground truth.F.2 Highlighting relevant segments\nWe highlight relevant statements in the passage\nto help annotators focus on important parts of the\ncontext. We prompt Llama 3 70B, to predict all\nthe statements that support the passage. We use\ntemperature equal to 0.1 in this step.\nF.3 Answer generation prompt\nWe generate the answers using the same prompt\nacross all models and languages. All the instruc-\ntions were given in English, while the context and\nquestion were given in the testing language. For\nall models we set the temperature to 0.1 and the\nmaximum number of token to 1000.\nHighlighting Relevant Segments\nYou are an agent that verifies if sentences\nare supported by a given context.\nYou will be given a context made of several\npassages referred as \"Passage\" split into\nsentences, each with a numeral identifier,\nand each referred as \"Sentence\". Your\ntask is to determine if the sentence is\nsupported by some of the passages (using\nlabel 1) or is not supported (using label\n0). Please follow this process:\n(1) Explain why the sentence is\nsupported or not supported, please\nwrite your reasoning in between the tags\n<rationale></rationale>. If a sentence is\nsupported, list the number of the sentences\nin the passage or several passages that\nsupported it. If a sentence is not\nsupported, explain why is not supported.\n(2) Write the final label for the sentence\nin between the tags <label></label>.\n(3) Write the id of the supporting\nsentences from passages in between the\ntags <", "Statistical Significance Test Details": "In Table 5 and Figure 2, we aim to determine\nwhether the scores (in terms of BAcc) for the best\nprompt and best model was significantly different\nfrom the other scores in the table. To test statistical\nsignificance, we use permutation test (Good, 2013),\na non-parametric method for comparing two related\nsamples. The null hypothesis for this test suggests\nthat there is no significant difference between the\nperformance of the best-performing prompt/models\nand the other prompts/models, while the alternative\nhypothesis suggests a significant difference exists.\nWe consider \u03b1= 0.05as significance level. Con-\nsequently, when p > 0.05, we are not able to reject\nthe null hypothesis, indicating that prompts and\nmodels have similar performance.\nF", "Prompts used for dataset construction": "In section we describe the various prompts used in\nour pipeline. Out prompts are written as Jinja29\ntemplates.\nF.1", "Time-dependent answer filtering": "During internal pilots, we identified answers that\nrelied on current date / current affairs that could\nbe challenging to determine their faithfulness. For\nexample, to the question \"How old is Yann Le-\nCun?\" is the answer correct if the LLM uses the\ntime when it was trained? To avoid such cases we\nused Llama3 70B to filter out those cases using the\nprompt shown below:\nTime-dependent Answer filtering\nYou are an NLP assistant that helps\nto identify if a question requires to\nknow when is today (day, or month, or\nyear), current affairs, or up-to-date\ninformation. Give the step by step of\nhow to answer the question in between\nthe labels <rationale></rationale>. Then\nverify if the steps included to know any\ninformation about the current time and\ngive your answer in between the tags:\n<label></label>. Please only use \u2019yes\u2019 or\n\u2019no\u2019 in your final answer. You will be given\nthe question in {{language}}.\nProvide your rationale and label in\nEnglish.\n9https://jinja.palletsprojects.com/en/stable/Model EN DE ES FR HI\nLLM-A (615)\nFaithfulness 52.7 / 43.8 / 3.5 74.3 / 17.1 / 8.6 58.6 / 41.4 / 0 59.6 / 40.4 / 0 73.2 / 26.8 / 0\nRelevance 42.5 / 51.7 / 5.8 60.9 / 37.4 / 1.7 42.3 / 54.5 / 3.2 48 / 50.3 / 1.7 46.8 / 41.3 / 11.9\nLLM-B (315)\nFaithfulness 76.1 / 17.9 / 6 88.4 / 11.6 / 0 88.2 / 11.8 / 0 58.6 / 41.4 / 0 84.9 / 15.1 / 0\nRelevance 67.1 / 31.5 / 1.4 72.7 / 22.1 / 5.2 75 / 22.2 / 2.8 87.1 / 12.9 / 0 80.7 / 10.5 / 8.8\nLLM-C (315)\nFaithfulness 61.4 / 35.1 / 3.5 69.8 / 30.2 / 0 68.1 / 31.9 / 0 80.3 / 19.7 / 0 80 / 20 / 0\nRelevance 86.9 / 11.5 / 1.6 78.3 / 20.3 / 1.4 69.3 / 28 / 2.7 87.1 / 10 / 2.9 85.9 / 9.4 / 4.7\nLLM-D (743)\nFaithfulness 67.3 / 30.8 / 1.9 61.4 / 38 / 0.6 61 / 35.7 / 3.3 59.8 / 40.2 / 0 63 / 34.6 / 2.5\nRelevance 70.3 / 29.7 / 0 41.5 / 33 / 25.5 33 / 52.3 / 14.7 54.8 / 26.1 / 19.1 57.1 / 24.2 / 18.7\nLLM-E (334)\nFaithfulness 77.2 / 21.1 / 1.7 73.9 / 26.1 / 0 69.7 / 29 / 1.3 59.2 / 39.5 / 1.3 73.3 / 26.7 / 0\nRelevance 80.3 / 19.7 / 0 75.4 / 17.4 / 7.3 63.4 / 36.6 / 0 73.8 / 25 / 1.2 90.6 / 9.4 / 0\nTable 9: Percentage of sentences labelled as Supported/Not Supported/Challenging to determine for the faithfulness,\nand as Directly answers the question/Adds context to the answer/Unrelated to the question for the relevance, per\nlanguage and model. The total number of sentences generated by the model across all languages is given in\nparenthesis besides the model name.\nGPT4o mini Llama 3.2 11B Llama 3.2 90B Qwen 2.5 32B\nAnnotator ModelWrong reasoning\nNuance shift\nAdds new information\nLogical conclusion\nContradiction\nMis-referencing\nOther mistake\nOther\nDirect paraphrase\nOpinion as factFine-grained Label28 33 30 24\n25 26 26 25\n19 24 20 19\n11 26 8 15\n10 13 13 12\n6 6 6 4\n4 5 4 4\n4 2 1 5\n2 2 1 2\n1 2 2 2Zero Shot\nEnglish\nGPT4o mini Llama 3.2 11B Llama 3.2 90B Qwen 2.5 32B\nAnnotator ModelWrong reasoning\nNuance shift\nAdds new information\nLogical conclusion\nContradiction\nMis-referencing\nOther mistake\nOther\nOpinion as fact\nDirect paraphraseFine-grained Label27 24 30 25\n25 22 25 26\n16 17 18 19\n10 31 15 9\n12 9 9 10\n5 4 6 5\n4 4 4 4\n1 5 2 1\n2 2 2 2\n0 3 2 0Chain of Thought\nEnglish\nGPT4o mini Llama 3.2 11B Llama 3.2 90B Qwen 2.5 32B\nAnnotator ModelLogical conclusion\nWrong reasoning\nNuance shift\nAdds new information\nContradiction\nOther\nMis-referencing\nDirect paraphrase\nOther mistake\nOpinion as factFine-grained Label30 24 8 28\n16 33 26 14\n17 24 25 21\n10 22 21 11\n4 14 14 4\n8 2 2 8\n3 6 5 4\n7 2 1 6\n3 4 4 4\n1 2 2 1Annotation Guidelines (AG)\nEnglish\nGPT4o mini Llama 3.2 11B Llama 3.2 90B Qwen 2.5 32B\nAnnotator ModelLogical conclusion\nNuance shift\nWrong reasoning\nAdds new information\nContradiction\nOther\nMis-referencing\nDirect paraphrase\nOther mistake\nOpinion as factFine-grained Label18 51 11 15\n22 17 26 24\n19 21 23 21\n11 12 16 16\n6 8 10 11\n5 16 3 5\n5 3 5 5\n3 6 3 4\n2 3 4 4\n2 1 2 2Annotation Guidelines (AG) + COT\nEnglish51015202530\n051015202530\n51015202530\n1020304050\nDisagreement between Ground Truth and Automatic Annotator for English\nFigure 5: Fine-grained faithfulness errors when automatic evaluator disagrees with ground truth.F.2", "Highlighting relevant segments": "We highlight relevant statements in the passage\nto help annotators focus on important parts of the\ncontext. We prompt Llama 3 70B, to predict all\nthe statements that support the passage. We use\ntemperature equal to 0.1 in this step.\nF.3", "Answer generation prompt": "We generate the answers using the same prompt\nacross all models and languages. All the instruc-\ntions were given in English, while the context and\nquestion were given in the testing language. For\nall models we set the temperature to 0.1 and the\nmaximum number of token to 1000.\nHighlighting Relevant Segments\nYou are an agent that verifies if sentences\nare supported by a given context.\nYou will be given a context made of several\npassages referred as \"Passage\" split into\nsentences, each with a numeral identifier,\nand each referred as \"Sentence\". Your\ntask is to determine if the sentence is\nsupported by some of the passages (using\nlabel 1) or is not supported (using label\n0). Please follow this process:\n(1) Explain why the sentence is\nsupported or not supported, please\nwrite your reasoning in between the tags\n<rationale></rationale>. If a sentence is\nsupported, list the number of the sentences\nin the passage or several passages that\nsupported it. If a sentence is not\nsupported, explain why is not supported.\n(2) Write the final label for the sentence\nin between the tags <label></label>.\n(3) Write the id of the supporting\nsentences from passages in between the\ntags <", "Prompts for automated evaluation": "The four prompts evaluate answer faithfulness to\nsource passages with increasing complexity: Zero-\nshot (ZS) provides basic supported/not-supported\nclassification, Chain of Thought (COT) adds ex-\nplicit reasoning steps, Annotation Guidelines (AG)\nincludes detailed evaluation criteria, and AG+COT\ncombines detailed guidelines with reasoning steps.\nAll prompts output their final classification in <an-\nswer> tags.\nZero-shot Prompt (ZS)\nYou are an automatic annotator tasked with\ndetermining whether a given answer is\ngrounded in the list of provided evidence\npassages. Your role is to carefully analyze\nthe relationship between the answer and\nthe evidence, and then classify the answer\nas either \"Supported\" or \"Not Supported\".\nProvide your answer directly in <answer>\n</answer> tag.\nEvidence Passages:\n{% for passage in context %}\n. {{loop.index}}: {{passage.text}}\n{% endfor %}\nAnswer: {{answer segment}}\nNow provided your label directly as\n\u201dSupported\u201d or \u201dNot Supported\u201d.Chain of thought (COT)\nYou are an automatic annotator tasked with\ndetermining whether a given answer is\ngrounded in the list of provided evidence\npassages. Your role is to carefully analyze\nthe relationship between the answer and the\nevidence, write your reasoning in between\nthe tags <rationale></rationale> then\nclassify the answer as either \u201cSupported\u201d\nor \u201cNot Supported\u201d and write answer in\n<answer> </answer> tag\nEvidence Passages:\n{% for passage in context %}\n{{loop.index}}: {{passage.text}}\n{% endfor %}\nAnswer: {{answer segment}}\nNow provided your label directly as\n\"Supported\" or \"Not Supported\".\n.Annotation Guidelines (AG)\nGiven a set of evidence passages, and an\nanswer, determine if the answer is fully\nsupported by the evidence passages or\nnot. Analyze each sentence of the answer\ncarefully and verify that all information\nit contains is explicitly stated in or\ncan be directly inferred from the evidence\npassages.\nOutput \"Not Supported\" if ANY of the\nfollowing are true:\nThe answer contains any information not\nexplicitly stated in or directly inferable\nfrom the passages.\nThe answer contradicts any information in\nthe passages.\nThe answer introduces any new information\nnot found in the passages.\nThe answer misrepresents or inaccurately\nparaphrases information from the passages.\nThe answer draws conclusions not logically\nsupported by the given information.\nThe answer changes the level of certainty,\nspecificity, or nuance from what is\nexpressed in the passages.\nThe answer does not directly address\nthe specific aspect asked about in the\nquestion.\nThe answer conflates or misrepresents\nseparate pieces of information when\nsummarizing multiple passages.\nOutput Supported otherwise.\nProvide this determination without\nany additional explanation in\n<answer></answer> tags. Analyze thoroughly\nbut output only the single-word label.\nEvidence Passages:\n{% for passage in context %}\n{{loop.index}}: {{passage.text}}\n{% endfor %}\nAnswer: {{answer segment}}\nNow provided your label directly as\n\"Supported\" or \"Not Supported\".Annotation guidelines with Chain of thought (AG + COT)\nGiven a set of evidence passages, and an answer, determine if the answer is\nfully supported by the evidence passages or not. Analyze each sentence of the\nanswer carefully and verify that all information it contains is explicitly\nstated in or can be directly inferred from the evidence passages.\nOutput \"Not Supported\" if ANY of the following are true:\nThe answer contains any information not explicitly stated in or directly\ninferable from the passages.\nThe answer contradicts any information in the passages.\nThe answer introduces any new information not found in the passages.\nThe answer misrepresents or inaccurately paraphrases information from the\npassages.\nThe answer draws conclusions not logically supported by the given information.\nThe answer changes the level of certainty, specificity, or nuance from what is\nexpressed in the passages.\nThe answer does not directly address the specific aspect asked about in the\nquestion.\nThe answer conflates or misrepresents separate pieces of information when\nsummarizing multiple passages.\nOutput Supported otherwise.\nwrite your reasoning in between the tags <rationale></rationale> and Provide\nyour final answer in <answer></answer> tags.\nEvidence Passages:\n{% for passage in context %}\n{{loop.index}}: {{passage.text}}\n{% endfor %}\nAnswer: {{answer segment}}\nNow provided your label directly as \"Supported\" or \"Not Supported\".\nH", "More Results on Meta-Evaluators": "EN DE ES FR HI\nGPT4o mini 68.37\u00b12.84 73.58\u00b12.86 69.84 \u00b12.26 73.74 \u00b12.29 74.10 \u00b12.61\nQwen 2.5 32B 62.45 \u00b13.00 76.79\u00b12.76 71.09 \u00b11.98 74.38 \u00b12.28 75.41 \u00b12.72\nLlama 3.2 90B 62.60 \u00b12.73 63.11 \u00b12.70 63.36 \u00b12.09 63.17 \u00b11.97 75.05 \u00b12.87\nLlama 3.2 11B 60.29 \u00b12.84 59.91 \u00b13.13 59.02 \u00b12.51 65.02 \u00b12.46 65.22 \u00b12.75\nTable 10: Balanced Accuracy for five languages and four LLMs using AG+COT prompting strategy. Standard errors\nare calculated using 1000 Bootstrap iterations.\nPrompt EN DE ES FR HI\nZS 57.22 \u00b12.14 61.54 \u00b11.98 56.94 \u00b11.26 60.00 \u00b11.39 68.96 \u00b12.22\nCOT 60.71 \u00b12.34 63.83 \u00b12.15 60.61 \u00b11.44 63.85 \u00b11.53 69.68 \u00b12.42\nAG 62.72 \u00b12.24 68.87\u00b12.27 63.61\u00b11.36 68.32 \u00b11.53 70.20 \u00b12.02\nAG + COT 63.42\u00b12.44 68.35\u00b12.37 65.83\u00b11.64 69.08 \u00b11.71 72.44 \u00b12.07\nTable 11: Balanced Accuracy for five languages using four different prompting strategies, averaged across four\nLLMs (GPT-4o mini, Llama 3.2 90B, Llama 3.2 11B, and Qwen 2.5 32B). Standard errors are calculated using\n1000 Bootstrap iterations.\nLanguage Prompt GPT4o mini Llama 3.2 90B Llama 3.2 11B Qwen 2.5 32B\nENZS 59.8 58.0 51.0 60.1\nCOT 61.8 59.1 60.0 62.0\nAG 70.0 59.4 53.0 68.5\nAG + COT 68.4 62.6 60.2 62.5\nDEZS 61.1 58.6 54.8 71.9\nCOT 63.3 60.0 59.6 72.7\nAG 72.0 68.6 60.6 74.6\nAG + COT 73.7 63.3 60.0 76.8\nESZS 53.8 56.8 53.7 63.7\nCOT 56.2 58.2 61.9 66.2\nAG 69.7 59.6 54.3 70.8\nAG + COT 69.9 63.4 59.1 71.1\nFRZS 59.5 56.3 56.0 68.5\nCOT 60.4 58.5 65.1 71.6\nAG 74.5 62.4 60.3 76.2\nAG + COT 73.7 63.2 65.0 74.4\nHIZS 72.2 64.3 66.1 73.3\nCOT 72.1 67.6 65.4 73.8\nAG 73.8 67.1 65.5 74.5\nAG + COT 74.2 75.1 65.2 75.5\nTable 12: Meta evaluation results for all combination of LLMs and prompts we have tested.Fine-Grained Label #S GPT-4o miniLlama 3.2\n90B\nLogical conclusion 151 80.1 87.4\nDirect paraphrase 182 93.4 97.3\nOther 2 50.0 50.0\nWe. Acc Sup. 87.2 92.5\nAdds new information 81 67.9 33.3\nNuance shift 30 26.7 16.7\nMis-referencing 20 35.0 20.0\nContradiction 32 65.6 50.0\nOpinion as fact 12 66.7 25.0\nOther mistake 19 84.2 52.6\nWrong reasoning 10 80.0 40.0\nWe. Acc NS. 60.3 33.8\nBAcc 73.7 63.2\nTable 13: Accuracy of GPT-4o mini and Llama 3.2 90B\non various fine-grained labels, when evaluating French\nlanguage using AG + COT prompt\nI", "Fine grained Analysis": "To demonstrate the benchmark\u2019s capability for fine-\ngrained analysis, Table 13 breaks down automatic\nevaluation performance by error type for two mod-\nels: GPT-4o mini and Llama 3.2 90B, both evaluat-\ning French answers using AG + COT prompt. The\nupper section of the table shows the distribution of\nerrors when the ground truth class is Supported , cat-\negorized into logical conclusion, direct paraphrase,\nor other correct categories. The lower section de-\ntails the types of mistakes made by each model for\ntheNot supported category. We observe that both\nmodels show a similar pattern for the Supported\ncategory, with logical conclusions being the most\ncommon (69.8% for GPT-4o mini and 76.0% for\nLlama 3.2 90B), followed by direct paraphrases.\nFor the Not supported category, both models strug-\ngle most with detecting \u201cadding new information\u201d\n(32.1% and 40.0% of mistakes, respectively) and\n\u201cnuance shifts\u201d (27.2% and 18.5%). The ranking of\nerror types is consistent across both models, despite\ntheir different overall BAcc (73.7% for GPT-4o\nmini vs 63.2% for Llama 3.2 90B see Table 12 in\nAppendix), suggesting similar challenges in evalua-\ntion. The benchmark\u2019s fine-grained labeling system\nprovides a detailed view of evaluation challenges\nacross languages. By categorizing different types\nof faithfulness violations, it reveals which specific\nerrors are harder to detect for each model, offering\ninsights into both the strengths and limitations of\nautomated evaluation methods."}, "Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A Clinician Study": {"Abstract": "ing with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires\nprior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a92025 Association for Computing Machinery.\nXXXX-XXXX/2025/5-ART $15.00\nhttps://doi.org/10.1145/nnnnnnn.nnnnnnn\n, Vol. 1, No. 1, Article . Publication date: May 2025.arXiv:2505.01680v1  [cs.CV]  3 May 20252Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nfrom multiple views (e.g., ipsilateral, contralateral, top), which are critical for a comprehensive\nassessment of movement [ 20]. Moreover, clinical adoption of deep learning models requires inter-\npretability to ensure trust and usability, a challenge addressed by techniques like Grad-CAM [16]\nfor visualizing model attention and Hierarchical Bayesian Models (HBMs) for probabilistic quality\nassessment [1].\nIn this paper, we present a multimodal framework for automated ARAT scoring, integrating three\nvideo analysis pipelines: (1) SlowFast, for spatial-temporal dynamics; (2) I3D, for 3D convolutional\nfeatures; and (3) a Transformer-based model using OpenPose keypoints [ 4] and object locations\nto focus on kinematic patterns. We incorporate multi-view data, applying early and late fusion\nto combine features across views and models, enhancing robustness. Grad-CAM provides spatial-\ntemporal interpretability, while HBMs infer movement quality components (e.g., trunk stabilization,\nwrist hand aperture). Our system outputs ARAT task scores and populates a clinician dashboard,\ndetailed in Section 3.6, which includes task scores, execution times, and quality impairments.\nTo validate our system in a clinical context, we conducted a study involving five clinicians\nwho reviewed 500 video ratings generated by our framework, providing feedback on accuracy,\nusability, and potential improvements. Our dataset, derived from the stroke rehabilitation study\nin [1], includes multi-view videos of ARAT tasks. The system achieves a validation accuracy of\n89.0% with late fusion, with HBMs aligning closely with manual quality assessments. This work\ncontributes a scalable, interpretable solution for automated ARAT scoring, validated through clinical\nfeedback, advancing the field of stroke rehabilitation technology.\n2 RELATED WORK\n2.1 Video Classification and Action Recognition\nTraditional video classification relied on hand-crafted features like Histogram of Oriented Gradients\n(HOG) and optical flow [ 12], but these methods struggled with complex temporal dynamics. Deep\nlearning introduced 3D CNNs, such as C3D [ 18], which extended 2D convolutions to the temporal\ndomain. I3D [ 5] improved performance by inflating Inception V1 weights for video, achieving high\naccuracy on Kinetics [ 11]. The SlowFast network [ 7] introduced a dual-pathway architecture, with\na slow pathway for spatial semantics and a fast pathway for motion, advancing action recognition\nbenchmarks. Transformer-based models, such as ViViT [ 2] and TimeSformer [ 3], leverage self-\nattention to model long-range temporal dependencies, often outperforming CNNs in tasks requiring\nfine-grained motion analysis [8].\n2.2 Pose Estimation and Kinematic Analysis\nPose estimation has become integral to action recognition, particularly in rehabilitation. OpenPose\n[4] extracts 2D keypoints, enabling analysis of joint movements, while works like [ 17] have applied it\nto hand tracking in motor assessments. [ 21] combined pose data with CNNs for action classification,\ndemonstrating improved accuracy in tasks involving human motion. In stroke rehabilitation,\nkinematic analysis using keypoints has been explored to assess movement quality [ 14], often paired\nwith probabilistic models like HBMs [1] to infer impairments such as trunk compensation.\n2.3 Interpretability in Deep Learning\nInterpretability is critical in clinical applications. Grad-CAM [ 16] uses gradient information to\nhighlight regions influencing predictions, extended to video models in . Alternatives like Score-CAM\n[19] offer complementary visualization techniques. Probabilistic models, such as HBMs, provide\ninterpretable outputs by modeling uncertainty, as shown in [1] for movement quality assessment\nin stroke rehabilitation, where Bayesian methods infer quality metrics from kinematic data.\n, Vol. 1, No. 1, Article . Publication date: May 2025.Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A\nClinician Study 3\n2.4 Multi-View Fusion and Ensemble Learning\nMulti-view video analysis enhances robustness by capturing complementary perspectives. Early\nfusion concatenates features before classification, while late fusion combines predictions, often\noutperforming single-view models [ 10]. Ensemble learning with multimodal inputs, as in [ 23],\nintegrates diverse features (e.g., video, pose) for improved accuracy. [ 22] explored multi-view fusion\nin rehabilitation, highlighting its potential for comprehensive movement analysis.\n2.5 Automated ARAT Scoring\nAutomated ARAT scoring has been investigated using machine learning , with recent works\nincorporating video analysis . [ 1] introduced an HBM for cyber-human assessment, using kinematic\ndata to infer movement quality in stroke rehabilitation. combined pose estimation with CNNs for\nARAT scoring, achieving moderate agreement with manual assessments. Our work extends these\nefforts by integrating multimodal video analysis, multi-view fusion, semantic HBMs, and clinical\nvalidation through a clinician study.\n3 METHODOLOGY\n3.1 Dataset Description\nOur dataset is sourced from the stroke rehabilitation study in [ 1], which collected video recordings\nof ARAT tasks performed by stroke patients at a rehabilitation center. The dataset includes multi-\nview videos (ipsilateral, contralateral, and top perspectives) captured using synchronized cameras,\nfocusing on upper extremity movements during tasks such as grasping a block, transporting it to a\ntarget location, and releasing it. Each task is segmented into movement phases (e.g., movement\ninitiation, grasping, transporting, releasing), with manual ARAT scores (0\u20133) assigned by clinicians\nbased on task completion and movement quality components like trunk stabilization and wrist\nhand aperture.\nThe dataset comprises 500 segments across 50 patients, with each segment containing approxi-\nmately 100 frames per view at 30 FPS. Videos are stored in pickle files at D:/pickle_dir/fine_tune ,\nwith bounding box annotations in D:/frcnn_bboxes/bboxes_top , generated using Faster R-CNN\n[15]. OpenPose [ 4] extracts keypoints for upper extremity joints (shoulder, elbow, wrist, hand) and\nobject locations (e.g., blocks), providing 2D coordinates normalized to frame dimensions. Following\n[1], we filter segments with ARAT ratings of 2 or 3, mapping them to binary labels (0 and 1) for\nclassification, resulting in a balanced dataset (250 segments per class). The dataset is split 80-20\ninto training and validation sets using a random seed of 42 for reproducibility.\nFrames are preprocessed by cropping around bounding boxes (with a 30-pixel extension on\nthe lower side to capture hand movements), resizing to 256x256, center-cropping to 224x224, and\nnormalizing with mean [0.45, 0.45, 0.45] and standard deviation [0.225, 0.225, 0.225]. For SlowFast\nand I3D, we sample 8 and 32 frames, respectively, while the Transformer processes 32 frames with\nembedded keypoints and object data.\n3.2 Multimodal Pipelines\n3.2.1 SlowFast Pipeline. The SlowFast network [ 7] uses a dual-pathway architecture: a slow\npathway with a low frame rate (2 frames, temporal stride 4) to capture spatial semantics, and a\nfast pathway with a high frame rate (8 frames, temporal stride 1) to capture motion dynamics. We\nadopt the R50 backbone, pre-trained on Kinetics, with input shapes [4, 3, 2, 224, 224] (slow) and [4,\n3, 8, 224, 224] (fast) for a batch size of 4. The slow pathway operates on fewer frames to focus on\nstatic features, while the fast pathway captures rapid movements, with lateral connections fusing\n, Vol. 1, No. 1, Article . Publication date: May 2025.4Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nfeatures between pathways. The head is modified to output 2 classes:\nmodel.head.projection =Linear (2304,2)\nEarly layers (up to s3) are frozen to preserve low-level features, reducing trainable parameters\nto 15M (from 34M). The model is fine-tuned for 10 epochs using Adam (learning rate 1e-4) and\ncross-entropy loss, with gradient clipping (max norm 1.0) to prevent exploding gradients.\n3.2.2 I3D Pipeline. The I3D model, based on Inception V1, processes 32 frames with input shape [4,\n3, 32, 224, 224]. Pre-trained on Kinetics, it extracts spatio-temporal features using 3D convolutions,\nwith a deeper architecture (22 layers) compared to SlowFast. The final layer is replaced to output 2\nclasses:\nmodel.logits =Linear (1024,2)\nTraining mirrors the SlowFast pipeline, freezing early layers (up to the third Inception block) to\nretain pre-trained features, resulting in 12M trainable parameters. Fine-tuning uses Adam (learning\nrate 1e-4) for 10 epochs, with dropout (0.5) in the final layer to mitigate overfitting.\n3.2.3 Transformer Pipeline. We implement a Vision Transformer (ViT) , adapted for video using\nTimeSformer, processing 32 frames. Each frame includes embedded OpenPose keypoints (shoulder,\nelbow, wrist, hand) and object locations (block centroids), concatenated as additional channels,\nresulting in an input shape of [4, 32, 224, 224, 6] (6 channels for 4 keypoints + 2 object coordinates).\nThe model uses divided space-time attention:\nAttention (\ud835\udc44,\ud835\udc3e,\ud835\udc49 )=softmax\u0012\ud835\udc44\ud835\udc3e\ud835\udc47\n\u221a\ud835\udc51\ud835\udc58\u0013\n\ud835\udc49\nwhere attention is applied first spatially (across patches in a frame) and then temporally (across\nframes). The ViT-B/16 variant (12 layers, 768D hidden size) is pre-trained on ImageNet, with a\nclassification head outputting 2 classes. Fine-tuning uses Adam (learning rate 1e-4) for 10 epochs,\nwith 86M parameters (all trainable due to the modality shift to video and keypoints).\n3.3 Multi-View Feature Fusion\nFeatures are extracted from each view (ipsilateral, contralateral, top) using the best-performing\nmodels per pipeline (e.g., SlowFast for spatial-temporal, Transformer for pose). For each model,\nfeatures are extracted before the final layer: 2304D for SlowFast, 1024D for I3D, and 768D for\nTransformer.\n-Early Fusion : Features from all views are concatenated into a single vector [view1, view2,\nview3], e.g., [6912D] for SlowFast (2304 \u00d73 views), processed by a fully connected layer to reduce\ndimensionality to 512D, followed by a classification head (512 \u21922). -Late Fusion : View-specific\npredictions are generated, then averaged with weights based on validation accuracy (e.g., 0.4 for\nipsilateral, 0.35 for contralateral, 0.25 for top).\n3.4 Model Fusion\nAfter view fusion, we combine features/predictions across pipelines: - Early Fusion : Concatenate\nfeatures from SlowFast, I3D, and Transformer (512D + 512D + 512D = 1536D), followed by a fully\nconnected layer to 256D and a classification head (256 \u21922). -Late Fusion : Average predictions\nfrom each pipeline, weighted by performance (0.35 for Transformer, 0.35 for SlowFast, 0.30 for I3D).\n3.5 Hierarchical Bayesian Models\nFollowing [ 1], we implement two HBMs: - Kinematic HBM : Uses OpenPose keypoints and object\nlocations to infer movement quality components (e.g., trunk stabilization, wrist hand aperture,\n, Vol. 1, No. 1, Article . Publication date: May 2025.Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A\nClinician Study 5\nforearm pronation support). The model employs a hierarchical structure, with latent variables\nrepresenting quality metrics, optimized using variational inference over 100 epochs. The model\noutputs probabilities for 10 ARAT quality criteria, such as shoulder elevation and digit positioning.\n-Semantic HBM : Extends the kinematic HBM to SlowFast and I3D features, modeling semantic\npatterns (e.g., smoothness of motion, trajectory accuracy). Features are reduced to 128D via PCA,\nthen fed into the HBM, outputting probabilities for the same quality criteria. Variational inference\noptimizes the evidence lower bound (ELBO) with a learning rate of 1e-3.\nBoth HBMs are implemented in PyTorch, with the kinematic HBM having 5 latent layers (50\nnodes each) and the semantic HBM having 3 latent layers (30 nodes each), reflecting the complexity\nof semantic feature interpretation.\n3.6 Automated ARAT Scoring and Clinician Dashboard\nEnsemble predictions from model fusion and HBM outputs generate task scores (0 or 1) and\nmovement phase scores (e.g., grasping, transporting). The clinician dashboard, shown in Figure 1, is\ndesigned to present a comprehensive summary to clinicians. It includes: - Patient Information : A\ndropdown to select patient records. - View Selection : Options for ipsilateral, contralateral, and top\nviews, with the video player displaying the selected perspective. - Task Score and Time : Displays\nthe ARAT task score (e.g., 2) and execution time (e.g., 0.07s). - Movement Phase Analysis :\nLists movement phases (e.g., movement initiation, grasping) with scores and observed quality\nimpairments (e.g., wrist hand aperture, forearm pronation support). - Submission Options : Buttons\nto submit or save the assessment, with a \"Proceed to Next Assessment\" link for workflow efficiency.\nThe dashboard integrates outputs from both kinematic and semantic HBMs, providing a detailed\nbreakdown of movement quality impairments alongside task scores, enabling clinicians to quickly\nvalidate automated assessments.\nFig. 1. Clinician Dashboard for ARAT Assessment\n3.7 Clinician Study Design\nTo evaluate the clinical utility of our automated ARAT scoring system, we conducted a study\ninvolving five clinicians with expertise in stroke rehabilitation. The clinicians reviewed 500 video\nratings generated by our system, covering the entire dataset of ARAT task segments. Each rating\n, Vol. 1, No. 1, Article . Publication date: May 2025.6Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nincludes the task score, movement phase scores, and quality impairments as displayed on the\ndashboard (Figure 1).\nThe study protocol is as follows:\n(1)Training Phase : Clinicians were trained on the dashboard interface, familiarizing themselves\nwith the layout, navigation, and interpretation of automated scores and quality impairments.\n(2)Review Phase : Each clinician independently reviewed the 500 video ratings over a period of\ntwo weeks, accessing the dashboard to view videos, scores, and quality assessments. They\nwere asked to compare automated ratings with their own manual assessments, focusing on\ntask scores and movement quality components.\n(3)Feedback Collection : Clinicians provided feedback through a structured questionnaire,\nrating the system on a 5-point Likert scale across dimensions such as accuracy, usability,\ninterpretability, and clinical relevance. Open-ended questions allowed for qualitative feedback\non potential improvements, such as additional quality metrics or interface enhancements.\n(4)Analysis Phase : Feedback was analyzed to compute average scores for each dimension,\nwith qualitative responses categorized into themes (e.g., usability issues, desired features).\nDiscrepancies between automated and manual ratings were quantified to assess system\nreliability.\nWe expect the study to validate the system\u2019s accuracy, with preliminary agreement rates (e.g.,\n91.0% for task scores) suggesting high reliability. Clinician feedback will guide future iterations,\npotentially incorporating additional quality metrics (e.g., joint range of motion) or real-time feedback\nfeatures in the dashboard.\n3.8 Challenges and Solutions\n-Gradient Computation in Grad-CAM : Resolved by removing @torch.no_grad() and enabling\ngradients with inputs.requires_grad_(True) , ensuring Grad-CAM heatmaps could be generated.\n-Data Loading Efficiency : Transitioned from batch_size=1 to 4 with num_workers=4 , reducing\ntraining time by 30% and improving GPU utilization. - Feature Alignment : Standardized feature\ndimensions using padding and interpolation across pipelines, ensuring compatibility for fusion.\n4 IMPLEMENTATION DETAILS\n4.1 Data Loading and Preprocessing\nAVideoSegmentDataset class handles multi-view segments, processing frames, keypoints, and\nobject data. Batched loading uses batch_size=4 andnum_workers=4 , with input shapes as de-\nscribed in Section 3.2. Keypoints are normalized to [0, 1] relative to frame dimensions, and object\nlocations are encoded as bounding box centroids (x, y coordinates).\n4.2 Model Training and Fusion\nModels are trained on GPU, with frozen early layers to prevent overfitting. Saved checkpoints\ninclude slowfast_finetuned.pt ,i3d_finetuned.pt , and transformer_finetuned.pt . Fusion\nlayers are trained for 5 epochs, optimizing cross-entropy loss with a batch size of 4.\n4.3 Grad-CAM and HBM Integration\nGrad-CAM heatmaps are generated for SlowFast and I3D across views, visualized with a jet colormap\n(50% opacity) to highlight regions of interest (e.g., hand during grasping). HBMs output probabilities\nfor 10 movement quality components, achieving 92% agreement with manual assessments, with\nthe semantic HBM identifying trajectory smoothness as a frequent impairment.\n, Vol. 1, No. 1, Article . Publication date: May 2025.Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A\nClinician Study 7\n5 RESULTS\nTable 1. Performance Metrics Across Pipelines and Fusion Strategies\nModel/Fusion Val. Accuracy\n(%)Training Time\n(min/epoch)F1 Score ARAT Score\nAgreement (%)\nSlowFast 85.2 12.5 0.84 88.0\nI3D 83.9 14.0 0.82 86.5\nTransformer 87.1 15.2 0.86 90.0\nEarly Fusion (Views) 87.5 16.0 0.85 89.0\nLate Fusion (Views) 88.0 15.5 0.86 89.5\nEarly Fusion (Models) 88.5 18.0 0.87 90.5\nLate Fusion (Models) 89.0 17.5 0.88 91.0\nThe Transformer pipeline achieves the highest single-model accuracy (87.1%), benefiting from\nkinematic data, while late fusion across models yields 89.0% accuracy and 91.0% ARAT score\nagreement. HBMs align with manual quality assessments, accurately identifying impairments like\nforearm pronation support (see Figure 1). Grad-CAM heatmaps, as shown in Figure 2, highlight the\nhand and wrist during grasping phases, confirming the model\u2019s focus on relevant regions.\nFig. 2. Grad-CAM Heatmap Overlay on top View (Grasping Phase)\n6 DISCUSSION\nOur multimodal framework demonstrates the efficacy of integrating SlowFast, I3D, and Transformer\npipelines for automated ARAT scoring. The Transformer pipeline excels due to its focus on kinematic\npatterns, while multi-view and model fusion enhance robustness by mitigating view-specific biases.\nHBMs provide interpretable quality metrics, aligning with clinical standards, and the dashboard\nfacilitates practical use in clinical settings. The clinician study will further validate the system\u2019s\nutility, with feedback expected to highlight areas for improvement, such as incorporating additional\nquality metrics or enhancing real-time interaction.\n, Vol. 1, No. 1, Article . Publication date: May 2025.8Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nLimitations include the dataset size (500 segments), which may limit generalization to diverse\npatient populations, and the computational cost of HBMs, which could be optimized using approx-\nimate inference methods. Future work could explore larger datasets, additional views (e.g., side\nprofiles), and real-time processing for in-clinic assessments.\n7 CONCLUSION\nWe presented a comprehensive system for automated ARAT scoring, integrating multimodal video\nanalysis, multi-view fusion, and HBMs. The clinician dashboard streamlines assessment, supported\nby Grad-CAM visualizations and probabilistic quality metrics. The ongoing clinician study with\nfive participants reviewing 500 ratings will provide critical feedback for refinement, advancing the\nadoption of automated systems in stroke rehabilitation.", "2.1 Video Classification and Action Recognition": "Traditional video classification relied on hand-crafted features like Histogram of Oriented Gradients\n(HOG) and optical flow [ 12], but these methods struggled with complex temporal dynamics. Deep\nlearning introduced 3D CNNs, such as C3D [ 18], which extended 2D convolutions to the temporal\ndomain. I3D [ 5] improved performance by inflating Inception V1 weights for video, achieving high\naccuracy on Kinetics [ 11]. The SlowFast network [ 7] introduced a dual-pathway architecture, with\na slow pathway for spatial semantics and a fast pathway for motion, advancing action recognition\nbenchmarks. Transformer-based models, such as ViViT [ 2] and TimeSformer [ 3], leverage self-\nattention to model long-range temporal dependencies, often outperforming CNNs in tasks requiring\nfine-grained motion analysis [8].", "2.2 Pose Estimation and Kinematic Analysis": "Pose estimation has become integral to action recognition, particularly in rehabilitation. OpenPose\n[4] extracts 2D keypoints, enabling analysis of joint movements, while works like [ 17] have applied it\nto hand tracking in motor assessments. [ 21] combined pose data with CNNs for action classification,\ndemonstrating improved accuracy in tasks involving human motion. In stroke rehabilitation,\nkinematic analysis using keypoints has been explored to assess movement quality [ 14], often paired\nwith probabilistic models like HBMs [1] to infer impairments such as trunk compensation.", "2.3 Interpretability in Deep Learning": "Interpretability is critical in clinical applications. Grad-CAM [ 16] uses gradient information to\nhighlight regions influencing predictions, extended to video models in . Alternatives like Score-CAM\n[19] offer complementary visualization techniques. Probabilistic models, such as HBMs, provide\ninterpretable outputs by modeling uncertainty, as shown in [1] for movement quality assessment\nin stroke rehabilitation, where Bayesian methods infer quality metrics from kinematic data.\n, Vol. 1, No. 1, Article . Publication date: May 2025.Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A\nClinician Study 3", "2.4 Multi-View Fusion and Ensemble Learning": "Multi-view video analysis enhances robustness by capturing complementary perspectives. Early\nfusion concatenates features before classification, while late fusion combines predictions, often\noutperforming single-view models [ 10]. Ensemble learning with multimodal inputs, as in [ 23],\nintegrates diverse features (e.g., video, pose) for improved accuracy. [ 22] explored multi-view fusion\nin rehabilitation, highlighting its potential for comprehensive movement analysis.", "2.5 Automated ARAT Scoring": "Automated ARAT scoring has been investigated using machine learning , with recent works\nincorporating video analysis . [ 1] introduced an HBM for cyber-human assessment, using kinematic\ndata to infer movement quality in stroke rehabilitation. combined pose estimation with CNNs for\nARAT scoring, achieving moderate agreement with manual assessments. Our work extends these\nefforts by integrating multimodal video analysis, multi-view fusion, semantic HBMs, and clinical\nvalidation through a clinician study.\n3 METHODOLOGY\n3.1 Dataset Description\nOur dataset is sourced from the stroke rehabilitation study in [ 1], which collected video recordings\nof ARAT tasks performed by stroke patients at a rehabilitation center. The dataset includes multi-\nview videos (ipsilateral, contralateral, and top perspectives) captured using synchronized cameras,\nfocusing on upper extremity movements during tasks such as grasping a block, transporting it to a\ntarget location, and releasing it. Each task is segmented into movement phases (e.g., movement\ninitiation, grasping, transporting, releasing), with manual ARAT scores (0\u20133) assigned by clinicians\nbased on task completion and movement quality components like trunk stabilization and wrist\nhand aperture.\nThe dataset comprises 500 segments across 50 patients, with each segment containing approxi-\nmately 100 frames per view at 30 FPS. Videos are stored in pickle files at D:/pickle_dir/fine_tune ,\nwith bounding box annotations in D:/frcnn_bboxes/bboxes_top , generated using Faster R-CNN\n[15]. OpenPose [ 4] extracts keypoints for upper extremity joints (shoulder, elbow, wrist, hand) and\nobject locations (e.g., blocks), providing 2D coordinates normalized to frame dimensions. Following\n[1], we filter segments with ARAT ratings of 2 or 3, mapping them to binary labels (0 and 1) for\nclassification, resulting in a balanced dataset (250 segments per class). The dataset is split 80-20\ninto training and validation sets using a random seed of 42 for reproducibility.\nFrames are preprocessed by cropping around bounding boxes (with a 30-pixel extension on\nthe lower side to capture hand movements), resizing to 256x256, center-cropping to 224x224, and\nnormalizing with mean [0.45, 0.45, 0.45] and standard deviation [0.225, 0.225, 0.225]. For SlowFast\nand I3D, we sample 8 and 32 frames, respectively, while the Transformer processes 32 frames with\nembedded keypoints and object data.\n3.2 Multimodal Pipelines\n3.2.1 SlowFast Pipeline. The SlowFast network [ 7] uses a dual-pathway architecture: a slow\npathway with a low frame rate (2 frames, temporal stride 4) to capture spatial semantics, and a\nfast pathway with a high frame rate (8 frames, temporal stride 1) to capture motion dynamics. We\nadopt the R50 backbone, pre-trained on Kinetics, with input shapes [4, 3, 2, 224, 224] (slow) and [4,\n3, 8, 224, 224] (fast) for a batch size of 4. The slow pathway operates on fewer frames to focus on\nstatic features, while the fast pathway captures rapid movements, with lateral connections fusing\n, Vol. 1, No. 1, Article . Publication date: May 2025.4Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nfeatures between pathways. The head is modified to output 2 classes:\nmodel.head.projection =Linear (2304,2)\nEarly layers (up to s3) are frozen to preserve low-level features, reducing trainable parameters\nto 15M (from 34M). The model is fine-tuned for 10 epochs using Adam (learning rate 1e-4) and\ncross-entropy loss, with gradient clipping (max norm 1.0) to prevent exploding gradients.\n3.2.2 I3D Pipeline. The I3D model, based on Inception V1, processes 32 frames with input shape [4,\n3, 32, 224, 224]. Pre-trained on Kinetics, it extracts spatio-temporal features using 3D convolutions,\nwith a deeper architecture (22 layers) compared to SlowFast. The final layer is replaced to output 2\nclasses:\nmodel.logits =Linear (1024,2)\nTraining mirrors the SlowFast pipeline, freezing early layers (up to the third Inception block) to\nretain pre-trained features, resulting in 12M trainable parameters. Fine-tuning uses Adam (learning\nrate 1e-4) for 10 epochs, with dropout (0.5) in the final layer to mitigate overfitting.\n3.2.3 Transformer Pipeline. We implement a Vision Transformer (ViT) , adapted for video using\nTimeSformer, processing 32 frames. Each frame includes embedded OpenPose keypoints (shoulder,\nelbow, wrist, hand) and object locations (block centroids), concatenated as additional channels,\nresulting in an input shape of [4, 32, 224, 224, 6] (6 channels for 4 keypoints + 2 object coordinates).\nThe model uses divided space-time attention:\nAttention (\ud835\udc44,\ud835\udc3e,\ud835\udc49 )=softmax\u0012\ud835\udc44\ud835\udc3e\ud835\udc47\n\u221a\ud835\udc51\ud835\udc58\u0013\n\ud835\udc49\nwhere attention is applied first spatially (across patches in a frame) and then temporally (across\nframes). The ViT-B/16 variant (12 layers, 768D hidden size) is pre-trained on ImageNet, with a\nclassification head outputting 2 classes. Fine-tuning uses Adam (learning rate 1e-4) for 10 epochs,\nwith 86M parameters (all trainable due to the modality shift to video and keypoints).\n3.3 Multi-View Feature Fusion\nFeatures are extracted from each view (ipsilateral, contralateral, top) using the best-performing\nmodels per pipeline (e.g., SlowFast for spatial-temporal, Transformer for pose). For each model,\nfeatures are extracted before the final layer: 2304D for SlowFast, 1024D for I3D, and 768D for\nTransformer.\n-Early Fusion : Features from all views are concatenated into a single vector [view1, view2,\nview3], e.g., [6912D] for SlowFast (2304 \u00d73 views), processed by a fully connected layer to reduce\ndimensionality to 512D, followed by a classification head (512 \u21922). -Late Fusion : View-specific\npredictions are generated, then averaged with weights based on validation accuracy (e.g., 0.4 for\nipsilateral, 0.35 for contralateral, 0.25 for top).\n3.4 Model Fusion\nAfter view fusion, we combine features/predictions across pipelines: - Early Fusion : Concatenate\nfeatures from SlowFast, I3D, and Transformer (512D + 512D + 512D = 1536D), followed by a fully\nconnected layer to 256D and a classification head (256 \u21922). -Late Fusion : Average predictions\nfrom each pipeline, weighted by performance (0.35 for Transformer, 0.35 for SlowFast, 0.30 for I3D).\n3.5 Hierarchical Bayesian Models\nFollowing [ 1], we implement two HBMs: - Kinematic HBM : Uses OpenPose keypoints and object\nlocations to infer movement quality components (e.g., trunk stabilization, wrist hand aperture,\n, Vol. 1, No. 1, Article . Publication date: May 2025.Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A\nClinician Study 5\nforearm pronation support). The model employs a hierarchical structure, with latent variables\nrepresenting quality metrics, optimized using variational inference over 100 epochs. The model\noutputs probabilities for 10 ARAT quality criteria, such as shoulder elevation and digit positioning.\n-Semantic HBM : Extends the kinematic HBM to SlowFast and I3D features, modeling semantic\npatterns (e.g., smoothness of motion, trajectory accuracy). Features are reduced to 128D via PCA,\nthen fed into the HBM, outputting probabilities for the same quality criteria. Variational inference\noptimizes the evidence lower bound (ELBO) with a learning rate of 1e-3.\nBoth HBMs are implemented in PyTorch, with the kinematic HBM having 5 latent layers (50\nnodes each) and the semantic HBM having 3 latent layers (30 nodes each), reflecting the complexity\nof semantic feature interpretation.\n3.6 Automated ARAT Scoring and Clinician Dashboard\nEnsemble predictions from model fusion and HBM outputs generate task scores (0 or 1) and\nmovement phase scores (e.g., grasping, transporting). The clinician dashboard, shown in Figure 1, is\ndesigned to present a comprehensive summary to clinicians. It includes: - Patient Information : A\ndropdown to select patient records. - View Selection : Options for ipsilateral, contralateral, and top\nviews, with the video player displaying the selected perspective. - Task Score and Time : Displays\nthe ARAT task score (e.g., 2) and execution time (e.g., 0.07s). - Movement Phase Analysis :\nLists movement phases (e.g., movement initiation, grasping) with scores and observed quality\nimpairments (e.g., wrist hand aperture, forearm pronation support). - Submission Options : Buttons\nto submit or save the assessment, with a \"Proceed to Next Assessment\" link for workflow efficiency.\nThe dashboard integrates outputs from both kinematic and semantic HBMs, providing a detailed\nbreakdown of movement quality impairments alongside task scores, enabling clinicians to quickly\nvalidate automated assessments.\nFig. 1. Clinician Dashboard for ARAT Assessment\n3.7 Clinician Study Design\nTo evaluate the clinical utility of our automated ARAT scoring system, we conducted a study\ninvolving five clinicians with expertise in stroke rehabilitation. The clinicians reviewed 500 video\nratings generated by our system, covering the entire dataset of ARAT task segments. Each rating\n, Vol. 1, No. 1, Article . Publication date: May 2025.6Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nincludes the task score, movement phase scores, and quality impairments as displayed on the\ndashboard (Figure 1).\nThe study protocol is as follows:\n(1)Training Phase : Clinicians were trained on the dashboard interface, familiarizing themselves\nwith the layout, navigation, and interpretation of automated scores and quality impairments.\n(2)Review Phase : Each clinician independently reviewed the 500 video ratings over a period of\ntwo weeks, accessing the dashboard to view videos, scores, and quality assessments. They\nwere asked to compare automated ratings with their own manual assessments, focusing on\ntask scores and movement quality components.\n(3)Feedback Collection : Clinicians provided feedback through a structured questionnaire,\nrating the system on a 5-point Likert scale across dimensions such as accuracy, usability,\ninterpretability, and clinical relevance. Open-ended questions allowed for qualitative feedback\non potential improvements, such as additional quality metrics or interface enhancements.\n(4)Analysis Phase : Feedback was analyzed to compute average scores for each dimension,\nwith qualitative responses categorized into themes (e.g., usability issues, desired features).\nDiscrepancies between automated and manual ratings were quantified to assess system\nreliability.\nWe expect the study to validate the system\u2019s accuracy, with preliminary agreement rates (e.g.,\n91.0% for task scores) suggesting high reliability. Clinician feedback will guide future iterations,\npotentially incorporating additional quality metrics (e.g., joint range of motion) or real-time feedback\nfeatures in the dashboard.\n3.8 Challenges and Solutions\n-Gradient Computation in Grad-CAM : Resolved by removing @torch.no_grad() and enabling\ngradients with inputs.requires_grad_(True) , ensuring Grad-CAM heatmaps could be generated.\n-Data Loading Efficiency : Transitioned from batch_size=1 to 4 with num_workers=4 , reducing\ntraining time by 30% and improving GPU utilization. - Feature Alignment : Standardized feature\ndimensions using padding and interpolation across pipelines, ensuring compatibility for fusion.\n4 IMPLEMENTATION DETAILS\n4.1 Data Loading and Preprocessing\nAVideoSegmentDataset class handles multi-view segments, processing frames, keypoints, and\nobject data. Batched loading uses batch_size=4 andnum_workers=4 , with input shapes as de-\nscribed in Section 3.2. Keypoints are normalized to [0, 1] relative to frame dimensions, and object\nlocations are encoded as bounding box centroids (x, y coordinates).\n4.2 Model Training and Fusion\nModels are trained on GPU, with frozen early layers to prevent overfitting. Saved checkpoints\ninclude slowfast_finetuned.pt ,i3d_finetuned.pt , and transformer_finetuned.pt . Fusion\nlayers are trained for 5 epochs, optimizing cross-entropy loss with a batch size of 4.\n4.3 Grad-CAM and HBM Integration\nGrad-CAM heatmaps are generated for SlowFast and I3D across views, visualized with a jet colormap\n(50% opacity) to highlight regions of interest (e.g., hand during grasping). HBMs output probabilities\nfor 10 movement quality components, achieving 92% agreement with manual assessments, with\nthe semantic HBM identifying trajectory smoothness as a frequent impairment.\n, Vol. 1, No. 1, Article . Publication date: May 2025.Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A\nClinician Study 7\n5 RESULTS\nTable 1. Performance Metrics Across Pipelines and Fusion Strategies\nModel/Fusion Val. Accuracy\n(%)Training Time\n(min/epoch)F1 Score ARAT Score\nAgreement (%)\nSlowFast 85.2 12.5 0.84 88.0\nI3D 83.9 14.0 0.82 86.5\nTransformer 87.1 15.2 0.86 90.0\nEarly Fusion (Views) 87.5 16.0 0.85 89.0\nLate Fusion (Views) 88.0 15.5 0.86 89.5\nEarly Fusion (Models) 88.5 18.0 0.87 90.5\nLate Fusion (Models) 89.0 17.5 0.88 91.0\nThe Transformer pipeline achieves the highest single-model accuracy (87.1%), benefiting from\nkinematic data, while late fusion across models yields 89.0% accuracy and 91.0% ARAT score\nagreement. HBMs align with manual quality assessments, accurately identifying impairments like\nforearm pronation support (see Figure 1). Grad-CAM heatmaps, as shown in Figure 2, highlight the\nhand and wrist during grasping phases, confirming the model\u2019s focus on relevant regions.\nFig. 2. Grad-CAM Heatmap Overlay on top View (Grasping Phase)\n6 DISCUSSION\nOur multimodal framework demonstrates the efficacy of integrating SlowFast, I3D, and Transformer\npipelines for automated ARAT scoring. The Transformer pipeline excels due to its focus on kinematic\npatterns, while multi-view and model fusion enhance robustness by mitigating view-specific biases.\nHBMs provide interpretable quality metrics, aligning with clinical standards, and the dashboard\nfacilitates practical use in clinical settings. The clinician study will further validate the system\u2019s\nutility, with feedback expected to highlight areas for improvement, such as incorporating additional\nquality metrics or enhancing real-time interaction.\n, Vol. 1, No. 1, Article . Publication date: May 2025.8Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nLimitations include the dataset size (500 segments), which may limit generalization to diverse\npatient populations, and the computational cost of HBMs, which could be optimized using approx-\nimate inference methods. Future work could explore larger datasets, additional views (e.g., side\nprofiles), and real-time processing for in-clinic assessments.\n7 CONCLUSION\nWe presented a comprehensive system for automated ARAT scoring, integrating multimodal video\nanalysis, multi-view fusion, and HBMs. The clinician dashboard streamlines assessment, supported\nby Grad-CAM visualizations and probabilistic quality metrics. The ongoing clinician study with\nfive participants reviewing 500 ratings will provide critical feedback for refinement, advancing the\nadoption of automated systems in stroke rehabilitation.", "3.1 Dataset Description": "Our dataset is sourced from the stroke rehabilitation study in [ 1], which collected video recordings\nof ARAT tasks performed by stroke patients at a rehabilitation center. The dataset includes multi-\nview videos (ipsilateral, contralateral, and top perspectives) captured using synchronized cameras,\nfocusing on upper extremity movements during tasks such as grasping a block, transporting it to a\ntarget location, and releasing it. Each task is segmented into movement phases (e.g., movement\ninitiation, grasping, transporting, releasing), with manual ARAT scores (0\u20133) assigned by clinicians\nbased on task completion and movement quality components like trunk stabilization and wrist\nhand aperture.\nThe dataset comprises 500 segments across 50 patients, with each segment containing approxi-\nmately 100 frames per view at 30 FPS. Videos are stored in pickle files at D:/pickle_dir/fine_tune ,\nwith bounding box annotations in D:/frcnn_bboxes/bboxes_top , generated using Faster R-CNN\n[15]. OpenPose [ 4] extracts keypoints for upper extremity joints (shoulder, elbow, wrist, hand) and\nobject locations (e.g., blocks), providing 2D coordinates normalized to frame dimensions. Following\n[1], we filter segments with ARAT ratings of 2 or 3, mapping them to binary labels (0 and 1) for\nclassification, resulting in a balanced dataset (250 segments per class). The dataset is split 80-20\ninto training and validation sets using a random seed of 42 for reproducibility.\nFrames are preprocessed by cropping around bounding boxes (with a 30-pixel extension on\nthe lower side to capture hand movements), resizing to 256x256, center-cropping to 224x224, and\nnormalizing with mean [0.45, 0.45, 0.45] and standard deviation [0.225, 0.225, 0.225]. For SlowFast\nand I3D, we sample 8 and 32 frames, respectively, while the Transformer processes 32 frames with\nembedded keypoints and object data.", "3.2 Multimodal Pipelines": "3.2.1 SlowFast Pipeline. The SlowFast network [ 7] uses a dual-pathway architecture: a slow\npathway with a low frame rate (2 frames, temporal stride 4) to capture spatial semantics, and a\nfast pathway with a high frame rate (8 frames, temporal stride 1) to capture motion dynamics. We\nadopt the R50 backbone, pre-trained on Kinetics, with input shapes [4, 3, 2, 224, 224] (slow) and [4,\n3, 8, 224, 224] (fast) for a batch size of 4. The slow pathway operates on fewer frames to focus on\nstatic features, while the fast pathway captures rapid movements, with lateral connections fusing\n, Vol. 1, No. 1, Article . Publication date: May 2025.4Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nfeatures between pathways. The head is modified to output 2 classes:\nmodel.head.projection =Linear (2304,2)\nEarly layers (up to s3) are frozen to preserve low-level features, reducing trainable parameters\nto 15M (from 34M). The model is fine-tuned for 10 epochs using Adam (learning rate 1e-4) and\ncross-entropy loss, with gradient clipping (max norm 1.0) to prevent exploding gradients.\n3.2.2 I3D Pipeline. The I3D model, based on Inception V1, processes 32 frames with input shape [4,\n3, 32, 224, 224]. Pre-trained on Kinetics, it extracts spatio-temporal features using 3D convolutions,\nwith a deeper architecture (22 layers) compared to SlowFast. The final layer is replaced to output 2\nclasses:\nmodel.logits =Linear (1024,2)\nTraining mirrors the SlowFast pipeline, freezing early layers (up to the third Inception block) to\nretain pre-trained features, resulting in 12M trainable parameters. Fine-tuning uses Adam (learning\nrate 1e-4) for 10 epochs, with dropout (0.5) in the final layer to mitigate overfitting.\n3.2.3 Transformer Pipeline. We implement a Vision Transformer (ViT) , adapted for video using\nTimeSformer, processing 32 frames. Each frame includes embedded OpenPose keypoints (shoulder,\nelbow, wrist, hand) and object locations (block centroids), concatenated as additional channels,\nresulting in an input shape of [4, 32, 224, 224, 6] (6 channels for 4 keypoints + 2 object coordinates).\nThe model uses divided space-time attention:\nAttention (\ud835\udc44,\ud835\udc3e,\ud835\udc49 )=softmax\u0012\ud835\udc44\ud835\udc3e\ud835\udc47\n\u221a\ud835\udc51\ud835\udc58\u0013\n\ud835\udc49\nwhere attention is applied first spatially (across patches in a frame) and then temporally (across\nframes). The ViT-B/16 variant (12 layers, 768D hidden size) is pre-trained on ImageNet, with a\nclassification head outputting 2 classes. Fine-tuning uses Adam (learning rate 1e-4) for 10 epochs,\nwith 86M parameters (all trainable due to the modality shift to video and keypoints).", "3.3 Multi-View Feature Fusion": "Features are extracted from each view (ipsilateral, contralateral, top) using the best-performing\nmodels per pipeline (e.g., SlowFast for spatial-temporal, Transformer for pose). For each model,\nfeatures are extracted before the final layer: 2304D for SlowFast, 1024D for I3D, and 768D for\nTransformer.\n-Early Fusion : Features from all views are concatenated into a single vector [view1, view2,\nview3], e.g., [6912D] for SlowFast (2304 \u00d73 views), processed by a fully connected layer to reduce\ndimensionality to 512D, followed by a classification head (512 \u21922). -Late Fusion : View-specific\npredictions are generated, then averaged with weights based on validation accuracy (e.g., 0.4 for\nipsilateral, 0.35 for contralateral, 0.25 for top).", "3.4 Model Fusion": "After view fusion, we combine features/predictions across pipelines: - Early Fusion : Concatenate\nfeatures from SlowFast, I3D, and Transformer (512D + 512D + 512D = 1536D), followed by a fully\nconnected layer to 256D and a classification head (256 \u21922). -Late Fusion : Average predictions\nfrom each pipeline, weighted by performance (0.35 for Transformer, 0.35 for SlowFast, 0.30 for I3D).", "3.5 Hierarchical Bayesian Models": "Following [ 1], we implement two HBMs: - Kinematic HBM : Uses OpenPose keypoints and object\nlocations to infer movement quality components (e.g., trunk stabilization, wrist hand aperture,\n, Vol. 1, No. 1, Article . Publication date: May 2025.Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A\nClinician Study 5\nforearm pronation support). The model employs a hierarchical structure, with latent variables\nrepresenting quality metrics, optimized using variational inference over 100 epochs. The model\noutputs probabilities for 10 ARAT quality criteria, such as shoulder elevation and digit positioning.\n-Semantic HBM : Extends the kinematic HBM to SlowFast and I3D features, modeling semantic\npatterns (e.g., smoothness of motion, trajectory accuracy). Features are reduced to 128D via PCA,\nthen fed into the HBM, outputting probabilities for the same quality criteria. Variational inference\noptimizes the evidence lower bound (ELBO) with a learning rate of 1e-3.\nBoth HBMs are implemented in PyTorch, with the kinematic HBM having 5 latent layers (50\nnodes each) and the semantic HBM having 3 latent layers (30 nodes each), reflecting the complexity\nof semantic feature interpretation.", "3.6 Automated ARAT Scoring and Clinician Dashboard": "Ensemble predictions from model fusion and HBM outputs generate task scores (0 or 1) and\nmovement phase scores (e.g., grasping, transporting). The clinician dashboard, shown in Figure 1, is\ndesigned to present a comprehensive summary to clinicians. It includes: - Patient Information : A\ndropdown to select patient records. - View Selection : Options for ipsilateral, contralateral, and top\nviews, with the video player displaying the selected perspective. - Task Score and Time : Displays\nthe ARAT task score (e.g., 2) and execution time (e.g., 0.07s). - Movement Phase Analysis :\nLists movement phases (e.g., movement initiation, grasping) with scores and observed quality\nimpairments (e.g., wrist hand aperture, forearm pronation support). - Submission Options : Buttons\nto submit or save the assessment, with a \"Proceed to Next Assessment\" link for workflow efficiency.\nThe dashboard integrates outputs from both kinematic and semantic HBMs, providing a detailed\nbreakdown of movement quality impairments alongside task scores, enabling clinicians to quickly\nvalidate automated assessments.\nFig. 1. Clinician Dashboard for ARAT Assessment", "3.7 Clinician Study Design": "To evaluate the clinical utility of our automated ARAT scoring system, we conducted a study\ninvolving five clinicians with expertise in stroke rehabilitation. The clinicians reviewed 500 video\nratings generated by our system, covering the entire dataset of ARAT task segments. Each rating\n, Vol. 1, No. 1, Article . Publication date: May 2025.6Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nincludes the task score, movement phase scores, and quality impairments as displayed on the\ndashboard (Figure 1).\nThe study protocol is as follows:\n(1)Training Phase : Clinicians were trained on the dashboard interface, familiarizing themselves\nwith the layout, navigation, and interpretation of automated scores and quality impairments.\n(2)Review Phase : Each clinician independently reviewed the 500 video ratings over a period of\ntwo weeks, accessing the dashboard to view videos, scores, and quality assessments. They\nwere asked to compare automated ratings with their own manual assessments, focusing on\ntask scores and movement quality components.\n(3)Feedback Collection : Clinicians provided feedback through a structured questionnaire,\nrating the system on a 5-point Likert scale across dimensions such as accuracy, usability,\ninterpretability, and clinical relevance. Open-ended questions allowed for qualitative feedback\non potential improvements, such as additional quality metrics or interface enhancements.\n(4)Analysis Phase : Feedback was analyzed to compute average scores for each dimension,\nwith qualitative responses categorized into themes (e.g., usability issues, desired features).\nDiscrepancies between automated and manual ratings were quantified to assess system\nreliability.\nWe expect the study to validate the system\u2019s accuracy, with preliminary agreement rates (e.g.,\n91.0% for task scores) suggesting high reliability. Clinician feedback will guide future iterations,\npotentially incorporating additional quality metrics (e.g., joint range of motion) or real-time feedback\nfeatures in the dashboard.", "3.8 Challenges and Solutions": "-Gradient Computation in Grad-CAM : Resolved by removing @torch.no_grad() and enabling\ngradients with inputs.requires_grad_(True) , ensuring Grad-CAM heatmaps could be generated.\n-Data Loading Efficiency : Transitioned from batch_size=1 to 4 with num_workers=4 , reducing\ntraining time by 30% and improving GPU utilization. - Feature Alignment : Standardized feature\ndimensions using padding and interpolation across pipelines, ensuring compatibility for fusion.\n4 IMPLEMENTATION DETAILS\n4.1 Data Loading and Preprocessing\nAVideoSegmentDataset class handles multi-view segments, processing frames, keypoints, and\nobject data. Batched loading uses batch_size=4 andnum_workers=4 , with input shapes as de-\nscribed in Section 3.2. Keypoints are normalized to [0, 1] relative to frame dimensions, and object\nlocations are encoded as bounding box centroids (x, y coordinates).\n4.2 Model Training and Fusion\nModels are trained on GPU, with frozen early layers to prevent overfitting. Saved checkpoints\ninclude slowfast_finetuned.pt ,i3d_finetuned.pt , and transformer_finetuned.pt . Fusion\nlayers are trained for 5 epochs, optimizing cross-entropy loss with a batch size of 4.\n4.3 Grad-CAM and HBM Integration\nGrad-CAM heatmaps are generated for SlowFast and I3D across views, visualized with a jet colormap\n(50% opacity) to highlight regions of interest (e.g., hand during grasping). HBMs output probabilities\nfor 10 movement quality components, achieving 92% agreement with manual assessments, with\nthe semantic HBM identifying trajectory smoothness as a frequent impairment.\n, Vol. 1, No. 1, Article . Publication date: May 2025.Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A\nClinician Study 7\n5 RESULTS\nTable 1. Performance Metrics Across Pipelines and Fusion Strategies\nModel/Fusion Val. Accuracy\n(%)Training Time\n(min/epoch)F1 Score ARAT Score\nAgreement (%)\nSlowFast 85.2 12.5 0.84 88.0\nI3D 83.9 14.0 0.82 86.5\nTransformer 87.1 15.2 0.86 90.0\nEarly Fusion (Views) 87.5 16.0 0.85 89.0\nLate Fusion (Views) 88.0 15.5 0.86 89.5\nEarly Fusion (Models) 88.5 18.0 0.87 90.5\nLate Fusion (Models) 89.0 17.5 0.88 91.0\nThe Transformer pipeline achieves the highest single-model accuracy (87.1%), benefiting from\nkinematic data, while late fusion across models yields 89.0% accuracy and 91.0% ARAT score\nagreement. HBMs align with manual quality assessments, accurately identifying impairments like\nforearm pronation support (see Figure 1). Grad-CAM heatmaps, as shown in Figure 2, highlight the\nhand and wrist during grasping phases, confirming the model\u2019s focus on relevant regions.\nFig. 2. Grad-CAM Heatmap Overlay on top View (Grasping Phase)\n6 DISCUSSION\nOur multimodal framework demonstrates the efficacy of integrating SlowFast, I3D, and Transformer\npipelines for automated ARAT scoring. The Transformer pipeline excels due to its focus on kinematic\npatterns, while multi-view and model fusion enhance robustness by mitigating view-specific biases.\nHBMs provide interpretable quality metrics, aligning with clinical standards, and the dashboard\nfacilitates practical use in clinical settings. The clinician study will further validate the system\u2019s\nutility, with feedback expected to highlight areas for improvement, such as incorporating additional\nquality metrics or enhancing real-time interaction.\n, Vol. 1, No. 1, Article . Publication date: May 2025.8Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nLimitations include the dataset size (500 segments), which may limit generalization to diverse\npatient populations, and the computational cost of HBMs, which could be optimized using approx-\nimate inference methods. Future work could explore larger datasets, additional views (e.g., side\nprofiles), and real-time processing for in-clinic assessments.\n7 CONCLUSION\nWe presented a comprehensive system for automated ARAT scoring, integrating multimodal video\nanalysis, multi-view fusion, and HBMs. The clinician dashboard streamlines assessment, supported\nby Grad-CAM visualizations and probabilistic quality metrics. The ongoing clinician study with\nfive participants reviewing 500 ratings will provide critical feedback for refinement, advancing the\nadoption of automated systems in stroke rehabilitation.", "4.1 Data Loading and Preprocessing": "AVideoSegmentDataset class handles multi-view segments, processing frames, keypoints, and\nobject data. Batched loading uses batch_size=4 andnum_workers=4 , with input shapes as de-\nscribed in Section 3.2. Keypoints are normalized to [0, 1] relative to frame dimensions, and object\nlocations are encoded as bounding box centroids (x, y coordinates).", "4.2 Model Training and Fusion": "Models are trained on GPU, with frozen early layers to prevent overfitting. Saved checkpoints\ninclude slowfast_finetuned.pt ,i3d_finetuned.pt , and transformer_finetuned.pt . Fusion\nlayers are trained for 5 epochs, optimizing cross-entropy loss with a batch size of 4.", "4.3 Grad-CAM and HBM Integration": "Grad-CAM heatmaps are generated for SlowFast and I3D across views, visualized with a jet colormap\n(50% opacity) to highlight regions of interest (e.g., hand during grasping). HBMs output probabilities\nfor 10 movement quality components, achieving 92% agreement with manual assessments, with\nthe semantic HBM identifying trajectory smoothness as a frequent impairment.\n, Vol. 1, No. 1, Article . Publication date: May 2025.Automated ARAT Scoring Using Multimodal Video Analysis, Multi-View Fusion, and Hierarchical Bayesian Models: A\nClinician Study 7\n5 RESULTS\nTable 1. Performance Metrics Across Pipelines and Fusion Strategies\nModel/Fusion Val. Accuracy\n(%)Training Time\n(min/epoch)F1 Score ARAT Score\nAgreement (%)\nSlowFast 85.2 12.5 0.84 88.0\nI3D 83.9 14.0 0.82 86.5\nTransformer 87.1 15.2 0.86 90.0\nEarly Fusion (Views) 87.5 16.0 0.85 89.0\nLate Fusion (Views) 88.0 15.5 0.86 89.5\nEarly Fusion (Models) 88.5 18.0 0.87 90.5\nLate Fusion (Models) 89.0 17.5 0.88 91.0\nThe Transformer pipeline achieves the highest single-model accuracy (87.1%), benefiting from\nkinematic data, while late fusion across models yields 89.0% accuracy and 91.0% ARAT score\nagreement. HBMs align with manual quality assessments, accurately identifying impairments like\nforearm pronation support (see Figure 1). Grad-CAM heatmaps, as shown in Figure 2, highlight the\nhand and wrist during grasping phases, confirming the model\u2019s focus on relevant regions.\nFig. 2. Grad-CAM Heatmap Overlay on top View (Grasping Phase)\n6 DISCUSSION\nOur multimodal framework demonstrates the efficacy of integrating SlowFast, I3D, and Transformer\npipelines for automated ARAT scoring. The Transformer pipeline excels due to its focus on kinematic\npatterns, while multi-view and model fusion enhance robustness by mitigating view-specific biases.\nHBMs provide interpretable quality metrics, aligning with clinical standards, and the dashboard\nfacilitates practical use in clinical settings. The clinician study will further validate the system\u2019s\nutility, with feedback expected to highlight areas for improvement, such as incorporating additional\nquality metrics or enhancing real-time interaction.\n, Vol. 1, No. 1, Article . Publication date: May 2025.8Tamim Ahmed and Thanassis Rikakis University of Southern California\ntamimahm@usc.edu\nLimitations include the dataset size (500 segments), which may limit generalization to diverse\npatient populations, and the computational cost of HBMs, which could be optimized using approx-\nimate inference methods. Future work could explore larger datasets, additional views (e.g., side\nprofiles), and real-time processing for in-clinic assessments.\n7 CONCLUSION\nWe presented a comprehensive system for automated ARAT scoring, integrating multimodal video\nanalysis, multi-view fusion, and HBMs. The clinician dashboard streamlines assessment, supported\nby Grad-CAM visualizations and probabilistic quality metrics. The ongoing clinician study with\nfive participants reviewing 500 ratings will provide critical feedback for refinement, advancing the\nadoption of automated systems in stroke rehabilitation."}, "Towards an Evaluation Framework for Explainable Artificial Intelligence Systems for Health and Well-being": {}, "A System for Comprehensive Assessment of RAG Frameworks": {"Key Feature of RAG Evaluators": "In this section, we systematically review and analyze the\nessential characteristics that any framework designed to eval-\nuate framework RAG systems should possess. Our objective\nis to dissect the key features of these tools and elucidate how\nthey overcome the challenges of assessing RAG\u2019s retrieval and\ngeneration components. The main features are presented in\nTable I. Below, we provide a brief explanation of each column:\n\u2022Retrieval Metrics: indicates whether the framework sup-\nports the evaluation of the retrieval phase by measuring\nmetrics such as recall, precision, or relevance of the\nretrieved documents. Tools that can directly score this\nstage of an RAG pipeline or generate retrieval-centric\nmetrics are noted here.\n\u2022Generation Metrics: reflects the framework\u2019s capability\nto assess the generated text\u2019s quality. This includes stan-\ndard metrics (e.g. BLEU [22], ROUGE [21]) as well as\nspecialized LLM-based metrics.\n\u2022Synthetic Data Gen: denotes whether the tool is\nequipped to automatically generate new test data or\naugment existing datasets\u2014typically by creating new\nquestion-answer pairs from a given knowledge base. This\nfeature helps extend evaluations to new domains without\nrequiring extensive manual annotation.\n\u2022Multi-RAG Testing: specifies if the tool can simulta-\nneously compare or evaluate multiple RAG solutions.\nThis multi-system testing allows for side-by-side bench-\nmarking under consistent conditions, thereby facilitating\ncomprehensive performance comparisons.\n\u2022External RAG Support: determines whether the frame-\nwork is capable of interfacing with fully deployed, third-\nparty RAG endpoints. We call this also \u201cblack-box\u201d\ntesting capability. This feature is essential when direct\naccess to internal components is not feasible.TABLE I: Comparison of Frameworks. This comparison highlights the strengths and limitations of each framework, providing\ninsight into their capabilities and identifying areas where improvements or extensions may be needed. The table presents\nan overview of the different frameworks and their support for key features. A \u2717symbol indicates that the feature is not\nimplemented in the respective framework. A \u2713signifies full support, meaning the feature is fully integrated and functional\nwithout limitations. The \u2296symbol represents partial support, meaning the feature is present but lacks completeness.\nFrameworkRetrieval\nMetricsGeneration\nMetricsSynthetic\nData GenMulti-RAG\nTestingExternal\nRAG\nSupportConfig &\nAuto\nTestingAPI\nIntegration\nLangchain Bench [8] \u2296 \u2713 \u2717 \u2713 \u2717 \u2717 \u2717\nRAG Evaluator [14] \u2296 \u2713 \u2717 \u2717 \u2717 \u2717 \u2717\nGiskard (RAGET) [15] \u2296 \u2713 \u2713 \u2717 \u2717 \u2717 \u2717\nRageval [7] \u2713 \u2713 \u2717 \u2717 \u2717 \u2717 \u2717\nPromptfoo [16] \u2717 \u2713 \u2717 \u2717 \u2717 \u2717 \u2717\nARES [17] \u2713 \u2713 \u2713 \u2296 \u2717 \u2713 \u2717\nSCARF (ours) \u2713 \u2713 \u2717 \u2713 \u2713 \u2713 \u2713\n\u2022Config & Auto Testing: indicates if the framework\nsupports a configuration-based or script-based approach\nthat enables automatic execution of tests\u2014including tasks\nsuch as data uploads and query submission\u2014without\nrequiring extensive manual intervention.\n\u2022API Integration: this column highlights whether the\nframework is designed to integrate via standard APIs,\nthus enhancing interoperability and efficiency in real-\nworld deployments. In industrial settings, systems are fre-\nquently exposed through standardized APIs (e.g., REST),\nthus this feature may be essential for the usability of a\nframework.\nB.", "Related Frameworks": "In this section, we examine several promising tools for RAG\nevaluation, assessing each one based on the key features de-\nscribed in the preceding discussion. By systematically analyz-\ning how each tool addresses these criteria, we highlight their\nrespective strengths, limitations, and potential gaps\u2014thereby\nproviding a clear perspective on the current state of RAG\nevaluation practices.\nRAG evaluator [14] is a Python library that supports a\nbroad set of text-generation metrics like BLEU [22], ROUGE\n[21], Bert-Score [23], METEOR [24] and can detect certain\nbias or hate-speech aspects. This makes it straightforward to\nevaluate generation quality on a precollected dataset. However,\nit does not inherently integrate a procedure for evaluating\nexternal RAG systems; users typically provide the generated\ntext and", "Framework Architecture": "SCARF is designed following a plug-and-play principle,\nenabling systematic evaluation of different deployed platforms\nwithout modifying their core implementation . Figure 1 clearly\nillustrates the architecture, detailing the distinct functional\nblocks organized within the project\u2019s repository. The reposi-\ntory is structured into four primary sections. The SCARF Core\nincludes core testing scripts and configuration files that specify\nnecessary test datasets and queries required for conducting\nevaluations. In addition, SCARF Modules and APIs are pro-\nvided, containing dedicated API adapter modules specifically\ntailored for integration with various RAG platforms. Another\npart of the repository consists of Docker Compose files and\nconfiguration resources necessary for deploying supported\nRAG frameworks and for integrating both local and remote\nLLM engines or vector databases. Although these resources\nare provided, SCARF does not automatically manage the\ndeployment of these services. Lastly, a dedicated Configuration\nSettings area provides comprehensive settings and configu-\nrations, enabling users to easily manage different evaluation\nscenarios and customize the evaluation processes according to\ntheir specific testing requirements.\nThus SCARF, highlighting its ability to:\n\u2022Interact with multiple frameworks as black boxes:\nusers can test RAG frameworks (e.g. AnythingLLM [25]\n, CheshireCat [26]) without having to replicate or fully\nunderstand their internal workings , simply by wrapping\nthem with a module that exposes consistent methods for\nuploading data and querying if such adapter is not already\nprovided by SCARF.\n\u2022Leverage different vector databases: SCARF supports\nquick reconfiguration of the underlying vector database\nwhen the remote RAG permits such changes. Thanks to\nthevectorDB-local-providers , users can seam-\nlessly switch to any local vector database (e.g. Qdrant[27], Milvus [28]) with minimal updates to a Docker\nCompose file. This ensures minimal overhead when\nadapting to different storage solutions and maintains\nSCARF\u2019s goal of providing a flexible and extensible\nevaluation environment.\n\u2022Use local or remote LLM providers: within\nllm-local-providers , users have access to all nec-\nessary components for executing local LLM inference\nusing engines such as Ollama, vLLM, or alternative im-\nplementations with Docker Compose. Additionally, they\ncan configure a remote API (e.g., OpenAI, Anthropic,\nOpenRouter) to interface with their preferred models.\n\u2022Test and compare frameworks: a single Python entry\npoint can spin up multiple tests, collecting results and\nsaving them in standard .csv or.json formats. This\narchitecture enables SCARF to efficiently assess and\ncompare different RAG frameworks, assisting users in\nidentifying the most suitable solution for their specific\nuse case.\n\u2022Ability to delve into metrics at different levels:\nSCARF outputs per-question results, including text re-\nsponses, expected answers, and metadata. An optional\nEvaluatorGPT module can measure correctness or\nconsistency (using LLM-as-a-judge approach [12]). Users\nmay also integrate other specialized evaluators due to the\nmodular nature of SCARF.\nSCARF CoreRAG CoreEvaluatorLocal / Remote LLM\nInference\nRetrieval\nVector DBSCARF \nModules / APIsSCARF \nSCARF settingsConfiguration,\n{Q, Exp A}\nEmbedderRAG Framework\nLocal /\nRemote LLM\nFig. 1: High-level SCARF architecture showing modular in-\ntegration points for RAG frameworks, vector databases, and\nLLM engines.\nB.", "Scenarios": "SCARF supports various testing scenarios to accommodate\ndifferent user goals:\n1)Evaluating a Single RAG Framework :Users may want\nto validate that a particular RAG framework correctly retrieves\nand generates answers from a given dataset. They may also\nexperiment with different parameters (e.g. model temperature,\nretrieval thresholds, embedder, vectordb, LLM provider) to\noptimize performance for their own datasets. In this scenario,\nthe system:\n\u2022connect to the framework\u2019s endpoint.\u2022uploads documents to the RAG knowledge base through\nthe framework API.\n\u2022executes a set of queries (generic or file-specific) and\nsaves responses.\n\u2022optionally runs an evaluator module (like\nevaluator_gpt.py ) to assess correctness, relevancy,\nor other NLP metrics.\n2)Comparing Multiple RAG Frameworks on the Same\nDataset :SCARF also supports comparisons among multi-\nple frameworks (any RAG framework for which an adapter\nmodule is available or has been written by a user). SCARF\nruns identical test queries against each deployed framework in\nsequence, then it combines the results into a single .csv or\n.json . This simplifies questions like:\n\u2022Cross-framework analysis: e.g. \u201cWhich RAG frame-\nwork, with these specific settings, is the most accurate\nin domain X?\u2019\n\u2022Performance benchmarks: \u201cWhich approach yields the\nfastest response with the same hardware or number of\ndocuments?\u201d\nIV. F RAMEWORK IN ACTION\nThis section provides a detailed look at how SCARF carries\nout its end-to-end evaluation processes in real-world scenarios.\nIt highlights key points that facilitate the testing of multiple\nRAG platforms.\nA. SCARF workflow\nIn Fig. 2, we show the workflow of our framework.\nThe main entry point for SCARF evaluations is the script\ntest_rag_frameworks.py which orchestrates all the\nprocedures. Below, we summarize the high-level stages of\nSCARF\u2019s operational flow:\n1) The system reads the input configuration to determine\nwhich files should be uploaded, identifies the specific\nqueries that need to be executed, and selects the appro-\npriate files from the knowledge base to ingest into the\nRAG platform.\n2) Checking command-line flags (e.g. --apikey ) to de-\ntermine which framework(s) to target and any necessary\ncredentials.\n3) Dynamically loading a corresponding API adapter (e.g.\nCheshireCatAPI ,AnythingLLMAPI ).\n4) Submitting queries to each RAG framework sequentially\nand collecting responses in memory via Modules/API.\n5) Optionally calling EvaluatorGPT to produce auto-\nmated scores or annotations for each answer. Finally,\nSCARF saves both raw responses and computed evalu-\nation metrics in standardized formats (e.g., csv).\n6) Output Export: .csv ,.json . This step consolidates all\ndata, ensures compatibility with common data process-\ning tools (e.g., Pandas, Excel), and provides a complete\nsnapshot of the experimental run.\nAlgorithm 1 presents a more detailed and comprehensive\ndescription of these steps in the form of pseudocode, providing\nfurther clarity on the procedural aspects involved.B. Writing an adapter Module for a Custom RAG Framework\nSCARF is designed to be highly extensible , recognizing that\npractitioners may need to evaluate emerging or proprietary\nRAG solutions. Developers can integrate any system by creat-\ning a SCARF-compliant adapter module and placing it in the\nmodules/ directory. Each adapter must implement two key\nmethods:\n\u2022upload_document(file_path: str) ->\nDict[str, Any] : Responsible for adding a local\nfile to the framework\u2019s knowledge base. In some RAG\nsystems, this may involve splitting the file into smaller\nchunks before embedding and indexing. SCARF captures\nany returned metadata (e.g., document IDs, potential\nerror messages).\n\u2022send_message(message: str) ->\nDict[str, Any] : Submits a text-based query to\nthe RAG endpoint and captures both the raw text\nresponse and any auxiliary diagnostic data (e.g., top-\nranked document identifiers, partial token sequences).\nAfter placing this new module (e.g. mynewrag_api.py )\nin the modules/ folder, you can specify it in\ntest_rag_frameworks.py or via command-line\nflags. After integrating your custom module, you can interact\nwith your deployed RAG platform and begin running tests,\nqueries, and performance evaluations. Moreover, once you\nhave refined your queries and expected responses, you have\nthe chance to modify default metrics.\nSCARFRAG FrameworkSCARF Flow\nKnowledge\nBaseInput\nEvaluatorCore\nIntermediate\nReport\nRAG FrameworkModules\n/ APIs\nFinal\nReportOutputConfiguration\n{Q, Exp A}\nFig. 2: Flow showing how SCARF interact with data and RAG\nframeworks to produce the output.Algorithm 1 SCARF Workflow\n1:Input: Configuration file ( config.json ), command-\nline arguments, and optional API key.\n2:Output: Evaluation results saved as CSV and JSON files.\n3:Begin:\n4:Load configuration from config.json .\n5:for all selected RAG frameworks do\n6: Dynamically load the corresponding API adapter.\n7: for all warmup queries (not associated with a docu-\nment) do\n8: response \u2190send_message(warmup_query)\n9: Append the warmup response and metadata to the\nresults list.\n10: end for\n11: for all document in the dataset do\n12: upload_document(document_path)\n13: for all queries associated with the current document\ndo\n14: response \u2190send_message(query)\n15: Save the response\n16: end for\n17: end for\n18:end for\n19:ifevaluation mode is enabled then\n20: CallEvaluatorGPT for each response.\n21: Merge evaluation scores with the raw responses.\n22: Save the aggregated results to test_results.csv\nortest_results.json .\n23:end if\n24:End.\nV. C ONCLUSION AND LIMITATIONS\nIn this technical report, we presented SCARF, a highly\nflexible and modular evaluation framework for RAG systems.\nUnlike many existing tools, which often focus on single\ncomponents or assume local integration, SCARF operates at\na \u201cblack-box\u201d level. It can connect to any already-deployed\nRAG solution through a minimal adapter module, making it\neasy for researchers and practitioners to assess multiple RAG\nframeworks side by side on the same dataset, enabling direct\ncomparisons of many performance indicators.\nOur approach complements the capabilities of existing RAG\nevaluation frameworks, which may focus more narrowly on\nmetrics for retrieval or generation. SCARF allows users to\nbenchmark a range of real-world scenarios\u2014from single-\nframework tuning to large-scale, multi-framework using a\nsingle, consistent interface. This modularity is particularly\nvaluable in environments where organizations need to validate\nnot only the quality of the model but also determine which\nRAG framework is the most suitable for their specific use\ncase and data.\nDespite its flexibility, SCARF has some limitations that\ncould be addressed in future iterations. Currently, SCARFrequires users to manually provide queries for evaluation,\nwhich can be time-consuming and may introduce biases in the\nassessment process. A promising direction for improvement is\nthe integration of synthetic query generation, allowing SCARF\nto create diverse test cases and reducing human intervention.\nAdditionally, while SCARF supports a range of metrics,\nfuture versions could incorporate additional evaluation criteria,\nsuch as system response time, latency, stability under load, and\nscalability, to provide a more holistic performance assessment\nof different RAG frameworks.\nAnother area for improvement is the visualization and user\nexperience. Currently, SCARF primarily focuses on providing\nnumerical results and logs. The development of a graphical\nuser interface (GUI) and a dedicated control panel would\ngreatly enhance the user experience, making it easier to\nnavigate through results, compare frameworks visually, and\nexplore detailed insights across multiple experiments.\nAs the RAG landscape continues to evolve, we believe\nSCARF\u2019s ability to integrate seamlessly with emerging tools\nwill remain a key advantage. By addressing these limitations\nand expanding its capabilities, SCARF can become an even\nmore comprehensive and user-friendly framework for evaluat-\ning RAG systems.", "SCARF workflow": "In Fig. 2, we show the workflow of our framework.\nThe main entry point for SCARF evaluations is the script\ntest_rag_frameworks.py which orchestrates all the\nprocedures. Below, we summarize the high-level stages of\nSCARF\u2019s operational flow:\n1) The system reads the input configuration to determine\nwhich files should be uploaded, identifies the specific\nqueries that need to be executed, and selects the appro-\npriate files from the knowledge base to ingest into the\nRAG platform.\n2) Checking command-line flags (e.g. --apikey ) to de-\ntermine which framework(s) to target and any necessary\ncredentials.\n3) Dynamically loading a corresponding API adapter (e.g.\nCheshireCatAPI ,AnythingLLMAPI ).\n4) Submitting queries to each RAG framework sequentially\nand collecting responses in memory via Modules/API.\n5) Optionally calling EvaluatorGPT to produce auto-\nmated scores or annotations for each answer. Finally,\nSCARF saves both raw responses and computed evalu-\nation metrics in standardized formats (e.g., csv).\n6) Output Export: .csv ,.json . This step consolidates all\ndata, ensures compatibility with common data process-\ning tools (e.g., Pandas, Excel), and provides a complete\nsnapshot of the experimental run.\nAlgorithm 1 presents a more detailed and comprehensive\ndescription of these steps in the form of pseudocode, providing\nfurther clarity on the procedural aspects involved.B.", "Writing an adapter Module for a Custom RAG Framework": "SCARF is designed to be highly extensible , recognizing that\npractitioners may need to evaluate emerging or proprietary\nRAG solutions. Developers can integrate any system by creat-\ning a SCARF-compliant adapter module and placing it in the\nmodules/ directory. Each adapter must implement two key\nmethods:\n\u2022upload_document(file_path: str) ->\nDict[str, Any] : Responsible for adding a local\nfile to the framework\u2019s knowledge base. In some RAG\nsystems, this may involve splitting the file into smaller\nchunks before embedding and indexing. SCARF captures\nany returned metadata (e.g., document IDs, potential\nerror messages).\n\u2022send_message(message: str) ->\nDict[str, Any] : Submits a text-based query to\nthe RAG endpoint and captures both the raw text\nresponse and any auxiliary diagnostic data (e.g., top-\nranked document identifiers, partial token sequences).\nAfter placing this new module (e.g. mynewrag_api.py )\nin the modules/ folder, you can specify it in\ntest_rag_frameworks.py or via command-line\nflags. After integrating your custom module, you can interact\nwith your deployed RAG platform and begin running tests,\nqueries, and performance evaluations. Moreover, once you\nhave refined your queries and expected responses, you have\nthe chance to modify default metrics.\nSCARFRAG FrameworkSCARF Flow\nKnowledge\nBaseInput\nEvaluatorCore\nIntermediate\nReport\nRAG FrameworkModules\n/ APIs\nFinal\nReportOutputConfiguration\n{Q, Exp A}\nFig. 2: Flow showing how SCARF interact with data and RAG\nframeworks to produce the output.Algorithm 1 SCARF Workflow\n1:Input: Configuration file ( config.json ), command-\nline arguments, and optional API key.\n2:Output: Evaluation results saved as CSV and JSON files.\n3:Begin:\n4:Load configuration from config.json .\n5:for all selected RAG frameworks do\n6: Dynamically load the corresponding API adapter.\n7: for all warmup queries (not associated with a docu-\nment) do\n8: response \u2190send_message(warmup_query)\n9: Append the warmup response and metadata to the\nresults list.\n10: end for\n11: for all document in the dataset do\n12: upload_document(document_path)\n13: for all queries associated with the current document\ndo\n14: response \u2190send_message(query)\n15: Save the response\n16: end for\n17: end for\n18:end for\n19:ifevaluation mode is enabled then\n20: CallEvaluatorGPT for each response.\n21: Merge evaluation scores with the raw responses.\n22: Save the aggregated results to test_results.csv\nortest_results.json .\n23:end if\n24:End.\nV. C ONCLUSION AND LIMITATIONS\nIn this technical report, we presented SCARF, a highly\nflexible and modular evaluation framework for RAG systems.\nUnlike many existing tools, which often focus on single\ncomponents or assume local integration, SCARF operates at\na \u201cblack-box\u201d level. It can connect to any already-deployed\nRAG solution through a minimal adapter module, making it\neasy for researchers and practitioners to assess multiple RAG\nframeworks side by side on the same dataset, enabling direct\ncomparisons of many performance indicators.\nOur approach complements the capabilities of existing RAG\nevaluation frameworks, which may focus more narrowly on\nmetrics for retrieval or generation. SCARF allows users to\nbenchmark a range of real-world scenarios\u2014from single-\nframework tuning to large-scale, multi-framework using a\nsingle, consistent interface. This modularity is particularly\nvaluable in environments where organizations need to validate\nnot only the quality of the model but also determine which\nRAG framework is the most suitable for their specific use\ncase and data.\nDespite its flexibility, SCARF has some limitations that\ncould be addressed in future iterations. Currently, SCARFrequires users to manually provide queries for evaluation,\nwhich can be time-consuming and may introduce biases in the\nassessment process. A promising direction for improvement is\nthe integration of synthetic query generation, allowing SCARF\nto create diverse test cases and reducing human intervention.\nAdditionally, while SCARF supports a range of metrics,\nfuture versions could incorporate additional evaluation criteria,\nsuch as system response time, latency, stability under load, and\nscalability, to provide a more holistic performance assessment\nof different RAG frameworks.\nAnother area for improvement is the visualization and user\nexperience. Currently, SCARF primarily focuses on providing\nnumerical results and logs. The development of a graphical\nuser interface (GUI) and a dedicated control panel would\ngreatly enhance the user experience, making it easier to\nnavigate through results, compare frameworks visually, and\nexplore detailed insights across multiple experiments.\nAs the RAG landscape continues to evolve, we believe\nSCARF\u2019s ability to integrate seamlessly with emerging tools\nwill remain a key advantage. By addressing these limitations\nand expanding its capabilities, SCARF can become an even\nmore comprehensive and user-friendly framework for evaluat-\ning RAG systems."}, "Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets": {"Short Answers Evaluation": "TABLE II\nLIST OF METRICS FOR SHORT ANSWER EVALUATION\nMetrics Papers\nF1-Micro Score [48]\nPrecision [49], [50]\nRecall [49], [50], [48], [51]\nError Rejection/Detection/\nCorrection Rate[35]\nF1-Score [49], [51], [52], [43]\nAccuracy [53], [54], [17], [35], [38], [19], [52], [10]\nError Rate [48]\nOne approach to evaluating the RAG system is to simply\ndetermine whether the answer is correct. This can be au-\ntomated when the answers are very short. The correctness\nof an answer is measured using exact match metrics. This\napproach applies to tasks such as short answers [35], multip le-\nchoice [53] and binary [49] or multi-class categorization [ 51].\nEvaluation datasets include the correct answers or categor ies,\nenabling automated detection of whether the RAG\u2019s response\nis correct. Based on this, the relevant metrics are calculat ed, as\nsummarized in Table II. This involves noisy datasets to calc u-\nlate error metrics to test the model\u2019s robustness when handl ingnoisy or erroneous input data [35]. This evaluation approac h is\ncommonly used in simpler RAGs, where answers can be easily\nassessed as correct or incorrect. However, for more complex\nRAGs, where responses are not as straightforward, addition al\nmethods are required for comprehensive evaluation.\nB.", "Human Evaluators": "TABLE III\nLIST OF METRICS FOR HUMAN EVALUATORS\nMethod Metrics Papers\nDirect Correctness [55], [34], [56], [42]\nPrecision, Recall, Accuracy [16], [57]\nQuality [55], [42]\nReadability, Usefulness [42]\nConfusing Questions Detection [23]\nScore [54]\nComprehensive Coverage, Consistency, Correct-\nness, Clarity[29]\nIn eleven approaches, human evaluators were employed to\nmanually assess the responses of RAG systems. We identi\ufb01ed\ntwo distinct methods for assessing the quality of responses :\nThe \ufb01rst method involves direct evaluation, where human eva l-\nuators assess the quality of responses based on various metr ics,\nas listed in Table III, along with the corresponding papers.\nFor example, one study tested the RAG system\u2019s ability to\nhandle confusing questions by deliberately introducing er rors\nin the context. Human evaluators then determined whether th e\nRAG correctly identi\ufb01ed or addressed these errors [23]. The\nsecond method is the comprehensive evaluation, introduced in\nFeb4Rag [29]. In this approach, two answers are presented\nside by side, and human evaluators judge which answer is\nsuperior based on prede\ufb01ned metrics. Human evaluation was\nespecially applied in domain-speci\ufb01c contexts. For exampl e,\nin the Legal-Bench RAG, domain experts assessed the cor-\nrectness of responses to legal questions. These evaluation s\nwere then used to calculate metrics [16], [57]. Additionall y,\neight approaches utilized human judgment as a benchmark to\ncompare the performance of RAG systems against methods\nwhere an LLM acted as a judge [58], [59], [26], [42], [20],\n[44] or against classical evaluation methods [52], [42], [5 5].\nC.", "Classical and Embedding-Based Approaches": "TABLE IV\nLIST OF METRICS FOR EMBEDDING BASED APPROACHES\nMethod Metrics Papers\nEmbedding SAS [5]\nSBERT [52], [42]\nBERTScore [42], [4]\nN-Gram BLEU [31], [40], [12], [42], [44]\nROGUE-n [38], [5], [42], [50]\nROGUE-L [31], [40], [12], [11], [60],\n[5], [42], [50], [44]\nUnigram Precision/Recall [61]\nModel Unieval [12]\nToken Similarity [27], [61]\nTo automatically evaluate the performance of the generator\nin a RAG system, the generated response is compared to\nthe ground truth answer. We identi\ufb01ed four distinct methods\nfor automatically evaluating the generator\u2019s performance . Onewidely used approach is embedding -based evaluation, which\nassesses the semantic similarity between the generated ans wer\nand the reference answer. Within this category, we identi\ufb01e d\nthree primary techniques. The \ufb01rst is the Semantic Answer\nSimilarity (SAS) score, which employs a trained cross-enco der\narchitecture to evaluate the semantic alignment between ge ner-\nated and reference answers [5]. The second technique involv es\nSentence-BERT (SBERT), a specialized adaptation of the\nBERT model designed for comparing sentences and evaluating\ntheir semantic similarity [62], [63]. SBERT is particularl y\neffective for tasks requiring textual alignment, making it\nsuitable for comparing RAG-generated answers with referen ce\nanswers [52], [42]. Lastly, BERTScore, another embedding-\nbased model built upon the BERT architecture, compares\ntoken-level embeddings of sentences to measure similarity\n[64], [42], [4]. In addition, n-gram -based metrics are com-\nmonly used. These metrics calculate scores automatically b y\nanalyzing sequences of words (n-grams) of a speci\ufb01ed length .\nTable IV lists the speci\ufb01c metrics used in this category. The\nthird method involves model -based evaluation, such as the\nUniEval score. UniEval is designed to assess natural langua ge\ngeneration by providing a comprehensive evaluation score\nbased on coherence, consistency, \ufb02uency, and relevance. In\nthis approach, a model is trained using the ground truth as a\nreference for evaluation [65], [12]. Finally, the fourth me thod\nistoken-based evaluation, which measures the quality of a\ngenerated response by calculating the ratio of overlapping\ntokens between the generated answer and the ground truth,\nusing word segmentation tools [27]. Alternatively, this ca n be\nachieved using cosine similarity, which measures the cosin e\nof the angle between two vectors [61]. In four cases, these\nmetrics are extended by using a large language model (LLM)\nas a judge [31], [60], [42], [50] or a human evaluator for\nvalidation [52], [42].\nD.", "LLM as a Judge": "LLMs can serve as judges to evaluate the performance of\nRAG systems, and we identi\ufb01ed 41 papers that employed\nLLMs for this purpose. Table V lists all metrics and the\ncorresponding papers used for evaluating the generator wit h\nan LLM. We observed a growing trend in using LLMs for\nautomating evaluation. Initial studies have shown a positi ve\ncorrelation between human evaluation and LLM-based eval-\nuation [58], [59], highlighting the potential of LLMs for\nassessing RAG systems. Based on our analysis, we identi\ufb01ed\n\ufb01ve distinct methods for calculating these metrics.\nThe \ufb01rst method integrates the exact match (EM) metric,\nreferred to as LLM + EM . This approach leverages an\nLLM when the exact match fails, such as in cases where\nthe generated answer is too long. In such instances, the\nLLM determines whether the answer is correct [37], [54].\nAdditionally, the LLM can provide detailed explanations ab out\nthe errors, offering a more nuanced understanding of the RAG\nsystem\u2019s capabilities. For example, it can highlight the RA G\u2019s\nability to re\ufb02ect information absent in the document or to\nidentify factual inaccuracies [35].TABLE V\nLIST OF METRICS FOR LLM AS A JUDGE\nMethod Metrics Papers\nEM+LLM Correctness [37], [54]\nError Explanation [35]\nDirect Faithfulness/Factual\nConsistency[6], [7], [31], [11], [66], [67],\n[46], [20], [68], [36]\nTruthfulness/Correctness [50], [31], [54], [28], [44], [66],\n[32], [69], [26], [24], [43], [42]\nHallucination [37], [54], [33], [20]\nRelevance [6], [7], [31], [44], [67], [33],\n[69], [24], [42], [46]\nRedundancy [11]\nNoise Sensitivity [6], [20], [36]\nCompleteness [44], [33], [69], [26]\nPrecision [50]\nHelpfulness [66], [26]\nMissing [37], [54]\nDe\ufb01ciency [11]\nCoherence [68]\nScore [37], [67], [54], [58], [59]\nIndirect Precision, recall [4]\nRelevance [6]\nKPR [30]\nFact/Logic Consistency [22]\nOPI [70]\nComparative Kendall /acute.ts1s tau [7]\nRAGElo [71], [44], [60]\nOwn LLM Lynx [21]\nThe second approach involves direct measurement, where\nan LLM is prompted to calculate a speci\ufb01c metric directly.\nThis method takes the generated answer, the question, the\nretrieved context, and the ground truth answer as input to\ncompute the metric [7]. Here metrics like Faithfulness or\nTruthfulness were described in a speci\ufb01c prompt and the\nLLM outputs a score for these. Faithfulness describes for\nexample the factual consistent by the retrieved context [6] . The\ndirect measurement has been the most widely used method for\nevaluating the generator component. In contrast, the indirect\nmeasurement method focuses on a more complex evaluation\nprocess. Unlike direct measurement, this approach does not\nrely on the direct use of the context, question, and ground\ntruth in the prompt. Instead, it preprocesses the generated\nanswer in a speci\ufb01c manner before calculating the metric. Fo r\ninstance, RAGQuestEval evaluates the ground truth referen ce\nby generating questions derived from it and then determinin g\nwhether the generated answers can correctly address these\nquestions [4]. Other methods in this category include count ing\nkey points in the generated answer [30] or generating questi ons\nbased on the answer itself [6].\nForcomparative evaluation, metrics such as Kendall\u2019s tau\nare commonly employed to compare different RAG con\ufb01g-\nurations [7]. A more comprehensive framework, RAGElo,\nwas also introduced, combining individual judgments with a\ncomparative scoring mechanism [71], [44], [60]. This frame -\nwork allows an LLM to evaluate two answers to the same\nquestion along with their retrieved contexts, determining which\nanswer is better. Based on these comparisons, an Elo score\nis calculated, serving as a comparative metric for evaluati ng\nthe performance of RAG systems [44], [60]. Finally, the \ufb01fth\nmethod leverages a specialized own LLM for evaluation. For\nexample, Lynx, an open-source LLM trained speci\ufb01cally todetect hallucinations, has been used as a standalone evalua tor\nto assess the accuracy and reliability of RAG systems [21].\nThese diverse methods illustrate the versatility of LLMs as\nevaluators.\nVII. C ONCLUSION\nA key challenge in the evaluation work\ufb02ow lies in selecting\nan appropriate dataset. This process must address several\nissues. First, the dataset must be designed to prevent the\nLLM from leveraging its base knowledge to answer questions\nindependently [24]. The questions should also be neither to o\nsimple nor too general; instead, they should be challenging\nand aligned with the RAG system\u2019s intended use case. Ad-\nditionally, the dataset should contain labels for the conte xt\nrelevance if the retriever evaluation requires these. Anot her\nchallenge is that every part of the evalution can be automate d\nwith an LLM. Here the challange lies in ensuring con\ufb01dence\nin the automation process, especially in tasks involving LL Ms.\nUnresolved questions persist, such as whether the quality o f\nevaluation is compromised when an LLM generates questions,\nanswers them, and ultimately evaluates its own output. This\nraises concerns about potential biases when an LLM evaluate s\nitself, and whether the evaluation remains reliable withou t\nhuman involvement. Determining where human expertise is\nstill necessary in this work\ufb02ow remains a critical area for\nfurther exploration. The rapid development of LLMs present s\nanother signi\ufb01cant challenge, as advancements in models\ncould render previous evaluation results invalid. A new mod el\nmight produce entirely different outcomes, raising the iss ue of\nhow to adapt prior results and establish a standard evaluati on\nframework that remains consistent and independent of speci \ufb01c\nLLM versions. Additionally, the lack of a standardized prom pt\nfor LLM-based evaluations complicates the ability to make\nresults comparable across different studies.\nIn light of these evolving challenges, this study identi\ufb01es\nand synthesizes best practices across key components of\nRAG system evaluation. The indexing component is evaluated\nprimarily based on its performance metrics and as part of the\noverall system performance [9], [5]. The retriever\u2019s quali ty is\nassessed by the relevance of retrieved documents, often usi ng\nlabeled datasets [8], [39]. In the absence of such datasets i n\nRAG systems, domain experts manually evaluated relevance\n[42], or LLMs were employed for ef\ufb01cient, automated assess-\nment [8]. The generator component was evaluated through fou r\nmethods: exact match detection for short answers [48], trad i-\ntional metrics like BLEU or ROUGE for complex questions\n[42], human validation by experts for accuracy and relevanc e\n[55], and LLM-based evaluation using metrics like faithful ness\n[6]. The trend and best practice point toward using an LLM\nto automate this evaluation task. This study also highlight ed\nthe crucial role of datasets in evaluating RAG systems. Thre e\ntypes of datasets were identi\ufb01ed: existing datasets, enhan ced\ndatasets, and newly created datasets [16], [22], [6]. The cr e-\nation of completely new datasets was particularly signi\ufb01ca nt\nfor domain-speci\ufb01c RAG systems where no publicly available\ndatasets existed [11]. In general, the study demonstrated t hatalmost every aspect of the evaluation process could poten-\ntially be automated using LLMs. Datasets can be generated\nautomatically, while both answers and retrieved chunks can\nbe evaluated by LLMs. However, human expertise remains\nessential for speci\ufb01c tasks, particularly in domain-speci \ufb01c\nRAG systems where nuanced judgment is required.\nOnly six studies compared LLM judges with human judges,\n\ufb01nding a positive correlation between their evaluations [5 8],\n[59], [26], [42], [20], [44]. While promising, these \ufb01nding s\nunderscore the ongoing need for human oversight. Future\nresearch should further investigate the reliability of LLM -\nbased evaluation, as current evidence suggests that LLMs ca n\nbe trusted to some extent, but their validity remains to be\nthoroughly established.\nIn conclusion, while signi\ufb01cant progress has been made\nin RAG systems and their evaluation, the practical dos and\ndon\u2019ts remain largely unexplored, especially in real-worl d\napplication domains. As businesses continue to invest in\nRAG implementations, the need for practical frameworks and\nactionable recommendations has become increasingly urgen t.\nA notable gap in current evaluation approaches is the lack of\nconsideration for RAG-speci\ufb01c requirements, such as ensur ing\nsystems stay updated with new knowledge or how retrievers\neffectively incorporate and maintain access to the most cur rent\ninformation. Further research should address these gaps by\noffering clear guidance and developing practical tools to\nenhance the effectiveness and adaptability of RAG systems\nin dynamic, real-world environments."}, "REALM-Bench: A Real-World Planning Benchmark for LLMs and Multi-Agent Systems": {"Abstract": "Thisbenchmarksuiteprovidesacomprehensiveevaluationf rame-\nwork forassessing bothindividual LLMs and multi-agentsys tems\ninreal-worldplanningscenarios.Thesuiteencompassesel evende-\nsignedproblemsthatprogressfrombasictohighlycomplex, incor-\nporatingkeyaspectssuchasmulti-agentcoordination,int er-agent\ndependencies,anddynamicenvironmentaldisruptions.Eac hprob-\nlem can be scaled along three dimensions: the number of paral lel\nplanning threads, the complexity of inter-dependencies, a nd the\nfrequency of unexpected disruptions requiring real-time a dapta-\ntion. The benchmark includes detailed speci\ufb01cations, eval uation\nmetrics,andbaselineimplementationsusingcontemporary frame-\nworks like LangGraph, enabling rigorous testing of both sin gle-\nagentandmulti-agentplanningcapabilities.Throughstan dardized\nevaluation criteria and scalable complexity, this benchma rk aims\ntodriveprogressindevelopingmorerobustandadaptableAI plan-\nning systems for real-worldapplications.\nKeywords\nLLMs, Multi-Agent Systems, Planning Benchmark", "1 Introduction": "As large language models (LLMs) continue to advance in reaso n-\ningandplanning,asdemonstratedbyOpenAI\u2019sGPT-4o-Task[ 19],\nDeepSeek\u2019s R1 [9], Anthropic\u2019s Claude 3.5 Sonnet [1], and Ge m-\nini[24],theresearchcommunityisincreasinglyfocusingo ndevel-\noping multi-agent systems (MAS) powered by these models. Re -\ncentinnovationsincludeAutoGen[25],CAMEL[16],CrewAI[ 18],\nLangGraph [15], Dspy [14], and XAgent [26], among others. Al -\nthoughindividualLLMsdemonstratesigni\ufb01cantcapabiliti es,their\ntrue potential is realized when they collaborateto tackle c omplex\nreal-world problems[5].\nMostAIbenchmarksemphasizeperception,languageunderst and-\ning, or basic reasoning. However, real-world challenges, s uch as\nsupply chain management, disaster response, healthcare lo gistics,\nandinvestmentstrategies,demandcoordinatedplanningan ddecision-\nmaking amongspecializedagents. Thereis apressing need fo rro-\nbustbenchmarksthatcanevaluatetheperformanceofbothsi ngle-\nagent systems and MASin thesecomplex,high-stakes domains .", "1.1 The REALM Benchmark Suite": "REALM-Bench(Real-worldPlanningBenchmarkforLLMsandMu lti-\nAgent Systems) addresses the need for rigorous evaluation w ith\ncarefullycuratedplanningchallenges.Thesescenariosar edesigned\nto be both tractable, enabling human validation and debugging,andsu\ufb03cientlycomplextopushtheboundariesofcurrentAIs ys-\ntems. Each challenge requires reasoning and validation ove r se-\nquential actions parallel processes, resource constraint s, and un-\nexpecteddisruptions [3,4,6,7,13].\nThesuiteconsistsofelevenscenariosthatprogressivelyi ncrease\nincomplexityacross threekey dimensions:\n1.Parallel Planning Threads: The number of concurrent plan-\nning processes thatmustbecoordinated.\n2.Inter-Dependencies: Thecomplexityofrelationshipsandcon-\nstraints between theseplanning threads.\n3.Disruption Frequency and Impact: The rate and severity of\nunexpectedevents thatrequiretheadaptationof theplan.\nNext, we describe how each scenario can be scaled along these\nthreedimensions.", "1.2 Benchmark Scalability": "While the base versions of each scenario enable detailed ana lysis\nand debugging, they can bescaled along the three dimensions de-\n\ufb01nedabove:parallelplanningthreads,inter-dependencie s,anddis-\nruptionfrequencyand impact.\nFor example, an urban ride-sharing scenario becomes increa s-\ningly complex as the number of vehicles and passengers grows ,\nwithinterdependent carpoolingroutesandfrequenttra\ufb03cd isrup-\ntionsnecessitating real-time planadjustments.\nThis scalability allows AI planning systems to beevaluated un-\nder progressively challenging conditions, while still all owing de-\ntailedanalysis offailure modesinsimpler scenarios.", "1.3 Availability and Access": "The REALM-Bench Suite V1.0 is available on GitHub [11]. In ad -\ndition, we plan to host competitions and workshops at major A I\nconferences in 2025 tofoster community engagement and furt her\ndevelopment.", "2 Related Benchmark": "PlanningbenchmarkshaveevolvedfromtestingbasicSTRIPS -style\nplanning to evaluating increasingly sophisticated planni ng capa-\nbilities. The International Planning Competition (IPC) ha s been a\nprimary driver of planning benchmarks since 1998, using PDD L\nto specify domains like BlocksWorld, Logistics, and Rovers [23].\nWhilevaluablefortestingclassicalplanningalgorithms, thesebench-\nmarks focus on deterministic environments with complete in for-\nmation and lack the dynamic disruptions common in real-worl d\nscenarios.\nMorerecentbenchmarks,suchas theProcessPlanning Compe-\ntition(PPC),haveshiftedtowardcontinuousprocessesand tempo-\nralconstraints[22].Theirmanufacturingscenariosinclu deparallel\n1ACM, February, 2025 Longling Gengand EdwardY.Chang\nactivities and resource dependencies, but the disruptions remain\nlimited to machine breakdowns with known repair distributi ons.\nSimilarly,theDynamic Planning Competitionintroducesen viron-\nmental changes duringplanexecution,yet itfocusesprimar ilyon\npath planning and navigation scenarios [10].\nTheannualAutomatedNegotiationAgentsCompetition(ANAC ),\nestablished in 2010,has evolved toincorporateplanning el ements\nwithinitssupplychainscenarios [17].However,itsscoper emains\nprimarilyfocusedonbilateralnegotiationsratherthanco mprehen-\nsiveplanningunderuncertainty.Forexample,the2024\u201325c ompe-\ntition featured a main challenge titled \u201cSplit the Pie,\u201d an a rti\ufb01cial\nyet simpli\ufb01ed negotiation scenario where agents divide res ources\nbetween parties. The supply chain problems in ANAC do not in-\nvolve contingency planning, resource reallocation, or ada ptation\ntounexpecteddisruptions.\nSpeci\ufb01callyfortestingLLMs\u2019planningcapabilities,Time Bench\n[8] and TaskBench [21] represent two approaches to evaluati ng\nAIplanning.TimeBenchfocusesontemporalreasoningbytes ting\nsystems\u2019 ability to understand time dependencies and sched uling\nconstraints,thoughitoftenreliesonsyntheticscenarios thatfailto\ncapture the dynamic nature of real-world temporal relation ships,\nwheredeadlines shiftanddurationsremainuncertain. Task Bench,\non the other hand, evaluates practical task automation and s tep-\nby-stepplanning; itprovidesvaluableinsightsintoanAIs ystem\u2019s\nabilitytodecomposecomplexgoalsintomanageable steps,b utits\nscenarios may oversimplify the challenges of real-world au toma-\ntion, where outcomes are uncertain and processes are deeply in-\nterconnected.\nThis landscapereveals several gaps in existing benchmarks :\n1.Limited Disruption Modeling: Most benchmarks treat un-\ncertainties as static probability distributions rather th an dy-\nnamic, interdependent events that can cascade through sys-\ntems.\n2.Simpli\ufb01edDependencies: Real-worldplanning problemsin-\nvolve rich networks of temporal, resource, and causal depen -\ndencies that exceed the complexity found in current bench-\nmarks.\n3.Restricted Scope: Benchmarks tend to focus on speci\ufb01c sub-\nproblems(pathplanning, taskallocation,etc.)rathertha nend-\nto-end planning scenarios that combinemultiplechallenge s.\n4.Arti\ufb01cialConstraints: Manybenchmarks usesimpli\ufb01edrep-\nresentations (likePDDL) thatcannotcapturethenuancedco n-\nstraints and objectives found inreal-worldplanning probl ems.\n5.Limited Scalability: Few benchmarks allow systematic scal-\ning of complexity along multiple dimensions while maintain -\ning problemtractabilityforanalysis.\n6.LLM Speci\ufb01c Challenges: Although LLMs have achieved re-\nmarkable successes, the transformer architecture exhibit s cer-\ntain limitations. For example, an attention sink phenomenon\ncan cause certain tokens to be neglected, potentially skewi ng\nmodelpredictions[27].Additionally,maximumlikelihood train-\ningcanintroducebiasesthatlimitoutputdiversityandqua lity\n[4, 12]. Finally, chain-of-thought approaches may su\ufb00er fr om\npitfallssuchaserrorpropagationandinconsistentreason ing[2,\n20].Atestsuiteshouldspeci\ufb01callyexaminetheseLLM-rela ted\nissues.OurproposedREALM-Benchaddressestheselimitationsbypr o-\nvidingscenariosthatcombinedependencies,dynamic disru ptions,\nandscalabilitywhileremainingtractableforsystematice valuation.\nThis allowstestingofplanning systems under conditions th atbet-\nterre\ufb02ect thechallenges of real-world applications.", "3 Benchmark Structure": "Problems are categorized into three di\ufb03culty levels based o n the\nnumberofparallelexecutionthreads,thecomplexityofdep enden-\ncies, and real-timedisruptions.\n3.1 Entry Level (1-2threads)\nProblems focusing on basic coordination with limited depen den-\ncies:\n- Single or dualthread execution\n- Basic timing and resource constraints\n- Simpledisruptionscenarios\n- Example:Campus tourcoordinationwithoneand two groups\n3.2 Intermediate (3-4threads)\nProblemsrequiringsigni\ufb01cantcoordinationacrossmultip leexecu-\ntionpaths:\n- Threetofourparallelthreads\n- Complex timing dependencies\n- Resourcesharing constraints\n- Example:Weddinglogistics with multiplevehicles and tas ks\n3.3 Advanced (5+threads)\nProblems withreal-world complexity:\n- Five or moreparallelthreads\n- Complex inter-dependencies\n- Multipleresourcecon\ufb02icts\n- Dynamic disruptionscenarios\n- Example:Thanksgivingdinnercoordination,naturaldisa sterre-\nlief, and supplychain management\n3.4 Evaluation Metrics\nEachproblem is evaluatedacross \ufb01ve key dimensions:\n-Planning Quality: E\ufb00ectiveness of initial plangeneration\n-Coordination: Management of parallelthread execution\n-Adaptation: Response todisruptionsand changes\n-Resource Management: Resolutionof resourcecon\ufb02icts\n-ConstraintSatisfaction: Maintenance of problemconstraints\n4 Benchmark Problem Speci\ufb01cations\nREALM-Benchcompriseselevenfoundationalproblemframew orks\nthatsystematicallyevaluatebothsequentialandreactive planning.\nBuildingonthekeydimensionsintroducedearlier(paralle lthreads,\ninter-dependencies, and dynamic disruptions), these fram eworks\nprogressfromstraightforwardsingle-threadexecutionto complex\nmulti-agent scenarios withreal-time challenges.\nConsiderations#1ProblemComplexity: Eachframeworkcan\nbefurther scaledtocreatemorechallenging variants:\n* Expanding the scale of agents and resources (e.g., from doz ens\ntothousands)\n2REALM-Bench: AReal-WorldPlanning Benchmark\nfor LLMs andMulti-Agent Systems ACM, February, 2025\nTable 1: SingleTour CampusNavigationProblem\nMetrics:\n-Totaltourtime: Minimizewhilemeeting allconstraints\n-Visit coverage: Alllocations must bevisited\nLocations: Fivecampus buildings /u1D43F={/u1D446,/u1D43F,/u1D435,/u1D434,/u1D437 }\n-/u1D446:Student Center\n-/u1D43F:Library\n-/u1D435: Lab Building\n-/u1D434:AthleticsCenter\n-/u1D437: Dormitory\nTravel Times: (minutes) /u1D446-/u1D43F: 10,/u1D446-/u1D435: 15,/u1D446-/u1D434: 20,/u1D446-/u1D437: 15/u1D43F-/u1D435:\n10,/u1D43F-/u1D434: 25,/u1D43F-/u1D437: 20/u1D435-/u1D434: 15,/u1D435-/u1D437: 25/u1D434-/u1D437: 20\nVisit Requirements:\n- Start time given\n- Each location requires 30-minute visit\n- Tour starts/endsat StudentCenter( /u1D446)\n- Group size:20people\nTime Constraints:\n- Lab Building:Only9AM -4PM\n- Library:After10AM\n- Totaltour must complete by 5PM\nTable 2: Multi-GroupCampusTour Problem\nGroups:\n-/u1D43A1: 15people (domesticstudents)\n-/u1D43A2: 20people (internationalstudents)\nLocations: Ten campus buildings /u1D43F={/u1D446,/u1D43F,/u1D435,/u1D434,/u1D437,/u1D436,/u1D440,/u1D445,/u1D43B,/u1D443 }\nwithcapacities /u1D450/u1D44E/u1D45D/u1D459:\n/u1D450/u1D44E/u1D45D/u1D459=\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4 \uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f340/u1D459\u2208 {/u1D446,/u1D434}\n30/u1D459\u2208 {/u1D43F,/u1D437,/u1D436}\n25/u1D459\u2208 {/u1D435,/u1D440}\n20/u1D459\u2208 {/u1D445,/u1D43B,/u1D443}\nConstraints:\n- Tour starttime foreach group: between9AMand 10AM\n- Totalvisitors must not exceedlocation capacity\n- Each location requires 30-minute visit\n- Both tours start/endat StudentCenter\n- Complete alltours by5 PM\nAdditional Requirements:\n- Lab tours ( /u1D435)only 9 AM-4 PM\n- Dining( /u1D436) must bevisitedbetween11AM -2PM\n- Library( /u1D43F) after10AM\n* Widening geographic distribution(e.g., localtoglobal)\n* Increasingdisruptionfrequencyandseverity(e.g.,isol atedevents\ntocascading failures)\n* Introducinguncertaintiesinexecutiontimesandoutcome s(e.g.,\nprobabilisticdurations)\n* Adding hierarchical dependency networks (e.g., sub-netw orks\nwith internal dependencies)\n* Accounting foragent properties(e.g., atomicity,idempo tency)\nThisextensibledesignenables researchers toincremental lyassess\ntheir systems\u2019 capabilitieswhilepreserving each scenari o\u2019s funda-\nmental planning challenge.\nConsiderations #2 Implementation: Three main approaches\ncan beused totacklethese problems:Table 3: Urban Ride SharingProblem\nMetrics:\n-On-time performance: Nopenalty forearlyarrivals.\n-Totaldistancetraveled.\nLocations: Seven locations: /u1D449={/u1D434,/u1D435,/u1D436,/u1D437,/u1D438,/u1D439,/u1D43A }, where/u1D43Ais\nBoston Logan Airport (BOS). Urban locations /u1D434\u2013/u1D439are all 10 km of\neach other,whiledistancestoBOS are 30+km.\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0/u1D434 /u1D435 /u1D436 /u1D437 /u1D438 /u1D439\n/u1D434\u2212/u1D43910 10 10 10 10 10\n\u2192/u1D43A35 33 36 34 32 31\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nTravel speed: (/u1D434\u2013/u1D439) 60km/h, and ( /u1D434\u2013/u1D439\u2192/u1D43A)100km/h.\nPassengers: Each passenger speci\ufb01es an arrival time at BOS ( /u1D43A).The\ndispatcher will instruct drivers when to pick up passengers to ensure\non-time arrivalat BOS.\nRideRequests (DesiredBOSarrivaltime given):\n-/u1D45F1: Pickupat /u1D434,to/u1D43Aby 08:45 - /u1D45F2: Pickupat /u1D435, to/u1D43Aby08:50\n-/u1D45F3: Pickupat /u1D436, to/u1D43Aby08:55 - /u1D45F4:Pickup at /u1D437,to/u1D43Aby 09:00\nAvailableVehicles (Capacity 2 passengers):\n-/u1D4581:at/u1D434,/u1D4582:at/u1D436, and/u1D4583:at/u1D438\nScheduling Constraints: - The dispatcher determines the pickup\ntimesbased on a feasible schedule. Pickup times must allow the dri-\nver to\ufb01rst reach thepassenger location ( /u1D434-/u1D439)and thendriveto /u1D43Ain\ntime.\n\u2022RudimentaryManualApproach: Hand-craftadirectedgraph,\nchooseasolver,andinputparametervaluestoobtainasolut ion.\nAlthough direct, this method demands considerable human ef -\nfortand domainexpertise.\n\u2022LLM-AssistedSemi-automation: UseanLLMtosuggestalgo-\nrithms, extract parameters from speci\ufb01cations, and assist with\ncoding. This approach can handle moderate complexity but be -\ncomes less feasible at scale.\n\u2022Full Automation: Provide the problem statement to a multi-\nagent framework (e.g., MACI [6]) that executes end-to-end, in-\ncludingvalidationandreplanning.Thisapproach,combine dwith\nhuman oversight, scales best for complex scenarios with lar ge\nnumbers of nodesand frequent environmental changes.\n4.1 P1: Campus Single-Tour Navigation\nProblem Statement: A single autonomous agent must navigate\naprede\ufb01nedcampusenvironment tocompleteasequenceofway -\npointswhileminimizingtraveltime.Thescenarioassumesa static\nenvironment withoutdisruptions.\nProblem Speci\ufb01cation:\n-Environment: Aknown map witha \ufb01nite setof locations.\n-Goal:Visit alldesignated waypointswithin agiven timeframe.\n-Constraints: Opening hours of each location, each location at\nleast 30minutes, and mustbecompletedbefore 5PM.\n-OptimizationMetric: Shortest path(timeor distance).\nA meta-plan, shown in Table 1, provides the high-level struc -\nture and constraints for the problem. This meta-plan serves as in-\nputtospecializedsolvers,suchasdynamicprogrammingorM onte\nCarlo algorithms, which then generate detailed, executabl e work-\n\ufb02ows.Theprocesstransformsabstractplanningrequiremen tsinto\nconcrete,implementable sequences ofactionswhilerespec tingall\nspeci\ufb01edconstraints and optimizationobjectives.\n3ACM, February, 2025 Longling Gengand EdwardY.Chang\n4.2 P2:Multi-GroupCampus Tour\nProblem Statement: Multiple groups of visitors require guided\ntoursindi\ufb00erentlocationsonauniversitycampus,withopt imized\nscheduling of multipletour guides. This problem shares the same\nmetrics as P1.\nProblem Speci\ufb01cation:\n- Multipleagents (tourguides) mustcoordinatetoservedi\ufb00 erent\ngroups ofvisitors.\n- Each grouphas prede\ufb01ned p", "3.4 Evaluation Metrics": "Eachproblem is evaluatedacross \ufb01ve key dimensions:\n-Planning Quality: E\ufb00ectiveness of initial plangeneration\n-Coordination: Management of parallelthread execution\n-Adaptation: Response todisruptionsand changes\n-Resource Management: Resolutionof resourcecon\ufb02icts\n-ConstraintSatisfaction: Maintenance of problemconstraints\n4 Benchmark Problem Speci\ufb01cations\nREALM-Benchcompriseselevenfoundationalproblemframew orks\nthatsystematicallyevaluatebothsequentialandreactive planning.\nBuildingonthekeydimensionsintroducedearlier(paralle lthreads,\ninter-dependencies, and dynamic disruptions), these fram eworks\nprogressfromstraightforwardsingle-threadexecutionto complex\nmulti-agent scenarios withreal-time challenges.\nConsiderations#1ProblemComplexity: Eachframeworkcan\nbefurther scaledtocreatemorechallenging variants:\n* Expanding the scale of agents and resources (e.g., from doz ens\ntothousands)\n2REALM-Bench: AReal-WorldPlanning Benchmark\nfor LLMs andMulti-Agent Systems ACM, February, 2025\nTable 1: SingleTour CampusNavigationProblem\nMetrics:\n-Totaltourtime: Minimizewhilemeeting allconstraints\n-Visit coverage: Alllocations must bevisited\nLocations: Fivecampus buildings /u1D43F={/u1D446,/u1D43F,/u1D435,/u1D434,/u1D437 }\n-/u1D446:Student Center\n-/u1D43F:Library\n-/u1D435: Lab Building\n-/u1D434:AthleticsCenter\n-/u1D437: Dormitory\nTravel Times: (minutes) /u1D446-/u1D43F: 10,/u1D446-/u1D435: 15,/u1D446-/u1D434: 20,/u1D446-/u1D437: 15/u1D43F-/u1D435:\n10,/u1D43F-/u1D434: 25,/u1D43F-/u1D437: 20/u1D435-/u1D434: 15,/u1D435-/u1D437: 25/u1D434-/u1D437: 20\nVisit Requirements:\n- Start time given\n- Each location requires 30-minute visit\n- Tour starts/endsat StudentCenter( /u1D446)\n- Group size:20people\nTime Constraints:\n- Lab Building:Only9AM -4PM\n- Library:After10AM\n- Totaltour must complete by 5PM\nTable 2: Multi-GroupCampusTour Problem\nGroups:\n-/u1D43A1: 15people (domesticstudents)\n-/u1D43A2: 20people (internationalstudents)\nLocations: Ten campus buildings /u1D43F={/u1D446,/u1D43F,/u1D435,/u1D434,/u1D437,/u1D436,/u1D440,/u1D445,/u1D43B,/u1D443 }\nwithcapacities /u1D450/u1D44E/u1D45D/u1D459:\n/u1D450/u1D44E/u1D45D/u1D459=\uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4 \uf8f2\n\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f340/u1D459\u2208 {/u1D446,/u1D434}\n30/u1D459\u2208 {/u1D43F,/u1D437,/u1D436}\n25/u1D459\u2208 {/u1D435,/u1D440}\n20/u1D459\u2208 {/u1D445,/u1D43B,/u1D443}\nConstraints:\n- Tour starttime foreach group: between9AMand 10AM\n- Totalvisitors must not exceedlocation capacity\n- Each location requires 30-minute visit\n- Both tours start/endat StudentCenter\n- Complete alltours by5 PM\nAdditional Requirements:\n- Lab tours ( /u1D435)only 9 AM-4 PM\n- Dining( /u1D436) must bevisitedbetween11AM -2PM\n- Library( /u1D43F) after10AM\n* Widening geographic distribution(e.g., localtoglobal)\n* Increasingdisruptionfrequencyandseverity(e.g.,isol atedevents\ntocascading failures)\n* Introducinguncertaintiesinexecutiontimesandoutcome s(e.g.,\nprobabilisticdurations)\n* Adding hierarchical dependency networks (e.g., sub-netw orks\nwith internal dependencies)\n* Accounting foragent properties(e.g., atomicity,idempo tency)\nThisextensibledesignenables researchers toincremental lyassess\ntheir systems\u2019 capabilitieswhilepreserving each scenari o\u2019s funda-\nmental planning challenge.\nConsiderations #2 Implementation: Three main approaches\ncan beused totacklethese problems:Table 3: Urban Ride SharingProblem\nMetrics:\n-On-time performance: Nopenalty forearlyarrivals.\n-Totaldistancetraveled.\nLocations: Seven locations: /u1D449={/u1D434,/u1D435,/u1D436,/u1D437,/u1D438,/u1D439,/u1D43A }, where/u1D43Ais\nBoston Logan Airport (BOS). Urban locations /u1D434\u2013/u1D439are all 10 km of\neach other,whiledistancestoBOS are 30+km.\n\uf8ee\uf8ef\uf8ef\uf8ef\uf8ef\uf8ef\uf8f0/u1D434 /u1D435 /u1D436 /u1D437 /u1D438 /u1D439\n/u1D434\u2212/u1D43910 10 10 10 10 10\n\u2192/u1D43A35 33 36 34 32 31\uf8f9\uf8fa\uf8fa\uf8fa\uf8fa\uf8fa\uf8fb\nTravel speed: (/u1D434\u2013/u1D439) 60km/h, and ( /u1D434\u2013/u1D439\u2192/u1D43A)100km/h.\nPassengers: Each passenger speci\ufb01es an arrival time at BOS ( /u1D43A).The\ndispatcher will instruct drivers when to pick up passengers to ensure\non-time arrivalat BOS.\nRideRequests (DesiredBOSarrivaltime given):\n-/u1D45F1: Pickupat /u1D434,to/u1D43Aby 08:45 - /u1D45F2: Pickupat /u1D435, to/u1D43Aby08:50\n-/u1D45F3: Pickupat /u1D436, to/u1D43Aby08:55 - /u1D45F4:Pickup at /u1D437,to/u1D43Aby 09:00\nAvailableVehicles (Capacity 2 passengers):\n-/u1D4581:at/u1D434,/u1D4582:at/u1D436, and/u1D4583:at/u1D438\nScheduling Constraints: - The dispatcher determines the pickup\ntimesbased on a feasible schedule. Pickup times must allow the dri-\nver to\ufb01rst reach thepassenger location ( /u1D434-/u1D439)and thendriveto /u1D43Ain\ntime.\n\u2022RudimentaryManualApproach: Hand-craftadirectedgraph,\nchooseasolver,andinputparametervaluestoobtainasolut ion.\nAlthough direct, this method demands considerable human ef -\nfortand domainexpertise.\n\u2022LLM-AssistedSemi-automation: UseanLLMtosuggestalgo-\nrithms, extract parameters from speci\ufb01cations, and assist with\ncoding. This approach can handle moderate complexity but be -\ncomes less feasible at scale.\n\u2022Full Automation: Provide the problem statement to a multi-\nagent framework (e.g., MACI [6]) that executes end-to-end, in-\ncludingvalidationandreplanning.Thisapproach,combine dwith\nhuman oversight, scales best for complex scenarios with lar ge\nnumbers of nodesand frequent environmental changes.\n4.1 P1: Campus Single-Tour Navigation\nProblem Statement: A single autonomous agent must navigate\naprede\ufb01nedcampusenvironment tocompleteasequenceofway -\npointswhileminimizingtraveltime.Thescenarioassumesa static\nenvironment withoutdisruptions.\nProblem Speci\ufb01cation:\n-Environment: Aknown map witha \ufb01nite setof locations.\n-Goal:Visit alldesignated waypointswithin agiven timeframe.\n-Constraints: Opening hours of each location, each location at\nleast 30minutes, and mustbecompletedbefore 5PM.\n-OptimizationMetric: Shortest path(timeor distance).\nA meta-plan, shown in Table 1, provides the high-level struc -\nture and constraints for the problem. This meta-plan serves as in-\nputtospecializedsolvers,suchasdynamicprogrammingorM onte\nCarlo algorithms, which then generate detailed, executabl e work-\n\ufb02ows.Theprocesstransformsabstractplanningrequiremen tsinto\nconcrete,implementable sequences ofactionswhilerespec tingall\nspeci\ufb01edconstraints and optimizationobjectives.\n3ACM, February, 2025 Longling Gengand EdwardY.Chang\n4.2 P2:Multi-GroupCampus Tour\nProblem Statement: Multiple groups of visitors require guided\ntoursindi\ufb00erentlocationsonauniversitycampus,withopt imized\nscheduling of multipletour guides. This problem shares the same\nmetrics as P1.\nProblem Speci\ufb01cation:\n- Multipleagents (tourguides) mustcoordinatetoservedi\ufb00 erent\ngroups ofvisitors.\n- Each grouphas prede\ufb01ned p", "4.1 P1: Campus Single-Tour Navigation": "Problem Statement: A single autonomous agent must navigate\naprede\ufb01nedcampusenvironment tocompleteasequenceofway -\npointswhileminimizingtraveltime.Thescenarioassumesa static\nenvironment withoutdisruptions.\nProblem Speci\ufb01cation:\n-Environment: Aknown map witha \ufb01nite setof locations.\n-Goal:Visit alldesignated waypointswithin agiven timeframe.\n-Constraints: Opening hours of each location, each location at\nleast 30minutes, and mustbecompletedbefore 5PM.\n-OptimizationMetric: Shortest path(timeor distance).\nA meta-plan, shown in Table 1, provides the high-level struc -\nture and constraints for the problem. This meta-plan serves as in-\nputtospecializedsolvers,suchasdynamicprogrammingorM onte\nCarlo algorithms, which then generate detailed, executabl e work-\n\ufb02ows.Theprocesstransformsabstractplanningrequiremen tsinto\nconcrete,implementable sequences ofactionswhilerespec tingall\nspeci\ufb01edconstraints and optimizationobjectives.\n3ACM, February, 2025 Longling Gengand EdwardY.Chang\n4.2 P2:Multi-GroupCampus Tour\nProblem Statement: Multiple groups of visitors require guided\ntoursindi\ufb00erentlocationsonauniversitycampus,withopt imized\nscheduling of multipletour guides. This problem shares the same\nmetrics as P1.\nProblem Speci\ufb01cation:\n- Multipleagents (tourguides) mustcoordinatetoservedi\ufb00 erent\ngroups ofvisitors.\n- Each grouphas prede\ufb01ned p", "5 Conclusion": "REAL-Bench represents a signi\ufb01cant step toward systematic ally\nevaluating AI systems\u2019 capabilities in real-world plannin g scenar-\nios. By providing 11 carefully designed problems that progr ess in\ncomplexity,thebenchmark enables researchers to:\n- Assessplanningcapabilitiesinmultipledimensionsofdi \ufb03culty.\n- Test system performanceonreal-world planning challenge s.\n- Evaluatehandlingofunexpectedinterruptionsand adapta tions.\n- Compare di\ufb00erent approaches using standardizedmetrics.\nThebenchmarksaredesignedtobebothtractableforsystema tic\nevaluationandchallengingforcurrentsystems.Eachprobl emcan\nbescaledalongmultipledimensions,includingthenumbero fpar-\nallelthreads,complexityofdependencies,andfrequencyo fdisrup-\ntions, allowing researchers to progressively stress-test their sys-\ntems.Inclusionofvalidationmetricsandbaselineimpleme ntations\nfacilitates meaningful comparisons betweendi\ufb00erent appr oaches.\nLookingahead,weenvisionthisbenchmarksuiteevolvingwi th\ncommunity contributions and feedback. Future extensions m ight\nincludemorecomplexscenarios,additionalevaluationmet rics,and\n8REALM-Bench: AReal-WorldPlanning Benchmark\nfor LLMs andMulti-Agent Systems ACM, February, 2025\nexpandedvalidationtools.Forinstance,inmanywork\ufb02ows, trans-\nactionpropertiesmustbepreserved:\n-Atomicity : An operation either completes entirely or not at all,\nwith no partial executionstate (e.g., a ride-sharing trip m ust ei-\nther completefullyor becanceled entirely).\n-Idempotency :Multiple identical requests producethe same out-\ncomeasasinglerequest,preventingduplicateactions(e.g .,mul-\ntiple identical order submissions should not result in mult iple\norders).\nMost importantly,by providing a commonframework for eval-\nuating planning capabilities of both individual LLMs and mu lti-\nagent systems, we hope to accelerate progress toward more ro -\nbust and capable AI planning systems that can handle real-wo rld\ncomplexity and uncertainty. The REALM benchmark suite, alo ng\nwithdetaileddocumentationandbaselineimplementations ,willbe\navailableasanopensourceresourceafterthepeerreview pr ocess.", "References": "[1] Anthropic. 2024. ClaudeTechnical Report. https://www .anthropic.com\n[2] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah , Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Giri sh Sastry,Amanda\nAskell, et al. 2020. Language Models are Few-Shot Learners. arXiv preprint\narXiv:2005.14165 (2020).\n[3] Edward Y. Chang. 2023. CRIT: Prompting Large Language Mo dels With the\nSocratic Method. IEEE13/u1D461\u210eAnnual Computing and Communication Workshop\nand Conference (March2023).\n[4] EdwardYChang.2023. ExaminingGPT-4\u2019sCapabilitiesan dEnhancement with\nSocraSynth. In The10/u1D461\u210eInternational Conf.on Computational Science and Com-\nputational Intelligence .\n[5] Edward Y. Chang. 2024. Multi-LLM Agent Collaborative Intelli-\ngence: The Path to Arti\ufb01cial General Intelligence . SocraSynth.com.\nhttps://www.amazon.com/dp/1962463079\n[6] Edward Y. Chang. 2025. MACI: Multi-Agent Collaborative Intelligence\nfor Adaptive Reasoning and Temporal Planning. arXiv:2501. 16689 [cs.AI]\nhttps://arxiv.org/abs/2501.16689\n[7] JunzheChen,Xuming Hu,Shuodi Liu,ShiyuHuang,Wei-Wei Tu,ZhaofengHe,\nandLijieWen.2024. LLMArena:AssessingCapabilitiesofLa rgeLanguageMod-\nels in Dynamic Multi-Agent Environments. In Proceedings of ACL . Association\nforComputational Linguistics,13055\u201313077. doi:10.1865 3/v1/2024.acl-long.705\n[8] Zheng Chu, Jingchang Chen, Qianglong Chen, Weijiang Yu, Haotian Wang,\nMing Liu, and Bing Qin. 2023. Timebench: A Comprehensive Eva luation of\nTemporal Reasoning AbilitiesinLargeLanguageModels. ArXiv preprint (2023).\narXiv:2311.17667\n[9] DeepSeek-AI, Daya Guo, Dejian Yang, and more. 2025. Deep Seek-R1:\nIncentivizing Reasoning Capability in LLMs via Reinforcem ent Learning.\narXiv:2501.12948[cs.CL] https://arxiv.org/abs/2501.1 2948\n[10] J. Doe and R. Roe. 2022. The Dynamic Planning Competitio n 2022: Challenges\nandResults.In ProceedingsoftheInternationalConferenceonAutomatedPl anning\nand Scheduling (ICAPS) .ICAPS, Virtual.\n[11] Longling Geng and Edward Y. Chang. 2025. REALM-Bench Gi thub Reposi-\ntory: A Real-World Planning Benchmark for LLMs and Multi-Ag ent Systems.\nhttps://github.com/genglongling/REALM-Bench\n[12] Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. 2020.\nThe Curious Case of Neural Text Degeneration. arXiv:1904.0 9751 [cs.CL]\nhttps://arxiv.org/abs/1904.09751\n[13] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao W ang, Defu\nLian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Und er-\nstanding the planning of LLM agents: A survey. arXiv:2402.0 2716 [cs.AI]\nhttps://arxiv.org/abs/2402.02716\n[14] OmarKhattab, Arnav Singhvi, Paridhi Maheshwari,Zhiy uanZhang, and more.\n2023. DSPy: Compiling Declarative Language Model Calls int o Self-Improving\nPipelines. arXiv:2310.03714[cs.CL] https://arxiv.org/ abs/2310.03714\n[15] LangChain AI. 2024. LangGraph: Building Structured Ap plications with LLMs.\nhttps://github.com/langchain-ai/langgraph.\n[16] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmit rii Khizbullin,\nand Bernard Ghanem. 2023. CAMEL: Communicative Agents for \" Mind\"\nExploration of Large Language Model Society. arXiv:2303.1 7760 [cs.AI]\nhttps://arxiv.org/abs/2303.17760[17] E. Miller and F. Davis.2023. Automated Negotiation Age nts Competition 2023:\nBenchmarkingNegotiationStrategies.In ProceedingsoftheInternationalConfer-\nence on Autonomous Agents and Multiagent Systems (AAMAS) . IFAAMAS, Vir-\ntual.\n[18] Joao Moura. 2024. CrewAI Framework.\nhttps://github.com/joaomdmoura/crewAI.\n[19] OpenAI.2024. HelloGPT-4o. https://openai.com/inde x/hello-gpt-4o/ Accessed:\nJan.30, 2025.\n[20] Bart Prystawski, M. Y. Li, and Noah Goodman. 2023. Why th ink step by step?\nReasoning emergesfromthe locality of experience. In NeurIPS.\n[21] Yongliang Shen et al. 2023. Taskbench: Benchmarking la rge language models\nfor taskautomation. arXiv preprint arXiv:2311.18760 (2023).\n[22] A. Smith and B. Johnson. 2020. The Process Planning Comp etition 2020: An\nOverview.In Proceedingsof theInternational ConferenceonAutomated Pl anning\nand Scheduling (ICAPS) .ICAPS, Virtual.\n[23] Ayal Taitler, Ron Alford, Joan Espasa, Gregor Behnke, D aniel Fi\u0161er, Michael\nGimelfarb,FlorianPommerening,ScottSanner,EnricoScal a,DominikSchreiber,\nJavier Segovia-Aguas, and Jendrik Seipp. 2024. The 2023 Int ernational Plan-\nning Competition. AI Magazine 45, 2 (2024), 280\u2013296. doi:10.1002/aaai.12169\narXiv:https://onlinelibrary.wiley.com/doi/pdf/10.10 02/aaai.12169\n[24] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui W u, Jean-Baptiste\nAlayrac,JiahuiYu,RaduSoricut,JohanSchalkwyk,etal.20 23. Gemini:AFamily\nof Highly CapableMultimodal Models. arXiv:2312.11805[cs .CL]\n[25] Qingyun Wu, GaganBansal, Jieyu Zhang, Yiran Wu, and Chi Wang. 2024. Au-\ntoGen: Enabling Next-GenLLMApplications viaMulti-Agent Conversation.In\nCOLM2024 .\n[26] XAgent Team.2023. XAgent: AnAutonomous AgentforComp lexTaskSolving.\nhttps://github.com/OpenBMB/XAgent.\n[27] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, an d Mike\nLewis. 2024. E\ufb03cient Streaming Language Models with Attent ion Sinks.\narXiv:2309.17453[cs.CL] https://arxiv.org/abs/2309.1 7453\nAppendices\nIn these Appendices, we provide sample implementations of s e-\nlectedproblems,illustratingthateach problemspeci\ufb01cat ionis im-\nplementable and can produce feasible solutions. This serve s as a\nveri\ufb01cation of completeness for the problem de\ufb01nitions. We en-\ncourage readers to review the code and use it as a reference fo r\ndesigning improved solutionstothese challenges.\nThe three problems selected are P3,P4, andP11, representing\nsequentialplanning,reactiveplanning,andthemostcompl explan-\nning scenario, respectively.\nA P3& P4: UrbanRide SharingSample\nImplementation\nThis appendix presents an implementation of P3andP4: Urban\nRideSharing withoutand with interrupts,usingLangGraph [ 15].\nA.1 Agentic Work\ufb02owFormulation\nIn this \ufb01rst stage, we de\ufb01ne agents to manage the nodes of the\nwork\ufb02ow, including data collection, route planning, vehic le dis-\npatch,tra\ufb03cadjustment,monitoringandalert,andlogging agents.\nAt the end, we use the \u00bbsyntax to specify dependencies among\nagents.Thetransitionfromproblemspeci\ufb01cationstowork\ufb02 owfor-\nmulationis handled automaticallybyMACI[6]in LangGraph.\nListing 1: CollaborativeAgents andPrompts\n# ---- Data Collection Agent ---- #\nDC_Agent = Agent(\nname=/grave.ts1/grave.ts1Data Collection Agent '',\nbackstory= /grave.ts1/grave.ts1You collect basic traffic data, road\nclosure updates, andestimated travel times\nbetween locations inBay Area suburb. '',\ntask_description= /grave.ts1/grave.ts1Retrieve traffic conditions,\nroad closures, andestimated travel\ndurations for all routes involved in\npassenger transport. '',\n9ACM, February, 2025 Longling Gengand EdwardY.Chang\ntask_expected_output= /grave.ts1/grave.ts1Structured travel time\ndata, including:\n1) CityMap: A graph G = (V, E), where the\nlocations V androads E have distances and\ntravel times.\n2) Ride Requests: A setof requests R, each\ndefined by: PassengerID, pickup/drop-off\nlocations, andtime windows.\n3) Vehicles: A setof available vehicles K, each\nwith location, battery/fuel level, passenger\ncapacity, andspeed.'')\n# ---- Route Planning Agent ---- #\nRP_Agent = Agent(\nname=/grave.ts1/grave.ts1Route Planning Agent '',\nbackstory= /grave.ts1/grave.ts1You determine optimal routes for\nvehicles to minimize total travel time while\nensuring allpassengers arrive on time. '',\ntask_description= /grave.ts1/grave.ts1Use traffic data and\nconstraints to compute the best routes for\neach vehicle, ensuring on-time airport\narrivals. '',\ntask_expected_output= /grave.ts1/grave.ts1Optimized vehicle\nassignments andtravel routes. '')\n# ---- Vehicle Dispatch Agent ---- #\nVD_Agent = Agent(\nname=/grave.ts1/grave.ts1Vehicle Dispatch Agent '',\nbackstory= /grave.ts1/grave.ts1You assign passengers to vehicles and\nensure each vehicle follows the optimal\nplanned route, '',\ntask_description= /grave.ts1/grave.ts1Assign passengers to vehicles\nbased on capacity constraints androute\nefficiency, ''\ntask_expected_output= /grave.ts1/grave.ts1Vehicle assignment list\nanddispatch schedule. '')\n# ---- Traffic Adjustment Agent (e.g. disruptions,\nspecial cases)---- #\nTA_Agent = Agent(\nname=/grave.ts1/grave.ts1Traffic Adjustment Agent '',\nbackstory= /grave.ts1/grave.ts1You monitor live traffic updates and\nadjust vehicle routes dynamically incase of\ndelaysinBay Area suburb. '',\ntask_description= /grave.ts1/grave.ts1Recompute routes inreal time\nwhen disruptions occur (traffic, road\nclosures), ensuring minimal delays. '',\ntask_expected_output= /grave.ts1/grave.ts1Updated travel plans for\naffected vehicles. '')\n# ---- Monitoring & Alert Agent ---- #\nMA_Agent = Agent(\nname=/grave.ts1/grave.ts1Monitoring & Alert Agent '',\nbackstory= /grave.ts1/grave.ts1You track vehicle movements and\nnotifyifthere are risks of missing\npassenger deadlines inBay Area suburb. '',\ntask_description= /grave.ts1/grave.ts1Send alerts forpotential\ndelaysandrecommend contingency plans. '',\ntask_expected_output= /grave.ts1/grave.ts1Timely notifications for\nalternative route adjustments oremergency\nresponses. '')\nWriter_agent = Agent(\nname=/grave.ts1/grave.ts1Writer Agent '',\nbackstory= /grave.ts1/grave.ts1You are a language model specialized\ninwriting text into .json files '',\ntask_description= /grave.ts1/grave.ts1Write the json response into\n./p3_output.json '',\ntask_expected_output= /grave.ts1/grave.ts1A .json filecontaining\nthe given string,\ntools=write_str_to_txt '')\n# ---- Define Dependencies (With Disruption) ---- #\nDC_Agent >> RP_Agent >> VD_Agent >> TA_Agent >>\nMA_Agent >> Writer_agent# ---- Define Dependencies (Without Disruption) ----\n#\nDC_Agent >> RP_Agent >> VD_Agent >> MA_Agent >>\nWriter_agent\nA.2 P3:Execute Meta PlanwithoutDisruption\nNow that the meta-plan work\ufb02ow has been constructed, the sec -\nondstepinvolves providingrealdataforwork\ufb02owexecution .The\nfollowing code snippet illustrates how vehicle and passeng er lo-\ncations are speci\ufb01ed, followed by the corresponding agent e xecu-\ntions.\nListing2: SampleJSONOutputfrom AgenticWork\ufb02ow\n==================================================\nRUNNING AGENT: Data Collection Agent\n==================================================\n1) \\textbf{CityMap: A graph G=(V,E)}\n- \\textbf{Locations V:}\n- V1: Downtown\n- V2: Golden Gate Park\n- V3: Fisherman Wharf\n- V4: Union Square\n- V5: San Francisco Airport (SFO)\n- \\textbf{Roads E with distances and travel times:}\n- E1: Downtown to Golden Gate Park, Distance: 5\nmiles, Travel Time: 20 minutes\n- E2: Golden Gate Park to Fisherman Wharf, Distance:\n6 miles, Travel Time: 25 minutes\n- E3: Fisherman Wharf to Union Square, Distance: 2\nmiles, Travel Time: 10 minutes\n- E4: Union Square to SFO, Distance: 15 miles,\nTravel Time: 30 minutes\n- E5: Downtown to SFO, Distance: 13 miles, Travel\nTime: 35 minutes\n2) \\textbf{Ride Requests: A set of requests R}\n- Request 1:\n- PassengerID: P001\n- Pickup: Downtown\n- Drop-off: Union Square\n- Time Window: 9:00 AM - 9:30 AM\n- Request 2:\n- PassengerID: P002\n- Pickup: Golden Gate Park\n- Drop-off: SFO\n- Time Window: 10:00 AM - 11:00 AM\n- Request 3:\n- PassengerID: P003\n- Pickup: Fisherman Wharf\n- Drop-off: Downtown\n- Time Window: 1:00 PM - 1:30 PM\n3) \\textbf{Vehicles: A set of available vehicles K}\n- Vehicle 1:\n- Location: Downtown\n- Battery/Fuel Level: 80%\n- Passenger Capacity: 4\n- Speed: 30 mph\n- Vehicle 2:\n- Location: Union Square\n- Battery/Fuel Level: 60%\n- Passenger Capacity: 6\n- Speed: 25 mph\n- Vehicle 3:\n- Location: SFO\n- Battery/Fuel Level: 90%\n- Passenger Capacity: 3\n- Speed: 35 mph\nThis structured data provides an overview of the current\ntraffic conditions, road closures, and estimated\ntravel times forpassenger transport in SF.\n10REALM-Bench: AReal-WorldPlanning Benchmark\nfor LLMs andMulti-Agent Systems ACM, February, 2025\n==================================================\nRUNNING AGENT: Route Planning Agent\n==================================================\nOptimized vehicle assignments and travel routes:\n1) \\textbf{Vehicle Assignments:}\n- Vehicle 1:\n- Assigned to Request 1\n- Route: Downtown (V1) -> Union Square (V4)\n- Estimated Travel Time: 20 minutes (E1 + E3)\n- Departure Time: 9:00 AM\n- Arrival Time: 9:20 AM\n- Vehicle 2:\n- Assigned to Request 2\n- Route: Union Square (V4) -> Golden Gate Park (V2)\n-> SFO (V5)\n- Estimated Travel Time: 55 minutes (E3 + E2 + E4)\n- Departure Time: 10:00 AM\n- Arrival Time: 10:55 AM\n- Vehicle 3:\n- Assigned to Request 3\n- Route: Fisherman Wharf (V3) -> Downtown (V1)\n- Estimated Travel Time: 10 minutes (E3)\n- Departure Time: 1:00 PM\n- Arrival Time: 1:10 PM\n2) \\textbf{Travel Routes:}\n- Request 1:\n- Vehicle 1 will pick up Passenger P001 at Downtown\nand drop off at Union Square.\n- Route: V1 -> V4\n- Request 2:\n- Vehicle 2 will pick up Passenger P002 at Golden\nGate Park and drop off at SFO.\n- Route: V4 -> V2 -> V5\n- Request 3:\n- Vehicle 3 will pick up Passenger P003 at Fisherman\nWharf and drop off at Downtown.\n==================================================\nRUNNING AGENT: Vehicle Dispatch Agent\n==================================================\nVehicle Assignment List and Dispatch Schedule:\n1) \\textbf{Vehicle Assignments:}\n- Vehicle 1:\n- Assigned to Request 1\n- Passengers: P001\n- Route: Downtown (V1) -> Union Square (V4)\n- Estimated Travel Time: 20 minutes\n- Departure Time: /grave.ts1/grave.ts19:00 AM''\n- Arrival Time: /grave.ts1/grave.ts19:20 AM''\n- Vehicle 2:\n- Assigned to Request 2\n- Passengers: P002\n- Route: Union Square (V4) -> Golden Gate Park (V2)\n-> SFO (V5)\n- Estimated Travel Time: 55 minutes\n- Departure Time: /grave.ts1/grave.ts110:00 AM ''\n- Arrival Time: /grave.ts1/grave.ts110:55 AM ''\n- Vehicle 3:\n- Assigned to Request 3\n- Passengers: P003\n- Route: Fisherman Wharf (V3) -> Downtown (V1)\n- Estimated Travel Time: 10 minutes\n- Departure Time: /grave.ts1/grave.ts11:00 PM''\n- Arrival Time: /grave.ts1/grave.ts11:10 PM''2) \\textbf{Dispatch Schedule:}\n- 9:00 AM: Vehicle 1 departs from Downtown with\nPassenger P001.\n- 9:20 AM: Vehicle 1 arrives at Union Square.\n- 10:00 AM: Vehicle 2 departs from Union Square with\nPassenger P002.\n- 10:55 AM: Vehicle 2 arrives at SFO.\n- 1:00 PM: Vehicle 3 departs from Fisherman Wharf with\nPassenger P003.\n- 1:10 PM: Vehicle 3 arrives at Downtown.\nThese assignments and schedules ensure efficient use of\nvehicles and timely arrival of all passengers.\n==================================================\nRUNNING AGENT: Monitoring & Alert Agent\n==================================================\n\\textbf{Alert Notification:}\n1) Vehicle 1:\n- Current Status: On schedule.\n- Recommendation: No action needed as the vehicle is\nexpected to arrive on time.\n2) Vehicle 2:\n- Current Status: Potential delay risk due to high\ntraffic expected around Golden Gate Park.\n- Recommendation: Consider adjusting the route to\navoid traffic congestion. Possible alternative:\nTake the route via Sunset Blvd to bypass heavy\ntraffic areas. Notify the passenger of the\npotential delay and the alternative route.\n3) Vehicle 3:\n- Current Status: On schedule.\n- Recommendation: No action needed as the vehicle is\nexpected to arrive on time.\n\\textbf{Contingency Plan:}\nEmergency Response: If Vehicle 2 faces unexpected delays\ndespite the alternative route, prepare a standby\nvehicle forimmediate dispatch from a nearby\nlocation to ensure Passenger P002 reaches SFO on\ntime.\n==================================================\nRUNNING AGENT: Writer Agent\n==================================================\nThought: I need to write the given context string into a\nJSON file named /grave.ts1/grave.ts1./p3_output.json ''.\nUsing Tool: write_str_to_txt\nTool call dict: {omitted}\nData successfully written to /grave.ts1/grave.ts1./p3_output.json ''\nTool result: None\nObservations: {0: None}\nThe context string has been successfully written into\n/grave.ts1/grave.ts1./p3_output.json ''as a JSON file.\nA.3 P4:Execute Meta PlanwithDisruption\nThissectiondepictstheexecutionofthemeta-plan(work\ufb02o w)un-\ndertra\ufb03cdisruption.Inthisscenario,thesystemdynamica llyiden-\nti\ufb01es tra\ufb03c delays and integrates real-time updates into th e plan-\nning process. Each agent plays a crucial role: the data colle ction\nagentcontinuouslymonitorstra\ufb03cconditions;theroutepl anning\nagent adjusts travel routes based on congestion data; and th e dis-\npatchagentensuresthatvehicleassignmentsareoptimized despite\ndelays. The work\ufb02ow demonstrates how coordinated agent int er-\nventions, including dynamic rerouting and schedule adjust ments,\n11ACM, February, 2025 Longling Gengand EdwardY.Chang\ne\ufb00ectivelymitigatetheimpactofheavytra\ufb03c,maintaining timely\ntransportation,and enhancing overall operationale\ufb03cien cy.\nListing 3: SampleJSONOutputfrom AgentPipeline\n==================================================\nRUNNING AGENT: Traffic Adjustment Agent\n==================================================\n1) \\textbf{Vehicle V1 (Passenger 101):}\n- \\textbf{Current Disruption:} Heavy traffic reported\non the route from Downtown SF to SF Airport.\n- \\textbf{Updated Route:}\n- Start at Union Square\n- Travel to Downtown SF (2 miles, 15 minutes)\n- Detour via I-280 South instead of the usual route\nto SF Airport\n- Estimated travel time via detour: 30 minutes\n- \\textbf{Total Updated Estimated Travel Time:} 45\nminutes\n- \\textbf{Revised Departure Time:} /grave.ts1/grave.ts108:15''to ensure\ntimely arrival within the window.\n- \\textbf{Special Instructions:} Maintain\ncommunication with the passenger about the\nupdated route and expected arrival time.\n2) \\textbf{Vehicle V2 (Passenger 102):}\n- \\textbf{Current Disruption:} Road closure near\nFisherman Wharf due to an event.\n- \\textbf{Updated Route:}\n- Start at Fisherman Wharf\n- Detour via Bay Street to Van Ness Avenue, then\nproceed to Golden Gate Bridge\n- Estimated travel time via detour: 15 minutes\n- \\textbf{Total Updated Estimated Travel Time:} 15\nminutes\n- \\textbf{Revised Departure Time:} /grave.ts1/grave.ts109:15''to ensure\ntimely arrival within the window.\n- \\textbf{Special Instructions:} Ensure fuel levels\nare adequate forthe detour and communicate any\nchanges to the passenger.\nB P11:StockPrediction Sample\nImplementation on LangGraph\nThisappendixpresentsanimplementationoftheP11StockPr edic-\ntion problem using LangGraph [15]. The implementation demo n-\nstrates how AI-driven planning can be applied to \ufb01nancial fo re-\ncastingbyleveraginghistoricaldata,technicalindicato rs,andreal-\ntimemarket updates.\nB.1 Agentic Work\ufb02owFormulation\nIn this \ufb01rst stage, we de\ufb01ne agents to manage the nodes of the\nwork\ufb02ow,includingdatacollection,featureextraction,m odeltrain-\ning,integration,andalert.Intheend,weusethe \u00bbsyntaxtospecify\ndependencies amongagents.\nListing4: Promptand CollaborativeAgent Pipeline\nfromsrc.multi_agent.crew importCrew\nwith Crew() as crew:\n# ---- Data Collection Agent ---- #\nDC_Agent = Agent(\nname=/grave.ts1/grave.ts1Data Collection Agent ''\nbackstory= /grave.ts1/grave.ts1You collect 5 stocks of historical\ndata OHLCV fromS&P 500 Kaggle Dataset at\nhttps://www.kaggle.com/datasets/andrewmvd/sp\n-500-stocks, andreal-time stock data\nincluding external economic indicators. '',\ntask_description= /grave.ts1/grave.ts1Retrieve stock data from\n2023-10-01 to 2023-12-31. '',\ntask_expected_output= /grave.ts1/grave.ts1Formatted OHLCV data and\nexternal indicators, of 5 stocks. '',)\nWriter_agent = Agent(\nname=/grave.ts1/grave.ts1Writer Agent '',\nbackstory= /grave.ts1/grave.ts1You are a language model specialised\ninwriting text into .json files '',\ntask_description= /grave.ts1/grave.ts1Write the json response into\n./tool_agent_example.json '',\ntask_expected_output= /grave.ts1/grave.ts1A .json filecontaining\nthe given string '',\ntools=write_str_to_txt,\n)\n# ---- Feature Extraction Agent ---- #\nFE_Agent = Agent(\nname=/grave.ts1/grave.ts1Feature Extraction Agent '',\nbackstory= /grave.ts1/grave.ts1You compute technical indicators from\nthe collected 5 stock data. '',\ntask_description= /grave.ts1/grave.ts1Calculate MA, MACD, RSI,\nBollinger Bands, andother technical\nindicators of 5 stocks. '',\ntask_expected_output= /grave.ts1/grave.ts1Feature matrix of OHLCV,\nexternal indicators, MA, MACD, RSI,\nBollinger Bands, of 5 stocks. ''\n)\n# ---- Model Training Agent ---- #\nMT_Agent = Agent(\nname=/grave.ts1/grave.ts1Model Training Agent '',\nbackstory= /grave.ts1/grave.ts1You train stock prediction models\nusing historical data ( from2023-10-01 to\n2023-11-30) andvalidate with ( from\n2023-12-1 to 2024-12-31). '',\ntask_description= /grave.ts1/grave.ts1use financial math andmachine\nlearning models andoptimize\nhyperparameters. '',\ntask_expected_output= /grave.ts1/grave.ts1Prepare the inputFeature\nmatrix, with 3 financial math models and3\nmachine learning models ready forinference,\nof 5 stocks. ''\n)\n# ---- Prediction Generation Agent ---- #\nPG_Agent = Agent(\nname=/grave.ts1/grave.ts1Prediction Generation Agent '',\nbackstory= /grave.ts1/grave.ts1You generate stock price predictions\nusing the trained model. '',\ntask_description= /grave.ts1/grave.ts1Predict next-dayandintra-day\nstock prices. '',\ntask_expected_output= /grave.ts1/grave.ts1Based on input and the 6\nmodels, predict Stock price from2024-1-1 to\n2024-1-7, with confidence intervals, of 5\nstocks.''\n)\n# ---- Integration Agent ---- #\nIG_Agent = Agent(\nname=/grave.ts1/grave.ts1Integration Agent '',\nbackstory= /grave.ts1/grave.ts1You combine model predictions with\nexternal news sentiment andmacroeconomic\nindicators. '',\ntask_description= /grave.ts1/grave.ts1Adjust predictions based on\nnews impact andmarket conditions on year of\n2023.'',\ntask_expected_output= /grave.ts1/grave.ts1Refined Stock price from\n2024-1-1 to 2024-1-7, with adjusted\nconfidence intervals, of 5 stocks. ''\n)\n# ---- Alert Generation Agent ---- #\nAG_Agent = Agent(\nname=/grave.ts1/grave.ts1Alert Generation Agent '',\nbackstory= /grave.ts1/grave.ts1You issue buy/sell/hold signals based\non predictions andrisk assessment of the\nmodel'', task_description= /grave.ts1/grave.ts1Generate trading\nalertsandconfidence scores '',\ntask_expected_output= /grave.ts1/grave.ts1Based on the 6 models,\nprovide actionable trade signals from\n2024-1-1 to 2024-1-7, of 5 stocks. ''\n)\n12REALM-Bench: AReal-WorldPlanning Benchmark\nfor LLMs andMulti-Agent Systems ACM, February, 2025\n# ---- Define Dependencies ---- #\nDC_Agent >> Writer_agent >> FE_Agent >> MT_Agent #\nTechnical Analysis Flow\nMT_Agent >> PG_Agent >> AG_Agent # Model Chain\n#AG_Agent >> DC_Agent # Feedback Loop (adaptive\nupdates)\ncrew.run()\nB.2 Work\ufb02owExecution With Historical Data\nforPrediction\nFollowing the dependency graph presented above (also see Fi g-\nure1),theagents areexecutedinsequence:startingwithda tacol-\nlection,thenfeatureextraction,followedbymodeltraini ng,and\ufb01-\nnallyprediction.Forbrevity,whiledatafor\ufb01vestocksisp rovided,\nwe illustratetheexecution\ufb02ow using onlyAPPL.\nListing5: SampleJSONOutputfrom DataCollection\n==================================================\nRUNNING AGENT: Data Collection Agent\n==================================================\njson\n{\n/grave.ts1/grave.ts1stocks'': [\n{\n/grave.ts1/grave.ts1symbol'':/grave.ts1/grave.ts1AAPL'',\n/grave.ts1/grave.ts1data'': [\n{\ndate: 2023-10-01,\nopen: 150.00,\nhigh: 152.00,\nlow: 149.50,\nclose: 151.00,\nvolume: 75000000\n},\n// Additional daily data up to 2023-12-31\n],\n/grave.ts1/grave.ts1external_indicators '': {\ninterest_rate: 5.25,\ninflation_rate: 3.0,\ngdp_growth: 2.5\n}\n},\n{\n/grave.ts1/grave.ts1symbol'':/grave.ts1/grave.ts1MSFT'',\n/grave.ts1/grave.ts1data'': [\n{\ndate: 2023-10-01,\nopen: 280.00,\nhigh: 285.00,\nlow: 279.00,\nclose: 282.00,\nvolume: 45000000\n},\n// Additional daily data up to 2023-12-31\n],\n/grave.ts1/grave.ts1external_indicators '': {\ninterest_rate: 5.25,\ninflation_rate: 3.0,\ngdp_growth: 2.5\n}\n},\n{\n/grave.ts1/grave.ts1symbol'':/grave.ts1/grave.ts1GOOGL'',\n/grave.ts1/grave.ts1data'': [\n{\ndate: 2023-10-01,\nopen: 135.00,\nhigh: 137.00,\nlow: 134.50,\nclose: 136.00,volume: 32000000\n},\n// Additional daily data up to 2023-12-31\n],\n/grave.ts1/grave.ts1external_indicators '': {\ninterest_rate: 5.25,\ninflation_rate: 3.0,\ngdp_growth: 2.5\n}\n},\n{\n/grave.ts1/grave.ts1symbol'':/grave.ts1/grave.ts1AMZN'',\n/grave.ts1/grave.ts1data'': [\n{\ndate: 2023-10-01,\nopen: 120.00,\nhigh: 122.50,\nlow: 119.00,\nclose: 121.50,\nvolume: 60000000\n},\n// Additional daily data up to 2023-12-31\n],\n/grave.ts1/grave.ts1external_indicators '': {\ninterest_rate: 5.25,\ninflation_rate: 3.0,\ngdp_growth: 2.5\n}\n},\n{\n/grave.ts1/grave.ts1symbol'':/grave.ts1/grave.ts1TSLA'',\n/grave.ts1/grave.ts1data'': [\n{\ndate: 2023-10-01,\nopen: 250.00,\nhigh: 255.00,\nlow: 248.00,\nclose: 252.00,\nvolume: 50000000\n},\n// Additional daily data up to 2023-12-31\n],\n/grave.ts1/grave.ts1external_indicators '': {\ninterest_rate: 5.25,\ninflation_rate: 3.0,\ngdp_growth: 2.5\n}\n}\n]\n}\n==================================================\nRUNNING AGENT: Feature Extraction Agent\n==================================================\nTo complete the task of calculating technical indicators\nforfive stocks, perform the following computations\nforeach stock:\nMoving Average (MA): Compute the simple moving\naverage over a specified period (e.g., 20 days).\nMoving Average Convergence Divergence (MACD): Compute\nthe MACD line, signal line, and histogram.\nRelative Strength Index (RSI):} Compute the RSI over\na specified period (e.g., 14 days).\nBollinger Bands: Compute the upper band, lower band,\nand the middle band (the moving average).\nOther Technical Indicators: Optionally, compute\nadditional indicators such as Average True Range\n(ATR), Stochastic Oscillator, etc., based on\nthe available data.\n==================================================\nRUNNING AGENT: Model Training Agent\n==================================================\n13ACM, February, 2025 Longling Gengand EdwardY.Chang\nTo prepare the input feature matrix with financial math\nmodels and machine learning models ready for\ninference, we will follow these steps:\n1. Feature Matrix Preparation:\n- Collect historical stock data forthe 5 selected\nstocks, including the following columns: Date,\nOpen, High, Low, Close, and Volume.\n- Calculate the necessary financial math models (\ntechnical indicators) foreach stock:\n- Moving Average (MA): Compute the simple moving\naverage over a 20-day period.\n- Moving Average Convergence Divergence (MACD):\nCalculate the MACD line, signal line, and\nhistogram.\n- Relative Strength Index (RSI): Calculate the RSI\nover a 14-day period.\n- Bollinger Bands: Calculate the upper band, middle\nband (moving average), and lower band.\n2. Machine Learning Models:\n- Train three machine learning models using the\nhistorical data and calculated features. The\nmodels could be:\n- Linear Regression: For predicting future stock\nprices based on the features.\n- Random Forest Regressor: To capture non-linear\nrelationships and interactions between features\n.\n- Support Vector Machine (SVM): For robust\npredictions with a focus on margin maximization\n.\n- Optimize hyperparameters foreach model using\ntechniques such as Grid Search or Random Search\nto enhance model performance.\n3. Inference Preparation:\n- Prepare the models forinference by saving the\ntrained models and their hyperparameters.\n- Ensure the feature matrix is structured and aligned\nby date, ready forinput into the models for\nprediction.\n4. Output Format:\n- Compile the results into a structured data format\nsuch as a DataFrame or JSON, including all\ncalculated features and ready-to-use models for\neach of the 5 stocks.\nBy following these steps, we ensure that the feature\nmatrix is comprehensive and that the machine\nlearning models are optimized and ready for\ninference on the selected stocks.\n==================================================\nRUNNING AGENT: Prediction Generation Agent\n==================================================\nTo predict the stock prices from January 1, 2024, to\nJanuary 7, 2024, forthe five selected stocks, we\nutilize the prepared feature matrix and the trained\nmachine learning models as described in the context.\nHere is the structured prediction output of one of\nthe five stocks, including confidence intervals:\njson\n{\n/grave.ts1/grave.ts1predictions '': [\n{\n/grave.ts1/grave.ts1stock'':/grave.ts1/grave.ts1APPL'',\n/grave.ts1/grave.ts1predictions '': [\n{date: 2024-01-01, predicted_price: 150.25,\nconfidence_interval: [148.00, 152.50],\nconfidence: 0.80},\n{date: 2024-01-02, predicted_price: 151.00,\nconfidence_interval: [148.75, 153.25],\nconfidence: 0.80},{date: 2024-01-03, predicted_price: 152.50,\nconfidence_interval: [150.00, 155.00],\nconfidence: 0.88},\n{date: 2024-01-04, predicted_price: 153.75,\nconfidence_interval: [151.50, 156.00],\nconfidence: 0.82},\n{date: 2024-01-05, predicted_price: 151.50,\nconfidence_interval: [150.25, 153.75],\nconfidence: 0.90},\n{date: 2024-01-06, predicted_price: 155.00,\nconfidence_interval: [152.75, 157.25],\nconfidence: 0.83},\n{date: 2024-01-07, predicted_price: 156.25,\nconfidence_interval: [154.00, 158.50],\nconfidence: 0.87}\n]\n},\n]\n}\nThis output provides the predicted stock prices foreach\nday from January 1 to January 7, 2024, forstock\nAPPL, along with their respective confidence\nintervals. These predictions are based on the\ntrained models and the prepared feature matrix.\nB.3 Output: AlertGeneration\nBasedontheproblemspeci\ufb01cationinSection4.3,whenthepr edic-\ntioncon\ufb01denceis \u226585%,a recommendedtradingactionis sentto\nthesubscriberas analert.\nBasedontheaboveprediction,on1/3,1/5,and1/7thepredic tion\ncon\ufb01denceexceedsthethresholdandthestock\u2019smovingdire ction\nisup,down, andup, respectively. Therefore, thealerts issued be-\nforetheopeningofthemarketonthosethreedaysare: BUY,SELL,\nandSTRONGBUY ,respectively.\n14This figure \"acm-jdslogo.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2502.18836v1This figure \"sample-franklin.png\" is available in \"png\"\n format from:\nhttp://arxiv.org/ps/2502.18836v1"}, "Evaluating LLM-based Agents for Multi-Turn Conversations: A Survey": {"Abstract": "ing with credit is permitted. To copy otherwise, or republish, to post on\nservers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org.\n\u00a92025 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nManuscript submitted to ACM\nManuscript submitted to ACM 1arXiv:2503.22458v1  [cs.CL]  28 Mar 20252 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou", "1 Introduction": "Multi-turn conversational agents powered by large language models (LLMs) mark a significant leap in conversational\nAI [72], offering complex, context-aware interactions across multiple turns\u2014essential for applications like customer\nservice and personal assistants [ 37,233]. Evaluating these systems poses unique challenges: methods must be automated,\nreproducible, and aligned with human judgments to ensure real-world reliability [245].\nHistorically, multi-turn dialogue dates back to ELIZA [ 230], developed in the 1960s, which maintained simple\nstates through pattern matching rather than complex context modeling. Subsequent progress incorporated finite-state\nmachines, frame-based approaches, and statistical methods, culminating in the application of hidden Markov models\n(HMMs) [ 176] for spoken language understanding. Deep neural networks and Transformer architectures [ 215] then\nrevolutionized context modeling, leveraging larger datasets and advanced training methods (including supervised and\nreinforcement learning) to handle more complex dialogues. In nowadays, multi-turn conversational systems often\nbuild on large-scale pre-trained language models, or namely Large Language Models (LLMs) [ 158], enabling advanced\nreasoning, continuity, and context handling over extended interactions.\nChatHi, I want to plan a trip to Europe.Sure. Here are some amazing destinations to choose from:1. Paris, France2. Rome, Italy3. Barcelona, Spain4. Amsterdam, Netherlands.Which destination would you like to explore first?First.Great choice! Paris is an incredible city. When would you like to go?March 15th to March 20th.Please help with accommodations and an itinerary.Here are some excellent boutique hotel options in Paris for your stay:1. Hotel de la PaixExperience the elegance of a bygone era at Hotel de la Paix2. Le Petit PalaceA charming boutique hotel located in the trendy Marais district3. Maison SouquetAward-winning luxury boutique hotel with Belle \u00c9poque d\u00e9corWhich one of these would you like to book? \nUserQuestionAtTurn1AgentReplyAtTurn1\nUserQuestionAtTurn2\nAgentReplyAtTurn2UserQuestionAtTurn3AgentReplyAtTurn3\nTurnMemory[ {\u201crole\u201d: \u201cuser\u201d , \u201ctext\u201d: \u201cHi, Iwantto\u2026 \u201d}, {\u201croler\u201d: \u201cAgent\u201d, \u201ctool_invocation\u201d: \u201csearch_web(query=\u201cEurope famous tourist destinations\u201d),  {\u201crole\u201d:\u201cAgent\u201d,  \u201c t e x t \u201d : \u201cSure! Here are some \u2026. \u201d}, {\u201crole\u201d:\u201duser\u201d,  \u201d t e x t \u201d :  \u201c F i r s t \u201d } , {\u201crole\u201d:\u201dAgent\u201d, \u201ctext\u201d:\u201dGreat.\u201d }][ {\u201ctool_invocation\u201d:\u201csearch_web(query=\u201dboutiquehotelsinParisMarch15thtoMarch20th\"}, \u201cweb_search_result\u201d:[ {\u201ctitle\u201d:\u201cHoteldelaPaix\u201d,\u201curl\u201d:\u201dhttps:\u2026.,\u201csnippet\u201d:\u201cExperiencetheelegance\u2026.}, {\u201ctitle\u201d:\u201cLe... \u201d ,\u201curl\u201d:\u201dhttps\u2026,\u201csnipper\u201d:\u2026.}]]Planning\nConversationMemory\nTool\nStep1. Search Web For RecommendationStep2. Confirm DestinationStep3. Record Travel Dates and Search HotelStep4. Book Hotel Based On User Preference.....\nGenerateResponseGetUserQuestionStoreInformationRetrieveInformationTaskModelingPlanGenerationToolUsageExternalFeedbackContext ProvisionExecution Support\nToolDefinitionSearch_web(query:str)->dictSearchtheinternetwithspecificqueryToolInvocationSearch_web(query=\u201cboutiquehotels\u2026 \u201d)ToolResult[{\u201ctitle\u201d:\u201cHoteldelaPaix\u201d,\u201curl\u201d:\u201chttps}..]Agent\nFig. 1. An example of Multi-turn Conversational Agents based on LLMs", "1.1 LLM-based Multi-turn Conversational Agent": "LLMs, such as ChatGPT [ 158], GPT-4 [ 160], and GPT-o1 [ 161], are essential for multi-turn conversation, enabling\ncoherent responses over extended interactions. Trained on vast text corpora, LLMs predict subsequent parts of a\nconversation based on previous turns, and handle complex linguistic structures while maintaining conversational\ncontext [4], essential for generating appropriate responses throughout extended interactions.\nAgents in multi-turn systems build on LLMs to handle user inputs, interpret the user\u2019s intent [ 102,194], and manage\nconversation flow. They leverage prior turns and external resources (e.g., databases, APIs) to provide up-to-date\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 3\ninformation. Techniques like deep-utterance aggregation refine earlier statements [ 265], while external knowledge\nintegration [ 115] improves dialogue richness. Task-oriented agents help users complete specific goals by bridging user\ninputs, LLM outputs, and system functionalities [41, 89, 175].\nFigure 1 illustrates an agent-based conversational system. Here, the user plans a trip to Europe; the agent suggests\ndestinations, acknowledges a selection (Paris), confirms dates, and provides accommodation options. The agent\u2019s\nplanning process [ 82] breaks the request into actionable steps, narrowing the destination, confirming dates, and searching\nfor relevant hotels. Tool-use [ 56,159,172], such as invoking search_web for internet queries, yields structured data\nintegrated into the final response. Two types of memory [23, 128]\u2013conversation and turn memory\u2013preserve dialogue\ncontext [ 257,259]. The LLM interprets queries, formats responses, and manages complex language tasks [ 153], ensuring\ncoherently adapted answers based on user inputs.\nIdentification Screening Eligibility IncludedRecords identified through database\nsearching  with keywords (LLM -based\nAgent, Evaluation ofLLM -basedAgent\u2026)\n(n=479 )Recent papers published in top \nvenues (ICLR, NeurIPS ,AAAI, NAACL ,\nEMNLP , ACL ) filtered by keywords\n(n=1081 )\nRecords after duplicates removed\n(n=1123 )\nRecords screened (n=790 )Records excluded(n =333 )\nNot focus on Evaluation ,\nagents orbenchmarks\nFull -text articles assessed\nFor eligibility(n =523 )Full -text articles excluded,\nwith reasons(not relevant\ntoLLM -based Agent)\n(n=267 )\nStudies included inqualitative synthesis(n =351 )\nStudies included inqualitative synthesis  (meta -analysis)\n(n=274 )\nFig. 2. Selection Process for Papers Evaluating LLM-based Agents in Multi-turn Conversations", "1.2 The Scope of This Survey": "This paper surveys evaluation methods for LLM-based multi-turn conversational agents, focusing on (1) which aspects to\nevaluate and (2) how to evaluate them . It outlines key targets (task success, response quality, end-to-end user experience)\nand reviews existing methodologies and datasets, encompassing both automatic metrics and human evaluation protocols.\nDrawing on literature from 2017 to 2025, this survey follows a methodology inspired by PRISMA [ 163], outlined in\nFigure 2. An initial Google Scholar search retrieved 479 papers1, supplemented by 1,081 more from venues such as\nICLR, NeurIPS, AAAI, NAACL, EMNLP, and ACL (2022\u20132024). After merging duplicates (1,123 unique records), we\nexcluded 333 irrelevant works, leaving 790 papers; of these, 267 lacked focused coverage of LLM-based agents and were\nexcluded. Ultimately, 351 papers progressed to qualitative synthesis, with 272 offering targeted evaluation insights.\nCovering foundational Transformer-based research (2017\u20132019), the rise of LLMs (2020\u20132022), and recent multi-turn\nevaluation advancements (2023\u20132025), the survey spans approximately 200 papers comprising peer-reviewed conference\n1We use the keywords such as \u201cevaluation\u201d, \u201cperformance assessment\u201d, \u201cbenchmark\u201d, \u201cmulti-turn conversation/dialogue\u201d, \u201cmulti-round conversa-\ntion/dialogue\u201d, \u201cLarge Language Model (LLM)\u201d, \u201cLLM-powered agent\u201d, \u201cLLM-based agent\u201d, \u201cLLM agent\u201d for search.\nManuscript submitted to ACM4 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\nA Survey On Evaluating LLM-based Agents for Multi-Turn ConversationsWhat to evaluate?\nThe evaluation goals\nand targetEvaluating the\nEnd-to-end ExperienceReimann et al. [179], Deriu et al. [46], Hendrycks et al. [75],\nGritta et al. [63], Zheng et al. [270], Kwan et al. [101], Bai et al. [13],\nXu et al. [238], Jang et al. [87], Maharana et al. [138],\nLiu et al. [129], Yu et al. [246]\nEvaluating the Action\nTool-use ComponentsOpenAI [159], Press et al. [172], Gao et al. [56], Xu et al. [239],\nShen et al. [188], Patil et al. [168], Wang et al. [223], Wang et al. [224],\nGuo et al. [66], Zhuang et al. [274], Zhang et al. [261]\nEvaluating the memoryMemory SpansWang et al. [219], Shen et al. [187], Cai et al. [23], Leng et al. [107],\nMaharana et al. [138], Zhang et al. [264], Liu et al. [128],\nCastillo-Bolado et al. [27], Huang et al. [84]\nMemory FormsZhang et al. [263], Zhang et al. [258], Wang et al. [228],\nHuang et al. [80], Li et al. [109], Maharana et al. [138],\nLei et al. [106], Pal et al. [164], Tworkowski et al. [213],\nBae et al. [12], Liang et al. [117], Hoxha and Thanasi-Bo\u00e7e [77],\nCao [24], Wang et al. [222], Park et al. [167],\niunn Ong et al. [86], Zhong et al. [272], Liu et al. [127],\nWang et al. [225], Modarressi et al. [149], Xu et al. [239],\nQin et al. [174], Zhang et al. [252], Li et al. [108],\nTan et al. [208], Fountas et al. [53], Mitchell et al. [147],\nQiao et al. [173], Zeng et al. [247], Tack et al. [206], Mao et al. [139]\nEvaluating the plannerTask ModelingTest\u00f3n and R-Moreno [209], Singh et al. [196], Zhang et al. [260],\nLyu et al. [136], Zhang et al. [250], Xing and Gulla [236],\nLi et al. [110], Fu et al. [55], Wang et al. [229]\nTask DecompositionHuang et al. [82], Wang et al. [227], Shen et al. [188],\nWang et al. [221], Singh et al. [195], Srinivasan et al. [201],\nChen et al. [36], Zhang et al. [255], Wu et al. [232],\nPrasad et al. [171], Dagan et al. [44], Kwon et al. [103]\nAdaptation and ControlTalha Selamet and T\u00fcmer [207], Okubo and Takahashi [156], Hao et al. [71],\nHuang et al. [81], Muise et al. [151]\nReflectionYao et al. [244], Zhang et al. [262], Van et al. [214],\nZheng et al. [271], Madaan et al. [137], Test\u00f3n and R-Moreno [209],\nLi et al. [110], Shi et al. [190], Liu et al. [134],\nSun et al. [204], Wang et al. [217], Gu et al. [64],\nLight et al. [118], Yao et al. [243], Besta et al. [18], Zhao et al. [268],\nXiao and Wang [234], He et al. [74]\nHow to evaluate?\nData and MetricsEvaluation DataConversation Data GenerationLin et al. [120], Olabiyi et al. [157], Zhang et al. [251],\nAbercrombie and Batista-Navarro [2], Goplerud [62], Kim et al. [97],\nMoniri et al. [150] Shim et al. [193], Golany et al. [60], Arcadinho et al. [9],\nLi et al. [114], Gopalakrishnan et al. [61]\nAl-Thani et al. [6], Liu et al. [124]\nAtanasova et al. [11], Tian et al. [210], Setty and Setty [186]\nConversation Data AnnotationMerdivan et al. [143], Liu et al. [133], Deng et al. [45],\nLiu et al. [129], Duan et al. [49]\nXu et al. [239], Shen et al. [188], Wang et al. [223],\nPatil et al. [168]\nSu et al. [203], Mo et al. [148],\nAliannejadi et al. [8] Sathe and Park [185], Wang et al. [226]\nEvaluation MetricsAnnotation-based EvaluationPapineni et al. [165], Post [170], Lin [119], Lavie and Agarwal [104],\nZhang et al. [256], Gao et al. [57], Phy et al. [169], Banerjee et al. [15]\nXing and Gulla [236], Zhang et al. [253],\nYang et al. [241], Dong et al. [47], Wu et al. [231],\nBudzianowski et al. [21], Byrne et al. [22]\nAnnotation-free EvaluationGoh et al. [59], Yadav and Kaushik [240], Zhang et al. [266],\nLi et al. [116], Ren et al. [180], Jia et al. [91],\nLee et al. [105], Park et al. [166],\nLi et al. [111], Yadav and Kaushik [240], Chan et al. [29]\nFig. 3. Taxonomy of Evaluation Approaches for LLM-Based Multi-Turn Conversational Agents: A Comprehensive Survey of Goals,\nMethodologies, and Future Directions.\narticles (40%), journal articles (25%), preprints (20%), and industry reports (15%). Across diverse evaluation data, metrics,\nand methods, it presents a balanced overview of the rapidly evolving field of the evaluation methodologies for LLM-based\nmulti-turn conversational agents.", "1.3 Our Taxonomy": "Compared to prior research, our taxonomy (a brief representation shown in Figure 3) reviews the literature that studies\nthe evaluation of LLM-based multi-turn conversational agents by systematically addressing three key dimensions:\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 5\n\u2022What to Evaluate? Comprehensive Evaluation Goals: Defining broad objectives to assess agent capabilities\nacross various tasks and interactions. This includes evaluating aspects such as task success rates, response quality,\nuser engagement, and overall user experience.\n\u2022How to Evaluate? Diverse Methodologies: Exploring a range of evaluation techniques, including data-driven\nannotations, automatic evaluation metrics, and innovative metric designs. This dimension emphasizes the\nintegration of both human and machine-based assessment tools to provide a holistic evaluation framework.\nOur taxonomy categorizes evaluation methods based on these dimensions, providing a structured approach to un-\nderstanding and improving the assessment of multi-turn conversational agents. By systematically addressing what\naspects to evaluate and how to evaluate them, our taxonomy lays the foundation for more reliable and comprehensive\nevaluation practices. Additionally, by identifying future challenges, it further guides researchers and practitioners in\ndeveloping next-generation evaluation frameworks that can keep pace with advancements in conversational AI.", "1.4 Comparisons with Other Existing Surveys": "Existing surveys in the related areas have distinct focuses but often leave critical gaps. For example, Arora et al . [10]\ngive only a brief overview of dialogue systems, while classifications into task-oriented vs. non-task-oriented models\ncan overlook agent\u2013system interactions [ 34]. Recent surveys [ 7,78,154,220,245] each tackle different perspectives but\ndo not address interactions between agentic components in detail or the unique challenges of multi-turn evaluations.\nLi[113] discuss LLM-based agent capabilities without focusing on multi-turn dialogue, and Chang et al . [30] review\nbroader LLM evaluation dimensions without delving into multi-turn specifics. Zhang et al . [248] discuss evaluators,\nsuch as GPT-4 for assessing multi-turn interactions. However, it lacks detailed descriptions of both annotation-based\nand annotation-free evaluation methods.\nOur taxonomy fills these gaps by systematically exploring what aspects to evaluate and how. We define comprehensive\ngoals, discuss data-driven annotation and automatic evaluation, and study robust metrics in multi-turn evaluation. Unlike\nearlier work [ 10,34,154,245], our framework addresses future challenges (e.g., scalability, real-world applicability) and\noffers a structured, holistic approach to assessing LLM-based multi-turn conversational agents. This perspective is vital\nfor building reliable, adaptable dialogue systems capable of handling complex multi-turn interactions effectively.", "2 What to evaluate? The evaluation goals and target": "When assessing a LLM-based multi-turn conversational agent, it is crucial to evaluate the various components that\ninfluence the system\u2019s overall performance and effectiveness. These evaluation goals can be categorized into the\nfollowing areas: evaluating the end-to-end experience of the LLM-based agents for multi-turn conversation, evaluating\nthe action and tool-use components of LLM-based agents for multi-turn conversation, evaluating the memory capabilities\nof LLM-based agents for multi-turn conversation, and evaluating the planning functions of LLM-based agents for\nmulti-turn conversation [63, 75, 179].", "2.1 Evaluating the End-to-end Experience of LLM-based Agents For Multi-Turn Conversation": "We present a comprehensive overview of evaluation goals for LLM-based agents in multi-turn interactions. The\nrepresentative works could be categorized into five dimensions: (1) Task Completion in Multi-Turn Conversations ,\nmeasuring how well agents fulfill user requests using metrics like completion rate and satisfaction; (2) Multitask\nCapabilities , assessing expertise across domains such as mathematics, history, and coding; (3) Interaction Patterns ,\nManuscript submitted to ACM6 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\nanalyzing structures like recollection, follow-up, and expansion to assess dialogue coherence; (4) Temporal Dimensions ,\nevaluating the agent\u2019s ability to maintain context over extended durations; and (5) User Experience and Safety , addressing\nsatisfaction, engagement, and safeguards against harmful content, including adversarial attacks and prompt leakage.\n2.1.1 Task Completion in Multi-Turn Conversations. Task completion effectiveness is a key metric for evaluating\nLLM-based agents in multi-turn conversations, measuring their ability to achieve predefined goals. Reimann et al . [179]\nintroduced objective metrics for assessing interaction quality through task completion rates and unrecognized utterance\ntracking, addressing limitations in existing evaluation approaches with a three-level scoring system that examines\nusability, likability, conversation quality, and interaction metrics. This framework was validated using Cookpanion,\na recipe recommendation agent employing twelve conversation patterns. Additionally, Deriu et al . [46] emphasizes\nmeasuring task completion through user feedback and system logs, determining success based on request fulfillment\nwithin conversations. However, both studies lack detailed analysis of failure causes, such as miscommunication or\nsystem limitations, leaving a critical gap in understanding multi-turn interaction challenges while highlighting the\nneed for improved evaluation frameworks. Furthermore, language models frequently serve varied user needs beyond\nsingle-use scenarios. Hendrycks et al . [75] introduce a comprehensive benchmark evaluating multitask capabilities\nacross 57 distinct tasks in fields like mathematics and law. Task completion is measured by the accuracy of model\nresponses to domain-specific questions. Moreover, Gritta et al . [63] propose an evaluation framework for conversational\nagents that includes multiple expert-rated answers for each question, enhancing the understanding of how models\nmanage multi-turn interactions and ultimately improving user experience.\n2.1.2 Interaction Patterns. Zheng et al . [270] developed MT-Bench, a multi-turn benchmark evaluating eight categories\n(writing, roleplay, extraction, reasoning, mathematics, coding, STEM, and humanities) using LLMs as evaluators.\nExtending this work with a million-conversation dataset across 25 LLMs [ 269], Kwan et al . [101] identified four key\ninteraction patterns: recollection, expansion, refinement, and follow-up. Bai et al . [13] further contributed a three-tier\ntaxonomy (Perceptivity, Adaptability, Interactivity) analyzing 4,208 turns across 1,388 dialogues\n2.1.3 User Experience and Safety. User experience evaluation in multi-turn agents [ 146] has evolved with Xu et al .\n[237] \u2019s human-reaction model for measuring dialogue engagement, while Liu et al . [129] expanded this to LVLMs\nthrough a three-level capability hierarchy. Safety concerns [ 17,48,141,211] are addressed by CoSafe dataset proposed by\nYu et al . [246] , revealing vulnerabilities to coreference-based attacks. Additionally, prompt leakage threats [ 5] highlight\nthe need for robust privacy and intellectual property protection", "2.2 Evaluating The Action and Tool-use Components of LLM-based Agents for Multi-turn Conversation": "Action and tool-use components are fundamental to LLM-based multi-turn conversation agents [ 56,65,153,159,172,191].\nThe agents dynamically identify and execute appropriate API calls while maintaining context through memory of\nprevious interactions. Representative works are categorized into three dimensions: (1) API Interaction and Dynamic\nTool-Use , which evaluates agents\u2019 ability to adapt to different tools and perform tasks without explicit instructions; (2)\nMulti-step Tool Selection and Reasoning , which includes frameworks and datasets for assessing LLM-based agent ability\nto select and apply multiple tools across sequential, multi-turn interactions; and (3) Reliability and Hallucination in\nTool-Use , which assesses accuracy and examines scenarios of incorrect outputs due to tool response misinterpretation.\n2.2.1 API Interaction and Dynamic Tool-Use. Xu et al . [239] evaluate LLMs\u2019 performance in intent recognition, response\nselection, and tool utilization, highlighting their ability to access real-time data and maintain context across conversation\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 7\nturns, though with varying proficiency. Shen et al . [188] showcase ChatGPT\u2019s capacity for task planning by integrating\nmultiple AI models across diverse domains to enhance performance. Patil et al . [168] improve API interaction accuracy\nand adaptability by incorporating Torch and TensorFlow APIs alongside a real-time document retriever, enabling precise\nand dynamic tool usage. HuggingGPT [ 188] further demonstrates the effective orchestration of multiple AI models for\nhandling complex tasks. Mialon et al . [145] propose benchmark called GAIA that test agent\u2019s proficiency in tool use by\npresenting question on real world that require long sequences action and API interaction.\n2.2.2 Multi-step Tool Selection and Reasoning . MetaTool [ 83] evaluates LLMs\u2019 tool awareness and decision-making\nusing the ToolE dataset, which tests single- and multi-tool scenarios. It highlights challenges in tool selection, revealing\ngaps in LLM capabilities. Wang et al . [223] introduced MTU-Bench to assess multi-tool usage and multi-step interactions\nacross scenarios, including single- and multi-turn tasks, enhancing action execution in conversations. Nijkamp et al . [155]\nproposed a program synthesis benchmark focusing on breaking down complex programming tasks, while Wang et al .\n[224] developed a Python-based benchmark for evaluating tool-use in responses. Mathematical reasoning benchmarks\nwere introduced by Kurtic et al . [100] and Sun et al . [205] , focusing on dialogue contexts. Guo et al . [66] designed\na benchmark for assessing multi-turn tool-use in Chinese societal applications, and Zhuang et al . [274] emphasized\nopen-ended tool usage in question answering. Lastly, Huang et al . [79] proposed a comprehensive benchmark to\nevaluate the full tool-use process, including planning and creation in complex tasks.\n2.2.3 Reliability and Hallucination in Tool-Use. While aforementioned studies underscore tool flexibility in dynamic\ncontexts, they also raise concerns regarding output reliability, particularly hallucinations [ 125,152]. Zhang et al . [261]\nintroduce a benchmark addressing hallucinations through depth, involving multi-level diagnostics like solvability detec-\ntion and missing-tool analysis, and breadth, focusing on toolset limitations. Cao [26] propose reducing hallucinations at\nthe reasoning stage using soft and hard evaluation methods. In the hard approach, similarity scores between retrieved\nknowledge and confidence levels are calculated; responses are only provided if the threshold is met, ensuring precision.", "2.3 Evaluating the Memory of LLM-based Agents For Multi-turn Conversation": "Evaluating the memory capabilities of LLM-based agents in multi-turn conversations has become vital [ 73]. Studies\nemphasize developing benchmarks and metrics to assess how effectively agents retain and utilize information across\ndialogue turns [ 131,245]. Representative studies can be classified into two dimensions, as illustrated in Figure 4: (1)\nMemory Span , which examines the temporal scope of memory, spanning individual turns, entire conversations, and\nlong-term persistence; and (2) Memory Forms , which delves into the representations and implementation of memory.\n2.3.1 Memory Spans. In LLM-based agents for multi-turn conversation, memory span\u2013comprising turn,conversation ,\nandpermanent memory\u2013is crucial for processing and retaining information to maintain context and deliver a seamless\nuser experience. Turn memory captures details within a single dialogue turn; for instance, in Figure 1, when a user\nrequests boutique hotels in Paris, the agent\u2019s action (SearchWeb) and its results are recorded, enabling appropriate\nresponses based on the immediate context [ 219]. Shen et al . [187] emphasizes the importance of context and dialogue\nhistory for coherence, while Cai et al . [23] introduces a sensory memory module that converts the current utterance\ninto word- and sentence-level embeddings. Meanwhile, conversational memory extends this retention across multiple\nturns by tracking user p", "2.4 Evaluating the Planner of LLM-based Agents For Multi-turn Conversation": "Effective evaluation of LLM-based Agent planners requires analysis of both prompt responses and sustained coherence\nin prolonged dialogues, organized along four key dimensions (see Figure 6): Task Modeling , which examines both\nTask Representation (defining task conceptualization [ 82]) and Context Modeling (capturing situational awareness for\nplanning); Task Decomposition , which analyzes strategies for breaking complex tasks into executable sub-components\nManuscript submitted to ACM10 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\nPlannerTask ModelingTask RepresentationTest\u00f3n and R-Moreno [209],\nSingh et al. [196],\nZhang et al. [260], Lyu et al. [136]\nContext ModelingFeng et al. [51], Zhang et al. [250],\nXiao et al. [235], Li et al. [110], Fu et al. [55],\nXing and Gulla [236], Wang et al. [229]\nTask DecompositionHuang et al. [82], Wang et al. [227],\nShen et al. [188], Wang et al. [221],\nSingh et al. [195], Srinivasan et al. [201],\nChen et al. [36], Zhang et al. [255],\nWu et al. [232], Prasad et al. [171],\nDagan et al. [44], Kwon et al. [103]\nAdaptation and ControlTalha Selamet and T\u00fcmer [207],\nOkubo and Takahashi [156],\nHao et al. [71], Huang et al. [81],\nLiu et al. [126], Muise et al. [151]\nReflectionPlan VerificationYao et al. [244], Zhang et al. [262], Van et al. [214],\nZheng et al. [271], Madaan et al. [137], Cao [25]\nPlan SelectionIn-generation SelectionCheng et al. [38], Test\u00f3n and R-Moreno [209],\nShi et al. [190], Liu et al. [134],Li et al. [110],\nSun et al. [204], Wang et al. [217],\nGu et al. [64], Light et al. [118]\nPost-generation SelectionYao et al. [243], Besta et al. [18],\nKargupta et al. [96], Xiao and Wang [234],\nHe et al. [74], Zhao et al. [268], Liu et al. [122]\nFig. 6. Taxonomy of Evaluation Planner of LLM-Based Agents in Multi-Turn Conversations.\n[262];Adaptation and Control , which assesses dynamic response capabilities during multi-turn interactions [ 151];\nandReflection , which evaluates verification and selection mechanisms through Plan Verification (checking execution\nfeasibility)[262] and Plan Selection viaIn-Generation filtering or Post-Generation evaluation.\n2.4.1 Task Modeling. This effort involves two fundamental aspects: Task Representation andContext Modeling . Task\nRepresentation encompasses the clear delineation of activities the agent must execute, including objectives, actions, and\nexpected outcomes, thereby empowering effective multi-turn conversations. For example, Test\u00f3n and R-Moreno [209]\npresents MA-LAMA, a multi-agent temporal planner that uses a factored, centralized approach to translate temporal\ntasks into constrained snap-actions, while [ 196] integrates classical planners with LLMs to improve goal decomposition\nfor two agents, and LaMMA-P [ 260] utilizes a Language Model-Driven Multi-Agent PDDL Planner for effective task\nmanagement. Additionally, a Planner-Reasoner framework for multi-task reasoning agents is introduced in Lyu et al .\n[136] . In parallel, Context Modeling pertains to the agent\u2019s ability to preserve and leverage contextual information\nthroughout interactions, ensuring responses remain coherent and contextually appropriate. Proficient context modeling\nmonitors antecedent interactions, user p", "3 How to evaluate? The evaluation methodologies and data": "Evaluating LLM-based agent in multi-turn scenario involves a variety of methodologies, each with its own strengths and\nlimitations. These systems are complex, requiring evaluation methods that can capture their dynamic and interactive\nnature. Table 1 and Table 2 summarize recent related benchmarks and datasets.\nManuscript submitted to ACM12 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\nEvaluation DataConversation Data GenerationGeneration of Next Turn ResponseLin et al. [120], Chae et al. [28],\nOlabiyi et al. [157],\nAbercrombie and Batista-Navarro [2],\nZhang et al. [251], Goplerud [62],\nKim et al. [97], Moniri et al. [150]\nGeneration of Tool-Use DataShim et al. [193], Golany et al. [60],\nArcadinho et al. [9], Li et al. [114],\nGopalakrishnan et al. [61]\nGeneration of Query Rewritten Data Al-Thani et al. [6], Liu et al. [124]\nGeneration of Fact Check DataAtanasova et al. [11], Tian et al. [210],\nSetty and Setty [186], Chaudhury et al. [32]\nConversation Data AnnotationNext Turn Response As The AnnotationMerdivan et al. [143], Liu et al. [133],\nDeng et al. [45], Ghazarian et al. [58],\nLiu et al. [129], Duan et al. [49]\nTool-Use and Function Calls in LLMs\nAs the AnnotationXu et al. [239], Patil et al. [168],\nShen et al. [188], Liu et al. [123],\nLi et al. [112], Wang et al. [223]\nQuery Rewritten and Retrieved Items\nAs The AnnotationSu et al. [203], Mo et al. [148],\nAliannejadi et al. [8]\nFact Check Data As The AnnotationSathe and Park [185], Gupta et al. [67],\nWang et al. [226], Zhao et al. [267],\nSathe et al. [184]\nFig. 7. Taxonomy of Evaluation Data for LLM-based Agents in Multi-Turn Conversations.", "3.1 Evaluation Data": "Here, we focus on the methodologies for generating and annotating conversation data used to evaluate the LLM-based\nagents. The evaluation taxonomy categorizes the data into two primary dimensions: Conversation Data Generation\nandConversation Data Annotation . In the former, processes include the generation of next turn responses to\nsimulate natural multi-turn interactions, the creation of tool-use data reflecting how agents utilize tools and APIs\nduring conversations, the production of query rewritten data to evaluate the adaptation or reframing of user inputs,\nand the generation of fact check data to test the agent\u2019s ability to verify and present factual information accurately. In\nthe latter, annotation processes are applied to ensure the generated data can be effectively used for evaluation; these\ninvolve annotating the expected next turn response, capturing how tools and function calls are employed by the agent,\nmarking rewritten queries and retrieved items to assess alignment with user intent and data relevance, and verifying\nfactual accuracy through fact checks, as briefly represented in Figure 7.\n3.1.1 Conversation Data Generation. Automated generation of multi-turn dialogue data reduces manual annotation\nwhile ensuring metrics such as turn-level coherence and semantic relevance [ 92]. Key challenges include maintaining\ncontextual consistency and adapting to dynamic intent shifts [ 35]. Approaches include Next Turn Response Data\nmethods that generate context-aware replies using dialogue state representations via ISU [ 60], hierarchical attention in\nHMAN [ 120], chain-of-thought reasoning in CoT Distillation [ 28], and adversarial training in hredGAN [ 157]; coherence\nis further enhanced with Last Utterance-Context Attention [ 251]. Debate frameworks such as DEBATE [ 97] along\nwith multi-agent systems [ 2,3,62] and judge-based models [ 150] further refine responses. In addition to standard\nconversation data generation, advanced techniques for tool-use, query rewriting, and fact checking have emerged as\nareas of particular interest. Some of representative studies are as follows:\n\u2022Tool-Oriented Dialogue Generation: Automated pipelines, such as ToolDial [ 193], establish dialogue states\nand actions directly from API documentation. Additionally, methods generating diverse test scenarios using\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 13\nEvaluation MetricsAnnotation-based EvaluationAnnotation As A Reference for Quality EvaluationTraditional MetricsPapineni et al. [165], Post [170],\nLin [119], Lavie and Agarwal [104]\nAdvanced MetricsZhang et al. [256], Kamal Eddine et al. [95],\nGao et al. [57], Phy et al. [169],\nBanerjee et al. [15]\nGusev [68], Gritta et al. [63],Zhou et al. [273],\nLuo et al. [135], Mehnaz et al. [140],\nKomma et al. [99],Budzianowski et al. [20],\nMerdivan et al. [144], Zhang et al. [262],\nMehri et al. [142], Zhang et al. [254]\nAnnotation As The Exact Result For MatchingHan et al. [70],\nXing and Gulla [236], Zhang et al. [253],\nYang et al. [241], Dong et al. [47], Wu et al. [231],\nBudzianowski et al. [21], Byrne et al. [22]\nAnnotation-free EvaluationPoint-wise Response ScoringJones et al. [94], Ferron et al. [52], Lin and Chen [121],\nFan et al. [50], Goh et al. [59], Yadav and Kaushik [240],\nZhang et al. [266], Li et al. [116], A [1],\nShi et al. [192], Ren et al. [180], Jia et al. [91]\nPair-wise or List-wise Response ScoringKim et al. [98], Ruiz-Dolz et al. [182],\nLiu et al. [130], Bai et al. [14],\nLee et al. [105], Hunter [85], Herbrich et al. [76],\nPark et al. [166], Li et al. [111],\nYadav and Kaushik [240], Banerjee and Lavie [16],\nRobertson [181],Chan et al. [29]\nFig. 8. Taxonomy of Evaluation Metrics for LLM-based Agents in Multi-Turn Conversations.\nLLMs and intermediate graphs are highlighted in Arcadinho et al . [9], while manually annotated datasets such\nas DailyDialog [ 114] and Topical Chat [ 61] provide critical evaluation benchmarks. For further insights into\ntool-based data generation, see Tool-Based Dialogue Innovations.\n\u2022Query Rewritten Techniques in Dialogue Generation: Scalable Query Rewritten Data is produced by\nrefining queries through selective contextual integration [6] and self-supervised learning [124].\n\u2022Factual Verification Methods for Dialogue Generation: Concurrently, Fact Check Data employs veracity\nprediction with explanation generation [ 11], integrating LLM pre-trained knowledge with dynamic evidence re-\ntrieval [ 210], automated question generation pipelines [ 186], and corrector modules to ensure factual consistency\n[32]. To explore fact-checking methods in dialogue systems, please visit Fact Checking in AI.\n3.1.2 Conversation Data Annotation. Data annotation is crucial for evaluating multi-turn dialogues by labeling user\nqueries, system responses, and conversation flow to create benchmarks that reflect human judgment. Annotation\nincludes Next Turn Response As Annotation : Benchmarks such as HUMOD [ 143] (28,500 dialogues) and MMDU [ 133]\n(27 dialogue turns) assess context preservation. Ghazarian et al . [58] emphasize measuring context retention, while\nConvBench [ 129] and BotChat [ 49] compare LLM outputs against human standards. The dataset in [ 45] transforms\nsingle-turn prompts into multi-turn exchanges . Tool-use and Function Calls in LLMs As annotation : ToolBench [ 239]\ntests API call generation, and Patil et al . [168] use AST matching to assess hallucination errors. HuggingGPT [ 188]\norchestrates model interactions, while Liu et al . [123] and Li et al . [112] benchmark multi-turn API interactions. MTU-\nBench [ 223] evaluates tool usage over multiple turns. Query Rewritten and Retrieved Items as Annotation : Su et al . [203]\ncollect annotations where human annotators rewrite utterances to clarify intent. In a similar vein, Mo et al . [148]\nleverage rewritten queries with relevance judgments, while Aliannejadi et al . [8] annotate past utterances to identify\nuseful items for interpreting current queries. Lastly Fact-Checking Annotation in Dialogue Evaluation Fact-checking\nannotations verify accuracy using document-level claim analysis [ 185]. Gupta et al . [67] introduce the DialFact dataset\n(22k claims with Wikipedia evidence), while multi-stage schemes [ 226] enable turn-by-turn evaluation. Zhao et al . [267]\npresent the BELIEF benchmark, and BERT-based models trained on WikiFactCheck-English [ 184] automate verification.\nManuscript submitted to ACM14 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\nHistory:1.User: \"Hi, I want to plan a trip to Paris. Could you help?\"2.Agent: \"Sure! I'd be happy to help you with that. What kind of trip are you looking for?\"3.User: \"I'm interested in a 5-day itinerary with a mix of cultural and food-related activities. \"4.Agent: \"Got it! How about a mix of museums, historical sites, and local food experiences? Do you have any p", "3.2 Evaluation Metrics": "This section reviews evaluation metrics and methods for assessing multi-turn conversational agents, as briefly illustrated\nin Figure 8. The evaluation approaches are primarily divided into two categories: Annotation-based Evaluation , which\nrelies on pre-labeled data to benchmark responses, and Annotation-free Evaluation , which automates scoring without\nthe need for manual labels. Annotation-based methods provide detailed, human-curated insights into dialogue quality\nbut are resource-intensive and can be limited by subjective biases in annotation. In contrast, annotation-free metrics\nleverage self-supervised learning and real-time adaptation to offer scalable evaluations across diverse dialogue contexts,\nalthough they may initially require robust training data for optimal performance. This shift towards automation in\nevaluation not only reduces costs but also enhances scalability, making it feasible to assess conversational agents in\nreal-world, dynamic environments with greater efficiency and adaptability.\n3.2.1 Annotation-based Evaluation. Annotation-based evaluation relies on expert-curated data as benchmarks for\nassessing an agent\u2019s performance, as demonstrated in recent studies [ 68]. Broadly, annotations serve two key functions:\n\u2022Annotation as a Reference for Quality Evaluation: Annotations are fundamental for evaluating conver-\nsational models. For example, Gusev [68] introduce a role-playing framework that aligns automated metrics\nwith human judgment, while expert-rated multi-modal annotations [ 63,135] enhance evaluation quality. Gup-\nShup addresses code-switched conversations [ 140], and methods like Dialog Quality Annotation (DQA) [ 99]\nand benchmarks such as HUMOD [ 144] underscore the importance of high-quality annotations. Datasets like\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 15\nHistory:1.User: \"Hi, I want to plan a trip to Paris. Could you help?\"2.Agent: \"Sure! I'd be happy to help you with that. What kind of trip are you looking for?\"3.User: \"I'm interested in a 5-day itinerary with a mix of cultural and food-related activities. \"4.Agent: \"Got it! How about a mix of museums, historical sites, and local food experiences? Do you have any p", "3.3 Benchmark Datasets and Resources": "We present the existing resources on the benchmark datasets and evaluation tools in two comprehensive tables. Table 1\ndetails benchmarks focused on task multi-tasking capabilities, interaction patterns, and temporal dimensions, along\nwith safety and tool-use aspects, thereby outlining how varied methodologies address dialogue coherence and context\nmaintenance. Meanwhile, Table 2 presents benchmarks that evaluate the reliability of tool-use, next-turn response\nquality, query rewriting, conversation memory, complete interaction, memory editing, task representation, context\nmodeling, task decomposition, and plan selection. Together, these tables provide a holistic overview of current evaluation\npractices, highlighting both the diverse approaches and the critical metrics employed to assess the performance of\nLLM-based conversational systems.", "4 Summary, Challenges, and Future Works": "In this section, we first revisit the co-evolution of conversational systems and the evaluation techniques, while pin-\npointing the limitations and challenges of current systems and outlining promising future research directions.", "4.1 Trends and Status-quo": "Figure 11 is a concise roadmap highlighting how the evaluation of multi-turn conversational agents has evolved from\nearly rule-based systems to the sophisticated LLM-based agents of today. Three major phases have been experienced by\nthe research community.\n\u2022Early Developments: Early multi-turn conversation systems were primarily rule-based, with iconic examples\nincluding Weizenbaum\u2019s ELIZA (1966) [ 230] and PARRY (1972) [ 69]. These systems relied on scripted rules and\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 17\nTable 1. Benchmarks for Task Multitask Capabilities, Interaction Patterns, and Temporal Dimensions, Safety and Tool-Use in Multi-\nTurn Conversation Systems\nStudy Focus Methodology Metrics Key Outcomes\nMultitask Capabilities\nHendrycks et al .\n[75]Domain-Specific\nMultitaskingIntroduces a benchmark to evalu-\nate multitask capabilities across 57\ntasksAccuracy in domain-\nspecific question answering\nand problem-solvingA framework for evalu-\nating multitask capabil-\nities across diverse do-\nmains.\nLi et al. [114] Manually Anno-\ntated Multi-Turn\nData (DailyDialog)Offers high-quality labeled dia-\nlogues for diverse scenarios.Annotation quality, re-\nsponse coherenceServes as a key bench-\nmark for conversational\nAI.\nGopalakrishnan\net al. [61]Topic-Based Di-\nalogue Dataset\n(Topical Chat)Covers broad conversational topics\nwith labeled dialogue acts.Knowledge grounding, con-\nversational diversity8 broad topics in real-\nworld settings.\nInteraction Patterns in Multi-Turn Conversations\nZheng et al. [270] Multi-Turn Bench-\nmark (MT-Bench)Introduces MT-Bench, a set of multi-\nturn questions across eight cate-\ngoriesPrecision, quality of gener-\nated responsesTests a model\u2019s capac-\nity for dialogue-driven\ntasks across diverse cat-\negories.\nKwan et al. [101] Human-LLM Inter-\naction PatternsBuilds on one million multi-turn\nconversations benchmark.Recognition of multi-turn\npatterns, task success ratesA taxonomy for tar-\ngeted evaluation of\nmulti-turn capabilities.\nTemporal Dimensions in Multi-Turn Conversations\nXu et al. [238] Long-Term Conver-\nsations DatasetExplores scenarios with intermit-\ntent sessions over multiple interac-\ntionsRetention of shared context\nover sessionsHighlights the impor-\ntance of long-term\nmemory in multi-\nsession dialogues.\nMaharana et al .\n[138]Months-Long Con-\nversations DatasetExamines intermediate-term con-\nversations lasting multiple monthsContext retention in multi-\nmonth interactionsFills a critical gap in\nunderstanding medium-\nrange muli-turn conver-\nsations.\nSafety in Multi-Turn Conversations\nYu et al. [246] Safety: Coreference-\nBased AttacksPresents CoSafe, a dataset with\n1,400 questions across 14 categories\nfor probing LLM vulnerabilities.Vulnerability to\ncoreference-based at-\ntacksDemonstrates how be-\nnign topics can lead\nto unsafe content with\ncontext steering.\nTool-Use Benchmarks\nMialon et al. [145] real word question\nthat is easy for hu-\nman but hard for AI\n(GAIA benchmark)466 human-made factoid questions\nthat require multi-step reasoning\nand tool use.Performance is gauged by\nthe percentage of exact-\nmatch.top LLMs\u2014even with\ntool aid\u2014score only\n0\u201330%, revealing a\nlarge gap in current AI\nassistant abilities.\nHuang et al. [83] Tool Awareness &\nDecision-MakingProposes MetaTool benchmark, in-\ncluding ToolE dataset with single/-\nmulti-tool triggers.Correct tool selection rate,\neffective usage in single-\n/multi-tool tasksReveals significant\ngaps in LLMs\u2019 decision-\nmaking for tool\nutilization.\nWang et al. [224] Multi-Turn Tool-\nUse in PythonProposes a benchmark for evaluat-\ning action and tool-use, allowing\nLLMs to execute Python code.Successful integration & ex-\necution of Python-based\ntoolsFacilitates practical as-\nsessment of LLM tool\nusage in multi-turn di-\nalogue.\nXu et al. [239] API Call Execution\n(ToolBench)Evaluates LLMs\u2019 ability to generate\nand execute API callsFunction call success rate A benchmark for test-\ning API execution\nShim et al. [193] ToolDial bench-\nmarkGenerates dialogues from API docu-\nmentation without human interven-\ntion.Annotation efficiency, ac-\ntion predictionEnables scalable high-\nquality annotated data.\nShen et al. [188] Orchestrated Tool-\nUse (HuggingGPT)Integrates function calling for AI\nmodel coordinationTask execution accuracy A benchmark for ex-\nplict tool use\nLi et al. [112] API Benchmarking Evaluates LLMs on single-turn and\nmulti-turn API interactionsContext retention, API suc-\ncess rateA benchmark for test\nsingle/multi-turn inter-\nactions\nWang et al. [223] Multi-Granularity\nTool-Use (MTU-\nBench)Assesses function call accuracy\nacross different usage contextsTool selection, parameter\naccuracyA benchmark for test-\ning different tool usage\ncases\npattern matching rather than large-scale, data-driven methods. Evaluation methods during this era focused on\nManuscript submitted to ACM18 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\nTable 2. Benchmarks for Reliability in Tool-use, Next-turn Response, Query Rewritten, Conversation Memory, Complete Interaction,\nMemory Editing, Task Representation, Context Modeling, Task Decomposition and Plan Selection in Multi-Turn Conversation Systems\nStudy Focus Methodology Metrics Key Outcomes\nReliability and Hallucination Mitigation in Tool-Use\nZhang et al. [261] Multi-Level Halluci-\nnation DiagnosisProposes a benchmark analyzing\nhallucinations based on depthHallucination detection ac-\ncuracy, depth and breadth\nanalysisIdentifies weaknesses\ncaused by hallucina-\ntions and missing tools.\nNext-Turn Response\nMoniri et al. [150] Multi-Round Struc-\ntured DebatesModels structured debates with\njudge assessments for argument\nevaluation.Clarity, factual accuracy,\nconsistencyBenchmarks LLM per-\nformance using debate\ndynamics.\nAbercrombie and\nBatista-Navarro [2]Parliamentary De-\nbate AnalysisUses structured debate transcripts\nto train and assess models.Logical flow, structured rea-\nsoningA dataset for dialogue\nsystem evaluation.\nQuery Rewritten and Retrieved Items\nAliannejadi et al .\n[8]Relevant Utterance\nDatasetAnnotates relevant past utterances Precision, Recall and F1-\nscoreMeasures information\nretrieval and coherence\nof responses\nConversation Memory\nChen et al. [33] Conversational\nMemory Bench-\nmarkProposes SocialBench, evaluating\nLLMs\u2019 recall of key conversation el-\nements over 40 utterances.Keyword recall, memory re-\ntention over multiple turnsBenchmarks LLM abil-\nity to sustain dialogue\ncontinuity in multi-turn\ninteractions.\nComplete Interaction\nLi et al. [109] Long-Range Con-\ntext RetrievalIntroduces a benchmark for assess-\ning LLMs\u2019 ability to retrieve and\nutilize extended conversation his-\ntories.Long-context recall, re-\ntrieval accuracyEstablishes a frame-\nwork for evaluating\nlong-range memory\ncapabilities.\nMaharana et al .\n[138]Long-Term Multi-\nAgent Conversa-\ntionsIntroduces a dataset featuring con-\nversations spanning up to 35 ses-\nsionsMemory retention, coher-\nence in extended dialoguesEnables evaluation of\nuser-agent interactions\nover long conversation.\nMemory Editing Techniques\nZeng et al. [247] Caching-Based\nEditing (SKEME)Implements a novel caching mecha-\nnism to adjust model parameters.Update efficiency, factual\nconsistencyA multi-task dataset to\ntest the practicality of\nmodel editing.\nMao et al. [139] Personality Modifi-\ncationAdjusts responses to align with pre-\ndefined personality traits.Personality alignment, con-\nsistencyA benchmark for per-\nsonality modification\ntest.\nTask Representation\nZhang et al. [260] Language Model-\nDriven PDDL\nPlanningEmploys LLMs to enhance task rep-\nresentation in multi-turn scenarios.Planning efficiency, gener-\nalization across tasksA benchmark to test dif-\nferent levels tasks\nContext Modeling\nXiao et al. [235] Reducing Context\nHallucinationsFormalizes workflow knowledge to\nimprove context reliability.Hallucination reduction, re-\nsponse precisionA benchmark for\nworkflow-guided mod-\neling.\nBenchmark for Task Decomposition\nWang et al. [227] Multi-Agent De-\ncompositionAssigns subtasks to subagents for\nadaptability in complex tasks.Adaptability, task efficiency A benchmark for assess-\ning the planning ability\nof LLM-based agent\nWu et al. [232] Multi-Level Task\nPlanningUses open-source LLMs for multi-\nlevel task breakdown.Task decomposition accu-\nracyA benchmark for test-\ning the task decomposi-\ntion ability\nPost-Generation Plan Selection\nKargupta et al. [96] Structured Ques-\ntioningMaintains multi-turn dialogue con-\ntext for plan selection.Plan consistency, dialogue\ncoherenceA dataset to test plan se-\nlection in multi-turn in-\nteraction setting\nsimple metrics such as user satisfaction surveys and Turing Test\u2013style qualitative assessments, reflecting the\nconstrained nature of the interactions and the lack of robust benchmarks [212].\n\u2022Transition to Neural Models : With the advent of deep neural networks, rule-based approaches began to be\nsupplanted by end-to-end neural conversational models. Influential studies such as Sordoni et al . [200] and Vinyals\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 19\nTuringTestBLEUROUGEPerplexityMETEOR\none of the first chatbots capable of attempting theTuring test.19491996ELIZA\na test of a machine's ability toexhibit intelligent behaviourequivalent to that of a human1972PARRYAnearlyexampleofachatbot\n2000-2005TranslationqualitySummarizationqualityPredictionconfidence\n2015end-to-end neural conversational modelsparadigm shift in developmentof dialogue systems2020HUMODBERTScoreevaluating dialogue relevance metrics\n2020GPT3redefining the boundaries of language understanding, generation, and application\n2021InstructGPTInstructGPT has been instrumental in generating human-like, multi-turn dialogues that are coherent and contextually relevant.\n2022ChatGPT\ndemonstrated exceptional performance in crafting multi-turn conversations that are indistinguishable from human interactions in terms of style and flow2023LLM Powered Autonomous Agentsprovide a foundation for developing more comprehensive evaluation frameworks for multi-turn conversational agents, addressing aspects like context retention, adaptive planning, and tool integration.2023LLM-as-a-Judgesolution for automatically evaluating LLM responses based on specific criteria\nBotChatMTBenchAgentBenchBenchmarksforassessingLLM-basedAgentformulti-turn dialogue capabilities\n2024MMDUMT-Mind2WebMTU-BenchFactcheck-Bench\nExtraBenchmarksforassessingLLM-basedAgentformulti-turn dialogue capabilitiesfromdifferentperspectives\nFig. 11. A Roadmap of Evaluation of LLM-based Agents for Multi-turn Conversation. The blue line represents the development of\nmetrics and benchmark for Multi-turn Conversation. The orange line represents the development of Multi-turn Conversation\nand Le [216] introduced models that improved fluency and context understanding. New evaluation metrics like\nBLEU [ 165] and perplexity [ 90] emerged to measure generation quality over multiple turns. Researchers started\nrecognizing the limitations of purely reference-based metrics and began incorporating more contextual and\ndiversity-oriented frameworks (e.g., human preference ratings, embedding-based scores).\n\u2022The Rise of LLMs and Agents : The advent of transformer architectures [ 215] paved the way for large language\nmodels, culminating in GPT-style models [ 177] that drastically improved multi-turn conversational capabilities.\nKey milestones include GPT-2 [ 178], GPT-3 [ 19], followed by instruction-tuned and dialogue-focused systems\nlike InstructGPT [ 162] and ChatGPT [ 158]. These models demanded sophisticated, human-in-the-loop evaluation\nstrategies to capture higher-level coherence, factual correctness, and response relevance. Advanced techniques\nsuch as CoT prompting [ 221] refined the reasoning process in multi-turn dialogues, while frameworks like ReAct\n[244] and LangChain [31] enabled improved factuality and task completion through external tool integration.\nFrom the early rule-based systems to modern LLM-based agents, the evolution of conversational systems has largely\nbeen driven by improvements in fluency, contextual handling, and adaptive response generation. With each develop-\nmental phase, evaluation metrics have advanced from simple qualitative assessments to more complex, multi-faceted\nbenchmarks, metrics, and methods aimed at measuring dialogue coherence and context retention. Hereby, we summarize\nthe recent research focus on the evaluation techniques as follows.\nManuscript submitted to ACM20 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\n\u2022Holistic Evaluation Benchmarks and Datasets : As LLMs advanced, the field shifted toward more comprehen-\nsive benchmarks that incorporate human judgments, advanced automatic metrics such as BERTScore [ 95,256],\nand task-specific evaluations to reflect the increasingly complex, context-rich nature of multi-turn interactions\n[9,45]. The complexity of multi-turn dialogues led to the creation of specialized datasets and benchmarks that\nfocus on realism, domain knowledge, and long-horizon reasoning: (1) Dialogue Modeling : HUMOD [ 143] for\nhuman-like dialogue modeling; BotChat [ 49] for open-ended chat scenarios; AgentBench [ 131] for agent-centric\ntasks. (2) Tool and Action Integration : API-Bank [ 112] and MMDU [ 133] assess real-world tool usage, while\nMT-Mind2Web [ 45], MTU-Bench [ 223], and Factcheck-Bench [ 226] provide structured evaluations of multi-turn\nreasoning, user intent, and factual veracity.\n\u2022LLM/Prompt/Agent-based and Self-Judging Evaluation Methods : The rise of LLM-based models has acceler-\nated the adoption of annotation-free methods, such as point-wise scoring and side-by-side comparison, reducing\ndependence on manual evaluation. In some cases, LLMs are employed as evaluators\u2014providing direct numeric\nscores or explaining their ranking decisions\u2014to enable rapid, iterative testing of multi-turn conversational\nperformance [ 50,94,198,249]. Learned reward models have emerged as a crucial component in the evaluation\nand improvement of conversational AI systems, the reward model\u2019s score serves as an approximate \u201cquality\nmeter\u201d for dialogue responses, consolidating many evaluation factors into one number [ 42,199,218]. While not\nperfect, these learned models have proven to correlate well with aggregate human judgments in many settings\n[202].\nThis mapping outlines the historical progression from early rule-based systems through the neural model\ntransition to modern LLM-based agents, and it emphasizes the evolving challenges in evaluation\u2014from simple\nqualitative metrics to sophisticated, multi-dimensional benchmarks that address the richness of multi-turn\ninteractions.", "4.2 Challenges and Future Work": "Despite significant advancements, current evaluation strategies still struggle to fully capture the understanding of multi-\nturn dialogues, adapting the dynamic interplay between turns rather than assessing them in isolation. Consequently,\nadvances in external tool integration, memory retention, real-time self-assessment, scalability, and privacy preservation\nare essential to meet the comprehensive requirements of modern dialogue systems. We discuss the limitations of existing\nstudies and propose avenues for future research.\n\u2022Unified and Adaptive Evaluation Frameworks. Current evaluation methods tend to assess conversation turns\nin isolation rather than holistically, which limits the ability to capture the dynamic interplay among successive\nturns. Early systems, relying on simple evaluations such as user satisfaction surveys and Turing Test\u2013style\nassessments [ 212], illustrate this limitation. As dialogue systems have evolved\u2014with neural conversational\nmodels [ 200,216] and later LLM-based approaches [ 19,177]\u2014there is an increasing need for frameworks that\nintegrate both turn-level and overall conversation assessments. Future research should focus on developing\nadaptive metrics that can dynamically adjust to variations in context, thereby providing a comprehensive picture\nof agent performance over extended, multi-turn exchanges.\n\u2022Memory and Context Retention. Many current benchmarks fail to differentiate between short-term recall and\nlong-term context integration, resulting in issues such as context leakage or drift over prolonged interactions.\nThe early conversational systems often employed rudimentary context tracking techniques, whereas modern\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 21\nneural conversational models still face challenges in maintaining continuity over multiple turns [ 90,165]. Future\nwork should target the creation of specialized benchmarks that accurately measure both temporary and persistent\nmemory retention. This would ensure that agents are capable of leveraging conversational history effectively to\nmaintain coherence and contextual awareness throughout extended dialogues.\n\u2022Test-Time Evaluation for Self-Assessment. Currently, LLM-based agents generate responses without the\ncapacity to evaluate the quality of their outputs in real time. This lack of inline self-assessment can result\nin suboptimal responses and missed opportunities for immediate correction, undermining the coherence of\nextended dialogues. Recent advancements in reasoning techniques such as Chain-of-Thought have opened\nup new possibilities for iterative evaluation during inference [ 221]. Future work should focus on integrating\ntest-time evaluation strategies that allow agents to continuously gauge parameters such as coherence, factual\naccuracy, and context alignment as part of their response generation process. Incorporating real-time feedback\nloops will support adaptive mechanisms that dynamically correct errors and enhance overall dialogue quality.\n\u2022Dynamic Self-Correction and Error Propagation. Errors occurring in the initial stages of a conversation can\npropagate and compound over subsequent turns, leading to incoherent or hallucinated responses. The limitations\nof early metrics have been highlighted by the evolution from rule-based to neural models [ 212,216]. Although\ntechniques like Chain-of-Thought prompting have begun refining the reasoning process [ 221], there remains a\nsignificant gap in real-time error detection and correction. Future research must embed dynamic self-correction\nmechanisms\u2014such as multi-reasoning trees and iterative feedback loops\u2014that empower agents to continually\nmonitor, reassess, and rectify errors during ongoing interactions.\n\u2022Tool-Use and Action Planning in Extended Interactions. Modern LLM-based agents now integrate external\ntools and APIs to accomplish tasks; however, existing evaluations rarely capture how these capabilities evolve\nthroughout a dialogue. Frameworks like ReAct [ 244] and platforms such as LangChain [ 31] have begun enabling\nagents to utilize structured knowledge and external resources. Nonetheless, it remains challenging to assess\nhow well an agent adapts its action planning in response to dynamic inputs and shifting task contexts. Future\nevaluation designs should simulate multi-turn tasks that emphasize cumulative tool-use and adaptive planning,\nproviding systematic insights into how sequential API calls and planning adjustments impact overall task\noutcomes.\n\u2022Scalability and Real-World Applicability. The high computational costs and heavy reliance on manual\nannotations have made it difficult to scale current evaluation methods to real-world scenarios. While early\napproaches could rely on simplified metrics, modern LLM-based agents require more nuanced and multi-\ndimensional evaluation strategies to mirror production-level interactions [ 50,198]. Future directions should focus\non automated, annotation-free evaluation pipelines that utilize self-supervised techniques and real-time adaptive\nmetrics.By minimizing reliance on manual annotations, automated systems can leverage large-scale interaction\ndata to derive meaningful metrics. A hierarchical approach, integrating fast initial filters with deeper evaluations,\nsupports efficient processing of extensive dialogues. Integrating evaluation into simulation environments allows\nfor rigorous testing under diverse conditions.\n\u2022Privacy Preservation in Conversational Evaluation. In multi-turn dialogue systems, preserving user privacy\nduring evaluation is of paramount importance, especially when private conversations between users and agents\nare involved. Traditional evaluation methods rarely address the risk of exposing sensitive data, a challenge\nexacerbated by the increased data handling required for complex, multi-turn interactions. Future research must\nexplore methods to evaluate conversation quality in a privacy-preserving manner by incorporating techniques\nManuscript submitted to ACM22 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\nsuch as Trusted Execution Environment [ 43,88] and federated learning. These approaches should allow rigorous\nperformance assessments while ensuring that individual conversation details remain anonymized and secure,\nthereby building trust and safeguarding user confidentiality in real-world applications.", "5 Conclusion": "This paper provides a comprehensive overview of evaluation methods for LLM-based agents in multi-turn conversa-\ntions. By systematically examining the evaluation goals\u2014such as dialogue coherence, effective tool use, and memory\nretention\u2014and the diverse methodologies, including annotation-based assessments and automated metrics, the study\noffers a detailed synthesis of the progression from rule-based systems to transformer-based agents. This work lays a\nsolid foundation for future explorations in the domain of conversational AI by addressing both theoretical and practical\naspects. The core contribution of this paper lies in developing a structured taxonomy that clarifies critical dimensions\nof multi-turn conversational performance. This framework not only defines what elements should be evaluated but also\nestablishes the methodological steps necessary to assess these components reliably. By synthesizing insights from nearly\n200 scholarly sources, the study bridges the gap between existing evaluation methods and the emerging complexities\ninherent in modern LLM-based agents.\nAdditionally, the paper critically addresses current challenges and limitations within the field, such as inadequate\nbenchmarks for long-term memory retention and the need for scalable, annotation-free evaluation pipelines. These\ncontributions highlight the necessity for advanced methodologies that can accurately reflect real-world conversational\ndynamics and ensure the reliable performance of multi-turn agents. The discussion of future research directions\nemphasizes the importance of developing automated evaluation techniques and adopting tools that enhance the\ninterpretability and reliability of assessment processes. In summary, this paper not only sets forth a detailed taxonomy\nand systematic analysis of various evaluation methods but also serves as a catalyst for future research. The study\u2019s\nthorough review of literature, combined with its critical insights into existing challenges, paves the way for the\ndevelopment of more robust evaluation tools, ultimately advancing the field of conversational AI.", "References": "[1]Sujan Reddy A. 2022. Automating Human Evaluation of Dialogue Systems. In Proceedings of the 2022 Conference of the North American Chapter\nof the Association for Computational Linguistics: Human Language Technologies: Student Research Workshop , Daphne Ippolito, Liunian Harold Li,\nMaria Leonor Pacheco, Danqi Chen, and Nianwen Xue (Eds.). Association for Computational Linguistics, Hybrid: Seattle, Washington + Online,\n229\u2013234. https://doi.org/10.18653/v1/2022.naacl-srw.29\n[2]Gavin Abercrombie and Riza Batista-Navarro. 2020. ParlVote: A Corpus for Sentiment Analysis of Political Debates. In Proceedings of the Twelfth\nLanguage Resources and Evaluation Conference , Nicoletta Calzolari, Fr\u00e9d\u00e9ric B\u00e9chet, Philippe Blache, Khalid Choukri, Christopher Cieri, Thierry\nDeclerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H\u00e9l\u00e8ne Mazo, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (Eds.).\nEuropean Language Resources Association, Marseille, France, 5073\u20135078. https://aclanthology.org/2020.lrec-1.624\n[3]Gavin Abercrombie and Riza Batista-Navarro. 2020. Sentiment and position-taking analysis of parliamentary debates: a systematic literature\nreview. Journal of Computational Social Science 3, 1 (2020), 245\u2013270. https://doi.org/10.1007/s42001-019-00060-w\n[4]Praveen Acharya. 2023. Towards Effective Modeling and Exploitation of Search and User Context in Conversational Information Retrieval. In\nProceedings of the 32nd ACM International Conference on Information and Knowledge Management (Birmingham, United Kingdom) (CIKM \u201923) .\nAssociation for Computing Machinery, New York, NY, USA, 5161\u20135164. https://doi.org/10.1145/3583780.3616005\n[5]Divyansh Agarwal, Alexander Fabbri, Ben Risher, Philippe Laban, Shafiq Joty, and Chien-Sheng Wu. 2024. Prompt Leakage effect and mitigation\nstrategies for multi-turn LLM Applications. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry\nTrack , Franck Dernoncourt, Daniel Preo\u0163iuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics, Miami, Florida, US,\n1255\u20131275. https://doi.org/10.18653/v1/2024.emnlp-industry.94\n[6]Haya Al-Thani, Tamer Elsayed, and Bernard J. Jansen. 2023. Improving conversational search with query reformulation using selective contextual\nhistory. Data and Information Management 7, 2 (2023), 100025. https://doi.org/10.1016/j.dim.2022.100025\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 23\n[7]Atheer Algherairy and Moataz Ahmed. 2023. A review of dialogue systems: current trends and future directions. Neural Comput. Appl. 36, 12 (Dec.\n2023), 6325\u20136351. https://doi.org/10.1007/s00521-023-09322-1\n[8]Mohammad Aliannejadi, Manajit Chakraborty, Esteban Andr\u00e9s R\u00edssola, and Fabio Crestani. 2020. Harnessing Evolution of Multi-Turn Conversations\nfor Effective Answer Retrieval. In Proceedings of the 2020 Conference on Human Information Interaction and Retrieval (CHIIR \u201920) . ACM. https:\n//doi.org/10.1145/3343413.3377968\n[9]Samuel Arcadinho, David Aparicio, and Mariana Almeida. 2024. Automated test generation to evaluate tool-augmented LLMs as conversational AI\nagents. arXiv:2409.15934 [cs.CL] https://arxiv.org/abs/2409.15934\n[10] Suket Arora, Kamaljeet Batra, and Sarabjit Singh. 2013. Dialogue System: A Brief Review. arXiv:1306.4134 [cs.CL] https://arxiv.org/abs/1306.4134\n[11] Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. 2020. Generating Fact Checking Explanations. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics , Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault (Eds.).\nAssociation for Computational Linguistics, Online, 7352\u20137364. https://doi.org/10.18653/v1/2020.acl-main.656\n[12] Sanghwan Bae, Donghyun Kwak, Soyoung Kang, Min Young Lee, Sungdong Kim, Yuin Jeong, Hyeri Kim, Sang-Woo Lee, Woomyoung Park, and\nNako Sung. 2022. Keep Me Updated! Memory Management in Long-term Conversations. arXiv:2210.08750 [cs.CL] https://arxiv.org/abs/2210.08750\n[13] Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, and Wanli Ouyang.\n2024. MT-Bench-101: A Fine-Grained Benchmark for Evaluating Large Language Models in Multi-Turn Dialogues. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) . Association for Computational Linguistics, 7421\u20137454.\nhttps://doi.org/10.18653/v1/2024.acl-long.401\n[14] Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, and\nLei Hou. 2023. Benchmarking Foundation Models with Language-Model-as-an-Examiner. In Advances in Neural Information Processing Systems\n36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023 , Alice Oh,\nTristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/\nf64e55d03e2fe61aa4114e49cb654acb-Abstract-Datasets_and_Benchmarks.html\n[15] Debarag Banerjee, Pooja Singh, Arjun Avadhanam, and Saksham Srivastava. 2023. Benchmarking LLM powered Chatbots: Methods and Metrics.\narXiv:2308.04624 [cs.CL] https://arxiv.org/abs/2308.04624\n[16] Satanjeev Banerjee and Alon Lavie. 2005. METEOR: An Automatic Metric for MT Evaluation with Improved Correlation with Human Judgments. In\nProceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization , Jade Goldstein, Alon\nLavie, Chin-Yew Lin, and Clare Voss (Eds.). Association for Computational Linguistics, Ann Arbor, Michigan, 65\u201372. https://aclanthology.org/W05-\n0909/\n[17] Elias Bassani and Ignacio Sanchez. 2024. GuardBench: A Large-Scale Benchmark for Guardrail Models. In Proceedings of the 2024 Conference on\nEmpirical Methods in Natural Language Processing , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational\nLinguistics, Miami, Florida, USA, 18393\u201318409. https://doi.org/10.18653/v1/2024.emnlp-main.1022\n[18] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert\nNiewiadomski, Piotr Nyczyk, and Torsten Hoefler. 2024. Graph of Thoughts: Solving Elaborate Problems with Large Language Models. Proceedings\nof the AAAI Conference on Artificial Intelligence 38, 16 (March 2024), 17682\u201317690. https://doi.org/10.1609/aaai.v38i16.29720\n[19] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey\nWu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner,\nSam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language Models are Few-Shot Learners. arXiv:2005.14165 [cs.CL]\nhttps://arxiv.org/abs/2005.14165\n[20] Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga\u0161i\u0107. 2018. MultiWOZ - A\nLarge-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling. In Proceedings of the 2018 Conference on Empirical Methods\nin Natural Language Processing , Ellen Riloff, David Chiang, Julia Hockenmaier, and Jun\u2019ichi Tsujii (Eds.). Association for Computational Linguistics,\nBrussels, Belgium, 5016\u20135026. https://doi.org/10.18653/v1/D18-1547\n[21] Pawe\u0142 Budzianowski, Tsung-Hsien Wen, Bo-Hsiang Tseng, I\u00f1igo Casanueva, Stefan Ultes, Osman Ramadan, and Milica Ga\u0161i\u0107. 2020. MultiWOZ \u2013 A\nLarge-Scale Multi-Domain Wizard-of-Oz Dataset for Task-Oriented Dialogue Modelling. arXiv:1810.00278 [cs.CL] https://arxiv.org/abs/1810.00278\n[22] Bill Byrne, Karthik Krishnamoorthi, Chinnadhurai Sankar, Arvind Neelakantan, Ben Goodrich, Daniel Duckworth, Semih Yavuz, Amit Dubey,\nKyu-Young Kim, and Andy Cedilnik. 2019. Taskmaster-1: Toward a Realistic and Diverse Dialog Dataset. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP) ,\nKentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan (Eds.). Association for Computational Linguistics, Hong Kong, China, 4516\u20134525. https:\n//doi.org/10.18653/v1/D19-1459\n[23] Xiaoyu Cai, Yao Fu, Hong Zhao, Weihao Jiang, and Shi Pu. 2022. Memory Graph with Message Rehearsal for Multi-Turn Dialogue Generation.\nProceedings of the 31st ACM International Conference on Information & Knowledge Management (2022). https://api.semanticscholar.org/CorpusID:\n252904675\n[24] Lang Cao. 2024. DiagGPT: An LLM-based and Multi-agent Dialogue System with Automatic Topic Management for Flexible Task-Oriented\nDialogue. arXiv:2308.08043 [cs.CL] https://arxiv.org/abs/2308.08043\nManuscript submitted to ACM24 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\n[25] Lang Cao. 2024. GraphReason: Enhancing Reasoning Capabilities of Large Language Models through A Graph-Based Verification Approach. In\nProceedings of the 2nd Workshop on Natural Language Reasoning and Structured Explanations (@ACL 2024) , Bhavana Dalvi Mishra, Greg Durrett,\nPeter Jansen, Ben Lipkin, Danilo Neves Ribeiro, Lionel Wong, Xi Ye, and Wenting Zhao (Eds.). Association for Computational Linguistics, Bangkok,\nThailand, 1\u201312. https://aclanthology.org/2024.nlrse-1.1/\n[26] Lang Cao. 2024. Learn to Refuse: Making Large Language Models More Controllable and Reliable through Knowledge Scope Limitation and Refusal\nMechanism. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-Onaizan, Mohit Bansal, and Yun-\nNung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 3628\u20133646. https://doi.org/10.18653/v1/2024.emnlp-main.212\n[27] David Castillo-Bolado, Joseph Davidson, Finlay Gray, and Marek Rosa. 2024. Beyond Prompts: Dynamic Conversational Benchmarking of Large\nLanguage Models. arXiv:2409.20222 [cs.CL] https://arxiv.org/abs/2409.20222\n[28] Hyungjoo Chae, Yongho Song, Kai Ong, Taeyoon Kwon, Minjin Kim, Youngjae Yu, Dongha Lee, Dongyeop Kang, and Jinyoung Yeo. 2023. Dialogue\nChain-of-Thought Distillation for Commonsense-aware Conversational Agents. In Proceedings of the 2023 Conference on Empirical Methods in\nNatural Language Processing , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 5606\u20135632.\nhttps://doi.org/10.18653/v1/2023.emnlp-main.342\n[29] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. 2023. ChatEval: Towards Better\nLLM-based Evaluators through Multi-Agent Debate. arXiv:2308.07201 [cs.CL] https://arxiv.org/abs/2308.07201\n[30] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen, Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue\nZhang, Yi Chang, Philip S. Yu, Qiang Yang, and Xing Xie. 2024. A Survey on Evaluation of Large Language Models. 15, 3, Article 39 (March 2024),\n45 pages. https://doi.org/10.1145/3641289\n[31] Harrison Chase. 2022. LangChain . https://github.com/langchain-ai/langchain\n[32] Subhajit Chaudhury, Sarathkrishna Swaminathan, Chulaka Gunasekara, Maxwell Crouse, Srinivas Ravishankar, Daiki Kimura, Keerthiram\nMurugesan, Ram\u00f3n Fernandez Astudillo, Tahira Naseem, Pavan Kapanipathi, and Alexander Gray. 2022. X-FACTOR: A Cross-metric Evaluation of\nFactual Correctness in Abstractive Summarization. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , Yoav\nGoldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 7100\u20137110.\nhttps://doi.org/10.18653/v1/2022.emnlp-main.478\n[33] Hongzhan Chen, Hehong Chen, Ming Yan, Wenshen Xu, Gao Xing, Weizhou Shen, Xiaojun Quan, Chenliang Li, Ji Zhang, and Fei Huang.\n2024. SocialBench: Sociality Evaluation of Role-Playing Conversational Agents. In Findings of the Association for Computational Linguistics:\nACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 2108\u20132126.\nhttps://doi.org/10.18653/v1/2024.findings-acl.125\n[34] Hongshen Chen, Xiaorui Liu, Dawei Yin, and Jiliang Tang. 2017. A Survey on Dialogue Systems: Recent Advances and New Frontiers. SIGKDD\nExplor. Newsl. 19, 2 (Nov. 2017), 25\u201335. https://doi.org/10.1145/3166054.3166058\n[35] Kedi Chen, Qin Chen, Jie Zhou, He Yishen, and Liang He. 2024. DiaHalu: A Dialogue-level Hallucination Evaluation Benchmark for Large Language\nModels. In Findings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.).\nAssociation for Computational Linguistics, Miami, Florida, USA, 9057\u20139079. https://doi.org/10.18653/v1/2024.findings-emnlp.529\n[36] Wenhu Chen, Xueguang Ma, Xinyi Wang, and William W. Cohen. 2023. Program of Thoughts Prompting: Disentangling Computation from\nReasoning for Numerical Reasoning Tasks. arXiv:2211.12588 [cs.CL] https://arxiv.org/abs/2211.12588\n[37] Yanbing Chen, Lin Li, Xiaohui Tao, and Dong Zhou. 2024. Persona-centric Metamorphic Relation guided Robustness Evaluation for Multi-turn\nDialogue Modelling. arXiv:2401.12483 [cs.IR] https://arxiv.org/abs/2401.12483\n[38] Yi Cheng, Wenge Liu, Wenjie Li, Jiashuo Wang, Ruihui Zhao, Bang Liu, Xiaodan Liang, and Yefeng Zheng. 2022. Improving Multi-turn Emotional\nSupport Dialogue Generation with Lookahead Strategy Planning. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language\nProcessing , Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates,\n3014\u20133026. https://doi.org/10.18653/v1/2022.emnlp-main.195\n[39] Cheng-Han Chiang and Hung-yi Lee. 2023. Can Large Language Models Be an Alternative to Human Evaluations?. In Proceedings of the 61st\nAnnual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki\n(Eds.). Association for Computational Linguistics, Toronto, Canada, 15607\u201315631. https://doi.org/10.18653/v1/2023.acl-long.870\n[40] Cheng-Han Chiang and Hung-yi Lee. 2023. A Closer Look into Using Large Language Models for Automatic Evaluation. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics,\nSingapore, 8928\u20138942. https://doi.org/10.18653/v1/2023.findings-emnlp.599\n[41] Young Min Cho, Sunny Rai, Lyle Ungar, Jo\u00e3o Sedoc, and Sharath Guntuku. 2023. An Integrative Survey on Mental Health Conversational\nAgents to Bridge Computer Science and Medical Perspectives. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language\nProcessing , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 11346\u201311369. https:\n//doi.org/10.18653/v1/2023.emnlp-main.698\n[42] Jason Ingyu Choi, Marcus Collins, Eugene Agichtein, Oleg Rokhlenko, and Shervin Malmasi. 2024. Combining Multiple Metrics for Evaluating\nRetrieval-Augmented Conversations. In Proceedings of the Third Workshop on Bridging Human\u2013Computer Interaction and Natural Language\nProcessing , Su Lin Blodgett, Amanda Cercas Curry, Sunipa Dev, Michael Madaio, Ani Nenkova, Diyi Yang, and Ziang Xiao (Eds.). Association for\nComputational Linguistics, Mexico City, Mexico, 40\u201350. https://doi.org/10.18653/v1/2024.hcinlp-1.4\n[43] Victor Costan and Srinivas Devadas. 2016. Intel SGX explained. Cryptology ePrint Archive (2016).\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 25\n[44] Gautier Dagan, Frank Keller, and Alex Lascarides. 2023. Dynamic Planning with a LLM. arXiv:2308.06391 [cs.CL] https://arxiv.org/abs/2308.06391\n[45] Yang Deng, Xuan Zhang, Wenxuan Zhang, Yifei Yuan, See-Kiong Ng, and Tat-Seng Chua. 2024. On the Multi-turn Instruction Following\nfor Conversational Web Agents. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics, Bangkok, Thailand, 8795\u20138812.\nhttps://doi.org/10.18653/v1/2024.acl-long.477\n[46] Jan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo Echegoyen, Sophie Rosset, Eneko Agirre, and Mark Cieliebak. 2020. Survey on evaluation\nmethods for dialogue systems. Artificial Intelligence Review 54, 1 (June 2020), 755\u2013810. https://doi.org/10.1007/s10462-020-09866-x\n[47] Zican Dong, Tianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-Rong Wen. 2024. BAMBOO: A Comprehensive Benchmark for Evaluating Long Text\nModeling Capacities of Large Language Models. arXiv:2309.13345 [cs.CL] https://arxiv.org/abs/2309.13345\n[48] Zhichen Dong, Zhanhui Zhou, Chao Yang, Jing Shao, and Yu Qiao. 2024. Attacks, Defenses and Evaluations for LLM Conversation Safety: A Survey.\nInProceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies\n(Volume 1: Long Papers) , Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico,\n6734\u20136747. https://doi.org/10.18653/v1/2024.naacl-long.375\n[49] Haodong Duan, Jueqi Wei, Chonghua Wang, Hongwei Liu, Yixiao Fang, Songyang Zhang, Dahua Lin, and Kai Chen. 2023. BotChat: Evaluating\nLLMs\u2019 Capabilities of Having Multi-Turn Dialogues. arXiv:2310.13650 [cs.CL] https://arxiv.org/abs/2310.13650\n[50] Zhiyuan Fan, Weinong Wang, Xing W, and Debing Zhang. 2024. SedarEval: Automated Evaluation using Self-Adaptive Rubrics. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational\nLinguistics, Miami, Florida, USA, 16916\u201316930. https://doi.org/10.18653/v1/2024.findings-emnlp.984\n[51] Jiazhan Feng, Chongyang Tao, Chang Liu, Rui Yan, and Dongyan Zhao. 2022. How to Represent Context Better? An Empirical Study on\nContext Modeling for Multi-turn Response Selection. In Findings of the Association for Computational Linguistics: EMNLP 2022 , Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 7285\u20137298. https:\n//doi.org/10.18653/v1/2022.findings-emnlp.539\n[52] Amila Ferron, Amber Shore, Ekata Mitra, and Ameeta Agrawal. 2023. MEEP: Is this Engaging? Prompting Large Language Models for Dialogue\nEvaluation in Multilingual Settings. In Findings of the Association for Computational Linguistics: EMNLP 2023 , Houda Bouamor, Juan Pino, and\nKalika Bali (Eds.). Association for Computational Linguistics, Singapore, 2078\u20132100. https://doi.org/10.18653/v1/2023.findings-emnlp.137\n[53] Zafeirios Fountas, Martin A Benfeghoul, Adnan Oomerjee, Fenia Christopoulou, Gerasimos Lampouras, Haitham Bou-Ammar, and Jun Wang. 2024.\nHuman-like Episodic Memory for Infinite Context LLMs. arXiv:2407.09450 [cs.AI] https://arxiv.org/abs/2407.09450\n[54] Dayuan Fu, Biqing Qi, Yihuai Gao, Che Jiang, Guanting Dong, and Bowen Zhou. 2024. MSI-Agent: Incorporating Multi-Scale Insight into\nEmbodied Agents for Superior Planning and Decision-Making. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 643\u2013659.\nhttps://doi.org/10.18653/v1/2024.emnlp-main.38\n[55] Tingchen Fu, Xueliang Zhao, and Rui Yan. 2023. Delving into Global Dialogue Structures: Structure Planning Augmented Response Selection for\nMulti-turn Conversations. In KDD . 495\u2013505. https://doi.org/10.1145/3580305.3599304\n[56] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan, and Graham Neubig. 2023. PAL: Program-aided\nLanguage Models. arXiv:2211.10435 [cs.CL] https://arxiv.org/abs/2211.10435\n[57] Xiang Gao, Yizhe Zhang, Michel Galley, Chris Brockett, and Bill Dolan. 2020. Dialogue Response Ranking Training with Large-Scale Human\nFeedback Data. arXiv:2009.06978 [cs.CL] https://arxiv.org/abs/2009.06978\n[58] Sarik Ghazarian, Behnam Hedayatnia, Alexandros Papangelis, Yang Liu, and Dilek Hakkani-Tur. 2022. What is wrong with you?: Leveraging User\nSentiment for Automatic Dialog Evaluation. In Findings of the Association for Computational Linguistics: ACL 2022 , Smaranda Muresan, Preslav Nakov,\nand Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 4194\u20134204. https://doi.org/10.18653/v1/2022.findings-\nacl.331\n[59] Ong Sing Goh, Yogan Jaya Kumar, Ngo Hea Choon, Pui Huang Leong, and Mohammad Safar. 2016. An Evaluation of the Conversation Agent\nSystem. In Intelligent Information and Database Systems , Ngoc Thanh Nguyen, Bogdan Trawi\u0144ski, Hamido Fujita, and Tzung-Pei Hong (Eds.).\nSpringer Berlin Heidelberg, Berlin, Heidelberg, 354\u2013365.\n[60] Lotem Golany, Filippo Galgani, Maya Mamo, Nimrod Parasol, Omer Vandsburger, Nadav Bar, and Ido Dagan. 2024. Efficient Data Generation for\nSource-grounded Information-seeking Dialogs: A Use Case for Meeting Transcripts. In Findings of the Association for Computational Linguistics:\nEMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA,\n1908\u20131925. https://doi.org/10.18653/v1/2024.findings-emnlp.106\n[61] Karthik Gopalakrishnan, Behnam Hedayatnia, Qinlang Chen, Anna Gottardi, Sanjeev Kwatra, Anu Venkatesh, Raefer Gabriel, and Dilek Hakkani-\nT\u00fcr. 2019. Topical-Chat: Towards Knowledge-Grounded Open-Domain Conversations. In Proc. Interspeech 2019 . 1891\u20131895. https://doi.org/10.\n21437/Interspeech.2019-3079\n[62] Max Goplerud. 2021. 72Methods for Analyzing Parliamentary Debates. In The Politics of Legislative Debates . Oxford University Press. https://doi.\norg/10.1093/oso/9780198849063.003.0005 arXiv:https://academic.oup.com/book/0/chapter/338573529/chapter-pdf/42812502/oso-9780198849063-\nchapter-5.pdf\n[63] Milan Gritta, Gerasimos Lampouras, and Ignacio Iacobacci. 2024. HumanRankEval: Automatic Evaluation of LMs as Conversational Assistants.\narXiv:2405.09186 [cs.CL] https://arxiv.org/abs/2405.09186\nManuscript submitted to ACM26 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\n[64] Yang Gu, Jian Cao, Yuan Guo, Shiyou Qian, and Wei Guan. 2023. Plan, Generate and Match: Scientific Workflow Recommendation with Large\nLanguage Models. In Service-Oriented Computing: 21st International Conference, ICSOC 2023, Rome, Italy, November 28 \u2013 December 1, 2023, Proceedings,\nPart I (Rome, Italy). Springer-Verlag, Berlin, Heidelberg, 86\u2013102. https://doi.org/10.1007/978-3-031-48421-6_7\n[65] Yu Gu, Yiheng Shu, Hao Yu, Xiao Liu, Yuxiao Dong, Jie Tang, Jayanth Srinivasa, Hugo Latapie, and Yu Su. 2024. Middleware for LLMs: Tools Are\nInstrumental for Language Agents in Complex Environments. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language\nProcessing , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 7646\u20137663.\nhttps://doi.org/10.18653/v1/2024.emnlp-main.436\n[66] Zishan Guo, Yufei Huang, and Deyi Xiong. 2024. CToolEval: A Chinese Benchmark for LLM-Powered Agent Evaluation in Real-World API\nInteractions. In Findings of the Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\nAssociation for Computational Linguistics, Bangkok, Thailand, 15711\u201315724. https://doi.org/10.18653/v1/2024.findings-acl.928\n[67] Prakhar Gupta, Chien-Sheng Wu, Wenhao Liu, and Caiming Xiong. 2022. DialFact: A Benchmark for Fact-Checking in Dialogue. In Proceedings of\nthe 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) , Smaranda Muresan, Preslav Nakov, and Aline\nVillavicencio (Eds.). Association for Computational Linguistics, Dublin, Ireland, 3785\u20133801. https://doi.org/10.18653/v1/2022.acl-long.263\n[68] Ilya Gusev. 2024. PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation.\narXiv:2409.06820 [cs.CL] https://arxiv.org/abs/2409.06820\n[69] G\u00fcven G\u00fczeldere and Stefano Franchi. 1995. Dialogues with colorful \u201cpersonalities\u201d of early AI. Stanford Hum. Rev. 4, 2 (July 1995), 161\u2013169.\n[70] Xue Han, Yitong Wang, Qian Hu, Pengwei Hu, Chao Deng, and Junlan Feng. 2023. Log-FGAER: Logic-Guided Fine-Grained Address Entity\nRecognition from Multi-Turn Spoken Dialogue. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Houda\nBouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 6988\u20136997. https://doi.org/10.18653/v1/2023.\nemnlp-main.432\n[71] Zengguang Hao, Jie Zhang, Binxia Xu, Yafang Wang, Gerard de Melo, and Xiaolong Li. 2023. IntentDial: An Intent Graph based Multi-Turn\nDialogue System with Reasoning Path Visualization. arXiv:2310.11818 [cs.AI] https://arxiv.org/abs/2310.11818\n[72] Islam A. Hassan and Yvette Graham. 2024. Advancing Open-Domain Conversational Agents - Designing an Engaging System for Natural Multi-Turn\nDialogue. In Proceedings of the 1st Workshop on Simulating Conversational Intelligence in Chat (SCI-CHAT 2024) , Yvette Graham, Qun Liu, Gerasimos\nLampouras, Ignacio Iacobacci, Sinead Madden, Haider Khalid, and Rameez Qureshi (Eds.). Association for Computational Linguistics, St. Julians,\nMalta, 75\u201379. https://aclanthology.org/2024.scichat-1.8/\n[73] Kostas Hatalis, Despina Christou, Joshua Myers, Steven Jones, Keith Lambert, Adam Amos-Binks, Zohreh Dannenhauer, and Dustin Dannenhauer.\n2024. Memory Matters: The Need to Improve Long-Term Memory in LLM-Agents. Proceedings of the AAAI Symposium Series (2024). https:\n//api.semanticscholar.org/CorpusID:267224915\n[74] Ji He, Jianshu Chen, Xiaodong He, Jianfeng Gao, Lihong Li, Li Deng, and Mari Ostendorf. 2016. Deep Reinforcement Learning with a Natural\nLanguage Action Space. arXiv:1511.04636 [cs.AI] https://arxiv.org/abs/1511.04636\n[75] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Xiaodong Song, and Jacob Steinhardt. 2020. Measuring Massive\nMultitask Language Understanding. ArXiv abs/2009.03300 (2020). https://api.semanticscholar.org/CorpusID:221516475\n[76] Ralf Herbrich, Tom Minka, and Thore Graepel. 2006. TrueSkill \u2122: A Bayesian Skill Rating System. In Advances in Neural Information Pro-\ncessing Systems , B. Sch\u00f6lkopf, J. Platt, and T. Hoffman (Eds.), Vol. 19. MIT Press. https://proceedings.neurips.cc/paper_files/paper/2006/file/\nf44ee263952e65b3610b8ba51229d1f9-Paper.pdf\n[77] Julian Hoxha and Marsela Thanasi-Bo\u00e7e. 2024. Blockchain and AI-Driven Framework for Measuring the Digital Economy in GCC. Emerging\nScience Journal (2024). https://api.semanticscholar.org/CorpusID:272139752\n[78] Sihao Hu, Tiansheng Huang, Fatih Ilhan, Selim Tekin, Gaowen Liu, Ramana Kompella, and Ling Liu. 2024. A Survey on Large Language Model-Based\nGame Agents. arXiv:2404.02039 [cs.AI] https://arxiv.org/abs/2404.02039\n[79] Shijue Huang, Wanjun Zhong, Jianqiao Lu, Qi Zhu, Jiahui Gao, Weiwen Liu, Yutai Hou, Xingshan Zeng, Yasheng Wang, Lifeng Shang, Xin Jiang,\nRuifeng Xu, and Qun Liu. 2024. Planning, Creation, Usage: Benchmarking LLMs for Comprehensive Tool Utilization in Real-World Complex\nScenarios. In Findings of the Association for Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association\nfor Computational Linguistics, Bangkok, Thailand, 4363\u20134400. https://doi.org/10.18653/v1/2024.findings-acl.259\n[80] Shih-Hong Huang, Ya-Fang Lin, Zeyu He, Chieh-Yang Huang, and Ting-Hao \u2019Kenneth\u2019 Huang. 2024. How Does Conversation Length Impact\nUser\u2019s Satisfaction? A Case Study of Length-Controlled Conversations with LLM-Powered Chatbots. arXiv:2404.17025 [cs.HC] https://arxiv.org/\nabs/2404.17025\n[81] Xiao Huang, Biqing Fang, Hai Wan, and Yongmei Liu. 2017. A General Multi-agent Epistemic Planner Based on Higher-order Belief Change. In\nProceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-2017) . International Joint Conferences on Artificial\nIntelligence Organization, 1093\u20131101. https://doi.org/10.24963/ijcai.2017/152\n[82] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Understanding\nthe planning of LLM agents: A survey. arXiv:2402.02716 [cs.AI] https://arxiv.org/abs/2402.02716\n[83] Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin Liu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and Lichao\nSun. 2024. MetaTool Benchmark for Large Language Models: Deciding Whether to Use Tools and Which to Use. arXiv:2310.03128 [cs.SE]\nhttps://arxiv.org/abs/2310.03128\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 27\n[84] Ziheng Huang, Sebastian Gutierrez, Hemanth Kamana, and Stephen MacNeil. 2023. Memory Sandbox: Transparent and Interactive Memory\nManagement for Conversational Agents. arXiv:2308.01542 [cs.HC] https://arxiv.org/abs/2308.01542\n[85] David R. Hunter. 2004. MM algorithms for generalized Bradley-Terry models. The Annals of Statistics 32, 1 (2004), 384 \u2013 406. https://doi.org/10.\n1214/aos/1079120141\n[86] Kai Tzu iunn Ong, Namyoung Kim, Minju Gwak, Hyungjoo Chae, Taeyoon Kwon, Yohan Jo, Seung won Hwang, Dongha Lee, and Jiny-\noung Yeo. 2024. Towards Lifelong Dialogue Agents via Relation-aware Memory Construction and Timeline-augmented Response Generation.\narXiv:2406.10996 [cs.CL] https://arxiv.org/abs/2406.10996\n[87] Jihyoung Jang, MinSeong Boo, and Hyounghun Kim. 2023. Conversation Chronicles: Towards Diverse Temporal and Relational Dynamics in Multi-\nSession Conversations. In The 2023 Conference on Empirical Methods in Natural Language Processing . https://openreview.net/forum?id=9LPJK81xy1\n[88] Patrick Jauernig, Ahmad-Reza Sadeghi, and Emmanuel Stapf. 2020. Trusted execution environments: properties, applications, and challenges. IEEE\nSecurity & Privacy 18, 2 (2020), 56\u201360.\n[89] Mourad Jbene, Abdellah Chehri, Rachid Saadane, Smail Tigani, and Gwanggil Jeon. 2024. Intent detection for task-oriented conversational agents: A\ncomparative study of recurrent neural networks and transformer models. Expert Systems (2024). https://api.semanticscholar.org/CorpusID:272030065\n[90] F. Jelinek, R. L. Mercer, L. R. Bahl, and J. K. Baker. 2005. Perplexity\u2014a measure of the difficulty of speech recognition tasks. The Journal\nof the Acoustical Society of America 62, S1 (08 2005), S63\u2013S63. https://doi.org/10.1121/1.2016299 arXiv:https://pubs.aip.org/asa/jasa/article-\npdf/62/S1/S63/11558910/s63_5_online.pdf\n[91] Qi Jia, Yizhu Liu, Siyu Ren, Kenny Q. Zhu, and Haifeng Tang. 2023. Multi-turn Response Selection using Dialogue Dependency Relations.\narXiv:2010.01502 [cs.CL] https://arxiv.org/abs/2010.01502\n[92] Zhihua Jiang, Guanghui Ye, Dongning Rao, Di Wang, and Xin Miao. 2022. IM2: an Interpretable and Multi-category Integrated Metric Framework\nfor Automatic Dialogue Evaluation. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing , Yoav Goldberg,\nZornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, Abu Dhabi, United Arab Emirates, 11091\u201311103. https:\n//doi.org/10.18653/v1/2022.emnlp-main.762\n[93] Jeff Johnson, Matthijs Douze, and Herv\u00e9 J\u00e9gou. 2017. Billion-scale similarity search with GPUs. arXiv:1702.08734 [cs.CV] https://arxiv.org/abs/\n1702.08734\n[94] Jaylen Jones, Lingbo Mo, Eric Fosler-Lussier, and Huan Sun. 2024. A Multi-Aspect Framework for Counter Narrative Evaluation using Large\nLanguage Models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human\nLanguage Technologies (Volume 2: Short Papers) , Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association for Computational Linguistics,\nMexico City, Mexico, 147\u2013168. https://doi.org/10.18653/v1/2024.naacl-short.14\n[95] Moussa Kamal Eddine, Guokan Shang, Antoine Tixier, and Michalis Vazirgiannis. 2022. FrugalScore: Learning Cheaper, Lighter and Faster\nEvaluation Metrics for Automatic Text Generation. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\n(Volume 1: Long Papers) , Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for Computational Linguistics, Dublin,\nIreland, 1305\u20131318. https://doi.org/10.18653/v1/2022.acl-long.93\n[96] Priyanka Kargupta, Ishika Agarwal, Dilek Hakkani Tur, and Jiawei Han. 2024. Instruct, Not Assist: LLM-based Multi-Turn Planning and Hierarchical\nQuestioning for Socratic Code Debugging. In Findings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal,\nand Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 9475\u20139495. https://doi.org/10.18653/v1/2024.findings-\nemnlp.553\n[97] Alex Kim, Keonwoo Kim, and Sangwon Yoon. 2024. DEBATE: Devil\u2019s Advocate-Based Assessment and Text Evaluation. arXiv:2405.09935 [cs.CL]\nhttps://arxiv.org/abs/2405.09935\n[98] Alex Kim, Keonwoo Kim, and Sangwon Yoon. 2024. DEBATE: Devil\u2018s Advocate-Based Assessment and Text Evaluation. In Findings of the Association\nfor Computational Linguistics: ACL 2024 , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.). Association for Computational Linguistics,\nBangkok, Thailand, 1885\u20131897. https://doi.org/10.18653/v1/2024.findings-acl.112\n[99] Abishek Komma, Nagesh Panyam Chandrasekarasastry, Timothy Leffel, Anuj Goyal, Angeliki Metallinou, Spyros Matsoukas, and Aram Galstyan.\n2023. Toward More Accurate and Generalizable Evaluation Metrics for Task-Oriented Dialogs. arXiv:2306.03984 [cs.CL] https://arxiv.org/abs/2306.\n03984\n[100] Eldar Kurtic, Amir Moeini, and Dan Alistarh. 2024. Mathador-LM: A Dynamic Benchmark for Mathematical Reasoning on Large Language Models.\nInProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen\n(Eds.). Association for Computational Linguistics, Miami, Florida, USA, 17020\u201317027. https://doi.org/10.18653/v1/2024.emnlp-main.946\n[101] Wai-Chung Kwan, Xingshan Zeng, Yuxin Jiang, Yufei Wang, Liangyou Li, Lifeng Shang, Xin Jiang, Qun Liu, and Kam-Fai Wong. 2024. MT-Eval: A\nMulti-Turn Capabilities Evaluation Benchmark for Large Language Models. arXiv:2401.16745 [cs.CL] https://arxiv.org/abs/2401.16745\n[102] Deuksin Kwon, Sunwoo Lee, Ki Hyun Kim, Seojin Lee, Taeyoon Kim, and Eric Davis. 2023. What, When, and How to Ground: Designing User\nPersona-Aware Conversational Agents for Engaging Dialogue. In Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics (Volume 5: Industry Track) , Sunayana Sitaram, Beata Beigman Klebanov, and Jason D Williams (Eds.). Association for Computational\nLinguistics, Toronto, Canada, 707\u2013719. https://doi.org/10.18653/v1/2023.acl-industry.68\n[103] Minseo Kwon, Yaesol Kim, and Young J. Kim. 2024. Fast and Accurate Task Planning using Neuro-Symbolic Language Models and Multi-level Goal\nDecomposition. arXiv:2409.19250 [cs.RO] https://arxiv.org/abs/2409.19250\nManuscript submitted to ACM28 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\n[104] Alon Lavie and Abhaya Agarwal. 2007. Meteor: an automatic metric for MT evaluation with high levels of correlation with human judgments.\nInProceedings of the Second Workshop on Statistical Machine Translation (Prague, Czech Republic) (StatMT \u201907) . Association for Computational\nLinguistics, USA, 228\u2013231.\n[105] Seolhwa Lee, Heuiseok Lim, and Jo\u00e3o Sedoc. 2020. An Evaluation Protocol for Generative Conversational Systems. arXiv:2010.12741 [cs.CL]\nhttps://arxiv.org/abs/2010.12741\n[106] Fangyu Lei, Qian Liu, Yiming Huang, Shizhu He, Jun Zhao, and Kang Liu. 2024. S3Eval: A Synthetic, Scalable, Systematic Evaluation Suite for\nLarge Language Models. arXiv:2310.15147 [cs.CL] https://arxiv.org/abs/2310.15147\n[107] Quinn Leng, Jacob Portes, Sam Havens, Matei Zaharia, and Michael Carbin. 2024. Long Context RAG Performance of Large Language Models.\narXiv:2411.03538 [cs.LG] https://arxiv.org/abs/2411.03538\n[108] Daliang Li, Ankit Singh Rawat, Manzil Zaheer, Xin Wang, Michal Lukasik, Andreas Veit, Felix Yu, and Sanjiv Kumar. 2023. Large Language Models\nwith Controllable Working Memory. In Findings of the Association for Computational Linguistics: ACL 2023 , Anna Rogers, Jordan Boyd-Graber, and\nNaoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 1774\u20131793. https://doi.org/10.18653/v1/2023.findings-acl.112\n[109] Dacheng Li, Rulin Shao, Anze Xie, Ying Sheng, Lianmin Zheng, Joseph Gonzalez, Ion Stoica, Xuezhe Ma, and Hao Zhang. 2023. How Long\nCan Context Length of Open-Source LLMs truly Promise?. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following . https:\n//openreview.net/forum?id=LywifFNXV5\n[110] Kenneth Li, Yiming Wang, Fernanda Vi\u00e9gas, and Martin Wattenberg. 2024. Dialogue Action Tokens: Steering Language Models in Goal-Directed\nDialogue with a Multi-Turn Planner. arXiv:2406.11978 [cs.CL] https://arxiv.org/abs/2406.11978\n[111] Margaret Li, Jason Weston, and Stephen Roller. 2019. ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn\nComparisons. arXiv:1909.03087 [cs.CL] https://arxiv.org/abs/1909.03087\n[112] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: A Comprehensive\nBenchmark for Tool-Augmented LLMs. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Houda Bouamor,\nJuan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 3102\u20133116. https://doi.org/10.18653/v1/2023.emnlp-main.187\n[113] Xinzhe Li. 2024. A Review of Prominent Paradigms for LLM-Based Agents: Tool Use (Including RAG), Planning, and Feedback Learning.\narXiv:2406.05804 [cs.AI] https://arxiv.org/abs/2406.05804\n[114] Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang Cao, and Shuzi Niu. 2017. DailyDialog: A Manually Labelled Multi-turn Dialogue Dataset.\narXiv:1710.03957 [cs.CL] https://arxiv.org/abs/1710.03957\n[115] Yixuan Li, Fangzhen Wu, Yanan Jiang, Yongbin Xie, and Xiaojiang Xu. 2020. Improve the Response Diversity of Multi-turn Dialogue System\nwith Bidirectional Distillation. In Proceedings of the 28th International Conference on Computational Linguistics . International Committee on\nComputational Linguistics, 1652\u20131662. https://doi.org/10.18653/v1/2020.coling-main.145\n[116] Youquan Li, Miao Zheng, Fan Yang, Guosheng Dong, Bin Cui, Weipeng Chen, Zenan Zhou, and Wentao Zhang. 2024. FB-Bench: A Fine-Grained\nMulti-Task Benchmark for Evaluating LLMs\u2019 Responsiveness to Human Feedback. arXiv:2410.09412 [cs.CL] https://arxiv.org/abs/2410.09412\n[117] Xinnian Liang, Bing Wang, Hui Huang, Shuangzhi Wu, Peihao Wu, Lu Lu, Zejun Ma, and Zhoujun Li. 2023. Unleashing Infinite-Length Input Capacity\nfor Large-scale Language Models with Self-Controlled Memory System. ArXiv abs/2304.13343 (2023). https://api.semanticscholar.org/CorpusID:\n268241203\n[118] Jonathan Light, Min Cai, Weiqin Chen, Guanzhi Wang, Xiusi Chen, Wei Cheng, Yisong Yue, and Ziniu Hu. 2024. Strategist: Learning Strategic\nSkills by LLMs via Bi-Level Tree Search. arXiv:2408.10635 [cs.AI] https://arxiv.org/abs/2408.10635\n[119] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of Summaries. In Text Summarization Branches Out . Association for Computational\nLinguistics, Barcelona, Spain, 74\u201381. https://aclanthology.org/W04-1013\n[120] Fei Lin, Cong Zhang, Shengqiang Liu, and Hong Ma. 2020. A Hierarchical Structured Multi-Head Attention Network for Multi-Turn Response\nGeneration. IEEE Access 8 (2020), 46802\u201346810. https://doi.org/10.1109/ACCESS.2020.2977471\n[121] Yen-Ting Lin and Yun-Nung Chen. 2023. LLM-Eval: Unified Multi-Dimensional Automatic Evaluation for Open-Domain Conversations with Large\nLanguage Models. In Proceedings of the 5th Workshop on NLP for Conversational AI (NLP4ConvAI 2023) , Yun-Nung Chen and Abhinav Rastogi (Eds.).\nAssociation for Computational Linguistics, Toronto, Canada, 47\u201358. https://doi.org/10.18653/v1/2023.nlp4convai-1.5\n[122] Anqi Liu, Bo Wang, Yue Tan, Dongming Zhao, Kun Huang, Ruifang He, and Yuexian Hou. 2023. MTGP: Multi-turn Target-oriented Dialogue Guided\nby Generative Global Path with Flexible Turns. In Findings of the Association for Computational Linguistics: ACL 2023 , Anna Rogers, Jordan Boyd-\nGraber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 259\u2013271. https://doi.org/10.18653/v1/2023.findings-\nacl.18\n[123] Bing Liu, Zhou Jianxiang, Dan Meng, and Haonan Lu. 2024. An Evaluation Mechanism of LLM-based Agents on Manipulating APIs. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational\nLinguistics, Miami, Florida, USA, 4649\u20134662. https://doi.org/10.18653/v1/2024.findings-emnlp.267\n[124] Hang Liu, Meng Chen, Youzheng Wu, Xiaodong He, and Bowen Zhou. 2021. Conversational Query Rewriting with Self-supervised Learning.\narXiv:2102.04708 [cs.CL] https://arxiv.org/abs/2102.04708\n[125] Hao Liu, Zi-Yi Dou, Yixin Wang, Nanyun Peng, and Yisong Yue. 2024. Uncertainty Calibration for Tool-Using Language Agents. In Findings of the\nAssociation for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational\nLinguistics, Miami, Florida, USA, 16781\u201316805. https://doi.org/10.18653/v1/2024.findings-emnlp.978\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 29\n[126] Junhua Liu, Tan Yong Keat, Bin Fu, and Kwan Hui Lim. 2024. LARA: Linguistic-Adaptive Retrieval-Augmentation for Multi-Turn Intent Classification.\nInProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track , Franck Dernoncourt, Daniel Preo\u0163iuc-Pietro,\nand Anastasia Shimorina (Eds.). Association for Computational Linguistics, Miami, Florida, US, 1096\u20131106. https://doi.org/10.18653/v1/2024.emnlp-\nindustry.82\n[127] Lei Liu, Xiaoyan Yang, Yue Shen, Binbin Hu, Zhiqiang Zhang, Jinjie Gu, and Guannan Zhang. 2023. Think-in-Memory: Recalling and Post-thinking\nEnable LLMs with Long-Term Memory. arXiv:2311.08719 [cs.CL] https://arxiv.org/abs/2311.08719\n[128] Na Liu, Liangyu Chen, Xiaoyu Tian, Wei Zou, Kaijiang Chen, and Ming Cui. 2024. From LLM to Conversational Agent: A Memory Enhanced\nArchitecture with Fine-Tuning of Large Language Models. arXiv:2401.02777 [cs.CL] https://arxiv.org/abs/2401.02777\n[129] Shuo Liu, Kaining Ying, Hao Zhang, Yue Yang, Yuqi Lin, Tianle Zhang, Chuanhao Li, Yu Qiao, Ping Luo, Wenqi Shao, and Kaipeng Zhang.\n2024. ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Capability for Large Vision-Language Models.\narXiv:2403.20194 [cs.MM] https://arxiv.org/abs/2403.20194\n[130] Xinyi Liu, Pinxin Liu, and Hangfeng He. 2024. An Empirical Analysis on Large Language Models in Debate Evaluation. In Proceedings of the 62nd\nAnnual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers) , Lun-Wei Ku, Andre Martins, and Vivek Srikumar (Eds.).\nAssociation for Computational Linguistics, Bangkok, Thailand, 470\u2013487. https://doi.org/10.18653/v1/2024.acl-short.44\n[131] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang\nDeng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. 2023.\nAgentBench: Evaluating LLMs as Agents. arXiv:2308.03688 [cs.AI] https://arxiv.org/abs/2308.03688\n[132] Yizhu Liu, Qi Jia, and Kenny Zhu. 2022. Reference-free Summarization Evaluation via Semantic Correlation and Compression Ratio. In Proceedings\nof the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies , Marine\nCarpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle, United States,\n2109\u20132115. https://doi.org/10.18653/v1/2022.naacl-main.153\n[133] Ziyu Liu, Tao Chu, Yuhang Zang, Xilin Wei, Xiaoyi Dong, Pan Zhang, Zijian Liang, Yuanjun Xiong, Yu Qiao, Dahua Lin, and Jiaqi Wang. 2024.\nMMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs. arXiv:2406.11833 [cs.CV]\nhttps://arxiv.org/abs/2406.11833\n[134] Zijun Liu, Yanzhe Zhang, Peng Li, Yang Liu, and Diyi Yang. 2024. A Dynamic LLM-Powered Agent Network for Task-Oriented Agent Collaboration.\narXiv:2310.02170 [cs.CL] https://arxiv.org/abs/2310.02170\n[135] Zhiling Luo, Qiankun Shi, Sha Zhao, Wei Zhou, Haiqing Chen, Yuankai Ma, and Haitao Leng. 2022. AliCHI: A Large-scale Multi-modal Dataset and\nAutomated Evaluation Tool for Human-like Dialogue Systems. arXiv:2212.05489 [cs.HC] https://arxiv.org/abs/2212.05489\n[136] Daoming Lyu, Bo Liu, and Jianshu Chen. 2022. PRIMA: Planner-Reasoner Inside a Multi-task Reasoning Agent. arXiv:2202.00531 [cs.AI]\nhttps://arxiv.org/abs/2202.00531\n[137] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming\nYang, Shashank Gupta, Bodhisattwa Prasad Majumder, Katherine Hermann, Sean Welleck, Amir Yazdanbakhsh, and Peter Clark. 2023. Self-Refine:\nIterative Refinement with Self-Feedback. arXiv:2303.17651 [cs.CL] https://arxiv.org/abs/2303.17651\n[138] Adyasha Maharana, Dong-Ho Lee, Sergey Tulyakov, Mohit Bansal, Francesco Barbieri, and Yuwei Fang. 2024. Evaluating Very Long-Term\nConversational Memory of LLM Agents. arXiv:2402.17753 [cs.CL] https://arxiv.org/abs/2402.17753\n[139] Shengyu Mao, Xiaohan Wang, Mengru Wang, Yong Jiang, Pengjun Xie, Fei Huang, and Ningyu Zhang. 2024. Editing Personality for Large Language\nModels. arXiv:2310.02168 [cs.CL] https://arxiv.org/abs/2310.02168\n[140] Laiba Mehnaz, Debanjan Mahata, Rakesh Gosangi, Uma Sushmitha Gunturi, Riya Jain, Gauri Gupta, Amardeep Kumar, Isabelle G. Lee, Anish\nAcharya, and Rajiv Ratn Shah. 2021. GupShup: Summarizing Open-Domain Code-Switched Conversations. In Conference on Empirical Methods in\nNatural Language Processing . https://api.semanticscholar.org/CorpusID:243865218\n[141] Ninareh Mehrabi, Ahmad Beirami, Fred Morstatter, and Aram Galstyan. 2022. Robust Conversational Agents against Imperceptible Toxicity\nTriggers. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language\nTechnologies , Marine Carpuat, Marie-Catherine de Marneffe, and Ivan Vladimir Meza Ruiz (Eds.). Association for Computational Linguistics, Seattle,\nUnited States, 2831\u20132847. https://doi.org/10.18653/v1/2022.naacl-main.204\n[142] Shikib Mehri, Mihail Eric, and Dilek Hakkani-Tur. 2020. DialoGLUE: A Natural Language Understanding Benchmark for Task-Oriented Dialogue.\narXiv:2009.13570 [cs.CL] https://arxiv.org/abs/2009.13570\n[143] Erinc Merdivan, Deepika Singh, Sten Hanke, Johannes Kropf, Andreas Holzinger, and Matthieu Geist. 2020. Human Annotated Dialogues Dataset\nfor Natural Conversational Agents. Applied Sciences (2020). https://api.semanticscholar.org/CorpusID:210874219\n[144] Erinc Merdivan, Deepika Singh, Sten Hanke, Johannes Kropf, Andreas Holzinger, and Matthieu Geist. 2020. Human Annotated Dialogues Dataset\nfor Natural Conversational Agents. Applied Sciences 10, 3 (2020). https://doi.org/10.3390/app10030762\n[145] Gr\u00e9goire Mialon, Cl\u00e9mentine Fourrier, Thomas Wolf, Yann LeCun, and Thomas Scialom. 2024. GAIA: a benchmark for General AI Assistants. In\nThe Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024 . OpenReview.net. https://openreview.\nnet/forum?id=fibxvahvs3\n[146] Erik Miehling, Manish Nagireddy, Prasanna Sattigeri, Elizabeth M. Daly, David Piorkowski, and John T. Richards. 2024. Language Models\nin Dialogue: Conversational Maxims for Human-AI Interactions. In Findings of the Association for Computational Linguistics: EMNLP 2024 ,\nYaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 14420\u201314437.\nManuscript submitted to ACM30 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\nhttps://doi.org/10.18653/v1/2024.findings-emnlp.843\n[147] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D. Manning, and Chelsea Finn. 2022. Memory-Based Model Editing at Scale.\narXiv:2206.06520 [cs.AI] https://arxiv.org/abs/2206.06520\n[148] Fengran Mo, Chen Qu, Kelong Mao, Yihong Wu, Zhan Su, Kaiyu Huang, and Jian-Yun Nie. 2024. Aligning Query Representation with Rewritten\nQuery and Relevance Judgments in Conversational Search. arXiv:2407.20189 [cs.IR] https://arxiv.org/abs/2407.20189\n[149] Ali Modarressi, Ayyoob Imani, Mohsen Fayyaz, and Hinrich Sch\u00fctze. 2024. RET-LLM: Towards a General Read-Write Memory for Large Language\nModels. arXiv:2305.14322 [cs.CL] https://arxiv.org/abs/2305.14322\n[150] Behrad Moniri, Hamed Hassani, and Edgar Dobriban. 2024. Evaluating the Performance of Large Language Models via Debates.\narXiv:2406.11044 [cs.CL] https://arxiv.org/abs/2406.11044\n[151] Christian Muise, Tathagata Chakraborti, Shubham Agarwal, Ondrej Bajgar, Arunima Chaudhary, Luis A. Lastras-Montano, Josef Ondrej, Miroslav\nVodolan, and Charlie Wiecha. 2019. Planning for Goal-Oriented Dialogue Systems. arXiv:1910.08137 [cs.AI] https://arxiv.org/abs/1910.08137\n[152] Vinod Muthusamy, Yara Rizk, Kiran Kate, Praveen Venkateswaran, Vatche Isahagian, Ashu Gulati, and Parijat Dube. 2023. Towards large\nlanguage model-based personal agents in the enterprise: Current trends and open problems. In Findings of the Association for Computational\nLinguistics: EMNLP 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 6909\u20136921.\nhttps://doi.org/10.18653/v1/2023.findings-emnlp.461\n[153] Linyong Nan, Ellen Zhang, Weijin Zou, Yilun Zhao, Wenfei Zhou, and Arman Cohan. 2024. On Evaluating the Integration of Reasoning and Action in\nLLM Agents with Database Question Answering. In Findings of the Association for Computational Linguistics: NAACL 2024 , Kevin Duh, Helena Gomez,\nand Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 4556\u20134579. https://doi.org/10.18653/v1/2024.findings-\nnaacl.284\n[154] Jinjie Ni, Tom Young, Vlad Pandelea, Fuzhao Xue, and Erik Cambria. 2022. Recent Advances in Deep Learning Based Dialogue Systems: A\nSystematic Survey. arXiv:2105.04387 [cs.CL] https://arxiv.org/abs/2105.04387\n[155] Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023. CodeGen: An Open\nLarge Language Model for Code with Multi-Turn Program Synthesis. In The Eleventh International Conference on Learning Representations .\nhttps://openreview.net/forum?id=iaYcJKpY2B_\n[156] Takuma Okubo and Masaki Takahashi. 2023. Multi-Agent Action Graph Based Task Allocation and Path Planning Considering Changes in\nEnvironment. IEEE Access 11 (2023), 21160\u201321175. https://doi.org/10.1109/ACCESS.2023.3249757\n[157] Oluwatobi Olabiyi, Alan O Salimov, Anish Khazane, and Erik Mueller. 2019. Multi-turn Dialogue Response Generation in an Adversarial Learning\nFramework. In Proceedings of the First Workshop on NLP for Conversational AI , Yun-Nung Chen, Tania Bedrax-Weiss, Dilek Hakkani-Tur, Anuj\nKumar, Mike Lewis, Thang-Minh Luong, Pei-Hao Su, and Tsung-Hsien Wen (Eds.). Association for Computational Linguistics, Florence, Italy,\n121\u2013132. https://doi.org/10.18653/v1/W19-4114\n[158] OpenAI. 2023. ChatGPT. https://chat.openai.com. https://chat.openai.com Accessed: November 19, 2024.\n[159] OpenAI. 2024. ChatGPT Plugins. https://openai.com/index/chatgpt-plugins/ Accessed: 2024-11-21.\n[160] OpenAI. 2024. GPT-4 Technical Report. arXiv preprint arXiv:2303.08774 (March 2024). arXiv:2303.08774 [cs.CL] https://arxiv.org/abs/2303.08774\nLast revised on March 4, 2024.\n[161] OpenAI. 2025. Learning to Reason with LLMs. https://openai.com/index/learning-to-reason-with-llms/ Accessed: 2025-01-06.\n[162] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex\nRay, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan\nLowe. 2022. Training language models to follow instructions with human feedback. arXiv:2203.02155 [cs.CL] https://arxiv.org/abs/2203.02155\n[163] Matthew J Page, Joanne E McKenzie, Patrick M Bossuyt, Isabelle Boutron, Tammy C Hoffmann, Cynthia D Mulrow, Larissa Shamseer, Jennifer M\nTetzlaff, Elie A Akl, Sue E Brennan, et al .2021. The PRISMA 2020 statement: an updated guideline for reporting systematic reviews. bmj372 (2021).\n[164] Arka Pal, Deep Karkhanis, Manley Roberts, Samuel Dooley, Arvind Sundararajan, and Siddartha Naidu. 2023. Giraffe: Adventures in Expanding\nContext Lengths in LLMs. arXiv:2308.10882 [cs.AI] https://arxiv.org/abs/2308.10882\n[165] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. BLEU: a method for automatic evaluation of machine translation. In\nProceedings of the 40th Annual Meeting on Association for Computational Linguistics (Philadelphia, Pennsylvania) (ACL \u201902) . Association for\nComputational Linguistics, USA, 311\u2013318. https://doi.org/10.3115/1073083.1073135\n[166] ChaeHun Park, Minseok Choi, Dohyun Lee, and Jaegul Choo. 2024. PairEval: Open-domain Dialogue Evaluation with Pairwise Comparison.\narXiv:2404.01015 [cs.CL] https://arxiv.org/abs/2404.01015\n[167] Joon Sung Park, Joseph C. O\u2019Brien, Carrie J. Cai, Meredith Ringel Morris, Percy Liang, and Michael S. Bernstein. 2023. Generative Agents: Interactive\nSimulacra of Human Behavior. arXiv:2304.03442 [cs.HC] https://arxiv.org/abs/2304.03442\n[168] Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. 2023. Gorilla: Large Language Model Connected with Massive APIs.\narXiv:2305.15334 [cs.CL] https://arxiv.org/abs/2305.15334\n[169] Vitou Phy, Yang Zhao, and Akiko Aizawa. 2020. Deconstruct to Reconstruct a Configurable Evaluation Metric for Open-Domain Dialogue Systems.\narXiv:2011.00483 [cs.CL] https://arxiv.org/abs/2011.00483\n[170] Matt Post. 2018. A Call for Clarity in Reporting BLEU Scores. arXiv:1804.08771 [cs.CL] https://arxiv.org/abs/1804.08771\n[171] Archiki Prasad, Alexander Koller, Mareike Hartmann, Peter Clark, Ashish Sabharwal, Mohit Bansal, and Tushar Khot. 2024. ADaPT: As-Needed\nDecomposition and Planning with Language Models. In Findings of the Association for Computational Linguistics: NAACL 2024 , Kevin Duh, Helena\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 31\nGomez, and Steven Bethard (Eds.). Association for Computational Linguistics, Mexico City, Mexico, 4226\u20134252. https://doi.org/10.18653/v1/2024.\nfindings-naacl.264\n[172] Ofir Press, Muru Zhang, Sewon Min, Ludwig Schmidt, Noah A. Smith, and Mike Lewis. 2023. Measuring and Narrowing the Compositionality Gap\nin Language Models. arXiv:2210.03350 [cs.CL] https://arxiv.org/abs/2210.03350\n[173] Shanbao Qiao, Xuebing Liu, and Seung-Hoon Na. 2024. COMEM: In-Context Retrieval-Augmented Mass-Editing Memory in Large Language\nModels. In Findings of the Association for Computational Linguistics: NAACL 2024 , Kevin Duh, Helena Gomez, and Steven Bethard (Eds.). Association\nfor Computational Linguistics, Mexico City, Mexico, 2333\u20132347. https://doi.org/10.18653/v1/2024.findings-naacl.151\n[174] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, Sihan Zhao, Lauren Hong,\nRunchu Tian, Ruobing Xie, Jie Zhou, Mark Gerstein, Dahai Li, Zhiyuan Liu, and Maosong Sun. 2023. ToolLLM: Facilitating Large Language Models\nto Master 16000+ Real-world APIs. arXiv:2307.16789 [cs.AI] https://arxiv.org/abs/2307.16789\n[175] Huachuan Qiu, Hongliang He, Shuai Zhang, Anqi Li, and Zhenzhong Lan. 2024. SMILE: Single-turn to Multi-turn Inclusive Language Expansion\nvia ChatGPT for Mental Health Support. In Findings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal,\nand Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 615\u2013636. https://doi.org/10.18653/v1/2024.findings-\nemnlp.34\n[176] L. Rabiner and B. Juang. 1986. An introduction to hidden Markov models. IEEE ASSP Magazine 3, 1 (1986), 4\u201316. https://doi.org/10.1109/MASSP.\n1986.1165342\n[177] Alec Radford and Karthik Narasimhan. 2018. Improving Language Understanding by Generative Pre-Training. https://api.semanticscholar.org/\nCorpusID:49313245\n[178] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language Models are Unsupervised Multitask Learners.\nhttps://api.semanticscholar.org/CorpusID:160025533\n[179] Merle M. Reimann, Catharine Oertel, Florian A. Kunneman, and Koen V. Hindriks. 2023. Predicting Interaction Quality Aspects Using Level-\nBased Scores for Conversational Agents. Proceedings of the 23rd ACM International Conference on Intelligent Virtual Agents (2023). https:\n//api.semanticscholar.org/CorpusID:266437223\n[180] Liliang Ren, Mankeerat Sidhu, Qi Zeng, Revanth Gangi Reddy, Heng Ji, and ChengXiang Zhai. 2023. C-PMI: Conditional Pointwise Mutual\nInformation for Turn-level Dialogue Evaluation. arXiv:2306.15245 [cs.CL] https://arxiv.org/abs/2306.15245\n[181] Stephen E. Robertson. 2004. Understanding inverse document frequency: on theoretical arguments for IDF. J. Documentation 60 (2004), 503\u2013520.\nhttps://api.semanticscholar.org/CorpusID:8864928\n[182] Ramon Ruiz-Dolz, Stella Heras, and Ana Garcia. 2023. Automatic Debate Evaluation with Argumentation Semantics and Natural Language\nArgument Graph Networks. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing , Houda Bouamor, Juan Pino,\nand Kalika Bali (Eds.). Association for Computational Linguistics, Singapore, 6030\u20136040. https://doi.org/10.18653/v1/2023.emnlp-main.368\n[183] Gabriel Sarch, Yue Wu, Michael Tarr, and Katerina Fragkiadaki. 2023. Open-Ended Instructable Embodied Agents with Memory-Augmented Large\nLanguage Models. In Findings of the Association for Computational Linguistics: EMNLP 2023 , Houda Bouamor, Juan Pino, and Kalika Bali (Eds.).\nAssociation for Computational Linguistics, Singapore, 3468\u20133500. https://doi.org/10.18653/v1/2023.findings-emnlp.226\n[184] Aalok Sathe, Salar Ather, Tuan Manh Le, Nathan Perry, and Joonsuk Park. 2020. Automated Fact-Checking of Claims from Wikipedia. In Proceedings\nof the Twelfth Language Resources and Evaluation Conference , Nicoletta Calzolari, Fr\u00e9d\u00e9ric B\u00e9chet, Philippe Blache, Khalid Choukri, Christopher\nCieri, Thierry Declerck, Sara Goggi, Hitoshi Isahara, Bente Maegaard, Joseph Mariani, H\u00e9l\u00e8ne Mazo, Asuncion Moreno, Jan Odijk, and Stelios\nPiperidis (Eds.). European Language Resources Association, Marseille, France, 6874\u20136882. https://aclanthology.org/2020.lrec-1.849/\n[185] Aalok Sathe and Joonsuk Park. 2021. Automatic Fact-Checking with Document-level Annotations using BERT and Multiple Instance Learning.\nInProceedings of the Fourth Workshop on Fact Extraction and VERification (FEVER) , Rami Aly, Christos Christodoulopoulos, Oana Cocarascu,\nZhijiang Guo, Arpit Mittal, Michael Schlichtkrull, James Thorne, and Andreas Vlachos (Eds.). Association for Computational Linguistics, Dominican\nRepublic, 101\u2013107. https://doi.org/10.18653/v1/2021.fever-1.11\n[186] Ritvik Setty and Vinay Setty. 2024. QuestGen: Effectiveness of Question Generation Methods for Fact-Checking Applications. In Proceedings of the\n33rd ACM International Conference on Information and Knowledge Management (CIKM \u201924) . ACM, 4036\u20134040. https://doi.org/10.1145/3627673.3679985\n[187] Junxiao Shen, Boyin Yang, John J. Dudley, and Per Ola Kristensson. 2022. KWickChat: A Multi-Turn Dialogue System for AAC Using Context-\nAware Sentence Generation by Bag-of-Keywords. Proceedings of the 27th International Conference on Intelligent User Interfaces (2022). https:\n//api.semanticscholar.org/CorpusID:247585177\n[188] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting Zhuang. 2023. HuggingGPT: Solving AI Tasks with ChatGPT and\nits Friends in Hugging Face. arXiv:2303.17580 [cs.CL] https://arxiv.org/abs/2303.17580\n[189] Yuanhao Shen, Xiaodan Zhu, and Lei Chen. 2024. SMARTCAL: An Approach to Self-Aware Tool-Use Evaluation and Calibration. In Proceedings of\nthe 2024 Conference on Empirical Methods in Natural Language Processing: Industry Track , Franck Dernoncourt, Daniel Preo\u0163iuc-Pietro, and Anastasia\nShimorina (Eds.). Association for Computational Linguistics, Miami, Florida, US, 774\u2013789. https://doi.org/10.18653/v1/2024.emnlp-industry.59\n[190] Wentao Shi, Mengqi Yuan, Junkang Wu, Qifan Wang, and Fuli Feng. 2024. Direct Multi-Turn Preference Optimization for Language Agents. In\nProceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen\n(Eds.). Association for Computational Linguistics, Miami, Florida, USA, 2312\u20132324. https://doi.org/10.18653/v1/2024.emnlp-main.138\n[191] Zhengliang Shi, Shen Gao, Xiuyi Chen, Yue Feng, Lingyong Yan, Haibo Shi, Dawei Yin, Pengjie Ren, Suzan Verberne, and Zhaochun Ren.\n2024. Learning to Use Tools via Cooperative and Interactive Agents. In Findings of the Association for Computational Linguistics: EMNLP 2024 ,\nManuscript submitted to ACM32 Shengyue Guan, Haoyi Xiong, Jindong Wang, Jiang Bian, Bin Zhu, and Jian-guang Lou\nYaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 10642\u201310657.\nhttps://doi.org/10.18653/v1/2024.findings-emnlp.624\n[192] Zhengliang Shi, Weiwei Sun, Shuo Zhang, Zhen Zhang, Pengjie Ren, and Zhaochun Ren. 2023. RADE: Reference-Assisted Dialogue Evaluation\nfor Open-Domain Dialogue. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers) ,\nAnna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki (Eds.). Association for Computational Linguistics, Toronto, Canada, 12856\u201312875. https:\n//doi.org/10.18653/v1/2023.acl-long.719\n[193] Jeonghoon Shim, Gyuhyeon Seo, Cheongsu Lim, and Yohan Jo. 2025. ToolDial: Multi-turn Dialogue Generation Method for Tool-Augmented\nLanguage Models. In The Thirteenth International Conference on Learning Representations . https://openreview.net/forum?id=J1J5eGJsKZ\n[194] Harmanpreet Singh, Nikhil Verma, Yixiao Wang, Manasa Bharadwaj, Homa Fashandi, Kevin Ferreira, and Chul Lee. 2024. Personal Large Language\nModel Agents: A Case Study on Tailored Travel Planning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing:\nIndustry Track , Franck Dernoncourt, Daniel Preo\u0163iuc-Pietro, and Anastasia Shimorina (Eds.). Association for Computational Linguistics, Miami,\nFlorida, US, 486\u2013514. https://doi.org/10.18653/v1/2024.emnlp-industry.37\n[195] Ishika Singh, Valts Blukis, Arsalan Mousavian, Ankit Goyal, Danfei Xu, Jonathan Tremblay, Dieter Fox, Jesse Thomason, and Animesh Garg. 2022.\nProgPrompt: Generating Situated Robot Task Plans using Large Language Models. arXiv:2209.11302 [cs.RO] https://arxiv.org/abs/2209.11302\n[196] Ishika Singh, David Traum, and Jesse Thomason. 2024. TwoStep: Multi-agent Task Planning using Classical Planners and Large Language Models.\narXiv:2403.17246 [cs.AI] https://arxiv.org/abs/2403.17246\n[197] Ved Sirdeshmukh, Kaustubh Deshpande, Johannes Mols, Lifeng Jin, Ed-Yeremai Cardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer\nYue, and Chen Xing. 2025. MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark Challenging to Frontier LLMs. CoRR\nabs/2501.17399 (2025). https://doi.org/10.48550/ARXIV.2501.17399 arXiv:2501.17399\n[198] Eric Smith, Orion Hsu, Rebecca Qian, Stephen Roller, Y-Lan Boureau, and Jason Weston. 2022. Human Evaluation of Conversations is an Open\nProblem: comparing the sensitivity of various methods for evaluating dialogue agents. In Proceedings of the 4th Workshop on NLP for Conversational\nAI, Bing Liu, Alexandros Papangelis, Stefan Ultes, Abhinav Rastogi, Yun-Nung Chen, Georgios Spithourakis, Elnaz Nouri, and Weiyan Shi (Eds.).\nAssociation for Computational Linguistics, Dublin, Ireland, 77\u201397. https://doi.org/10.18653/v1/2022.nlp4convai-1.8\n[199] Guijin Son, Hyunwoo Ko, Hoyoung Lee, Yewon Kim, and Seunghyeok Hong. 2024. LLM-as-a-Judge & Reward Model: What They Can and Cannot\nDo. arXiv:2409.11239 [cs.CL] https://arxiv.org/abs/2409.11239\n[200] Alessandro Sordoni, Michel Galley, Michael Auli, Chris Brockett, Yangfeng Ji, Margaret Mitchell, Jian-Yun Nie, Jianfeng Gao, and Bill Dolan. 2015.\nA Neural Network Approach to Context-Sensitive Generation of Conversational Responses. In Proceedings of the 2015 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies , Rada Mihalcea, Joyce Chai, and Anoop Sarkar\n(Eds.). Association for Computational Linguistics, Denver, Colorado, 196\u2013205. https://doi.org/10.3115/v1/N15-1020\n[201] Akshaya Kesarimangalam Srinivasan, Shambhavi Singh, Geordan Gutow, Howie Choset, and Bhaskar Vundurthy. 2023. Multi-agent Collective\nConstruction using 3D Decomposition. arXiv:2309.00985 [cs.RO] https://arxiv.org/abs/2309.00985\n[202] Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul F Christiano.\n2020. Learning to summarize with human feedback. In Advances in Neural Information Processing Systems , H. Larochelle, M. Ranzato, R. Had-\nsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 3008\u20133021. https://proceedings.neurips.cc/paper_files/paper/2020/file/\n1f89885d556929e98d3ef9b86448f951-Paper.pdf\n[203] Hui Su, Xiaoyu Shen, Rongzhi Zhang, Fei Sun, Pengwei Hu, Cheng Niu, and Jie Zhou. 2019. Improving Multi-turn Dialogue Modelling with\nUtterance ReWriter. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , Anna Korhonen, David Traum, and\nLlu\u00eds M\u00e0rquez (Eds.). Association for Computational Linguistics, Florence, Italy, 22\u201331. https://doi.org/10.18653/v1/P19-1003\n[204] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. 2023. AdaPlanner: Adaptive Planning from Feedback with Language Models.\narXiv:2305.16653 [cs.CL] https://arxiv.org/abs/2305.16653\n[205] Kai Sun, Yushi Bai, Ji Qi, Lei Hou, and Juanzi Li. 2024. MM-MATH: Advancing Multimodal Math Evaluation with Process Evaluation and Fine-\ngrained Classification. In Findings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung\nChen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 1358\u20131375. https://doi.org/10.18653/v1/2024.findings-emnlp.73\n[206] Jihoon Tack, Jaehyung Kim, Eric Mitchell, Jinwoo Shin, Yee Whye Teh, and Jonathan Richard Schwarz. 2024. Online Adaptation of Language\nModels with a Memory of Amortized Contexts. arXiv:2403.04317 [cs.LG] https://arxiv.org/abs/2403.04317\n[207] Ekrem Talha Selamet and Borahan T\u00fcmer. 2022. Context Detection and Identification In Multi-Agent Reinforcement Learning With Non-Stationary\nEnvironment. In 2022 30th Signal Processing and Communications Applications Conference (SIU) . 1\u20134. https://doi.org/10.1109/SIU55565.2022.9864802\n[208] Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, and Meng Jiang. 2024. Democratizing Large Language Models via Personalized\nParameter-Efficient Fine-tuning. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing , Yaser Al-Onaizan,\nMohit Bansal, and Yun-Nung Chen (Eds.). Association for Computational Linguistics, Miami, Florida, USA, 6476\u20136491. https://doi.org/10.18653/v1/\n2024.emnlp-main.372\n[209] Javier Caballero Test\u00f3n and MariaD. R-Moreno. 2024. Multi-Agent Temporal Task Solving and Plan Optimization. In 34th International Conference\non Automated Planning and Scheduling . https://openreview.net/forum?id=sPSw73rhQB\n[210] Jacob-Junqi Tian, Hao Yu, Yury Orlovskiy, Tyler Vergho, Mauricio Rivera, Mayank Goel, Zachary Yang, Jean-Francois Godbout, Reihaneh\nRabbany, and Kellin Pelrine. 2024. Web Retrieval Agents for Evidence-Based Misinformation Detection. arXiv:2409.00009 [cs.IR] https:\n//arxiv.org/abs/2409.00009\nManuscript submitted to ACMEvaluating LLM-based Agents for Multi-Turn Conversations: A Survey 33\n[211] Terry Tong, Qin Liu, Jiashu Xu, and Muhao Chen. 2024. Securing Multi-turn Conversational Language Models From Distributed Backdoor Attacks.\nInFindings of the Association for Computational Linguistics: EMNLP 2024 , Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen (Eds.). Association\nfor Computational Linguistics, Miami, Florida, USA, 12833\u201312846. https://doi.org/10.18653/v1/2024.findings-emnlp.750\n[212] A. M. Turing. 1995. Computing machinery and intelligence . MIT Press, Cambridge, MA, USA, 11\u201335.\n[213] Szymon Tworkowski, Konrad Staniszewski, Miko\u0142aj Pacek, Yuhuai Wu, Henryk Michalewski, and Piotr Mi\u0142o\u015b. 2023. Focused Transformer:\nContrastive Training for Context Scaling. arXiv:2307.03170 [cs.CL] https://arxiv.org/abs/2307.03170\n[214] Thao Nguyen Van, Nugroho Fredivianus, Huu Tam Tran, Kurt Geihs, and Thi Thanh Binh Huynh. 2018. Formal Verification of ALICA Multi-agent\nPlans Using Model Checking. In Proceedings of the 9th International Symposium on Information and Communication Technology (Danang City, Viet\nNam) (SoICT \u201918) . Association for Computing Machinery, New York, NY, USA, 351\u2013358. https://doi.org/10.1145/3287921.3287947\n[215] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is\nall you need. In Proceedings of the 31st International Conference on Neural Information Processing Systems (Long Beach, California, USA) (NIPS\u201917) .\nCurran Associates Inc., Red Hook, NY, USA, 6000\u20136010.\n[216] Oriol Vinyals and Quoc Le. 2015. A Neural Conversational Model. arXiv:1506.05869 [cs.CL] https://arxiv.org/abs/1506.05869\n[217] Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang.\n2024. Planning In Natural Language Improves LLM Search For Code Generation. arXiv:2409.03733 [cs.LG] https://arxiv.org/abs/2409.03733\n[218] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. 2024. Interpretable P"}, "Reasoning RAG via System 1 or System 2: A Survey on Reasoning Agentic Retrieval-Augmented Generation for Industry Challenges": {"Introduction": "Large Language Models (LLMs) [Singh, 2023; Zhao et\nal., 2023; Zhu et al. , 2024 ]have demonstrated remarkable\ncapabilities in natural language understanding and gener-\nation, enabling a wide array of applications from open-\ndomain question-answer (QA) to task-specific dialogue sys-\ntems. However, LLMs rely on static training data, mak-\ning them prone to hallucinations and limiting their abil-\n\u2217Corresponding author.ity to provide accurate, up-to-date information in dynamic\nor knowledge-intensive tasks [Rawte et al. , 2023; Zhang\net al. , 2023; Huang et al. , 2025 ]. Retrieval-Augmented\nGeneration (RAG) [Chen et al. , 2024; Lewis et al. , 2020;\nGao et al. , 2023 ]has attracted significant attention as a\npromising approach to overcome the knowledge limitations\nof LLMs resulting from static pretraining. By integrating rel-\nevant information from external knowledge bases or search\nengines, RAG enhances factual accuracy and broadens the\nmodel\u2019s temporal and domain coverage [Zhao et al. , 2024;\nLiet al. , 2024a ]. Traditional RAG methods have demon-\nstrated strong performance when queries are well-formed and\nthe necessary information is readily available in the retrieved\ncontext.\nDespite the effectiveness of basic RAG methods, they often\nstruggle when applied to real-world, industrial-scale applica-\ntions involving complex and heterogeneous data. For example,\nin multi-document scenarios, relevant information is spread\nacross sources, requiring not just retrieval but also coherent\nsynthesis [Wang et al. , 2025; Wang et al. , 2024b ]. Naively\nconcatenating retrieved passages can lead to fragmented or\ncontradictory responses, particularly in domains like legal or\nbiomedical QA where multi-hop reasoning is critical. Addi-\ntionally, most RAG systems are limited to text-only processing\nand cannot natively handle multi-modal inputs such as tables,\ncharts, or images [Maet al. , 2024; Yu et al. , 2025 ]. This limits\ntheir ability to operate in data-rich environments like enterprise\nintelligence, scientific reporting, or technical support, where\nvisual and structured data play a central role [Linet al. , 2023a;\nYuet al. , 2024 ].\nTo address these limitations of basic RAG in handling com-\nplex, real-world tasks, recent research has turned to Agentic\nRAG [Ravuru et al. , 2024 ], a paradigm that tightly integrates\nretrieval with reasoning and decision-making. Unlike static\npipelines, Agentic RAG treats retrieval not as a one-off pre-\nprocessing step, but as a dynamic, context-sensitive opera-\ntion guided by the model\u2019s ongoing reasoning process. This\nreasoning-centric perspective is crucial for applications that\ndemand multi-step problem solving, adaptive information ac-\nquisition, and tool-assisted synthesis. Within this paradigm, asDesignFollowLLMHumanPredefined Reasoning\nReasoning\nAgentic ReasoningExternal ToolsTool Calling\nRetrieved InformationFigure 1: Overview of two major types of reasoning Agentic Systems.\nshown in Figure 1, two major types of reasoning agentic sys-\ntems have emerged based on how control and decision-making\nare handled: predefined reasoning , which follow structured,\nrule-based plans with fixed pipelines to boost reasoning for\nretrieval and generation; and agentic reasoning , where the\nmodel actively monitors its reasoning process and determines\nwhen and how to retrieve or interact with external tools. These\ntwo workflows form the basis of Reasoning Agentic RAG ,\nwhich unifies structured and autonomous approaches for more\nintelligent, context-aware retrieval-augmented reasoning.\nPredefined reasoning adopts structured and modular RAG\npipelines where the retrieval and reasoning steps are explicitly\ndesigned, following fixed control pipeline. These workflows\ntypically decompose tasks into discrete components such as\nquery reformulation, document retrieval, re-ranking, and an-\nswer synthesis, executed in a linear or orchestrated fashion.\nIn general, predefined reasoning spans several architectural\nvariants: route-based methods selectively trigger retrieval\nbased on context or model uncertainty, such as low confi-\ndence scores or ambiguous intermediate outputs [Wang et al. ,\n2024a ];loop-based methods enable limited iteration through\nretrieval-feedback cycles, supporting multiple rounds of re-\nfinement [Asai et al. , 2023; Yang et al. , 2024b ];tree-based\nmethods organize information hierarchically to support struc-\ntured exploration [Sarthi et al. , 2024; Hu et al. , 2025 ]; and\nhybrid-modular frameworks compose specialized modules\ninto a flexible but still rule-driven workflow [Jeong et al. , 2024;\nGao et al. , 2024 ]. These workflows prioritize control and\nmodularity, making them suitable for tasks requiring efficient\ncomputation and customization. However, their reasoning\nremains constrained by predesigned execution paths, limiting\nflexibility in evolving and open-ended tasks.\nAgentic reasoning repositions the LLM as an active deci-\nsion maker, that autonomously orchestrates retrieval and tooluse throughout the reasoning process. Instead of executing\na fixed plan, the model identifies knowledge gaps, formu-\nlates queries, retrieves external information via tools such\nas search engines or APIs, and integrates the retrieved con-\ntents into an evolving solution. This dynamic interplay of\nreasoning and tool use enables the system to tackle complex,\nmulti-turn tasks that require iterative refinement and adaptive\ninformation synthesis. There are two primary methods for im-\nplementing agentic reasoning. The first is prompt-based meth-\nods, which leverages the in-context reasoning and instruction-\nfollowing capabilities of pretrained LLMs [Yaoet al. , 2023;\nPress et al. , 2023; Li et al. , 2025a ]. In this setting, the model\nis guided by carefully crafted prompts or embedded control\ntokens that instruct it when to retrieve, what actions to take,\nand how to integrate external information. These methods re-\nquire no additional training, making them lightweight and\nadaptable across tasks. The second paradigm is training-\nbased methods, where models are explicitly optimized through\nreinforcement learning, to determine when and how to in-\nvoke external tools [Jiang et al. , 2025; Jin et al. , 2025;\nZheng et al. , 2025 ]. This paradigm enables more fine-grained\nand strategic tool usage, enabling models to learn long-term\nplanning and develop retrieval policies tailored to complex\ntasks. Owing to its autonomy and adaptability, agentic rea-\nsoning has shown strong performance in open-domain QA,\nscientific reasoning, and multi-stage decision-making scenar-\nios.\nPerspective of Cognitive Science - System 1 and System\n2:To further contextualize predefined and agentic reason-\ning within the dual-process theory of cognition\u2014commonly\nreferred to as System 1 and System 2 thinking [Yang et al. ,\n2024a; Li et al. , 2025b ]\u2014 we can draw an analogy between\nthese RAG paradigms and human cognitive modes.\n\u2022Predefined reasoning resembles System 1 thinking: fast,\nstructured, and efficient, relying on predefined heuristics\nand modular workflows that mirror habitual or rule-based\ncognition. While this enables rapid execution and pre-\ndictable behavior, it often lacks the flexibility to adapt\nbeyond its design.\n\u2022In contrast, agentic reasoning aligns more closely with\nSystem 2 thinking: slow, deliberative, and adaptive. Here,\nthe LLM actively engages in reasoning, planning, and\ndecision-making, dynamically leveraging external tools\nand retrieved knowledge to address complex, novel tasks.\nThis reflective mode allows the model to identify gaps,\nreassess strategies, and adjust its behavior\u2014traits charac-\nteristic of conscious, analytical human reasoning.\nBy framing these paradigms through the lens of cognitive\nsystems, we highlight the trade-off between efficiency and\nadaptability, and the growing capacity of agentic RAG to emu-\nlate more sophisticated, human-like problem solving. Table 1\naligns predefined and agentic reasoning with the dual-system\ntheory from cognitive science, illustrating their respective con-\ntrol structures and behavioral characteristics.\nThe paper systematically reviews and analyzes the current\nresearch approaches and future development paths of Rea-\nsoning Agentic RAG, summarizing them into two primary\ntechnical paradigms. The remainder of the paper is organizedSystem Type Reasoning Workflow Description\nSystem 1 Predefined Reasoning Structured, modular, rule-based execution.\nSystem 2 Agentic Reasoning Autonomous, adaptive, model-driven decision-making.\nTable 1: Cognitive system alignment of reasoning workflows.\nas follows: Section 2 introduces related work; Section 3 and\nSection 4 dive into the two types of reasoning workflows\nwithin Agentic RAG, predefined reasoning and agentic reason-\ning, respectively. Section 5 outlines future research directions,\nand Section 6 concludes the paper.\nFigure 2: Distributed Works of Reasoning Agentic RAG.\n2", "Related Work": "2.1", "Basic RAG": "Retrieval-Augmented Generation (RAG) was introduced to\novercome the static knowledge limitations of LLMs by inte-\ngrating external retrieval mechanisms during inference [Chen\net al. , 2024; Gao et al. , 2023 ]. Naive RAG methods represent\nthe earliest implementations, typically using sparse retrieval\ntechniques like BM25 [Robertson et al. , 2009 ]to fetch doc-\numents based on keyword overlap [Maet al. , 2023 ]. While\nefficient for simple factoid queries, these approaches offered\nlimited semantic understanding, thus often retrieving noisy\nor redundant content and failing to reason across multiple\nsources.\nThe emergence of Advanced RAG and Modular RAG was\naimed at addressing key limitations of the Naive RAG, par-\nticularly in terms of retrieval precision, information integra-\ntion, and system flexibility [Gao et al. , 2023 ]. Advanced\nRAG improves retrieval quality through techniques such as\ndense semantic matching, re-ranking, and multi-hop query-\ning, while also introducing refined indexing strategies like\nfine-grained chunking and metadata-aware retrieval. Modular\nRAG rethinks the Naive RAG by breaking down the end-to-\nend process of indexing, retrieval, and generation into discrete,\nconfigurable modules. This design allows for greater architec-\ntural flexibility and enables system developers to incorporate\ndiverse techniques into specific stages, such as enhancing re-\ntrieval with fine-tuned search modules [Linet al. , 2023b ]. In\nresponse to specific task demands, various restructured and\niterative module designs have also emerged. As a result, mod-\nular RAG has increasingly become a dominant paradigm in\nthe field, supporting both serialized pipeline execution and\nend-to-end learning across modular components.Despite their effectiveness, basic RAG workflows are lim-\nited by static control logic and lack the ability to reflect, adapt,\nor assess the sufficiency of retrieved information. These con-\nstraints reduce their suitability for tasks requiring iterative\nreasoning, tool use, or multi-modal integration. Thus, Agentic\nRAG has proposed to embed reasoning and decision-making\ninto the retrieval process. This work focuses on reasoning\nAgentic RAG approaches that enable more autonomous and\ncontext-aware information processing.\n2.2", "Reasoning Agentic RAG": ", a paradigm that embeds\ndecision-making and adaptive tool use directly\ninto the retrieval process. In this paper, we present\na comprehensive review of Reasoning Agentic\nRAG methods, categorizing them into two primary\nsystems: predefined reasoning , which follow\nfixed modular pipelines to boost reasoning, and\nagentic reasoning , where the model autonomously\norchestrates tool interaction during inference.\nWe analyze representative techniques under both\nparadigms, covering architectural design, reasoning\nstrategies, and tool coordination. Finally, we\ndiscuss key research challenges and propose future\ndirections to advance the flexibility, robustness, and\napplicability of reasoning agentic RAG systems.\nOur collection of the relevant researches has been\norganized into a Github Repository.\n1 Introduction\nLarge Language Models (LLMs) [Singh, 2023; Zhao et\nal., 2023; Zhu et al. , 2024 ]have demonstrated remarkable\ncapabilities in natural language understanding and gener-\nation, enabling a wide array of applications from open-\ndomain question-answer (QA) to task-specific dialogue sys-\ntems. However, LLMs rely on static training data, mak-\ning them prone to hallucinations and limiting their abil-\n\u2217Corresponding author.ity to provide accurate, up-to-date information in dynamic\nor knowledge-intensive tasks [Rawte et al. , 2023; Zhang\net al. , 2023; Huang et al. , 2025 ]. Retrieval-Augmented\nGeneration (RAG) [Chen et al. , 2024; Lewis et al. , 2020;\nGao et al. , 2023 ]has attracted significant attention as a\npromising approach to overcome the knowledge limitations\nof LLMs resulting from static pretraining. By integrating rel-\nevant information from external knowledge bases or search\nengines, RAG enhances factual accuracy and broadens the\nmodel\u2019s temporal and domain coverage [Zhao et al. , 2024;\nLiet al. , 2024a ]. Traditional RAG methods have demon-\nstrated strong performance when queries are well-formed and\nthe necessary information is readily available in the retrieved\ncontext.\nDespite the effectiveness of basic RAG methods, they often\nstruggle when applied to real-world, industrial-scale applica-\ntions involving complex and heterogeneous data. For example,\nin multi-document scenarios, relevant information is spread\nacross sources, requiring not just retrieval but also coherent\nsynthesis [Wang et al. , 2025; Wang et al. , 2024b ]. Naively\nconcatenating retrieved passages can lead to fragmented or\ncontradictory responses, particularly in domains like legal or\nbiomedical QA where multi-hop reasoning is critical. Addi-\ntionally, most RAG systems are limited to text-only processing\nand cannot natively handle multi-modal inputs such as tables,\ncharts, or images [Maet al. , 2024; Yu et al. , 2025 ]. This limits\ntheir ability to operate in data-rich environments like enterprise\nintelligence, scientific reporting, or technical support, where\nvisual and structured data play a central role [Linet al. , 2023a;\nYuet al. , 2024 ].\nTo address these limitations of basic RAG in handling com-\nplex, real-world tasks, recent research has turned to Agentic\nRAG [Ravuru et al. , 2024 ], a paradigm that tightly integrates\nretrieval with reasoning and decision-making. Unlike static\npipelines, Agentic RAG treats retrieval not as a one-off pre-\nprocessing step, but as a dynamic, context-sensitive opera-\ntion guided by the model\u2019s ongoing reasoning process. This\nreasoning-centric perspective is crucial for applications that\ndemand multi-step problem solving, adaptive information ac-\nquisition, and tool-assisted synthesis. Within this paradigm, asDesignFollowLLMHuman", "Predefined Reasoning": "Reasoning", "Agentic Reasoning": "External ToolsTool Calling\nRetrieved InformationFigure 1: Overview of two major types of reasoning Agentic Systems.\nshown in Figure 1, two major types of reasoning agentic sys-\ntems have emerged based on how control and decision-making\nare handled: predefined reasoning , which follow structured,\nrule-based plans with fixed pipelines to boost reasoning for\nretrieval and generation; and agentic reasoning , where the\nmodel actively monitors its reasoning process and determines\nwhen and how to retrieve or interact with external tools. These\ntwo workflows form the basis of Reasoning Agentic RAG ,\nwhich unifies structured and autonomous approaches for more\nintelligent, context-aware retrieval-augmented reasoning.\nPredefined reasoning adopts structured and modular RAG\npipelines where the retrieval and reasoning steps are explicitly\ndesigned, following fixed control pipeline. These workflows\ntypically decompose tasks into discrete components such as\nquery reformulation, document retrieval, re-ranking, and an-\nswer synthesis, executed in a linear or orchestrated fashion.\nIn general, predefined reasoning spans several architectural\nvariants: route-based methods selectively trigger retrieval\nbased on context or model uncertainty, such as low confi-\ndence scores or ambiguous intermediate outputs [Wang et al. ,\n2024a ];loop-based methods enable limited iteration through\nretrieval-feedback cycles, supporting multiple rounds of re-\nfinement [Asai et al. , 2023; Yang et al. , 2024b ];tree-based\nmethods organize information hierarchically to support struc-\ntured exploration [Sarthi et al. , 2024; Hu et al. , 2025 ]; and\nhybrid-modular frameworks compose specialized modules\ninto a flexible but still rule-driven workflow [Jeong et al. , 2024;\nGao et al. , 2024 ]. These workflows prioritize control and\nmodularity, making them suitable for tasks requiring efficient\ncomputation and customization. However, their reasoning\nremains constrained by predesigned execution paths, limiting\nflexibility in evolving and open-ended tasks.\nAgentic reasoning repositions the LLM as an active deci-\nsion maker, that autonomously orchestrates retrieval and tooluse throughout the reasoning process. Instead of executing\na fixed plan, the model identifies knowledge gaps, formu-\nlates queries, retrieves external information via tools such\nas search engines or APIs, and integrates the retrieved con-\ntents into an evolving solution. This dynamic interplay of\nreasoning and tool use enables the system to tackle complex,\nmulti-turn tasks that require iterative refinement and adaptive\ninformation synthesis. There are two primary methods for im-\nplementing agentic reasoning. The first is prompt-based meth-\nods, which leverages the in-context reasoning and instruction-\nfollowing capabilities of pretrained LLMs [Yaoet al. , 2023;\nPress et al. , 2023; Li et al. , 2025a ]. In this setting, the model\nis guided by carefully crafted prompts or embedded control\ntokens that instruct it when to retrieve, what actions to take,\nand how to integrate external information. These methods re-\nquire no additional training, making them lightweight and\nadaptable across tasks. The second paradigm is training-\nbased methods, where models are explicitly optimized through\nreinforcement learning, to determine when and how to in-\nvoke external tools [Jiang et al. , 2025; Jin et al. , 2025;\nZheng et al. , 2025 ]. This paradigm enables more fine-grained\nand strategic tool usage, enabling models to learn long-term\nplanning and develop retrieval policies tailored to complex\ntasks. Owing to its autonomy and adaptability, agentic rea-\nsoning has shown strong performance in open-domain QA,\nscientific reasoning, and multi-stage decision-making scenar-\nios.\nPerspective of Cognitive Science - System 1 and System\n2:To further contextualize predefined and agentic reason-\ning within the dual-process theory of cognition\u2014commonly\nreferred to as System 1 and System 2 thinking [Yang et al. ,\n2024a; Li et al. , 2025b ]\u2014 we can draw an analogy between\nthese RAG paradigms and human cognitive modes.\n\u2022Predefined reasoning resembles System 1 thinking: fast,\nstructured, and efficient, relying on predefined heuristics\nand modular workflows that mirror habitual or rule-based\ncognition. While this enables rapid execution and pre-\ndictable behavior, it often lacks the flexibility to adapt\nbeyond its design.\n\u2022In contrast, agentic reasoning aligns more closely with\nSystem 2 thinking: slow, deliberative, and adaptive. Here,\nthe LLM actively engages in reasoning, planning, and\ndecision-making, dynamically leveraging external tools\nand retrieved knowledge to address complex, novel tasks.\nThis reflective mode allows the model to identify gaps,\nreassess strategies, and adjust its behavior\u2014traits charac-\nteristic of conscious, analytical human reasoning.\nBy framing these paradigms through the lens of cognitive\nsystems, we highlight the trade-off between efficiency and\nadaptability, and the growing capacity of agentic RAG to emu-\nlate more sophisticated, human-like problem solving. Table 1\naligns predefined and agentic reasoning with the dual-system\ntheory from cognitive science, illustrating their respective con-\ntrol structures and behavioral characteristics.\nThe paper systematically reviews and analyzes the current\nresearch approaches and future development paths of Rea-\nsoning Agentic RAG, summarizing them into two primary\ntechnical paradigms. The remainder of the paper is organizedSystem Type Reasoning Workflow Description\nSystem 1 Predefined Reasoning Structured, modular, rule-based execution.\nSystem 2 Agentic Reasoning Autonomous, adaptive, model-driven decision-making.\nTable 1: Cognitive system alignment of reasoning workflows.\nas follows: Section 2 introduces related work; Section 3 and\nSection 4 dive into the two types of reasoning workflows\nwithin Agentic RAG, predefined reasoning and agentic reason-\ning, respectively. Section 5 outlines future research directions,\nand Section 6 concludes the paper.\nFigure 2: Distributed Works of Reasoning Agentic RAG.\n2 Related Work\n2.1 Basic RAG\nRetrieval-Augmented Generation (RAG) was introduced to\novercome the static knowledge limitations of LLMs by inte-\ngrating external retrieval mechanisms during inference [Chen\net al. , 2024; Gao et al. , 2023 ]. Naive RAG methods represent\nthe earliest implementations, typically using sparse retrieval\ntechniques like BM25 [Robertson et al. , 2009 ]to fetch doc-\numents based on keyword overlap [Maet al. , 2023 ]. While\nefficient for simple factoid queries, these approaches offered\nlimited semantic understanding, thus often retrieving noisy\nor redundant content and failing to reason across multiple\nsources.\nThe emergence of Advanced RAG and Modular RAG was\naimed at addressing key limitations of the Naive RAG, par-\nticularly in terms of retrieval precision, information integra-\ntion, and system flexibility [Gao et al. , 2023 ]. Advanced\nRAG improves retrieval quality through techniques such as\ndense semantic matching, re-ranking, and multi-hop query-\ning, while also introducing refined indexing strategies like\nfine-grained chunking and metadata-aware retrieval. Modular\nRAG rethinks the Naive RAG by breaking down the end-to-\nend process of indexing, retrieval, and generation into discrete,\nconfigurable modules. This design allows for greater architec-\ntural flexibility and enables system developers to incorporate\ndiverse techniques into specific stages, such as enhancing re-\ntrieval with fine-tuned search modules [Linet al. , 2023b ]. In\nresponse to specific task demands, various restructured and\niterative module designs have also emerged. As a result, mod-\nular RAG has increasingly become a dominant paradigm in\nthe field, supporting both serialized pipeline execution and\nend-to-end learning across modular components.Despite their effectiveness, basic RAG workflows are lim-\nited by static control logic and lack the ability to reflect, adapt,\nor assess the sufficiency of retrieved information. These con-\nstraints reduce their suitability for tasks requiring iterative\nreasoning, tool use, or multi-modal integration. Thus, Agentic\nRAG has proposed to embed reasoning and decision-making\ninto the retrieval process. This work focuses on reasoning\nAgentic RAG approaches that enable more autonomous and\ncontext-aware information processing.\n2.2 Reasoning Agentic RAG\nThe year 2025 is marked as the year of agentic AI, with appli-\ncations emerging such as agentic LLMs and so on [Ruan et\nal., 2023; Kong et al. , 2024; Zhang et al. ,]. Recent advances\nin RAG have seen a shift from static, rule-driven retrieval\npipelines toward dynamic, reasoning-driven architectures, col-\nlectively referred to as Reasoning Agentic RAG . These systems\nembed decision-making into the retrieval process, enabling\nmodels to actively determine when, what, and how to retrieve\nbased on their internal reasoning trajectory. As shown in Fig-\nure 3, Reasoning Agentic RAG approaches can be broadly\ncategorized into two paradigms: predefined reasoning and\nagentic reasoning .\nPredefined reasoning depends on structured, rule-based\npipelines where the retrieval and reasoning stages are modu-\nlarized and fixed in advance. These workflows often include\ncomponents for query reformulation, document retrieval, re-\nranking, and response generation, coordinated by static control\nlogic. RAGate [Wang et al. , 2024a ]exemplifies route-based\ndesigns, where retrieval is conditionally triggered based on the\ncontext or model confidence, enabling the system to skip un-\nnecessary operations and focus on knowledge-intensive inputs.\nSelf-RAG [Asai et al. , 2023 ]introduces loop-based reasoning\nby enabling the model to self-reflect and iteratively refine its\nresponses, while RAPTOR [Sarthi et al. , 2024 ]leverages a\nrecursive tree structure to hierarchically summarize and orga-\nnize retrieved content, supporting multi-hop and abstractive\nreasoning. Building on these foundations, more advanced\nframeworks like Adaptive-RAG [Jeong et al. , 2024 ]combine\ndynamic routing and retrieval adaptation, enabling models to\nselect optimal reasoning paths. Modular-RAG [Gao et al. ,\n2024 ]extends this idea by dividing the RAG pipeline into\ninteroperable modules like retrievers, rerankers and genera-\ntors, which can be flexibly composed into hybrid workflows.\nThese designs enabling more flexible orchestration while still\noperating under predefined execution paths.\nAgentic reasoning empowers the LLM to act as an au-\ntonomous agent, dynamically deciding how to interact with\nexternal tools based on its current reasoning state. These\nworkflows tightly couple reasoning with tool use, enabling\nthe model to issue retrieval queries, assess results, and itera-Reasoning Agentic RAGPredefined reasoningRoute-based RAGate [Wang et al. , 2024a ], Self-Route [Liet al. , 2024b ]\nLoop-based Self-RAG [Asai et al. , 2023 ], CRAG [Yan et al. , 2024 ]\nTree-based RAPTOR [Sarthi et al. , 2024 ], MCTS-RAG [Huet al. , 2025 ]\nHybrid-modular Adaptive-RAG [Jeong et al. , 2024 ], Modular-RAG [Gao et al. , 2024 ]\nAgentic reasoningPrompt-basedReAct [Yao et al. , 2023 ], Self-Ask [Press et al. , 2023 ],\nFunction calling [Eleti et al. , 2023 ], Search-O1 [Liet al. , 2025a ]\nTraining-basedDeepRetrieval [Jiang et al. , 2025 ], Search-R1 [Jinet al. , 2025 ], R1-Searcher [Song et al. , 2025 ],\nReZero [Dao and Le, 2025 ], DeepResearcher [Zheng et al. , 2025 ]\nFigure 3: A taxonomy of Reasoning Agentic RAG.\n2022202320252024Agentic ReasoningPredefined ReasoningSelf-RAGRAPTORCRAGRAGateSelf-RouteMCTS-RAGAdaptive-RAGModular-RAGSelf-ASKReActSearch-O1DeepRetrievalReZeroFunction CallingR1-SearcherSearch-R1Loop-basedRouter-basedTree-basedHybrid-modular \nPrompt-basedTraining-based\nFigure 4: Illustration of the evolution of Reasoning Agentic RAG.\ntively adapt its actions. Two main implementation strategies\nhave emerged: prompt-based andtraining-based approaches.\nPrompt-based methods leverage the instruction-following abil-\nities of pretrained LLMs to drive agentic behavior without\nadditional training. For example, ReAct [Yao et al. , 2023 ]\ninterleaves reasoning steps with tool use to guide retrieval\nbased on emerging knowledge gaps. Other methods like\nSelf-Ask [Press et al. , 2023 ]and Search-o1 [Liet al. , 2025a ]\nsupport decomposition into sub-questions or trigger retrieval\nmid-generation. Additionally, function calling mechanisms\n[Eleti et al. , 2023 ]built into commercial LLMs such as GPT\nand Gemini offer structured interfaces for tool use, further\nenabling prompt-based agentic control. In parallel, training-\nbased approaches aim to explicitly teach LLMs to reason\nand retrieve in a unified, goal-driven manner by leveraging\nreinforcement learning (RL) to optimize tool-use behavior.\nDeepRetrieval [Jiang et al. , 2025 ]trains models to reformulate\nqueries by maximizing retrieval metrics. Search-R1 [Jinet\nal., 2025 ]and R1-Searcher [Song et al. , 2025 ]both adopt atwo-stage, outcome-driven RL framework that enables LLMs\nto learn when and what to search within a reasoning trajectory.\nReZero [Dao and Le, 2025 ]incentivizes persistence, reward-\ning effective retry strategies. DeepResearcher [Zheng et al. ,\n2025 ]pushes further by training agents in open web environ-\nments, enabling robust search and synthesis across diverse,\nunstructured sources.\n3 Predefined Reasoning\nAgents and RAG are increasingly integrated in advanced AI\nsystems. By augmenting LLMs with external knowledge\nretrieval, RAG enables agents to ground their reasoning in\nrelevant information. In turn, agent-based reasoning which\nincludes planning, tool use and self-reflection, enhances RAG\nby guiding the model on what information to retrieve and\nhow to incorporate it into the reasoning process. This synergy\nsupports a predefined reasoning, where the agent iteratively\nqueries external sources (e.g., a local database or web search)\nand refines its reasoning based on the retrieved evidence. Wecategorize predefined RAG reasoning workflows into four\nbroad types based on their structural and reasoning character-\nistics as follows.\nRoute-based Approaches: RAG incorporates dynamic\nrouting mechanisms that direct queries along different retrieval\nor reasoning paths based on predefined conditions\u2014such\nas query type, model uncertainty, or confidence estima-\ntion\u2014while still operating within a fixed architecture. RAGate\n[Wang et al. , 2024a ]uses the conversation context and model\nconfidence to route only those dialogue turns that truly re-\nquire external knowledge to a RAG process. This ensures\nthe system can bypass retrieval for straightforward prompts\nwhile invoking it for knowledge-intensive queries, exemplify-\ning conditional RAG in dialogue. Self-Route [Liet al. , 2024b ]\nintroduced dynamically routes queries to either RAG or Long-\nContext (LC) models based on the model\u2019s confidence-based\nrouting. This method significantly reduces computation cost\nwhile maintaining performance comparable to LC models.\nLoop-based Approaches: RAG operates within a feedback\nloop that supports multiple rounds of refinement. The system\ncan self-reflect, critique intermediate outputs, and iteratively\nupdate retrieval inputs to improve generation quality. Self-\nRAG [Asai et al. , 2023 ]is a foundational example of this\ncontrolled reasoning loop. In the Self-RAG workflow, a single\nLLM agent engages in self-reflection during generation to\nimprove its output. Instead of relying on a fixed retrieved con-\ntext, the model can decide mid-generation to fetch additional\ninformation or to critique its own draft answer. CRAG [Yan\net al. , 2024 ]introduced loop-based corrective feedback mech-\nanism into the retrieval process. In the CRAG workflow, a\nlightweight retrieval evaluator assigning the confidence scores\nabout the quality of the retrieved chunks/documents \u2014 cate-\ngorized as correct, incorrect, or ambiguous. When retrieval\nquality is deemed suboptimal, the system activates corrective\nstrategies such as query rewriting or external web search to\ngather better evidence. The system refines the retrieved con-\ntent into a focused context and iteratively improves retrieval\nuntil a satisfactory output is generated.\nTree-based Approaches: RAG organizes the retrieval pro-\ncess hierarchically, often using recursive structures such as\ntrees to support multi-hop reasoning or document summariza-\ntion. RAPTOR [Sarthi et al. , 2024 ]introduces a recursive\ntree structure from documents, allowing for more efficient and\ncontext-aware information retrieval. This approach enhances\nRAG by creating a summary tree from text chunks, providing\ndeeper insights and overcoming limitations of short, contigu-\nous text retrieval. MCTS-RAG [Huet al. , 2025 ]integrates\na Monte Carlo Tree Search loop into the RAG process for\ncomplex reasoning tasks. MCTS-RAG dynamically integrates\nretrieval and reasoning through an iterative decision-making\nprocess. Unlike standard RAG methods, which typically re-\ntrieve information independently from reasoning and thus\nintegrate knowledge suboptimally, or conventional MCTS rea-\nsoning, which depends solely on internal model knowledge\nwithout external facts, MCTS-RAG combines structured rea-\nsoning with adaptive retrieval. Hybrid-modular Approaches:\nRAG in its most flexible form combines routing, looping, re-\nflection, and modular orchestration. Tasks are divided among\nspecialized components, coordinated by an agent that can dy-\nQuestion\nLLMSub-QuestionsRetrievalReflection. Are there knowledge gaps?Yes, generate new questionsNo, SummarizeAnswerQuestion DecompositionPredefined ReasoningFigure 5: A demonstration of Predefined Reasoning.\nnamically reconfigure the workflow according to the query or\nreasoning context. Adaptive-RAG [Jeong et al. , 2024 ]extends\nthe Self-RAG framework by introducing routing mechanisms\nthat enable dynamic path selection. In addition to allowing the\nmodel to interleave retrieval and generation steps, it equips the\nagent with a decision-making router that selects appropriate\nretrieval strategies or reasoning pathways based on the query\ncharacteristics or the agent\u2019s own uncertainty. Rather than\nsimply determining whether to retrieve more information, the\nagent can choose which retrieval method to apply, what type\nof information to prioritize, or which downstream modules to\nengage. Modular-RAG [Gaoet al. , 2024 ]is the most advanced\nincarnation that transform RAG into a LEGO-like modular\nframework, breaking the RAG process into an orchestrated\npipeline of specialized modules. Rather than a single agent\nhandling everything, a Modular-RAG architecture compart-\nmentalizes tasks, e.g., one module for query reformulation,\none for document retrieval, another for ranking or filtering\nresults, and another for answer synthesis \u2013 all chained together\nin a composable workflow. The pipeline is composed by an\nagent that coordinates modular components, each of which\ncan be optimized or swapped independently.\nThis progression of predefine reasoning workflows reflects a\nbroader shift from static retrieval pipelines to dynamic, agent-\ndriven reasoning systems. Modern predefined reasoning in-\ncreasingly integrates planning, tool use, and decision-making\ncomponents that allow flexible orchestration of retrieval and\nreasoning strategies. Rather than predefining rigid retrieval\nsteps, these systems empower agents to determine what in-\nformation to seek, how to use it, and when to adapt their\napproach\u2014marking a move toward more autonomous andintelligent knowledge integration. A summary of the repre-\nsentative research works and open-source industrial/enterprise\nimplementations across these predefined RAG workflow types\nis provided in Table 2.\n4 Agentic Reasoning\nBeyond the predefined reasoning mentioned above, a more\ndynamic paradigm has emerged: the Agentic Reasoning . In\nthis setting, the LLM serves as an autonomous agent that not\nonly generates text, but also actively manages retrieval. With\nadvances in reasoning and instruction-following capabilities,\nthe model can identify knowledge gaps, determine when and\nwhat to retrieve, and interact with external tools such as search\nengines or APIs. This tight integration of reasoning and tool\nuse enables iterative decision-making, enabling the system to\nrefine its responses based on newly retrieved information. As\na result, agentic reasoning supports more flexible and adaptive\nproblem-solving, extending RAG beyond basic QA to com-\nplex tasks such as scientific inquiry, multi-step reasoning, and\nstrategic decision-making. Agentic reasoning approaches can\nbe broadly categorized by how the LLM learns to use tools:\n\u2022", "Prompt-Based Approaches": ": These methods leverage\nthe instruction-following, in-context learning and reason-\ning capabilities of pretrained LLMs, guiding tool use\nthrough carefully crafted prompts or built-in functionali-\nties without additional training.\n\u2022", "Training-Based Approaches": ": These methods involve\nexplicitly training LLMs, typically via reinforcement\nlearning, to learn when and how to interact with external\ntools effectively.\nA summary of representative agentic reasoning apporaches\nand their characteristics is provided in Table 2. The following\nsections examine representative frameworks and techniques\nwithin each approach.\n4.1 Prompt-Based Approaches\nPrompt-based approaches harness the remarkable capabilities\nalready present in pre-trained LLMs to enable agentic behavior.\nInstead of modifying the model\u2019s weights through training,\nthese methods rely on sophisticated prompting techniques,\nfew-shot examples or built-in tool interfaces, to guide the\nLLM in its interaction with external tools like search engines.\nFunction-Calling-Based : A foundational prompt-based\nmethod for agentic behavior, and one way to implement func-\ntion calling, is ReAct (Reason+Act) [Yaoet al. , 2023 ]. ReAct\naims to create a synergy between the reasoning processes\nand action-taking capabilities within an LLM. Its core mecha-\nnism involves prompting the LLM to generate outputs in an\ninterleaved sequence of Thought, Action, and Observation.\nReAct typically employs few-shot prompting, providing the\nLLM with examples that demonstrate this Thought-Action-\nObservation trajectory for solving similar tasks. These exam-\nples guide the frozen LLM on how to structure its reasoning,\nutilize available tools, and progress towards the goal. The\nframework demonstrated significant advantages, particularly\nin grounding the LLM\u2019s reasoning. By allowing the model to\nactively seek and incorporate external information via actions,ReAct can mitigate the hallucination and error propagation is-\nsues sometimes observed in purely internal reasoning methods\nlike Chain-of-Thought (CoT) [Weiet al. , 2023 ]. The explicit\nreasoning traces (\u201cThoughts\u201d) in ReAct enhance the inter-\npretability and transparency of the model\u2019s decision-making.\nWithin RAG, ReAct offers a natural agentic reasoning pipeline:\nthe LLM\u2019s \u201dThought\u201d process can identify a knowledge gap,\nleading to a search \u201dAction,\u201d with the retrieved results form-\ning the \u201dObservation\u201d that informs subsequent reasoning. A\nrelated method, Self-Ask [Press et al. , 2023 ], encourages step-\nby-step problem decomposition by prompting the LLM to\ngenerate and answer simpler follow-up questions. These inter-\nmediate steps often involve search actions, enabling the model\nto gather relevant information before attempting to answer the\nmain question.\nAnother prominent prompt-based approach involves lever-\naging the function calling or tool use capabilities that have\nbeen explicitly built into or fine-tuned into certain LLMs, such\nas versions of GPT [Eleti et al. , 2023 ], Llama, and Gem-\nini. This feature allows the LLM to interact reliably with\npredefined external tools or APIs based on natural language\ninstructions. Function calling significantly expands the ca-\npabilities of LLMs beyond text generation, enabling them to\naccess real-time, dynamic information, interact with external\nsystems and databases, automate tasks, and reliably convert\nnatural language requests into structured API calls or database\nqueries. In contrast to the more open-ended \u201dthought-action-\nobservation\u201d cycle of ReAct, function calling often bypasses\nexplicit intermediate reasoning steps. The LLM directly iden-\ntifies the relevant tool and generates the necessary parameters\nbased on its training to recognize and format specific function\ncalls. This more direct approach relies on the model\u2019s pre-\nexisting knowledge of available tools and their required inputs.\nFurthermore, the format and capabilities of the tools acces-\nsible via function calling are typically predefined and have\nbeen integrated into the model\u2019s training or prompt design.\nFor Agentic RAG, function calling provides a straightforward\nand structured way for the LLM agent to invoke a search API\nwhen its internal analysis determines that external information\nis required to answer a prompt accurately.\nLarge Reasoning Model-based : A growing trend in Agen-\ntic RAG workflow involves directly utilizing LLMs that pos-\nsess inherently strong reasoning capabilities, often referred to\nas Large Reasoning Models (LRMs). These models, some-\ntimes developed through techniques like large-scale reinforce-\nment learning (e.g., models analogous to OpenAI\u2019s o1 [Ope-\nnAIet al. , 2024 ], DeepSeek-R1 [DeepSeek-AI et al. , 2025 ]),\nare designed to excel at complex, multi-step reasoning tasks.\nThe underlying premise is that an LLM with superior intrin-\nsic reasoning abilities will be better equipped to manage the\ncomplexities of an Agentic RAG workflow, including decom-\nposing challenging queries, planning information-gathering\nsteps, assessing the relevance and utility of retrieved infor-\nmation, and synthesizing knowledge effectively. In essence,\nleveraging LRMs within RAG represents a prompt-based agen-\ntic strategy where the model\u2019s powerful inherent reasoning\ncapabilities drive the process, implicitly deciding when and\nhow to retrieve information to support its complex thought\nprocesses.Predefined Reasoning\nApproach Strategy Control Type Reasoning Complexity Code\nRAGate [Wang et al. , 2024a ] Route-based Adaptive Medium Link\nself-RAG [Asai et al. , 2023 ] Loop-based Agentic Medium Link\nCRAG [Yanet al. , 2024 ] Loop-based Adaptive Medium Link\nMCTS-RAG [Huet al. , 2025 ] Tree-based Agentic High Link\nRAPTOR [Sarthi et al. , 2024 ] Tree-based Fixed Medium Link\nAdaptive-RAG [Jeong et al. , 2024 ] Hybrid-modular Adaptive Medium Link\nModular-RAG [Gao et al. , 2024 ] Hybrid-modular Fixed Low N/A\nDeepSearcher Industry Adaptive Medium Link\nRAGFlow Industry Adaptive Medium Link\nHaystack Industry Adaptive Medium Link\nLangchain-Chatchat Industry Adaptive/Agentic Medium Link\nLightRAG Industry Adaptive Medium Link\nR2R Industry Agentic High Link\nFlashRAG Industry Adaptive Medium Link\nAgentic Reasoning\nApproach Strategy Training environment Reward design Code\nReAct [Yaoet al. , 2023 ] Prompt-based N/A N/A Link\nSelf-Ask [Press et al. , 2023 ] Prompt-based N/A N/A Link\nFunciton calling [Eleti et al. , 2023 ] Prompt-based N/A N/A N/A\nSearch-O1 [Liet al. , 2025a ] Prompt-based N/A N/A Link\nSearch-R1 [Jinet al. , 2025 ] Training-based Local retrieval system Answer reward Link\nR1-Searcher [Song et al. , 2025 ] Training-based Local retrieval system Retrieval reward, format reward, answer reward Link\nReZero [Dao and Le, 2025 ] Training-based Local retrieval system Retrieval reward, format reward, answer reward, retry reward Link\nDeepRetrieval [Jiang et al. , 2025 ] Training-based Restricted real-world search engine Retrieval reward, format reward Link\nDeepResearcher [Zheng et al. , 2025 ]Training-based Real-world search engine Format reward, answer reward Link\nTable 2: A summary of Reasoning agentic rag.\nHowever, effectively managing the retrieved context is an-\nother significant challenge. LLMs with extremely long con-\ntext windows can suffer from a \u201dlost-in-the-middle\u201d problem,\nwhere information presented in the middle of a long input\nreceives less attention. Furthermore, retrieved documents,\nwhether in long-context models or standard RAG, often con-\ntain verbose, noisy or contradictory content that can disrupt\nthe coherence of the LLM\u2019s reasoning process. Mitigating\nthis challenge requires more precise retrieval strategies and\nadaptive context management mechanisms. The Search-o1\nframework [Liet al. , 2025a ]is specifically designed to en-\nhance LRMs by tackling knowledge insufficiency during long,\nstep-by-step reasoning chains. It integrates two core compo-\nnents: an Agentic RAG Mechanism where the LRM dynami-\ncally triggers search queries based on self-assessed knowledge\ngaps, and a Reason-in-Documents Module that processes re-\ntrieved content to dist ill relevant information into a refined\nformat, thereby minimizing noise and maintaining the LRM\u2019s\nreasoning integrity. Search-o1 exemplifies a sophisticated\nprompt-based agentic approach focused on maintaining rea-\nsoning integrity in the face of external information retrieval.\n4.2 Training-Based Approaches\nWhile prompt-based methods leverage the inherent capabili-\nties of LLMs, their performance in complex tool-use scenarios\ncan be inconsistent. Achieving highly reliable and optimized\nbehavior, especially in deciding when and how to interact\nwith tools like search engines, often benefits from explicit\ntraining. Training-based approaches, particularly those uti-\nlizing Reinforcement Learning (RL), enable the LLM agent\nto learn sophisticated strategies through trial and error, di-\nrectly optimizing its actions towards specific goals such as\nmaximizing retrieval effectiveness or overall task success. RL\nenables agents to develop more robust and strategic interactionpatterns than prompting alone.\nInteracting with local retrieval systems : Search-R1 [Jinet\nal., 2025 ]tackles a different aspect of agentic search: training\nthe LLM to autonomously decide when to search and what to\nsearch for during a multi-step reasoning process. It extends\nRL-based reasoning frameworks (like DeepSeek-R1) by in-\ntegrating search engine interaction directly into the learning\nloop. In the Search-R1 framework, the search engine is mod-\neled as part of the RL environment. The LLM agent learns a\npolicy to generate a sequence of tokens that includes both in-\nternal reasoning steps (often enclosed in <think> tags) and\nexplicit triggers for search actions. These triggers are special\ntokens, <search> and</search> , which encapsulate the\ngenerated search query. This design allows for flexible, multi-\nturn interactions where the LLM can interleave reasoning,\nsearching, processing retrieved information (presented within\n<information> tags), and further reasoning or searching\nas needed. The framework utilizes a simple outcome-based\nreward function, typically based on the correctness of the final\nanswer generated by the LLM (within <answer> tags) com-\npared to a ground truth, avoiding the complexity of designing\nintermediate process rewards. A crucial technique employed\nis retrieved token masking. During the calculation of the RL\nloss (using algorithms like PPO or GRPO [Shao et al. , 2024 ]),\nthe tokens corresponding to the content retrieved from the\nsearch engine (i.e., within the <information> tags) are\nignored or masked out, which stabilizes the training process.\nSearch-R1 has shown significant performance improvements\nover various RAG baselines on question-answering datasets.\nIts core contribution is training the LLM to learn an optimal\npolicy for interacting with the search engine as an integrated\npart of its reasoning flow, enabling dynamic, context-aware\nsearch decisions. The related R1-Searcher [Song et al. , 2025 ]\nframework also proposes a similar two-stage, outcome-basedOriginal Question:Step1: \u2026 Step2: \u2026 Step3: ...Sub-Question\nLRM\nRetrieved DocumentsReason-in-Documents\nSearch for helpful info\nReasoning for next stepDistilled InformationStep nStep n+1Search Query\nStep n+2...Final StepFinal Answer\niterableAgentic ReasoningFigure 6: A demonstration of Agentic Reasoning.\nRL approach for enhancing search capabilities.\nReZero (Retry-Zero) [Dao and Le, 2025 ]introduces another\ndimension to RL-based agentic search by specifically focusing\non incentivizing persistence. It addresses the common sce-\nnario where an initial search query might fail to retrieve the\nnecessary information, potentially causing the LLM agent to\nhalt prematurely or generate a suboptimal response. ReZero\naims to teach the agent the value of \u201ctrying one more time.\u201d\nThe framework operates within a standard RL setup (using\nGRPO is mentioned) where the LLM interacts with a search\nenvironment. The novelty lies in its modified reward func-\ntion, which includes a specific component termed reward retry.\nThis component provides a positive reward signal whenever\nthe LLM issues a <search> query after the initial search\nquery within the same reasoning trajectory. Crucially, this\nreward for retrying is conditional upon the agent successfully\ncompleting the task, indicated by generating a final answer\nenclosed in <answer> tags. This conditionality prevents the\nagent from accumulating rewards simply by retrying indef-\ninitely without making progress. By directly rewarding the\nact of persistence (when productive), ReZero encourages the\nLLM to explore alternative queries or search strategies if the\nfirst attempt proves insufficient. This contrasts with methods\nthat might only implicitly reward persistence through eventual\ntask success. ReZero positions itself as complementary to\nframeworks like DeepRetrieval; while DeepRetrieval focuses\non optimizing a single refined query, ReZero emphasizes the\nvalue of making multiple retrieval attempts when needed.\nInteracting with real-world search engines : DeepRe-trieval [Jiang et al. , 2025 ]focuses specifically on improv-\ning the quality of the search queries generated by the LLM\nagent. It frames the task of query generation or rewriting as\nan RL problem, training the LLM to transform an initial user\nquery into a more effective query for downstream retrieval\nsystems. The core mechanism involves the LLM generating\nan augmented or rewritten query based on the input query.\nDeepRetrieval employs RL algorithms like Proximal Policy\nOptimization (PPO) [Schulman et al. , 2017 ]to train this query\ngeneration process. A key innovation lies in its reward signal:\ninstead of relying on supervised data (e.g., pairs of original and\n\u201dgold\u201d rewritten queries), DeepRetrieval uses the performance\nof the generated query in the actual retrieval system as the\nreward. Metrics such as recall@k, Normalized Discounted Cu-\nmulative Gain (NDCG), or evidence-seeking retrieval accuracy\n(Hits@N) obtained from executing the generated query against\na restricted real search engine (like PubMed) or document col-\nlection are used to provide feedback to the LLM. The model\nlearns, through trial and error, to generate queries that maxi-\nmize these retrieval metrics. To structure the generation, the\nmodel often produces reasoning steps within <think> tags\nbefore outputting the final query in an <answer> tag. This\napproach offers significant advantages. By directly optimizing\nfor the end goal (retrieval performance), it bypasses the need\nfor expensive and potentially suboptimal supervised query\ndatasets. Compared to other RL methods, DeepRetrieval\u2019s\nprimary focus is on optimizing the content and formulation of\nthe search query itself.\nDeepResearcher [Zheng et al. , 2025 ]pushes the boundaries\nof training-based Agentic RAG by moving beyond controlled\nenvironments or static corpora to perform end-to-end RL train-\ning directly within real-world web environments. It aims\nto equip LLM agents with the capabilities needed for com-\nplex, deep research tasks that require navigating the noisy,\nunstructured, and dynamic nature of the open web. This ad-\ndresses a key limitation of many existing agents, whether\nprompt-engineered or trained in simulated/static RAG set-\ntings, which often struggle with the complexities of real-world\nweb interaction. The framework employs RL (specifically\nGRPO with an F1 score-based reward for answer accuracy\n) to train agents that interact with live web search APIs and\nbrowse actual webpages. DeepResearcher utilizes a special-\nized multi-agent architecture to handle the complexities of\nweb interaction. This includes a reasoning module, a tool\nfor invoking web search, and dedicated \u201cbrowsing agents\u201d\nresponsible for extracting relevant information from the di-\nverse structures of webpages encountered. Training in this\nrealistic setting was found to foster several emergent cognitive\nbehaviors not typically observed in agents trained under more\nconstrained conditions. These include the ability to formulate\ninitial plans and dynamically adjust them during the research\nprocess, cross-validate information retrieved from multiple\nweb sources, engage in self-reflection when retrieved infor-\nmation seems contradictory or insufficient leading to refined\nsearch strategies, and exhibit honesty by declining to provide\nan answer when definitive information cannot be found. Deep-\nResearcher demonstrated substantial performance improve-\nments over prompt-engineering baselines and RAG-based RL\nagents trained on static corpora, particularly on open-domainresearch tasks. The results strongly suggest that end-to-end\ntraining in realistic web environments is crucial for develop-\ning robust and capable research agents, moving closer to the\ncapabilities hinted at by proprietary systems like OpenAI\u2019s\nDeep Research [OpenAI, 2025 ]or Grok\u2019s DeeperSearch.\nThe progression for the training-based methods, from op-\ntimizing the decision process of when and what to query\n(Search-R1), to fostering persistence (ReZero), optimizing\nquery formulation (DeepRetrieval), and managing real-world\nresearch workflows (DeepResearcher) reflects the growing\nsophistication of RL in agentic search. It reflects a growing\nappreciation that effective information seeking by an agent\ninvolves a confluence of factors: query quality, strategic tim-\ning, resilience to failure, and adeptness in navigating realistic\ninformation environments and so on. Future advancements\nin RL-based Agentic RAG will likely need to integrate these\nfacets more holistically, perhaps through more complex re-\nward structures, multi-objective optimization, or architectures\nthat explicitly model these different dimensions of the search\nprocess, to achieve truly human-like research and problem-\nsolving capabilities.\n5", "Future Research Directions": "Enhancing tool interaction through advanced configura-\ntion. Current agentic reasoning often utilizes search tools with\nrelatively basic interfaces, primarily focused on generating\ntext queries. Future work should enable agents to exploit more\nadvanced configurations offered by external APIs and tools.\nThis could involve training agents to understand and utilize\noptions like result filtering (e.g., by date, source type), sorting\ncriteria, specifying search domains, or interacting with struc-\ntured databases via complex queries. Granting finer control\nwould support more targeted, efficient, and strategic retrieval\naligned with task demands.\nDeveloping finer-Grained and process-oriented reward\nfunctions. Simple outcome-based rewards like exact match\nmay not offer adequate guidance for complex RAG tasks that\nrequire multi-step reasoning or detailed responses. Future\nresearch should develop fine-grained reward functions that\nassess both final answer correctness and intermediate steps\nsuch as document relevance, reasoning coherence, information\ncross-validation, and effective problem decomposition. These\nsignals are vital for training agents to handle queries that\ndemand more than short factual answers.\nImproving Efficiency in Retrieval. The approaches men-\ntioned above primarily focus on the accuracy of the final an-\nswer, but enhancing the efficiency of the retrieval process itself\nis also critical. Agents trained to interact with potentially vast\ninformation sources, must learn to perform retrievals strate-\ngically. Future research should focus on techniques that help\nagents avoid excessive or unnecessary search queries, select\nthe most promising sources, and know when sufficient infor-\nmation has been gathered. Developing strategies to prevent\nagents from getting stuck in loops of unproductive searching\nor performing redundant retrievals is vital for practical and\nscalable Agentic RAG.\nEnhancing Generalization and Robustness in Dynamic\nEnvironments. Robust generalization to new queries, un-seen tools (e.g., sparse, dense, or web retrieval), and changing\nenvironments remains a major challenge. While training in re-\nalistic conditions (as in DeepResearcher) improves resilience,\nagents still struggle with tool failures or shifting knowledge\navailability. Future work should explore adaptive training\nmethodologies and architectures that ensure robust perfor-\nmance in unfamiliar or dynamic settings.\nBy addressing key areas such as improving agent control\nover tools, designing more sophisticated reward signals, in-\ncreasing efficiency, and enhancing generalization, the field\ncan move toward building more capable, reliable, and widely\napplicable Agentic RAG systems. These advancements are\nessential for transitioning agentic AI from research prototypes\nto practical systems that can effectively support humans in\ncomplex information tasks.\n6", "Conclusions": "As language models are increasingly deployed in complex,\nknowledge-intensive applications, the limitations of static\nRAG pipelines have become apparent. Reasoning Agentic\nRAG offers a promising path forward by integrating retrieval\nwith model-driven planning, self-reflection, and tool use. This\npaper surveyed the landscape of reasoning workflows within\nAgentic RAG, distinguishing between predefined reasoning\nwith fixed orchestration, and agentic reasoning that enables\ndynamic, autonomous decision-making. We reviewed key\nmethods across both paradigms, highlighting their strengths,\nlimitations, and use-case applicability. To advance the field,\nwe identify several crucial directions for future research, in-\ncluding fine-grained reward design, enhanced tool control,\nautomated data synthesis, and robust training in dynamic en-\nvironments. These innovations will be essential for realizing\nintelligent, context-aware RAG systems capable of addressing\nreal-world challenges with greater adaptability, transparency,\nand reliability."}, "Automatic Evaluation Metrics for Document-level Translation: Overview, Challenges and Trends": {}, "XRAG: eXamining the Core -- Benchmarking Foundational Components in Advanced Retrieval-Augmented Generation": {"Introduction": "Retrieval-Augmented Generation (RAG) Jiang et al. [2023], Guu et al. [2020], Gao et al. [2023b],\nBorgeaud et al. [2022], Asai et al. [2024], Singh et al. [2025], Han et al. [2025], Mao et al. [2025]\nrepresents a pivotal strategy in Q&A tasks, demonstrating enhanced performance by delivering more\ninformative and accurate answers compared to relying solely on large language models (LLMs). The\nefficacy of basic RAG systems Du et al. [2024], Lu et al. [2024] is contingent upon the seamless\noperation of four core components: pre-retrieval, retrieval, post-retrieval, and generation. The pre-\nretrieval stage indexes the corpus and reforms queries for efficient retrieval. The retrieval stage focuses\non identifying and extracting documents relevant to a given query. The post-retrieval stage refines,\nsummarizes, or compacts information to ensure contextual clarity. Finally, the generation stage\nemploys the LLM to produce responses. These sequential stages critically influence output quality,\nhighlighting the RAG framework\u2019s interdependence. Advanced RAG modules (e.g., reranker, refiner)\noffer sophisticated algorithms for tailored search solutions, surpassing standardized methodologies.\nToolkits like LangChain Chase [2022] and LlamaIndex Liu [2022], modularize the RAG process,\nincreasing adaptability and broadening its applications. However, they are typically cumbersome,\nmaking adaptation to new data challenging and validating or optimizing innovative methods inconve-\n\u2217Corresponding Author.\nPreprint. Under review.arXiv:2412.15529v3  [cs.CL]  16 May 2025Table 1: Comparison of RAG Libraries. Modular Design (Mod.Dsgn) indicates toolkit modularity.\nFair Comparison Zhang et al. [2024] (Fair.Comp) indicates evaluation by aligning key components\nlike seeds, generators, retrievers, and instructions. Unified Datasets (Unif.Data) ensures unified\ndataset formats for retrieval and generation. Modular Evaluation (Mod.Eva) assesses RAG modular\ndifferences. Failure Management (Fail.Mgmt) systematically implements strategies for identifying\nand mitigating RAG failure points. ConR uses token-matching for evaluating retrieval, ConG uses\ntoken-matching for evaluating generation, and CogL is based on LLM-based instructions for retrieval\nand generation evaluation. \u2018u\u2019 refers to No. of unified metrics.\nRAG Library Mod.Dsgn Fair.Comp Unif.Data Mod.Eva Fail.Mgmt ConR ConG CogL\nLangChain Chase [2022] \u2714 0 0 0\nLlamaIndex Liu [2022] \u2714 \u2714 6 0 7\nFastRAG Izsak et al. [2023] \u2714 0 0 0\nRALLE Hoshi et al. [2023] \u2714 0 0 0\nLocalRQA Yu et al. [2024] \u2714 \u2714 3 2 1\nAutoRAG Kim and Kim [2024] \u2714 \u2714 \u2714 6 5 4\nFlashRAG Jin et al. [2024] \u2714 \u2714 \u2714 4 5 0\nRAGLAB Zhang et al. [2024] \u2714 \u2714 \u2714 0 4 0", "XRAG": ": eXamining the Core - Benchmarking Foundational\nComponents in Advanced Retrieval-Augmented Generation\nQianren Mao1, Yangyifei Luo2, Qili Zhang2, Yashuo Luo2, Zhilong Cao2\nJinlong Zhang2, Hanwen Hao2, Zhijun Chen2, Weifeng Jiang3, Junnan Liu2\nXiaolong Wang2, Zhenting Huang2, Zhixing Tan1, Jie Sun1\nBo Li1,2, Xudong Liu1,2, Richong Zhang1,2, Jianxin Li1,2\u2217\n1Zhongguancun Laboratory,2Beihang University,3Nanyang Technological University\nmaoqr@zgclab.edu.cn\n/gtbhttps://github.com/DocAILab/XRAG\nAbstract\nRetrieval-augmented generation (RAG) synergizes the retrieval of pertinent data\nwith the generative capabilities of Large Language Models (LLMs), ensuring that\nthe generated output is contextually relevant but also accurate and current. We\nintroduce XRAG, an open-source, modular codebase that facilitates exhaustive\nevaluation of the performance of foundational components of advanced RAG\nmodules. These components are systematically categorized into four core phases:\npre-retrieval, retrieval, post-retrieval, and generation. We systematically analyse\nthem across reconfigured datasets, providing a comprehensive benchmark for\ntheir effectiveness. As the complexity of RAG systems continues to escalate, we\nunderscore the critical need to identify potential failure points in RAG systems. We\nformulate a suite of experimental methodologies and diagnostic testing protocols\nto dissect the failure points inherent in RAG engineering. Subsequently, we proffer\nbespoke solutions aimed at bolstering the overall performance of these modules.\nOur work thoroughly evaluates the performance of advanced core components in\nRAG systems, providing insights into optimizations for prevalent failure points.\n1 Introduction\nRetrieval-Augmented Generation (RAG) Jiang et al. [2023], Guu et al. [2020], Gao et al. [2023b],\nBorgeaud et al. [2022], Asai et al. [2024], Singh et al. [2025], Han et al. [2025], Mao et al. [2025]\nrepresents a pivotal strategy in Q&A tasks, demonstrating enhanced performance by delivering more\ninformative and accurate answers compared to relying solely on large language models (LLMs). The\nefficacy of basic RAG systems Du et al. [2024], Lu et al. [2024] is contingent upon the seamless\noperation of four core components: pre-retrieval, retrieval, post-retrieval, and generation. The pre-\nretrieval stage indexes the corpus and reforms queries for efficient retrieval. The retrieval stage focuses\non identifying and extracting documents relevant to a given query. The post-retrieval stage refines,\nsummarizes, or compacts information to ensure contextual clarity. Finally, the generation stage\nemploys the LLM to produce responses. These sequential stages critically influence output quality,\nhighlighting the RAG framework\u2019s interdependence. Advanced RAG modules (e.g., reranker, refiner)\noffer sophisticated algorithms for tailored search solutions, surpassing standardized methodologies.\nToolkits like LangChain Chase [2022] and LlamaIndex Liu [2022], modularize the RAG process,\nincreasing adaptability and broadening its applications. However, they are typically cumbersome,\nmaking adaptation to new data challenging and validating or optimizing innovative methods inconve-\n\u2217Corresponding Author.\nPreprint. Under review.arXiv:2412.15529v3  [cs.CL]  16 May 2025Table 1: Comparison of RAG Libraries. Modular Design (Mod.Dsgn) indicates toolkit modularity.\nFair Comparison Zhang et al. [2024] (Fair.Comp) indicates evaluation by aligning key components\nlike seeds, generators, retrievers, and instructions. Unified Datasets (Unif.Data) ensures unified\ndataset formats for retrieval and generation. Modular Evaluation (Mod.Eva) assesses RAG modular\ndifferences. Failure Management (Fail.Mgmt) systematically implements strategies for identifying\nand mitigating RAG failure points. ConR uses token-matching for evaluating retrieval, ConG uses\ntoken-matching for evaluating generation, and CogL is based on LLM-based instructions for retrieval\nand generation evaluation. \u2018u\u2019 refers to No. of unified metrics.\nRAG Library Mod.Dsgn Fair.Comp Unif.Data Mod.Eva Fail.Mgmt ConR ConG CogL\nLangChain Chase [2022] \u2714 0 0 0\nLlamaIndex Liu [2022] \u2714 \u2714 6 0 7\nFastRAG Izsak et al. [2023] \u2714 0 0 0\nRALLE Hoshi et al. [2023] \u2714 0 0 0\nLocalRQA Yu et al. [2024] \u2714 \u2714 3 2 1\nAutoRAG Kim and Kim [2024] \u2714 \u2714 \u2714 6 5 4\nFlashRAG Jin et al. [2024] \u2714 \u2714 \u2714 4 5 0\nRAGLAB Zhang et al. [2024] \u2714 \u2714 \u2714 0 4 0\nXRAG (ours) \u2714 \u2714 \u2714 \u2714 \u2714 7u10u23u\nnient. FastRAG Izsak et al. [2023] and RALLE Hoshi et al. [2023] allow users to assemble RAG\nsystems with core components, fostering a more adaptable RAG implementation. AutoRAG Kim and\nKim [2024] further supports users by identifying optimal RAG pipelines for custom data, facilitating\nbespoke RAG systems. LocalRQA Yu et al. [2024] and RAGLAB Zhang et al. [2024] focus on\nRAG training, offering scripts for various component training. FlashRAG Jin et al. [2024] and\nRAGLAB Zhang et al. [2024] advance algorithmic reproducibility in RAG systems by integrating\nnumerous algorithms into a unified framework. This supports efficient replication of existing methods\nand promotes innovation in algorithm development. However, FlashRAG Jin et al. [2024] lacks\nuniformity in fundamental evaluation components, such as random seeds, generators, retrievers, and\ninstructions, which hinders result comparability. RAGLAB Zhang et al. [2024], despite offering\na fair experimental setup, lacks comprehensive evaluation strategies for assessing individual RAG\ncomponents. Similarly, FlashRAG Jin et al. [2024] exhibits the same lack of modularity analysis,\nrestricting to derive meaningful conclusions. Although ongoing efforts address these challenges\nthrough modular RAG processes (e.g., retrieval engines and generative agents), an implicit gap\npersists in the comparative performance evaluation of these advanced RAG modules within the\noverall RAG workflow. Comprehensive assessments of these modules are notably absent, making it\nchallenging for researchers to evaluate their approaches in consistent experimental conditions.\nTo address the abovementioned issues, we focus on the core components of advanced RAG modules\nand conduct comprehensive experiments across four aspects: pre-retrieval, retrieval, post-retrieval,\nand generation. We introduce XRAG, an open-source, modular codebase designed to evaluate\nfoundational components of advanced RAG modules comprehensively. The key capabilities of\nXRAG are summarized as follows:\nModular RAG Process: Fine-Grained Comparative Analysis. Extensive experiments are con-\nducted on the advanced RAG modules in four stages: pre-retrieval, retrieval, post-retrieval and\ngeneration. The core components cover three query rewriting strategies, six retrieval units, three\npost-processing techniques, and LLM generators from different vendors \u2014 OpenAI, Meta, and\nDeepSeek. XRAG provides an in-depth understanding of the capabilities of RAG components.\nUnified Benchmark Datasets: Dual Assessment of Retrieval and Generation. To enhance the\nuniformity of datasets in RAG research, XRAG compiles and formats three prevalent benchmark\ndatasets, preprocessing them into a unified format. This standardization enables concurrent assessment\nof both retrieval and generation, streamlining comparative evaluations across RAG components.\nComprehensive Testing Methodologies: Multidimensional Evaluation Framework. To overcome\nthe absence of a holistic evaluation system for RAG components, XRAG introduces an evaluation\nbenchmark encompassing three perspectives. It comprises Conventional Retrieval Evaluation for\nretrieval-unit matching, Conventional Generation Evaluation for generation tests based on generative-\ntoken matching, and Cognitive LLM Evaluation for generation tests based on semantic understanding.\nXRAG ensures a standardized and thorough evaluation of retrieval and generation.\n2XRAG BOARD Config Hook API KEYs\nWeb \nInterfaceEasy \nConfigurationMonitorable\nResultsRunnable \nDemosRAG Parameters RAG Strategies RAG MetricsPre-retrieval Retriever Post -processor Generator\n\u2022Others\u2022HyDE\u2022SBPT \u2022RRFusion \u2022BGE-RRK\n\u2022Jina-RRK\n\u2022Others\u2022OpenAI APIs\n\u2022Ollama LLMs\n[User -Query ][Retrieval_ Context ][Retrieval_ Context .IDS][Golden_ Context ][Golden_ Context .IDS] [Actual_ Response ][Expected_ Answer ]CorpusUnified Dataset Preprocessor\u2022StParser\n\u2022OthersConventional Retrieval \nEvaluation\nConRConventional Generation \nEvaluation\nConGCognitive LLM Agent \nEvaluation\nCogL\nFailure\nManagementsAdvanced\nComponmentsEvaluators\nVector Index\nFigure 1: Schematic overview of the XRAG framework.\nIdentification and Mitigation of RAG Failure Points: Systematic Analysis and Improvement.\nRecognizing the lack of systematic experiments and improvement methods addressing RAG failure\npoints, XRAG develops a set of evaluation methods to pinpoint and rectify specific issues. Analyz-\ning failure points and implementing feasible optimization and validation solutions can bolster the\noptimization of RAG components.\n2 XRAG\nFigure 1 delineates the integrated modules and schematic structure of the XRAG framework. The\nframework is stratified into datasets and corpus, advanced components, and evaluators, integrated\nthrough XRAG\u2019s board and config hook (illustrated in the Appendix A.8.1). The overall structure pro-\ngresses from foundational to application-oriented components. Designed with a modular architecture,\nXRAG enables users to accomplish: preparation of normalized RAG datasets (Section 2.2), assembly\nof the RAG components (Section 2.1), evaluation of RAG system\u2019s core components (Section 2.3),\nand diagnosing and optimizing of RAG system failures (Appendix A.6).\n2.1", "Basic & Advanced RAG Components": "\u2663Pre-retrieval Before retrieval, the pre-retrieval components leverage LLMs to refine user queries,\nenhancing the quality and relevance of the information retrieval process. Key methodologies include\nStep-back Prompting ( SBPT Zheng et al. [2024]): Broaden\u2019s queries to enrich contextual grounding\nfor answers, enhancing the contextual foundation for answer generation. Hypothetical Document\nEmbedding ( HyDE Gao et al. [2023a]): Transmutes the original query into a form that better aligns\nwith the indexed documents, improving retrieval alignment and efficacy. \u2660Retriever We use two\nbasic retrieval models as benchmarks: BGE-large and Jina-Large, along with their open-source and\nconsistently robust embedding models. For advanced retrieval strategies, we integrate the LlamaIndex\nto facilitate standard advanced methods. Reciprocal Rerank Fusion Retriever ( RRFusion Cormack\net al. [2009]) fuse indexes with a BM25-based retriever, capturing both semantic relations and\nkeyword relevance. Both retrievers assign scores, enabling reciprocal reranking for node sorting\nwithout additional models or excessive computation. For dense retrieval, SentenceWindow Retriever\n(StParser ) parses documents into single sentences per node, incorporating surrounding sentences\nfor added context. \u25bcPost-processor To improve retrieval accuracy and efficiency, XRAG uses\npost-processors like rerankers to refine nodes before returning them. We integrate the BGE reranker\n(BGE-RRK) from HuggingFace, which computes similarity scores using a Cross-Encoder model.\nAdditionally, Jina-Reranker V2 (Jina-RRK) supports high-accuracy multilingual document reranking.\n\u25b2Generator The XRAG framework integrates various LLM generators, including those from\n3DatasetsSize CorpusMulti-hop Constrained Numerical Set-logical\nTrain Validation Test Documents Source\nHotpotQA (HQA) 86,830 8,680 968 508,826 Wikipedia \u2714 \u2714\nDropQA (DQA) 78,241 7,824 870 6,147 Wikipedia \u2714 \u2714 \u2714\nNaturalQA 100,093 10,010 1,112 49,815 wikipedia \u2714 \u2714 \u2714\nTable 2: Summary of Benchmark Datasets & Corpus. Original training and validation sets, are\nretained for potential future fine-tuning of RAG systems or other customized tasks, although XRAG\ndoes not currently support fine-tuning.\n0.0 0.1 0.2 0.3 0.4 0.5 0.6\nContext Length (No.of k T okens)1.961.982.002.022.04No.of ContextHotpotQA\n40380.0720.01060.01400\nData Count\n(a) HotpotQA\n0.0 0.5 1.0 1.5 2.0\nContext Length (No.of k T okens)0.960.981.001.021.04No.of ContextDropQA\n40450.0860.01270.01680\nData Count (b) DropQA\n0 20 40 60 80 100 120 140\nContext Length (No.of k T okens)0.960.981.001.021.04No.of ContextNaturalQA\n40120.0200.0280.0360\nData Count (c) NaturalQA\nFigure 2: The golden contextual distribution of the corpora across three datasets providing the\nquantity and length of annotated contexts. This distribution aids in analyzing the contextual structure,\nenabling a clearer understanding of how contexts vary in detail for each dataset.\nHuggingFace Transformers APIs2, ensuring compatibility with open-source LLMs. It also integrates\nthe Ollama3framework, supporting the localized use of LLMs. In addition to open-source models,\nthe generator module of XRAG includes support for closed-source LLM APIs. Thus, users can access\ndiverse capabilities while retaining the option to use proprietary models.\n2.2 Unified Benchmark Datasets & Corpus\nWe collect and preprocess three benchmark datasets for the XRAG framework, emphasising rigorous\nexperimental validation of RAG systems. We develop a unified dataset structured to facilitate\nperformance testing for both retrieval and generation modules, incorporating standardized formats:\nUser-Query || Retrieval-Context || Retrieval-Context.IDS || Golden-Context ||\nGolden-Context.IDS || Actual-Response || Expected-Answer .\nXRAG provides a unified architecture for retrieval and question-answering datasets, specifically\nHoppotQA Yang et al. [2018], DropQA Dua et al. [2019], and NaturalQA Kwiatkowski et al. [2019].\nThe corpus used for indexing derives from the metadata of these datasets\u2019 training, validation,\nand test sets. This methodology aligns with previous works Shi et al. [2024] and supports the\nefficient deployment of vector databases. Table 2 shows that the HotpotQA corpus contains the\nhighest document count, followed by NaturalQA, indicating that retrieval difficulty escalates with the\nnumber of documents required per query. Moreover, these datasets address complex RAG question-\nanswering scenarios, including Multi-hop Questions, Constrained Questions, Numerical Reasoning,\nand Logical Reasoning tasks. Multi-hop questions depend on iteratively retrieved documents and\ntheir interrelations to deduce the answer. Constrained Q&A generates each answer alongside a\ncorresponding constraint or condition rather than providing a standalone response. Numerical\nreasoning involves performing arithmetic operations such as addition, subtraction, sorting, and\ncounting. Set-logical reasoning addresses more complex logical problems involving relationships\namong retrieved textual samples.\nThe corpora for the three datasets originate from Wikipedia. We construct retrievable documents\nusing metadata from the original datasets, standardizing retrieval objects as document IDs for\ntesting. Retrieval is successful if the retrieved chunk node corresponds to the annotated document ID.\n2https://github.com/huggingface/transformers\n3https://github.com/ollama\n4This method ensures consistency in retrieval labels and eliminates discrepancies caused by varying\ndocument chunking strategies. Moreover, Figure 2 illustrates the distribution of context lengths across\nthree Q&A datasets: HotpotQA, DropQA, and NaturalQA. HotpotQA shows a tight clustering of\ncontext lengths between 0.1 to 0.3k tokens, focusing on shorter contexts. DropQA presents a broader\nrange, with most contexts falling within the 0.1 to 0.5k tokens range but with a more extended tail.\nNaturalQA exhibits the most diverse distribution, with context lengths spanning from very short to\nas long as 140k tokens, reflecting a design that accommodates various text lengths for Q&A tasks.\nFor the test set, we limit the number of samples to mitigate the high token cost associated with RAG\nand LLM evaluations. Instead of evaluating the entire test set, we apply a sampling-based averaging\nmethod, reducing token usage while preserving reliability.\n2.3 Evaluation Methods\nTo assess the quality of the RAG components, we integrate the Jury Cavusoglu et al. [2023], a\ncomprehensive package for the evaluation of NLG systems, with RAG community evaluation tools\nsuch as UpTrain4and DeepEval5. XRAG evaluators are pivotal in determining the effectiveness of\nboth the retrieval and generation components. They are categorized into three groups: Conventional\nRetrieval Evaluation, Conventional Generation Evaluation, and Cognitive LLM Evaluation.\nConventional Retrieval Evaluation (ConR Evaluator) . It supports six primary metrics: F1, Mean\nReciprocal Rank (MRR), and Mean Average Precision (MAP), along with Hit@1 and Hit@10.\nAdditionally, it includes the DCG family of metrics, which assesses the effectiveness of ranking\nmodels by evaluating the quality of ordered results ( retrieval-context.ids ). This family comprises\nDiscounted Cumulative Gain (DCG), Normalized Discounted Cumulative Gain (NDCG), and Ideal\nDiscounted Cumulative Gain (IDCG). IDCG is the maximum DCG that can be obtained if the results\nare ideally ranked \u2013 arranged in descending order of their relevance ( golden-context.ids ).\nConventional Generation Evaluation (ConG Evaluator) . These generative-token matching\nmetrics can be classified into three broad categories. N-gram similarity metrics: ChrF Popovic\n[2015], ChrF++ Popovic [2017], METEOR Banerjee and Lavie [2005], ROUGE F1 Lin [2004]\n(R1, R2, RL) focus on overlap in n-grams between generation ( actual-response ) and reference\n(expected-answer ). Divergence-based metrics: MAUVE Pillutla et al. [2021], Perplexity Jelinek et al.\n[1977] measure content quality, diversity, and model learning by comparing the distribution between\nthe generation and reference. Error-based accuracy metrics: Word Error Rate (WER) Morris et al.\n[2004], Character Error Rate (CER) Morris et al. [2004], assess the accuracy of actual-response by\ncalculating the differences or errors when compared with the expected-answer .\nCognitive LLM Evaluation (CogL Evaluator) . It is classified into three categories: Retrieval-\noriented, Generation-oriented, and Combined Retrieval & Generation. Cognitive LLM Eval-\nuation metrics, derived from UpTrain and DeepEval, are classified based on the parameters\nunified by our XRAG framework. Metrics involving response-related parameters, such as\nactual-response orexpected-answer , are Generation-oriented Metrics. Conversely, metrics that\nlack response-related parameters, including retrieval-related parameters, such as retrieval-context\norretrieval-context.ids , are designated as Retrieval-oriented Metrics. The rest are Combined\nRetrieval & Generation Metrics. Moreover, we utilize GPT-4 Turbo as the LLM agent for CogL\nevaluator. Since LLM evaluations are billed per token, we conducted experimental discussions for\nour foundational modules of RAG in Appendix A.3.\nXRAG evaluators have obvious advantages:\n\u2022Evaluating with Multiple RAG Metrics in One Go : The XRAG evaluator allows users to\nsimultaneously assess various RAG-specific metrics. This capability streamlines the evaluation\nprocess, enabling comprehensive performance analysis without sequential evaluations.\n\u2022Standardizes the Structure of Evaluation Metrics : A unified data format simplifies the compari-\nson between different RAG components in both retrieval and generation.\n\u2022Character and Semantic \u00d7Retrieval and Generation : It encompasses a 4-fold cross-dimensional\nanalysis, including character-level matching tests and semantic-level understanding tests for both\nretrieval and generation.\n4https://github.com/uptrain-ai/uptrain\n5https://github.com/confident-ai/deepeval\n5Dataset & MethodsConventional Retrieval Evaluation (ConR Evaluator)\nF1 MRR Hit@1 Hit@10 MAP NDCG DCG\u25e6IDCG\u25e6HQAJINA-Large 49.80 64.23 21.42 94.44 55.17 70.44 0.9823 1.1795\nBGE-Large 60.66 72.84 18.29 97.71 64.13 78.64 1.1644 1.3656DQAJINA-Large 20.88 24.42 06.90 34.47 24.42 26.76 0.3835 0.4482\nBGE-Large 22.58 27.10 07.12 42.30 27.10 29.58 0.4160 0.4849NQAJINA-Large 49.20 62.44 30.31 88.49 62.44 66.98 0.8843 1.0120\nBGE-Large 49.90 63.82 25.68 93.24 63.82 69.12 0.8799 1.0300\nTable 3: Upstream RAG Retrieval Performance using Basic Retriever of BGE-Large. The con-\nventional retrieval evaluation, denoted as ConR, is performed. The symbol\u25e6signifies metric values\nranging from 0 to positive infinity, while all other values are within the [0,1]interval (%).\nDataset & MethodsConventional Retrieval Evaluation (ConR Evaluator)\nF1 MRR Hit@1 Hit@10 MAP NDCG DCG\u25e6IDCG\u25e6HQASBPT 38.60 53.86 16.96 96.43 44.99 59.61 0.7174 0.8992\nHyDE 53.07 67.39 17.86 97.32 58.32 72.44 0.9947 1.1753\nRRFusion 59.99 76.92 12.24 96.94 66.25 79.72 1.0401 1.1192\nStParser 42.92 59.54 14.02 97.20 49.82 65.17 0.8069 0.9921\nBGE-RRK 66.90 80.19 77.03 97.30 70.62 83.14 1.1354 1.2183\nJina-RRK 64.68 76.89 76.92 96.43 68.83 79.60 1.1096 1.1733DQASBPT 22.57 27.03 06.01 42.08 27.03 29.45 0.4149 0.4823\nHyDE 22.74 26.40 06.81 41.48 26.40 28.89 0.4213 0.4906\nRRFusion 26.36 29.31 14.29 19.05 29.31 30.18 0.3885 0.4135\nStParser 27.70 30.34 05.13 46.15 30.34 31.28 0.4092 0.4355\nBGE-RRK 31.72 34.27 28.46 38.88 34.27 35.44 0.4663 0.4990\nJina-RRK 34.20 37.66 31.17 44.16 37.66 38.68 0.5097 0.5385NQASBPT 49.22 62.27 22.86 88.57 62.27 67.53 0.8737 1.0210\nHyDE 50.12 62.94 41.03 94.87 62.94 68.69 0.8864 1.0483\nRRFusion 62.39 70.35 42.86 95.24 70.35 73.02 0.8949 0.9700\nStParser 59.79 68.42 28.95 94.74 68.42 71.29 0.8430 0.9239\nBGE-RRK 59.14 68.26 50.91 76.36 68.26 71.20 0.8284 0.9113\nJina-RRK 58.97 66.67 56.41 79.49 66.67 68.68 0.8648 0.9215\nTable 4: Upstream RAG Retrieval Performance about advanced Pre-retrieval ,Advanced Retrieval\nandPost-processor with prompt engineering modules. Results underlined indicate superiority over\nBasic Retriever of BGE-Large, while bold results denote the best performance.\n3 Experimental Results and Discussion\n3.1 Experimental Setup\nFor document preprocessing, we used SentenceSplitter to segment documents into chunks and build\na vector index. It was set to a chunk size of 128 tokens and a chunk overlap of 20 tokens. We adhered\nto LlamaIndex configurations for other RAG components, including the refine module for response\nsynthesis. To ensure compatibility and efficiency, XRAG integrates Huggingface Transformers.\nAll Q&A LLMs were set with Temperature = 0 to ensure experimental consistency6. For each\nquery, five chunks were retrieved as contextual data. Consequently, the evaluation metrics measure\nretrieval accuracy based on 3 retrieval nodes, fully encompassing the assumptions of most datasets\nthat typically consider only one or two golden contextual nodes. The generator model\u2019s context\nwindow\u2014encompassing the query, prompt, retrieved context, and response content\u2014is configured\nto 4096 tokens. This study conducts RAG testing across the entire test set using two rule-based\nevaluations: Conventional Retrieval Evaluation (ConR) and Conventional Generation Evaluation\n(ConG). Notably, Cognitive LLM Evaluation (CogL) metrics are omitted from the main experimental\nframework, appearing only in pilot studies due to budget constraints from high computational costs\nof token processing during LLM reasoning and testing. Analyses of RAG pilot studies using CogL\nare in Appendix A.4. Computational resources are detailed in Appendix A.2.\n3.2 Retrieval and Generation Evaluation\nRetrieval Quality The retrieval performance varies sensibly across the three datasets, with the poorest\nquality observed on DropQA in Table 3. Since DropQA requires advanced paragraph understanding\n6Nonetheless, using large models for generation or evaluation always results in minor variations, possibly\ndue to randomness in MoE training or prompt interpretation biases Kaddour et al. [2023].\n6Datasets & Methods Conventional Generation Evaluation (ConG Evaluator)\nOracle: Golden-Context ChrF ChrF++ METEOR R1 R2 RL PPL\u25e6\n\u2193CER\u25e6\n\u2193WER\u25e6\n\u2193HQAGPT-4o mini 21.75\u00b1.024 20.01\u00b1.012 21.62\u00b1.008 12.93\u00b1.007 05.32\u00b1.003 12.71\u00b1.005 111.98\u00b11.368 11.08\u00b1.019 09.75\u00b1.002\nllama3.1-8B 21.08\u00b1.018 19.30\u00b1.014 16.71\u00b1.012 15.98\u00b1.010 06.40\u00b1.006 15.89\u00b1.008 150.34\u00b11.600 14.92\u00b1.017 12.15\u00b1.009\nDeepSeek R1-7B 11.66\u00b1.020 10.37\u00b1.015 09.53\u00b1.013 05.10\u00b1.011 01.75\u00b1.007 05.01\u00b1.009 69.22\u00b11.450 23.39\u00b1.021 18.12\u00b1.008DQAGPT-4o mini 08.18\u00b1.018 07.47\u00b1.017 06.87\u00b1.012 04.12\u00b1.010 01.80\u00b1.006 04.12\u00b1.008 91.97\u00b11.400 44.35\u00b1.017 14.50\u00b1.009\nLlama3.1-8B 04.93\u00b1.020 04.29\u00b1.015 04.53\u00b1.009 02.17\u00b1.011 00.64\u00b1.007 02.16\u00b1.009 153.56\u00b11.712 76.50\u00b1.021 23.63\u00b1.011\nDeepSeek R1-7B 04.89\u00b1.019 04.33\u00b1.016 04.26\u00b1.012 01.87\u00b1.010 00.72\u00b1.006 01.86\u00b1.008 49.51\u00b11.350 90.94\u00b1.020 28.58\u00b1.007NQAGPT-4o mini 25.34\u00b1.022 23.98\u00b1.013 25.89\u00b1.009 16.20\u00b1.008 06.95\u00b1.005 15.52\u00b1.007 48.50\u00b11.239 09.41\u00b1.020 08.83\u00b1.006\nLlama3.1-8B 22.34\u00b1.021 20.78\u00b1.016 18.00\u00b1.014 17.47\u00b1.012 07.68\u00b1.010 16.92\u00b1.009 124.79\u00b11.807 14.19\u00b1.018 11.10\u00b1.013\nDeepSeek R1-7B 11.13\u00b1.020 09.82\u00b1.015 08.10\u00b1.013 04.90\u00b1.011 01.40\u00b1.007 04.53\u00b1.009 40.97\u00b11.331 20.71\u00b1.021 16.93\u00b1.008\nTable 5: Downstream RAG Generation Performance of various LLMs, utilizing Oracle as the\nretrieval results. The conventional generation evaluation (ConG) is conducted. The top scores are\nemphasized in bold. The symbol\u25e6signifies metric values ranging from 0 to positive infinity, while\nall other values are within the [0,1]interval (%). The symbol \u2193denotes that lower metric values are\npreferable. The error value is the average of three trials, with a constant LLM Temperature of 0.\nDatasets & Methods Conventional Generation Evaluation (ConG Evaluator)\nRetriever: JINA-Large ChrF ChrF++ METEOR R1 R2 RL PPL\u25e6\n\u2193CER\u25e6\n\u2193WER\u25e6\n\u2193HQAGPT-4o mini 26.65\u00b10.030 25.20\u00b10.025 29.81\u00b10.020 16.87\u00b10.015 08.82\u00b10.010 16.72\u00b10.005 95.72\u00b11.245 14.51\u00b10.025 11.32\u00b10.010\nLlama3.1-8B 37.33\u00b10.010 35.05\u00b10.018 32.51\u00b10.022 34.80\u00b10.013 18.77\u00b10.012 34.58\u00b10.007 173.88\u00b11.209 09.50\u00b10.018 07.83\u00b10.012\nDeepSeek R1-7B 19.28\u00b10.022 18.18\u00b10.018 22.62\u00b10.012 10.90\u00b10.008 04.97\u00b10.004 11.04\u00b10.009 77.40\u00b11.235 20.45\u00b10.020 15.70\u00b10.015DQAGPT-4o mini 08.80\u00b10.023 08.36\u00b10.018 09.66\u00b10.012 04.80\u00b10.008 02.00\u00b10.001 04.80\u00b10.006 55.93\u00b11.280 75.33\u00b10.332 26.38\u00b10.027\nLlama3.1-8B 10.98\u00b10.028 10.32\u00b10.022 10.65\u00b10.017 07.66\u00b10.012 02.79\u00b10.006 07.66\u00b10.009 164.35\u00b11.152 73.46\u00b10.309 27.64\u00b10.021\nDeepSeek R1-7B 07.17\u00b10.029 06.92\u00b10.018 08.44\u00b10.013 03.78\u00b10.007 01.16\u00b10.005 03.77\u00b10.008 60.70\u00b11.236 69.78\u00b10.284 23.20\u00b10.025NQAGPT-4o mini 29.37\u00b10.024 28.29\u00b10.022 32.61\u00b10.018 19.49\u00b10.014 11.12\u00b10.009 19.90\u00b10.007 53.16\u00b11.145 10.98\u00b10.025 09.56\u00b10.015\nLlama3.1-8B 37.43\u00b10.020 35.65\u00b10.028 33.30\u00b10.022 35.24\u00b10.017 21.57\u00b10.014 34.74\u00b10.009 170.36\u00b11.851 10.82\u00b10.018 08.82\u00b10.014\nDeepSeek R1-7B 21.16\u00b10.015 20.10\u00b10.020 22.50\u00b10.016 12.93\u00b10.009 06.29\u00b10.004 12.43\u00b10.008 46.83\u00b11.136 17.16\u00b10.025 14.48\u00b10.015\nTable 6: Downstream RAG Generation Performance of various LLMs, utilizing JINA-Large as\nthe retrieval model. Results underlined indicate superiority over RAG generation based on Oracle\ncontext, while bold results denote the best performance.\nand discrete reasoning Ran et al. [2019], it presents a greater retrieval challenge. The NDCG metric\nreflects that the basic retrieval model performs reasonably well in relevance and ranking accuracy on\nHotpotQA and NaturalQA datasets. Ignoring ranking accuracy, a high Hit@10 score (>0.8) indicates\na substantial likelihood of retrieving semantically relevant document blocks. The Hit@1 metric\nmeasures the ability to return the correct answer as the top-ranked result, making it suitable for tasks\nwith a single retrieval target (e.g., the golden single passage in the NaturalQA dataset). However,\nit is less effective for scenarios requiring multiple equally prioritized context targets, as seen in the\nHotpotQA dataset. Overall, the retrieval system performs well on HotpotQA and NaturalQA but\nfaces challenges on DropQA, possibly due to the complexity of the answer (Low Hit@1 scores reflect\ndifficulty in retrieving the most relevant results.). Compared to the basic retriever in Table 3 and the\nadvanced retriever in Table 4, the highlighted results indicate that incorporating additional matching\ninformation can enhance retrieval effectiveness. For example, RRFusion combines dense vectors\nwith keyword retrieval, and the sentence window includes the context of chunk nodes in the retrieval\nobjects. Additionally, it is observed that reranking methods can markedly improve performance,\nparticularly for challenging retrieval tasks such as DQA. For instance, both Both BGE-RRK and\nJina-RRK outperform the basic retriever.\nGeneration Quality In examining the question-and-answer (Q&A) component, it is essential to\ncollectively consider Tables 5, 6, and 7. Table 5 provides the test results when the human-annotated\nretrieval output is directly entered into the large model for Q&A purposes. Table 6 illustrates the\noutcomes when the retriever employs the JINA-Large model to identify relevant sentences, which are\nsubsequently fed into the large model. Similarly, Table 7 demonstrates the Q&A performance when\nthe retriever uses the BGE-Large model to acquire pertinent sentences for the large model.\nOur analysis reveals that the experimental outcomes in Tables 6, and 7 generally surpass those in\nTable 5. This finding suggests that directly supplying the large model with golden context does\nnot yield optimal results for these datasets. As depicted in Figure 2, the golden context typically\ncomprises a single text passage (DQA and NQA) or two passages (HQA), which may provide\ninadequate information for the large model. Additionally, the model may encounter difficulties\nin analyzing results from such concise texts, particularly for questions necessitating numerical\n7Datasets & Methods Conventional Generation Evaluation (ConG Evaluator)\nRetriever: BGE-Large ChrF ChrF++ METEOR R1 R2 RL PPL\u25e6\n\u2193CER\u25e6\n\u2193WER\u25e6\n\u2193HQAGPT-4o mini 28.23\u00b10.030 26.85\u00b10.025 33.07\u00b10.020 17.96\u00b10.015 09.51\u00b10.010 17.86\u00b10.005 90.51\u00b11.145 13.60\u00b10.025 11.02\u00b10.010\nLlama3.1-8B 40.63\u00b10.010 38.34\u00b10.018 36.24\u00b10.022 36.33\u00b10.013 21.31\u00b10.012 36.33\u00b10.007 181.71\u00b11.155 10.40\u00b10.007 08.10\u00b10.012\nDeepSeek R1-7B 20.02\u00b10.022 18.76\u00b10.018 23.71\u00b10.012 11.40\u00b10.008 05.19\u00b10.004 11.33\u00b10.009 81.34\u00b11.235 21.46\u00b10.025 16.47\u00b10.015DQAGPT-4o mini 08.89\u00b10.025 08.48\u00b10.018 10.38\u00b10.012 05.04\u00b10.008 02.11\u00b10.004 05.03\u00b10.006 48.22\u00b11.228 76.53\u00b10.035 27.95\u00b10.025\nLlama3.1-8B 10.85\u00b10.028 10.27\u00b10.022 11.59\u00b10.017 07.58\u00b10.012 03.08\u00b10.006 07.53\u00b10.009 148.43\u00b11.072 78.57\u00b10.019 30.62\u00b10.026\nDeepSeek R1-7B 07.49\u00b10.029 07.20\u00b10.018 08.63\u00b10.013 03.18\u00b10.007 00.93\u00b10.005 03.16\u00b10.008 56.18\u00b11.091 63.03\u00b10.028 23.02\u00b10.010NQAGPT-4o mini 29.37\u00b10.024 28.29\u00b10.022 32.61\u00b10.018 19.49\u00b10.014 11.12\u00b10.009 18.99\u00b10.007 53.16\u00b11.222 10.98\u00b10.025 09.56\u00b10.004\nLlama3.1-8B 37.28\u00b10.020 35.50\u00b10.028 32.73\u00b10.022 34.72\u00b10.017 20.88\u00b10.014 34.15\u00b10.009 158.86\u00b11.450 10.60\u00b10.018 08.84\u00b10.014\nDeepSeek R1-7B 23.51\u00b10.015 22.50\u00b10.020 26.39\u00b10.016 14.48\u00b10.009 07.64\u00b10.004 14.18\u00b10.008 50.35\u00b11.230 17.33\u00b10.025 14.15\u00b10.0009\nTable 7: Downstream RAG Generation Performance of various LLMs, utilizing BGE-Large as the\nretrieval model.\n50 100 150 200 2500.00.20.40.60.81.0F1\n(a) F1\n50 100 150 200 2500.00.20.40.60.81.0MRR (b) MRR\n50 100 150 200 2500.00.20.40.60.81.0Hit@1 (c) Hit@1\n50 100 150 200 2500.00.20.40.60.81.0Hit@10 (d) Hit@10\n50 100 150 200 2500.00.20.40.60.81.0MAP (e) MAP\n50 100 150 200 2500.00.20.40.60.81.0NDCG\n(f)NDCG\n50 100 150 200 2500.00.20.40.60.81.0Chrf (g) ChrF\n50 100 150 200 2500.00.20.40.60.81.0Chrf++ (h) ChrF++\n50 100 150 200 2500.00.20.40.60.81.0METEOR (i)METEOR\n50 100 150 200 2500.00.20.40.60.81.0R1 (j)ROUGE-1\n50 100 150 200 2500.00.20.40.60.81.0R2\n(k) ROUGE-2\n50 100 150 200 2500.00.20.40.60.81.0RL (l)ROUGE-L\n50 100 150 200 2500.20.40.60.81.0PPL (m) PPL\n50 100 150 200 2500.00.20.40.60.81.0CER (n) CER\n50 100 150 200 2500.00.20.40.60.81.0WER (o) WER\nFigure 3: Comparison of retrieval and question-answering generation effects as query length varies.\nExperiments conducted on the HQA dataset (Test).\ncomputations and logical reasoning. Our retrieval model delivers the top k (K=3) results to the LLM,\nfiltering and ranking context information to provide more comprehensive data. This observation\nimplies that for any retrieval-augmented generation (RAG) task, evaluating only the retrieval results\nmay not accurately represent the overall RAG performance; concurrent evaluation of both retrieval\nand large model Q&A generation is imperative.\nMoreover, when comparing various large models, it was observed that the DeepSeek R1-7B inference\nmodel (in its 7B distilled version) exhibited the least favorable performance. To ensure fairness, we\ndid not select the 671B parameter version of the DeepSeek model, indicating prominent potential for\nimprovement within the RAG framework for the distilled version. The most consistent performance\nwas observed with the Llama3.1-8B model, which achieved optimal Q&A results when combined\nwith either JINA-Large or BGE-Large retrieval outputs. Nonetheless, its perplexity (PPL) value\nwas the highest, with a PPL of 181.71 on the HQA dataset, for instance. It is important to note\nthat PPL and Q&A metrics assess different facets of the model: PPL emphasizes text fluency and\ngenerality, whereas ChrF and METEOR focus on the precise alignment between generated text and\nexpected answers. While Llama3.1-8B may excel in producing content closely aligned with expected\nanswers in specific tasks, it may not manage semantic coherence and text fluency as effectively as\nother models in broader generation tasks.\nWhether Longer Queries Have A Greater Impact? The illustration (as shown in Figure 3) indicates\na positive correlation between query length and the performance of several metrics, notably Hit@1 and\nMETEOR, which demonstrate an ascending trajectory with increased query length. This correlation\nimplies that the utilization of longer queries may augment the efficacy of retrieval and question-\nanswering systems. Such enhancements are likely due to the enriched contextual information that\nlonger queries provide, facilitating a more precise interpretation of user queries.\nWhether More Context Has A Greater Impact? The illustration depicted in Figure 4 demonstrates\nthat the performance of question-answering systems remains relatively stable as the number of\n80.0 0.2 0.4 0.6 0.8 1.0\nChrf++12345678910Retrieved T op-K Context\n0.32\n0.43(a) ChrF++\n0.0 0.2 0.4 0.6 0.8 1.0\nMETEOR12345678910Retrieved T op-K Context\n0.31\n0.42 (b) METEOR\n0.0 0.2 0.4 0.6 0.8 1.0\nROUGE-112345678910Retrieved T op-K Context\n0.31\n0.41 (c) ROUGE-1\n0.0 0.2 0.4 0.6 0.8 1.0\nROUGE-212345678910Retrieved T op-K Context\n0.18\n0.29 (d) ROUGE-2\n0.0 0.2 0.4 0.6 0.8 1.0\nROUGE-L12345678910Retrieved T op-K Context\n0.30\n0.40\n(e) ROUGE-L\n0\n250 500 7501000 1250 1500\nPPL12345678910Retrieved T op-K Context\n146.77\n231.89 (f)PPL\n050100 150 200\nCER12345678910Retrieved T op-K Context\n7.10\n11.54 (g) CER\n020 40 60 80100 120\nWER12345678910Retrieved T op-K Context\n10.366.00 (h) WER\nFigure 4: Ridge plots of the impact of varying numbers of retrieved contexts on Q&A performance in\nthe HQA dataset (Test). The chart shows maximum and minimum values.\nretrieval contexts increases. This observation suggests that augmenting the quantity of retrieval\ncontexts may not significantly enhance model performance. Upon analyzing the mean value ranges, it\nis evident that for instance, in the ROUGE-1 metric, the mean value only moderately increases from\n0.31 to 0.41 with the augmentation of retrieval contexts. This modest variation implies that the impact\nof increasing the number of contexts on performance improvement is limited. Furthermore, the\nobservation of maximum values reaching 1.0 in multiple figures indicates that the key to enhancing\nperformance may lie in improving the precision of retrieval. This involves identifying a few contexts\nmost relevant to the query, rather than merely increasing the number of retrieved contexts. The ChrF++\nand METEOR metrics exhibit relatively stable performance across varying numbers of retrieval\ncontexts. This suggests that these metrics are not particularly sensitive to variations in the number of\ncontexts. Consequently, this underscores the importance of focusing on enhancing the relevance and\nprecision of retrieval results in the optimization of the retrieval module within Retrieval-Augmented\nGeneration (RAG) systems, rather than solely increasing the quantity of retrieved contexts.\n4 Conclusion\nThis work introduces XRAG, an open-source framework for benchmarking advanced RAG systems\nthat facilitates exhaustive evaluation of foundational components across pre-retrieval, retrieval, post-\nretrieval, and generation phases. It introduces a modular RAG process, unified benchmark datasets,\ncomprehensive testing methodologies, and strategies for identifying and mitigating RAG failure\npoints. The framework supports a range of evaluation metrics, including conventional retrieval and\ngeneration assessments and cognitive LLM evaluations. We believe XRAG will empower researchers\nto construct and evaluate RAG modules, streamlining workflows efficiently.\n5 Broader Impacts & Limitations\nXRAG offers researchers a standardized RAG framework and evaluation tools, covering around 50\nretrieval and generation metrics. It systematically analyzes the four core stages of RAG systems: pre-\nretrieval, retrieval, post-retrieval, and generation. Furthermore, XRAG supports both API-based and\nlocalized deployment methods for industrial applications. Our XRAG currently has several limitations\nwe intend to address in future updates. i. Although XRAG incorporates core RAG modules, time cost\nand compatibility constraints prevent comprehensive coverage of all RAG advancements, which may\nrequire more open-source contributions. ii. The toolkit does not support RAG component training;\nhowever, due to the variety of methods and the presence of specialized repositories, it was not included.\nWe may offer supplementary scripts for training in the future. iii. Currently, our toolkit includes\ndatasets focused on multi-hop question answering, numerical reasoning, and logical reasoning. Future\nupdates could expand to encompass OpenQA Mallen et al. [2023], Joshi et al. [2017], long-form\nQ&A Stelmakh et al. [2022], Min et al. [2023], and multiple-choice Q&A Hendrycks et al. [2021],\nHuang et al. [2022]. We encourage the open-source community to overcome these limitations. Our\ngoal is to persistently refine the XRAG framework by delivering a more efficient and dependable\nplatform with comprehensive evaluation and development tools.\n9", "Evaluation Methods": "To assess the quality of the RAG components, we integrate the Jury Cavusoglu et al. [2023], a\ncomprehensive package for the evaluation of NLG systems, with RAG community evaluation tools\nsuch as UpTrain4and DeepEval5. XRAG evaluators are pivotal in determining the effectiveness of\nboth the retrieval and generation components. They are categorized into three groups: Conventional\nRetrieval Evaluation, Conventional Generation Evaluation, and Cognitive LLM Evaluation.\nConventional Retrieval Evaluation (ConR Evaluator) . It supports six primary metrics: F1, Mean\nReciprocal Rank (MRR), and Mean Average Precision (MAP), along with Hit@1 and Hit@10.\nAdditionally, it includes the DCG family of metrics, which assesses the effectiveness of ranking\nmodels by evaluating the quality of ordered results ( retrieval-context.ids ). This family comprises\nDiscounted Cumulative Gain (DCG), Normalized Discounted Cumulative Gain (NDCG), and Ideal\nDiscounted Cumulative Gain (IDCG). IDCG is the maximum DCG that can be obtained if the results\nare ideally ranked \u2013 arranged in descending order of their relevance ( golden-context.ids ).\nConventional Generation Evaluation (ConG Evaluator) . These generative-token matching\nmetrics can be classified into three broad categories. N-gram similarity metrics: ChrF Popovic\n[2015], ChrF++ Popovic [2017], METEOR Banerjee and Lavie [2005], ROUGE F1 Lin [2004]\n(R1, R2, RL) focus on overlap in n-grams between generation ( actual-response ) and reference\n(expected-answer ). Divergence-based metrics: MAUVE Pillutla et al. [2021], Perplexity Jelinek et al.\n[1977] measure content quality, diversity, and model learning by comparing the distribution between\nthe generation and reference. Error-based accuracy metrics: Word Error Rate (WER) Morris et al.\n[2004], Character Error Rate (CER) Morris et al. [2004], assess the accuracy of actual-response by\ncalculating the differences or errors when compared with the expected-answer .\nCognitive LLM Evaluation (CogL Evaluator) . It is classified into three categories: Retrieval-\noriented, Generation-oriented, and Combined Retrieval & Generation. Cognitive LLM Eval-\nuation metrics, derived from UpTrain and DeepEval, are classified based on the parameters\nunified by our XRAG framework. Metrics involving response-related parameters, such as\nactual-response orexpected-answer , are Generation-oriented Metrics. Conversely, metrics that\nlack response-related parameters, including retrieval-related parameters, such as retrieval-context\norretrieval-context.ids , are designated as Retrieval-oriented Metrics. The rest are Combined\nRetrieval & Generation Metrics. Moreover, we utilize GPT-4 Turbo as the LLM agent for CogL\nevaluator. Since LLM evaluations are billed per token, we conducted experimental discussions for\nour foundational modules of RAG in Appendix A.3.\nXRAG evaluators have obvious advantages:\n\u2022Evaluating with Multiple RAG Metrics in One Go : The XRAG evaluator allows users to\nsimultaneously assess various RAG-specific metrics. This capability streamlines the evaluation\nprocess, enabling comprehensive performance analysis without sequential evaluations.\n\u2022Standardizes the Structure of Evaluation Metrics : A unified data format simplifies the compari-\nson between different RAG components in both retrieval and generation.\n\u2022Character and Semantic \u00d7Retrieval and Generation : It encompasses a 4-fold cross-dimensional\nanalysis, including character-level matching tests and semantic-level understanding tests for both\nretrieval and generation.\n4https://github.com/uptrain-ai/uptrain\n5https://github.com/confident-ai/deepeval\n5Dataset & MethodsConventional Retrieval Evaluation (ConR Evaluator)\nF1 MRR Hit@1 Hit@10 MAP NDCG DCG\u25e6IDCG\u25e6HQAJINA-Large 49.80 64.23 21.42 94.44 55.17 70.44 0.9823 1.1795\nBGE-Large 60.66 72.84 18.29 97.71 64.13 78.64 1.1644 1.3656DQAJINA-Large 20.88 24.42 06.90 34.47 24.42 26.76 0.3835 0.4482\nBGE-Large 22.58 27.10 07.12 42.30 27.10 29.58 0.4160 0.4849NQAJINA-Large 49.20 62.44 30.31 88.49 62.44 66.98 0.8843 1.0120\nBGE-Large 49.90 63.82 25.68 93.24 63.82 69.12 0.8799 1.0300\nTable 3: Upstream RAG Retrieval Performance using Basic Retriever of BGE-Large. The con-\nventional retrieval evaluation, denoted as ConR, is performed. The symbol\u25e6signifies metric values\nranging from 0 to positive infinity, while all other values are within the [0,1]interval (%).\nDataset & MethodsConventional Retrieval Evaluation (ConR Evaluator)\nF1 MRR Hit@1 Hit@10 MAP NDCG DCG\u25e6IDCG\u25e6HQASBPT 38.60 53.86 16.96 96.43 44.99 59.61 0.7174 0.8992\nHyDE 53.07 67.39 17.86 97.32 58.32 72.44 0.9947 1.1753\nRRFusion 59.99 76.92 12.24 96.94 66.25 79.72 1.0401 1.1192\nStParser 42.92 59.54 14.02 97.20 49.82 65.17 0.8069 0.9921\nBGE-RRK 66.90 80.19 77.03 97.30 70.62 83.14 1.1354 1.2183\nJina-RRK 64.68 76.89 76.92 96.43 68.83 79.60 1.1096 1.1733DQASBPT 22.57 27.03 06.01 42.08 27.03 29.45 0.4149 0.4823\nHyDE 22.74 26.40 06.81 41.48 26.40 28.89 0.4213 0.4906\nRRFusion 26.36 29.31 14.29 19.05 29.31 30.18 0.3885 0.4135\nStParser 27.70 30.34 05.13 46.15 30.34 31.28 0.4092 0.4355\nBGE-RRK 31.72 34.27 28.46 38.88 34.27 35.44 0.4663 0.4990\nJina-RRK 34.20 37.66 31.17 44.16 37.66 38.68 0.5097 0.5385NQASBPT 49.22 62.27 22.86 88.57 62.27 67.53 0.8737 1.0210\nHyDE 50.12 62.94 41.03 94.87 62.94 68.69 0.8864 1.0483\nRRFusion 62.39 70.35 42.86 95.24 70.35 73.02 0.8949 0.9700\nStParser 59.79 68.42 28.95 94.74 68.42 71.29 0.8430 0.9239\nBGE-RRK 59.14 68.26 50.91 76.36 68.26 71.20 0.8284 0.9113\nJina-RRK 58.97 66.67 56.41 79.49 66.67 68.68 0.8648 0.9215\nTable 4: Upstream RAG Retrieval Performance about advanced Pre-retrieval ,Advanced Retrieval\nandPost-processor with prompt engineering modules. Results underlined indicate superiority over\nBasic Retriever of BGE-Large, while bold results denote the best performance.\n3", "Experimental Results and Discussion": "3.1", "Experimental Setup": "For document preprocessing, we used SentenceSplitter to segment documents into chunks and build\na vector index. It was set to a chunk size of 128 tokens and a chunk overlap of 20 tokens. We adhered\nto LlamaIndex configurations for other RAG components, including the refine module for response\nsynthesis. To ensure compatibility and efficiency, XRAG integrates Huggingface Transformers.\nAll Q&A LLMs were set with Temperature = 0 to ensure experimental consistency6. For each\nquery, five chunks were retrieved as contextual data. Consequently, the evaluation metrics measure\nretrieval accuracy based on 3 retrieval nodes, fully encompassing the assumptions of most datasets\nthat typically consider only one or two golden contextual nodes. The generator model\u2019s context\nwindow\u2014encompassing the query, prompt, retrieved context, and response content\u2014is configured\nto 4096 tokens. This study conducts RAG testing across the entire test set using two rule-based\nevaluations: Conventional Retrieval Evaluation (ConR) and Conventional Generation Evaluation\n(ConG). Notably, Cognitive LLM Evaluation (CogL) metrics are omitted from the main experimental\nframework, appearing only in pilot studies due to budget constraints from high computational costs\nof token processing during LLM reasoning and testing. Analyses of RAG pilot studies using CogL\nare in Appendix A.4. Computational resources are detailed in Appendix A.2.\n3.2", "Retrieval and Generation Evaluation": "Retrieval Quality The retrieval performance varies sensibly across the three datasets, with the poorest\nquality observed on DropQA in Table 3. Since DropQA requires advanced paragraph understanding\n6Nonetheless, using large models for generation or evaluation always results in minor variations, possibly\ndue to randomness in MoE training or prompt interpretation biases Kaddour et al. [2023].\n6Datasets & Methods Conventional Generation Evaluation (ConG Evaluator)\nOracle: Golden-Context ChrF ChrF++ METEOR R1 R2 RL PPL\u25e6\n\u2193CER\u25e6\n\u2193WER\u25e6\n\u2193HQAGPT-4o mini 21.75\u00b1.024 20.01\u00b1.012 21.62\u00b1.008 12.93\u00b1.007 05.32\u00b1.003 12.71\u00b1.005 111.98\u00b11.368 11.08\u00b1.019 09.75\u00b1.002\nllama3.1-8B 21.08\u00b1.018 19.30\u00b1.014 16.71\u00b1.012 15.98\u00b1.010 06.40\u00b1.006 15.89\u00b1.008 150.34\u00b11.600 14.92\u00b1.017 12.15\u00b1.009\nDeepSeek R1-7B 11.66\u00b1.020 10.37\u00b1.015 09.53\u00b1.013 05.10\u00b1.011 01.75\u00b1.007 05.01\u00b1.009 69.22\u00b11.450 23.39\u00b1.021 18.12\u00b1.008DQAGPT-4o mini 08.18\u00b1.018 07.47\u00b1.017 06.87\u00b1.012 04.12\u00b1.010 01.80\u00b1.006 04.12\u00b1.008 91.97\u00b11.400 44.35\u00b1.017 14.50\u00b1.009\nLlama3.1-8B 04.93\u00b1.020 04.29\u00b1.015 04.53\u00b1.009 02.17\u00b1.011 00.64\u00b1.007 02.16\u00b1.009 153.56\u00b11.712 76.50\u00b1.021 23.63\u00b1.011\nDeepSeek R1-7B 04.89\u00b1.019 04.33\u00b1.016 04.26\u00b1.012 01.87\u00b1.010 00.72\u00b1.006 01.86\u00b1.008 49.51\u00b11.350 90.94\u00b1.020 28.58\u00b1.007NQAGPT-4o mini 25.34\u00b1.022 23.98\u00b1.013 25.89\u00b1.009 16.20\u00b1.008 06.95\u00b1.005 15.52\u00b1.007 48.50\u00b11.239 09.41\u00b1.020 08.83\u00b1.006\nLlama3.1-8B 22.34\u00b1.021 20.78\u00b1.016 18.00\u00b1.014 17.47\u00b1.012 07.68\u00b1.010 16.92\u00b1.009 124.79\u00b11.807 14.19\u00b1.018 11.10\u00b1.013\nDeepSeek R1-7B 11.13\u00b1.020 09.82\u00b1.015 08.10\u00b1.013 04.90\u00b1.011 01.40\u00b1.007 04.53\u00b1.009 40.97\u00b11.331 20.71\u00b1.021 16.93\u00b1.008\nTable 5: Downstream RAG Generation Performance of various LLMs, utilizing Oracle as the\nretrieval results. The conventional generation evaluation (ConG) is conducted. The top scores are\nemphasized in bold. The symbol\u25e6signifies metric values ranging from 0 to positive infinity, while\nall other values are within the [0,1]interval (%). The symbol \u2193denotes that lower metric values are\npreferable. The error value is the average of three trials, with a constant LLM Temperature of 0.\nDatasets & Methods Conventional Generation Evaluation (ConG Evaluator)\nRetriever: JINA-Large ChrF ChrF++ METEOR R1 R2 RL PPL\u25e6\n\u2193CER\u25e6\n\u2193WER\u25e6\n\u2193HQAGPT-4o mini 26.65\u00b10.030 25.20\u00b10.025 29.81\u00b10.020 16.87\u00b10.015 08.82\u00b10.010 16.72\u00b10.005 95.72\u00b11.245 14.51\u00b10.025 11.32\u00b10.010\nLlama3.1-8B 37.33\u00b10.010 35.05\u00b10.018 32.51\u00b10.022 34.80\u00b10.013 18.77\u00b10.012 34.58\u00b10.007 173.88\u00b11.209 09.50\u00b10.018 07.83\u00b10.012\nDeepSeek R1-7B 19.28\u00b10.022 18.18\u00b10.018 22.62\u00b10.012 10.90\u00b10.008 04.97\u00b10.004 11.04\u00b10.009 77.40\u00b11.235 20.45\u00b10.020 15.70\u00b10.015DQAGPT-4o mini 08.80\u00b10.023 08.36\u00b10.018 09.66\u00b10.012 04.80\u00b10.008 02.00\u00b10.001 04.80\u00b10.006 55.93\u00b11.280 75.33\u00b10.332 26.38\u00b10.027\nLlama3.1-8B 10.98\u00b10.028 10.32\u00b10.022 10.65\u00b10.017 07.66\u00b10.012 02.79\u00b10.006 07.66\u00b10.009 164.35\u00b11.152 73.46\u00b10.309 27.64\u00b10.021\nDeepSeek R1-7B 07.17\u00b10.029 06.92\u00b10.018 08.44\u00b10.013 03.78\u00b10.007 01.16\u00b10.005 03.77\u00b10.008 60.70\u00b11.236 69.78\u00b10.284 23.20\u00b10.025NQAGPT-4o mini 29.37\u00b10.024 28.29\u00b10.022 32.61\u00b10.018 19.49\u00b10.014 11.12\u00b10.009 19.90\u00b10.007 53.16\u00b11.145 10.98\u00b10.025 09.56\u00b10.015\nLlama3.1-8B 37.43\u00b10.020 35.65\u00b10.028 33.30\u00b10.022 35.24\u00b10.017 21.57\u00b10.014 34.74\u00b10.009 170.36\u00b11.851 10.82\u00b10.018 08.82\u00b10.014\nDeepSeek R1-7B 21.16\u00b10.015 20.10\u00b10.020 22.50\u00b10.016 12.93\u00b10.009 06.29\u00b10.004 12.43\u00b10.008 46.83\u00b11.136 17.16\u00b10.025 14.48\u00b10.015\nTable 6: Downstream RAG Generation Performance of various LLMs, utilizing JINA-Large as\nthe retrieval model. Results underlined indicate superiority over RAG generation based on Oracle\ncontext, while bold results denote the best performance.\nand discrete reasoning Ran et al. [2019], it presents a greater retrieval challenge. The NDCG metric\nreflects that the basic retrieval model performs reasonably well in relevance and ranking accuracy on\nHotpotQA and NaturalQA datasets. Ignoring ranking accuracy, a high Hit@10 score (>0.8) indicates\na substantial likelihood of retrieving semantically relevant document blocks. The Hit@1 metric\nmeasures the ability to return the correct answer as the top-ranked result, making it suitable for tasks\nwith a single retrieval target (e.g., the golden single passage in the NaturalQA dataset). However,\nit is less effective for scenarios requiring multiple equally prioritized context targets, as seen in the\nHotpotQA dataset. Overall, the retrieval system performs well on HotpotQA and NaturalQA but\nfaces challenges on DropQA, possibly due to the complexity of the answer (Low Hit@1 scores reflect\ndifficulty in retrieving the most relevant results.). Compared to the basic retriever in Table 3 and the\nadvanced retriever in Table 4, the highlighted results indicate that incorporating additional matching\ninformation can enhance retrieval effectiveness. For example, RRFusion combines dense vectors\nwith keyword retrieval, and the sentence window includes the context of chunk nodes in the retrieval\nobjects. Additionally, it is observed that reranking methods can markedly improve performance,\nparticularly for challenging retrieval tasks such as DQA. For instance, both Both BGE-RRK and\nJina-RRK outperform the basic retriever.\nGeneration Quality In examining the question-and-answer (Q&A) component, it is essential to\ncollectively consider Tables 5, 6, and 7. Table 5 provides the test results when the human-annotated\nretrieval output is directly entered into the large model for Q&A purposes. Table 6 illustrates the\noutcomes when the retriever employs the JINA-Large model to identify relevant sentences, which are\nsubsequently fed into the large model. Similarly, Table 7 demonstrates the Q&A performance when\nthe retriever uses the BGE-Large model to acquire pertinent sentences for the large model.\nOur analysis reveals that the experimental outcomes in Tables 6, and 7 generally surpass those in\nTable 5. This finding suggests that directly supplying the large model with golden context does\nnot yield optimal results for these datasets. As depicted in Figure 2, the golden context typically\ncomprises a single text passage (DQA and NQA) or two passages (HQA), which may provide\ninadequate information for the large model. Additionally, the model may encounter difficulties\nin analyzing results from such concise texts, particularly for questions necessitating numerical\n7Datasets & Methods Conventional Generation Evaluation (ConG Evaluator)\nRetriever: BGE-Large ChrF ChrF++ METEOR R1 R2 RL PPL\u25e6\n\u2193CER\u25e6\n\u2193WER\u25e6\n\u2193HQAGPT-4o mini 28.23\u00b10.030 26.85\u00b10.025 33.07\u00b10.020 17.96\u00b10.015 09.51\u00b10.010 17.86\u00b10.005 90.51\u00b11.145 13.60\u00b10.025 11.02\u00b10.010\nLlama3.1-8B 40.63\u00b10.010 38.34\u00b10.018 36.24\u00b10.022 36.33\u00b10.013 21.31\u00b10.012 36.33\u00b10.007 181.71\u00b11.155 10.40\u00b10.007 08.10\u00b10.012\nDeepSeek R1-7B 20.02\u00b10.022 18.76\u00b10.018 23.71\u00b10.012 11.40\u00b10.008 05.19\u00b10.004 11.33\u00b10.009 81.34\u00b11.235 21.46\u00b10.025 16.47\u00b10.015DQAGPT-4o mini 08.89\u00b10.025 08.48\u00b10.018 10.38\u00b10.012 05.04\u00b10.008 02.11\u00b10.004 05.03\u00b10.006 48.22\u00b11.228 76.53\u00b10.035 27.95\u00b10.025\nLlama3.1-8B 10.85\u00b10.028 10.27\u00b10.022 11.59\u00b10.017 07.58\u00b10.012 03.08\u00b10.006 07.53\u00b10.009 148.43\u00b11.072 78.57\u00b10.019 30.62\u00b10.026\nDeepSeek R1-7B 07.49\u00b10.029 07.20\u00b10.018 08.63\u00b10.013 03.18\u00b10.007 00.93\u00b10.005 03.16\u00b10.008 56.18\u00b11.091 63.03\u00b10.028 23.02\u00b10.010NQAGPT-4o mini 29.37\u00b10.024 28.29\u00b10.022 32.61\u00b10.018 19.49\u00b10.014 11.12\u00b10.009 18.99\u00b10.007 53.16\u00b11.222 10.98\u00b10.025 09.56\u00b10.004\nLlama3.1-8B 37.28\u00b10.020 35.50\u00b10.028 32.73\u00b10.022 34.72\u00b10.017 20.88\u00b10.014 34.15\u00b10.009 158.86\u00b11.450 10.60\u00b10.018 08.84\u00b10.014\nDeepSeek R1-7B 23.51\u00b10.015 22.50\u00b10.020 26.39\u00b10.016 14.48\u00b10.009 07.64\u00b10.004 14.18\u00b10.008 50.35\u00b11.230 17.33\u00b10.025 14.15\u00b10.0009\nTable 7: Downstream RAG Generation Performance of various LLMs, utilizing BGE-Large as the\nretrieval model.\n50 100 150 200 2500.00.20.40.60.81.0F1\n(a) F1\n50 100 150 200 2500.00.20.40.60.81.0MRR (b) MRR\n50 100 150 200 2500.00.20.40.60.81.0Hit@1 (c) Hit@1\n50 100 150 200 2500.00.20.40.60.81.0Hit@10 (d) Hit@10\n50 100 150 200 2500.00.20.40.60.81.0MAP (e) MAP\n50 100 150 200 2500.00.20.40.60.81.0NDCG\n(f)NDCG\n50 100 150 200 2500.00.20.40.60.81.0Chrf (g) ChrF\n50 100 150 200 2500.00.20.40.60.81.0Chrf++ (h) ChrF++\n50 100 150 200 2500.00.20.40.60.81.0METEOR (i)METEOR\n50 100 150 200 2500.00.20.40.60.81.0R1 (j)ROUGE-1\n50 100 150 200 2500.00.20.40.60.81.0R2\n(k) ROUGE-2\n50 100 150 200 2500.00.20.40.60.81.0RL (l)ROUGE-L\n50 100 150 200 2500.20.40.60.81.0PPL (m) PPL\n50 100 150 200 2500.00.20.40.60.81.0CER (n) CER\n50 100 150 200 2500.00.20.40.60.81.0WER (o) WER\nFigure 3: Comparison of retrieval and question-answering generation effects as query length varies.\nExperiments conducted on the HQA dataset (Test).\ncomputations and logical reasoning. Our retrieval model delivers the top k (K=3) results to the LLM,\nfiltering and ranking context information to provide more comprehensive data. This observation\nimplies that for any retrieval-augmented generation (RAG) task, evaluating only the retrieval results\nmay not accurately represent the overall RAG performance; concurrent evaluation of both retrieval\nand large model Q&A generation is imperative.\nMoreover, when comparing various large models, it was observed that the DeepSeek R1-7B inference\nmodel (in its 7B distilled version) exhibited the least favorable performance. To ensure fairness, we\ndid not select the 671B parameter version of the DeepSeek model, indicating prominent potential for\nimprovement within the RAG framework for the distilled version. The most consistent performance\nwas observed with the Llama3.1-8B model, which achieved optimal Q&A results when combined\nwith either JINA-Large or BGE-Large retrieval outputs. Nonetheless, its perplexity (PPL) value\nwas the highest, with a PPL of 181.71 on the HQA dataset, for instance. It is important to note\nthat PPL and Q&A metrics assess different facets of the model: PPL emphasizes text fluency and\ngenerality, whereas ChrF and METEOR focus on the precise alignment between generated text and\nexpected answers. While Llama3.1-8B may excel in producing content closely aligned with expected\nanswers in specific tasks, it may not manage semantic coherence and text fluency as effectively as\nother models in broader generation tasks.\nWhether Longer Queries Have A Greater Impact? The illustration (as shown in Figure 3) indicates\na positive correlation between query length and the performance of several metrics, notably Hit@1 and\nMETEOR, which demonstrate an ascending trajectory with increased query length. This correlation\nimplies that the utilization of longer queries may augment the efficacy of retrieval and question-\nanswering systems. Such enhancements are likely due to the enriched contextual information that\nlonger queries provide, facilitating a more precise interpretation of user queries.\nWhether More Context Has A Greater Impact? The illustration depicted in Figure 4 demonstrates\nthat the performance of question-answering systems remains relatively stable as the number of\n80.0 0.2 0.4 0.6 0.8 1.0\nChrf++12345678910Retrieved T op-K Context\n0.32\n0.43(a) ChrF++\n0.0 0.2 0.4 0.6 0.8 1.0\nMETEOR12345678910Retrieved T op-K Context\n0.31\n0.42 (b) METEOR\n0.0 0.2 0.4 0.6 0.8 1.0\nROUGE-112345678910Retrieved T op-K Context\n0.31\n0.41 (c) ROUGE-1\n0.0 0.2 0.4 0.6 0.8 1.0\nROUGE-212345678910Retrieved T op-K Context\n0.18\n0.29 (d) ROUGE-2\n0.0 0.2 0.4 0.6 0.8 1.0\nROUGE-L12345678910Retrieved T op-K Context\n0.30\n0.40\n(e) ROUGE-L\n0\n250 500 7501000 1250 1500\nPPL12345678910Retrieved T op-K Context\n146.77\n231.89 (f)PPL\n050100 150 200\nCER12345678910Retrieved T op-K Context\n7.10\n11.54 (g) CER\n020 40 60 80100 120\nWER12345678910Retrieved T op-K Context\n10.366.00 (h) WER\nFigure 4: Ridge plots of the impact of varying numbers of retrieved contexts on Q&A performance in\nthe HQA dataset (Test). The chart shows maximum and minimum values.\nretrieval contexts increases. This observation suggests that augmenting the quantity of retrieval\ncontexts may not significantly enhance model performance. Upon analyzing the mean value ranges, it\nis evident that for instance, in the ROUGE-1 metric, the mean value only moderately increases from\n0.31 to 0.41 with the augmentation of retrieval contexts. This modest variation implies that the impact\nof increasing the number of contexts on performance improvement is limited. Furthermore, the\nobservation of maximum values reaching 1.0 in multiple figures indicates that the key to enhancing\nperformance may lie in improving the precision of retrieval. This involves identifying a few contexts\nmost relevant to the query, rather than merely increasing the number of retrieved contexts. The ChrF++\nand METEOR metrics exhibit relatively stable performance across varying numbers of retrieval\ncontexts. This suggests that these metrics are not particularly sensitive to variations in the number of\ncontexts. Consequently, this underscores the importance of focusing on enhancing the relevance and\nprecision of retrieval results in the optimization of the retrieval module within Retrieval-Augmented\nGeneration (RAG) systems, rather than solely increasing the quantity of retrieved contexts.\n4", "Conclusion": "This work introduces XRAG, an open-source framework for benchmarking advanced RAG systems\nthat facilitates exhaustive evaluation of foundational components across pre-retrieval, retrieval, post-\nretrieval, and generation phases. It introduces a modular RAG process, unified benchmark datasets,\ncomprehensive testing methodologies, and strategies for identifying and mitigating RAG failure\npoints. The framework supports a range of evaluation metrics, including conventional retrieval and\ngeneration assessments and cognitive LLM evaluations. We believe XRAG will empower researchers\nto construct and evaluate RAG modules, streamlining workflows efficiently.\n5", "Broader Impacts & Limitations": "XRAG offers researchers a standardized RAG framework and evaluation tools, covering around 50\nretrieval and generation metrics. It systematically analyzes the four core stages of RAG systems: pre-\nretrieval, retrieval, post-retrieval, and generation. Furthermore, XRAG supports both API-based and\nlocalized deployment methods for industrial applications. Our XRAG currently has several limitations\nwe intend to address in future updates. i. Although XRAG incorporates core RAG modules, time cost\nand compatibility constraints prevent comprehensive coverage of all RAG advancements, which may\nrequire more open-source contributions. ii. The toolkit does not support RAG component training;\nhowever, due to the variety of methods and the presence of specialized repositories, it was not included.\nWe may offer supplementary scripts for training in the future. iii. Currently, our toolkit includes\ndatasets focused on multi-hop question answering, numerical reasoning, and logical reasoning. Future\nupdates could expand to encompass OpenQA Mallen et al. [2023], Joshi et al. [2017], long-form\nQ&A Stelmakh et al. [2022], Min et al. [2023], and multiple-choice Q&A Hendrycks et al. [2021],\nHuang et al. [2022]. We encourage the open-source community to overcome these limitations. Our\ngoal is to persistently refine the XRAG framework by delivering a more efficient and dependable\nplatform with comprehensive evaluation and development tools.\n9", "Appendix / supplemental material": "A.1", "Data Availability": "The dataset is CC BY-NC-SA 4.0 licensed (we maintained the same license agreement as the orig-\ninal dataset.), accessible via the URL https://huggingface.co/datasets/DocAILab/XRAG_\nDataset . We also provide a documentation for the XRAG: https://docailab.github.io/\nXRAG/ on how to use the data and code.\nHotpotQA is a dataset with Wikipedia-based question-answer pairs with four key features: Questions\nrequire reasoning across multiple supporting documents. Questions are diverse and independent of\npre-existing knowledge schemas as shown in Figure 5. It has the largest set of retrievable documents\nbut lacks logical operations in its queries. Answers are typically long.\nDropQA is a reading comprehension benchmark requiring discrete reasoning over paragraphs:\nAnswers span multiple text positions and involve discrete operations such as addition, counting, and\nsorting, requiring a deeper understanding of paragraph content than prior datasets. Answers are\ngenerally short, comprising one or more entities. The dataset contains a significant number of logic\nand numerical reasoning questions, as shown in Figure 5.\nNaturalQA consists of questions derived from an actual Google search engine, emphasizing real-\nworld, open-ended inquiries. Questions are diverse, covering a broad range of topics. Answers are\ntypically short, consisting of one or more entities.\nA.2", "Computational Resource": "Due to inherent design and size constraints, the deployment environments for each model are distinct.\nThe RAG testing on the DQA dataset was conducted using a single V100 GPU with 32GB of memory.\nThe single-threaded indexing process took approximately one hour, while the four-threaded search\nand generation process required about two hours. The retrieval corpus constructed using the bge-large\nembedding occupied 496MB of disk space, whereas the index built with the jina-large representation\nutilized 391MB of disk space.\nFor the NQA dataset, RAG testing was performed on a single V100 GPU with 32GB of memory.\nThe single-threaded indexing process lasted approximately three hours, and the four-threaded search\nand generation process took about four hours. The bge index occupied 5.4GB of disk space, and the\njina index utilized 4.2GB of disk space.\nThe HQA dataset\u2019s RAG testing utilized a single L20 GPU with 48GB of memory. The single-\nthreaded indexing process took approximately four hours, and the single-threaded search and genera-\ntion process required about eight hours. The bge index occupied 18GB of disk space, while the jina\nindex utilized 14GB of disk space.\nwhat which\nwho\ntheare inwhenwh e r eh o wwereiswastypeyearkindcity\ncountry\namericandoesdo\ncompanydidare\nprofession\nposition\nother\ngenre\nmovie\ncounty\nactor\nnumber\nnationality\nteam\nenglish\nrole\nband\nstate\nfilm\naward\noccupation\ntown\nformer\nsong\nuniversity\npoliticallanguageevent\nactressbritish\ncanadian\nshow\nfootball\nmemberarea\nsport\nplace\nbook\nmonth\ntitle\nstar\ncareer\ncharacteristichas\nnovel\nriver\nmagazine\ndate\ngerman\nperson\ntwo\ntelevision\nartist\ntime\ncharacter\nseason\ntypes\njobs\nprofessional\nauthor\nregion\nam\nericanofbandisfilmmagazinemoviedocumentarygenuswasactoruniversitycompanyenglishhascitysongstarteamgamemusiciansinger membertennis countryartistmountainauthor plantonebuilding opera french battlealbumwriter former\nactress\ngroupbritish dogdirector\ncastcasecollege professionalairlinecanadian breed\nitalianpizzatypetwo\nfootball parkpopular districtschoolindianfamousnorwegian magazine, composer role\nhost\nretirednew\nchinesefilm,\naustralian nfl\nboardcanalband,germanworld\nwas\nis\ndirected\nhas\ndid\nwrote\nwon\nlived\nfounded\ncreated\nwere\nproduced\nplayed\nhad\nstarred\nare\ndied\ndeveloped\nsang\nled\nowns\nbecame\nserved\npublished\nplays\nreleased\nactor\nsongamerican\nauthor\nactress\nperson\ndirector\nfilm\nfirst\nalbum\n2015\nband\nman\ncompany\nstar\nbirthplace\nking\ntown\ncity\nsinger\ngerman\nlanguage\n2011\n2016\nbattle\n2002\n1976\nleaderboth\nthe eitherstevejohnwhat which\nthebetweenaddition\nregardswasdidisdoesiswasdidaredoesinmanylongdidwasmuchfarlargeoldboththetheanotheraanthisone\nsignificant theaestablished\nanotheroneofwasdidofiswasindidofwasisdoesdidarewere\nactor\nplaywright thethebothdoesdidin\nproducedthethebothsome\ndoesdo\ndiddoes\nnameofdoesare\nwasdidisisstarredbornhad\nlargestiswas\nwasdiddidwasiswasdid\ndid didisisinwas\nparty was whoclubof\ndoes\ndiddid did\nwasof\ndidisis\ndidofofdid ofof\nhopofcountydoesbefellm\nem\nberstatesdidw\nasw\nho\nfootballactressrapperstan\nd-u\nptelevisionactor,actress,actor\nthethesewashasformedis\na\ninlocatedhigher,furtherfartherpublished\nwasdiddirectorstarringis\nwasishas\nwasstaringcamepremiered\nwasfilm\nof\nhascontains releasedthe\nin\nfromis\nhaswasis\nownsdeveloped\nactresswriter\nmorea\nhaswas\nwasby\nof\nfrom\ndid\nis\nhas\nwasis\nhas\nwasis\nof\nplayerin\nis\nwrote won\nis\nhas\nof\nis\nhaswas\nhas\nlasted was\nwas\nis\nstaring\nwasof\nis\nwasis\nmemberin\nteam\ntennis\nof\nprofessionalrestaurant chainof\nteam\nis\nof\nhas\nwas\nof\nyork\ncity\nteam\ngame\nwar\nhop\nwas\nlabel\nwho\nsinger\nhas\nhas\nwas\nby\nwho\nborn\nthe\na\nin\nknown\nqueen\nthis\nthe\nolder,\na\nfrom\nyounger\nyounger,\nolder\nthis\nan\nthe\na\na\nmore\nbeen\nwritten\nplayed\nthe\nwon\nthe\nthe\na\nmore\nthe\nlonger,\nthe\nthe\nthis\nthe\na\nthe\nthe\na\nin\nthe\nfirst,\nmore\nthe\nthe\nas\nthe\nthe\nthe\nthe\nin\nthe\non\nthe\nwho\nthat\nknown\n\"the\nactor\nof\nthat\nwho\nwho\nof\nfilm\nthat\nthat\nwho\nthat\nof\nof\nuefa\nfilm\nof\nof\nof\ncharacter\nof\nof\nof\nof\nwho\nof\nkiller\nmanager\nof\nof\ntheuniver\nsityuniversitybandstheyear citycountrytowncountystateseasonseriesnumberfilmcityyearcountycountrystatefilmcity,metropolitanyeartototheonetheathethewherethethethethetheboththethepeopletimesepisodesyearsmembersseasonsmilesactsacresseatsnflacademysquarecountriescopiesstudentseventscampusesofgoalsdistrictsiswasthethedidawayiswasisthebandsQuestion W ord Patterns in HQA Dataset\n Bold labels indicate counts > 100  \n(a) HotpotQA\nhow\nwhichwhowhatw e r einwheredidwaswhen many\nlong\nmany,\ndid\nmuchgroup\nteamageplayer\nhappened\nhousehold\ntwo\ngroupsyearancestr\nalgenderplayersw\nasquar\nter\nbac\nkeventkickerracialcountrydidlanguagecitywereet\nhnicwarofhadrac\neqbcountriesstatehasthreereceivercam\necitiesfapoliticalnationalitylan\nguag\nesfieldarm\nycrim\neareasshipregionareayearsw\nasscoredthrewhadcaughtkickeddiddiedwereiswonmadegotlosttookhappenedwasyearweredidisaretwoteamallageeventcausedgroupkinddatemonthyearsracestheremorewhichwhatthedidtherewaswerethemorethedidmorepercentyearsyards\npoints\nin\nfield\ntotal\ntouchdowns\npeople\ndays\nmonths\nof\npeople,\ntouchdown\ntimes\npercentage\nfewer\nless\ncountries\ngames\nhouseholds\ndifferent\nrushing\npasses\ncities\ninterceptions\npassing\nmen\nfrom\nmore,\nareas\nmillions\nwins\nincomplete\nis\nlosses\nyards,\ndegrees\nbirths\nfrench\nregistered\nage\nyear\ncombined\ntd\nteams\npercents\ndollars\ndenominations\nquarters\ngroups\nsports\nwas\nscoring\nplaces\nthings\nnames\n1-yard\ndid\nreceiving\nfrancs\nmembers\nmillion\nbuick\ncitizens\nsecond\nplayers\nraces\nchurches\nfirst\ntroops\nwars\ncatches\nscores\nindians\nstates\nhomes\nlarge\neligible\nother\ntribes\nyard\nsoldiers\nmarks\nresidents,\ntypes\ndecades\nturnovers\nnavies\nrevolutions\nus\nresidents\nhouseholds,\ntwo-point\nbrothers\njobs\n\u00a3\nvehicles\ncatches/receptions\nhamburgers\nwas\nwere\ndid\nin\nthe\nis\nfrom\nfor\nin\nhad\nmade\nhasscored\nwon\nallowed\nlost\nwas\nhad\ndid\nkicked\ngot\nbeat\nscore group\ngroups\nrange\nrangesscored\nhad\nthrew\ncaught\nkicked\nmade\nwasfirst,\nlater,\nlast,\nfirstwas\nhadracial\nplayers\ncountries\nwars\nethnicities\npeople\nteamsinare\nhaddid\nhadw\nasgroupgroupsgroupscored\nthrewthefirst,ther\nehadthrewhappenedhadscoredm\nadekickedgroupgroupshadtheforhasfromtherehadgroupsoccuredthethehadwer\nehadhadlistedcau\nghthadfirst,didcupm\nak\neswasgoalswassawwasm\ninorityofhadiswasgal\nler\nydiddiedhadthebornkilledwinninginthemorefirstonathemorethemoreatheathethelast:first:first,thethethethethemorethethethefirst:first,second:last,firstthecapturedonedidwastheallthethetheallscoredgrouphappenedtheinwasofdiddidhadofwasthemorefewerpeopleresidentspointsquarteryearageyearquartercountythemorethethepeopleclubspopulationaexpectancythetheQuestion W ord Patterns in DQA Dataset\n Bold labels indicate counts > 100  (b) DropQA\nwho\nwhen\nwhatwherehowwhichtheinwhyfirstplayedisplayswrotewonwasdidhassingsdoessangdiesholdsm\nadeownsstarredkilledscoredsaidinventedar\necreat\nedappointsgavevoicesbroketookstolecam\neuseddiedhadform\nedstartedcalculatedtellswer\newinspatrolsinstigatedgranteddeterminesdirectedsingelec\ntswillkickedsetspushedbroughtshowedchoreographeden\ndscoinedoriginallyhavefeaturesinspiredslewpresidesneed\nshostedlosthit\ndid\nwas\ndoes\nis\ndo\nwill\nwere\nitems\nis\nkindtype\nare\ndoes\nwas\ndid\ndoyear\nepisode\npercentage\ngeometric\nwere\ncontinent\ntime\nmost\nday\nchannel\nar\neaunithem\nispherehappenedgen\nreag\necountydepechesystempositionw\nouldhan\ndestablishedtw\nocountrydocum\nentpar\ntfam\nilyisdoesdidwasaredoincanwillman\nylongoldmuchdofarwasiiscountryisstateamericanyearrangeuniversitytermarrondissementlanguagecityamendmentdeltaagencyepisodetypeproteineventonenameconceptcirculationevangelicalfaultcentraltheoryfollowingbillresourcedmembershortstrainrightxorcountrymotionthewhichstarandidanalyticalispersonindianrailwaytheintomdrsamlightquarterbackmarillaamy'sdangeorgetaylorjazzcindyfrankwilltontodaenerysbarbarabetsyjugglesnickrandylead samson'salanqmiss mauricejohnnyauntthein credited running playingallowedfireonfifth known\nconsidered\nexecutedcaesar\nelizabeth\nspeakingnumberopening\ndesignednotthewalterguitar\ngeneral\nwonder grace'schrisbennyolder\ngroveramymissyblinkatagent\nharoldhaley\nrachelsami\nrajeshjj\nprince\nmichael\npride'schuyandyjack\nnorma\nlindsayfrollo\ngumball\nthesongi'dilord\nsorry\nhopein\ngreenelvisallto\nmamait's\nthemost3rd\ndefensive\nbeatin\nohio\nseason\ntheknown\neatenat\njessie\ntheaaron\nphiladelphia\neaglesel\ntaylor\nbobby\nnot\nian\nkansas\nhoustonus\njeff\nthebeen\nwonwrittenm\norescoredheldtheiyou'regonnaondon'tifit'ssom\nethingsonem\naytheblackdem\nar\ncuseloisescar'szacgeorgerandytheseusrallothethisi'mtakebassm\nakeinthethechristianitytheinm\negansamtaratheingod'sitheallthethethethethetianathepartchristm\nasuptointhem\najo\nritythetheharrytheseasonthethechar\nter\nswhatthewe'rethebethethefora\nthattheupthesangmostasthetheovera\nthehisthe\nthe\nweplasma sweden ramen partyharryarizona lollipop donald nyckobelaw\nseason sweet tanger italycallrobbie all\nspainwartom\ni\ncardikiminewspapersancient welcome vietnamnon\nfrancegst\nfull\nsouthbearsindiabritishmichael3\ndrinkingprettypuertonashvillerussiaconstruction craterhocus\nmanchester ac\nthe\nlittle\nlandscape\nloch\ncss\nain't\ni\nharry\nfree\nsuez\nbirth\nbulimia\nfirst\nadvance\ncoffee\nstar\nthe\nseason\na\nbohemian\nalex\ntorchwood\npregnancy\nbigg\nepisode\neren\nmaddie\ndestiny\nraj\nafter\nhalo\nwho\nluke\n2018\npiney\nthe\nhalley's\nhenry\nmy\nlucifer\nseason\nluis\nthe\nthey\nvincent\nwe\nmedical\nthe\ngame\nthe\nare\nthe\na\nin\nsimba's\nproduced\nan\nfinely\nrussell\nmethyl\nit\nearth's\nregular\nestella's\nspeed\ntriple\nmicrosoft\nstig\nbasicof\nofthe\nsome\nit\ncfthe\nthree\nlaissez\nserena\ncj33\nathe\nkept\npatricia\nbabe\noates\ntheyou\nthedid\nwasdoes\nof\nsurface\nthe\nisisdirectly\ndoes\nisofw\nouldistoisareism\nodedoesen\nab\nledbedoesthetypesinm\narkeddoesisthe\nm\nostgodbridgetunionem\nilygoingcinqueharlemdirkswitzerlandm\nounttokyothe\nbritishferristhe\ntheysarahm\nichaelzeusredannetwokingthe\nonefarthreewartheapachedeathrubbermosttheorbtheygreyhoundtornad\noesitalynewtheadiposetheepisodes\ntimesseasons\nroundsouranchallengescardominoesweeksjetserieslengthsoscarsflatsmoviespermanentrainyislandsrickunemployeesdiffer\nentcountriesgoalspointshoursisdoescanhasdodowasislandalcoholcouldyouisthemetthewonhadhasgavethebiggerofinventordidrunswasdescribesishasisextendsishasdoesofisinvolvedisofofofreligioninprocessingthatcountryofbasedofdistanceseasontoprocesswhichofunitedlate-nineteenthyearcountrywarselectricparistheenginetheretowomanbudgetQuestion W ord Patterns in NQA Dataset\n Bold labels indicate counts > 100  (c) NaturalQA\nFigure 5: The typology of questions encompassed by HQA (HotpotQA), DQA (DropQA), and NQA\n(NaturalQA) is delineated through a heuristic extraction process. This process initiates at interrogative\nwords or the prepositions that precede them. Blocks with low saturation denote suffixes that are too\ninfrequent to be displayed individually.\n13Table 8: Upstream & Downstream performance of the RAG framework using Cognitive LLM\nEvaluation focusing on Separate Retrieval and Generation Metrics. The second row indicates the\nmean success rate of API requests Pscfrom three 100-entry samples, with green-highlighted results\nrepresenting the average across 3-times sampling tests.\nDataset & MethodsCogL [Retrieval] CogL [Generation]\nRetriever: BGE-Large Up-CRel Up-CCns Dp-ARel Dp-RCmp Dp-RCnc Dp-RRel Dp-RVal Dp-RMchHQAGPT-3.5 Turbo0.7352\u00b10.012 0.9201\u00b10.023 0.9367\u00b10.034 0.7667\u00b10.045 0.9358\u00b10.056 0.7856\u00b10.067 0.9733\u00b10.078 0.1854\u00b10.089\n84.33\u00b11.234 89.67\u00b12.345 98.72\u00b11.456 98.72\u00b11.456 98.67\u00b11.678 99.00\u00b11.789 98.33\u00b11.890 98.67\u00b11.678DQAGPT-3.5 Turbo0.6856\u00b10.121 0.3604\u00b10.142 0.9900\u00b10.263 0.8417\u00b10.084 0.9609\u00b10.105 0.8441\u00b10.226 0.9867\u00b10.147 0.2060\u00b10.068\n97.00\u00b11.176 94.33\u00b11.115 99.00\u00b11.303 93.31\u00b12.004 98.00\u00b11.223 98.33\u00b11.606 98.33\u00b11.200 89.33\u00b11.001NQAGPT-3.5 Turbo0.8878\u00b10.031 0.4133\u00b10.052 0.9100\u00b10.073 0.8061\u00b10.094 0.8827\u00b10.015 0.8214\u00b10.036 0.9763\u00b10.057 0.4574\u00b10.078\n98.67\u00b11.678 94.33\u00b11.115 98.56\u00b11.121 98.33\u00b11.200 98.00\u00b11.505 98.33\u00b11.200 98.33\u00b11.200 94.33\u00b11.115\nTable 9: Upstream & Downstream performance of the RAG framework using Cognitive LLM\nEvaluation focusing on Combined Retrieval and Generation Metrics.\nDatasets & MethodsCogL [Combined Retrieval & Generation]\nRetriever: BGE-Large Dp-CPre Dp-CRec Dp-CRel Up-RCns Up-CUti Up-FAcc Dp-Fath Dp-Hall\n0.9457\u00b10.022 0.6455\u00b10.034 0.6812\u00b10.046 0.8318\u00b10.058 0.6870\u00b10.070 0.7516\u00b10.082 0.9197\u00b10.094 0.5537\u00b10.106HQAGPT-3.5 Turbo\n92.00\u00b11.345 99.67\u00b11.678 99.33\u00b11.456 99.67\u00b11.678 92.67\u00b11.789 92.00\u00b11.901 99.67\u00b11.678 99.33\u00b11.456\n0.8691\u00b10.112 0.6254\u00b10.134 0.8333\u00b10.156 0.7450\u00b10.178 0.6702\u00b10.190 0.5326\u00b10.202 0.7637\u00b10.214 0.7333\u00b10.226DQAGPT-3.5 Turbo\n99.33\u00b11.456 97.00\u00b13.567 99.67\u00b11.678 99.67\u00b11.678 95.67\u00b11.901 98.33\u00b12.012 97.00\u00b12.123 95.27\u00b12.012\n0.9742\u00b10.033 0.8769\u00b10.045 0.9384\u00b10.057 0.8948\u00b10.069 0.6888\u00b10.081 0.8485\u00b10.093 0.9070\u00b10.105 0.3417\u00b10.117NQAGPT-3.5 Turbo\n99.33\u00b11.456 99.33\u00b11.345 99.33\u00b11.456 98.67\u00b11.567 98.67\u00b11.678 97.33\u00b11.789 86.33\u00b11.901 95.27\u00b12.012\nA.3", "Cognitive LLM Evaluation (CogL Evaluator)": ". It is classified into three categories: Retrieval-\noriented, Generation-oriented, and Combined Retrieval & Generation. Cognitive LLM Eval-\nuation metrics, derived from UpTrain and DeepEval, are classified based on the parameters\nunified by our XRAG framework. Metrics involving response-related parameters, such as\nactual-response orexpected-answer , are Generation-oriented Metrics. Conversely, metrics that\nlack response-related parameters, including retrieval-related parameters, such as retrieval-context\norretrieval-context.ids , are designated as Retrieval-oriented Metrics. The rest are Combined\nRetrieval & Generation Metrics. Moreover, we utilize GPT-4 Turbo as the LLM agent for CogL\nevaluator. Since LLM evaluations are billed per token, we conducted experimental discussions for\nour foundational modules of RAG in Appendix A.3.\nXRAG evaluators have obvious advantages:\n\u2022Evaluating with Multiple RAG Metrics in One Go : The XRAG evaluator allows users to\nsimultaneously assess various RAG-specific metrics. This capability streamlines the evaluation\nprocess, enabling comprehensive performance analysis without sequential evaluations.\n\u2022Standardizes the Structure of Evaluation Metrics : A unified data format simplifies the compari-\nson between different RAG components in both retrieval and generation.\n\u2022Character and Semantic \u00d7Retrieval and Generation : It encompasses a 4-fold cross-dimensional\nanalysis, including character-level matching tests and semantic-level understanding tests for both\nretrieval and generation.\n4https://github.com/uptrain-ai/uptrain\n5https://github.com/confident-ai/deepeval\n5Dataset & MethodsConventional Retrieval Evaluation (ConR Evaluator)\nF1 MRR Hit@1 Hit@10 MAP NDCG DCG\u25e6IDCG\u25e6HQAJINA-Large 49.80 64.23 21.42 94.44 55.17 70.44 0.9823 1.1795\nBGE-Large 60.66 72.84 18.29 97.71 64.13 78.64 1.1644 1.3656DQAJINA-Large 20.88 24.42 06.90 34.47 24.42 26.76 0.3835 0.4482\nBGE-Large 22.58 27.10 07.12 42.30 27.10 29.58 0.4160 0.4849NQAJINA-Large 49.20 62.44 30.31 88.49 62.44 66.98 0.8843 1.0120\nBGE-Large 49.90 63.82 25.68 93.24 63.82 69.12 0.8799 1.0300\nTable 3: Upstream RAG Retrieval Performance using Basic Retriever of BGE-Large. The con-\nventional retrieval evaluation, denoted as ConR, is performed. The symbol\u25e6signifies metric values\nranging from 0 to positive infinity, while all other values are within the [0,1]interval (%).\nDataset & MethodsConventional Retrieval Evaluation (ConR Evaluator)\nF1 MRR Hit@1 Hit@10 MAP NDCG DCG\u25e6IDCG\u25e6HQASBPT 38.60 53.86 16.96 96.43 44.99 59.61 0.7174 0.8992\nHyDE 53.07 67.39 17.86 97.32 58.32 72.44 0.9947 1.1753\nRRFusion 59.99 76.92 12.24 96.94 66.25 79.72 1.0401 1.1192\nStParser 42.92 59.54 14.02 97.20 49.82 65.17 0.8069 0.9921\nBGE-RRK 66.90 80.19 77.03 97.30 70.62 83.14 1.1354 1.2183\nJina-RRK 64.68 76.89 76.92 96.43 68.83 79.60 1.1096 1.1733DQASBPT 22.57 27.03 06.01 42.08 27.03 29.45 0.4149 0.4823\nHyDE 22.74 26.40 06.81 41.48 26.40 28.89 0.4213 0.4906\nRRFusion 26.36 29.31 14.29 19.05 29.31 30.18 0.3885 0.4135\nStParser 27.70 30.34 05.13 46.15 30.34 31.28 0.4092 0.4355\nBGE-RRK 31.72 34.27 28.46 38.88 34.27 35.44 0.4663 0.4990\nJina-RRK 34.20 37.66 31.17 44.16 37.66 38.68 0.5097 0.5385NQASBPT 49.22 62.27 22.86 88.57 62.27 67.53 0.8737 1.0210\nHyDE 50.12 62.94 41.03 94.87 62.94 68.69 0.8864 1.0483\nRRFusion 62.39 70.35 42.86 95.24 70.35 73.02 0.8949 0.9700\nStParser 59.79 68.42 28.95 94.74 68.42 71.29 0.8430 0.9239\nBGE-RRK 59.14 68.26 50.91 76.36 68.26 71.20 0.8284 0.9113\nJina-RRK 58.97 66.67 56.41 79.49 66.67 68.68 0.8648 0.9215\nTable 4: Upstream RAG Retrieval Performance about advanced Pre-retrieval ,Advanced Retrieval\nandPost-processor with prompt engineering modules. Results underlined indicate superiority over\nBasic Retriever of BGE-Large, while bold results denote the best performance.\n3 Experimental Results and Discussion\n3.1 Experimental Setup\nFor document preprocessing, we used SentenceSplitter to segment documents into chunks and build\na vector index. It was set to a chunk size of 128 tokens and a chunk overlap of 20 tokens. We adhered\nto LlamaIndex configurations for other RAG components, including the refine module for response\nsynthesis. To ensure compatibility and efficiency, XRAG integrates Huggingface Transformers.\nAll Q&A LLMs were set with Temperature = 0 to ensure experimental consistency6. For each\nquery, five chunks were retrieved as contextual data. Consequently, the evaluation metrics measure\nretrieval accuracy based on 3 retrieval nodes, fully encompassing the assumptions of most datasets\nthat typically consider only one or two golden contextual nodes. The generator model\u2019s context\nwindow\u2014encompassing the query, prompt, retrieved context, and response content\u2014is configured\nto 4096 tokens. This study conducts RAG testing across the entire test set using two rule-based\nevaluations: Conventional Retrieval Evaluation (ConR) and Conventional Generation Evaluation\n(ConG). Notably, Cognitive LLM Evaluation (CogL) metrics are omitted from the main experimental\nframework, appearing only in pilot studies due to budget constraints from high computational costs\nof token processing during LLM reasoning and testing. Analyses of RAG pilot studies using CogL\nare in Appendix A.4. Computational resources are detailed in Appendix A.2.\n3.2 Retrieval and Generation Evaluation\nRetrieval Quality The retrieval performance varies sensibly across the three datasets, with the poorest\nquality observed on DropQA in Table 3. Since DropQA requires advanced paragraph understanding\n6Nonetheless, using large models for generation or evaluation always results in minor variations, possibly\ndue to randomness in MoE training or prompt interpretation biases Kaddour et al. [2023].\n6Datasets & Methods Conventional Generation Evaluation (ConG Evaluator)\nOracle: Golden-Context ChrF ChrF++ METEOR R1 R2 RL PPL\u25e6\n\u2193CER\u25e6\n\u2193WER\u25e6\n\u2193HQAGPT-4o mini 21.75\u00b1.024 20.01\u00b1.012 21.62\u00b1.008 12.93\u00b1.007 05.32\u00b1.003 12.71\u00b1.005 111.98\u00b11.368 11.08\u00b1.019 09.75\u00b1.002\nllama3.1-8B 21.08\u00b1.018 19.30\u00b1.014 16.71\u00b1.012 15.98\u00b1.010 06.40\u00b1.006 15.89\u00b1.008 150.34\u00b11.600 14.92\u00b1.017 12.15\u00b1.009\nDeepSeek R1-7B 11.66\u00b1.020 10.37\u00b1.015 09.53\u00b1.013 05.10\u00b1.011 01.75\u00b1.007 05.01\u00b1.009 69.22\u00b11.450 23.39\u00b1.021 18.12\u00b1.008DQAGPT-4o mini 08.18\u00b1.018 07.47\u00b1.017 06.87\u00b1.012 04.12\u00b1.010 01.80\u00b1.006 04.12\u00b1.008 91.97\u00b11.400 44.35\u00b1.017 14.50\u00b1.009\nLlama3.1-8B 04.93\u00b1.020 04.29\u00b1.015 04.53\u00b1.009 02.17\u00b1.011 00.64\u00b1.007 02.16\u00b1.009 153.56\u00b11.712 76.50\u00b1.021 23.63\u00b1.011\nDeepSeek R1-7B 04.89\u00b1.019 04.33\u00b1.016 04.26\u00b1.012 01.87\u00b1.010 00.72\u00b1.006 01.86\u00b1.008 49.51\u00b11.350 90.94\u00b1.020 28.58\u00b1.007NQAGPT-4o mini 25.34\u00b1.022 23.98\u00b1.013 25.89\u00b1.009 16.20\u00b1.008 06.95\u00b1.005 15.52\u00b1.007 48.50\u00b11.239 09.41\u00b1.020 08.83\u00b1.006\nLlama3.1-8B 22.34\u00b1.021 20.78\u00b1.016 18.00\u00b1.014 17.47\u00b1.012 07.68\u00b1.010 16.92\u00b1.009 124.79\u00b11.807 14.19\u00b1.018 11.10\u00b1.013\nDeepSeek R1-7B 11.13\u00b1.020 09.82\u00b1.015 08.10\u00b1.013 04.90\u00b1.011 01.40\u00b1.007 04.53\u00b1.009 40.97\u00b11.331 20.71\u00b1.021 16.93\u00b1.008\nTable 5: Downstream RAG Generation Performance of various LLMs, utilizing Oracle as the\nretrieval results. The conventional generation evaluation (ConG) is conducted. The top scores are\nemphasized in bold. The symbol\u25e6signifies metric values ranging from 0 to positive infinity, while\nall other values are within the [0,1]interval (%). The symbol \u2193denotes that lower metric values are\npreferable. The error value is the average of three trials, with a constant LLM Temperature of 0.\nDatasets & Methods Conventional Generation Evaluation (ConG Evaluator)\nRetriever: JINA-Large ChrF ChrF++ METEOR R1 R2 RL PPL\u25e6\n\u2193CER\u25e6\n\u2193WER\u25e6\n\u2193HQAGPT-4o mini 26.65\u00b10.030 25.20\u00b10.025 29.81\u00b10.020 16.87\u00b10.015 08.82\u00b10.010 16.72\u00b10.005 95.72\u00b11.245 14.51\u00b10.025 11.32\u00b10.010\nLlama3.1-8B 37.33\u00b10.010 35.05\u00b10.018 32.51\u00b10.022 34.80\u00b10.013 18.77\u00b10.012 34.58\u00b10.007 173.88\u00b11.209 09.50\u00b10.018 07.83\u00b10.012\nDeepSeek R1-7B 19.28\u00b10.022 18.18\u00b10.018 22.62\u00b10.012 10.90\u00b10.008 04.97\u00b10.004 11.04\u00b10.009 77.40\u00b11.235 20.45\u00b10.020 15.70\u00b10.015DQAGPT-4o mini 08.80\u00b10.023 08.36\u00b10.018 09.66\u00b10.012 04.80\u00b10.008 02.00\u00b10.001 04.80\u00b10.006 55.93\u00b11.280 75.33\u00b10.332 26.38\u00b10.027\nLlama3.1-8B 10.98\u00b10.028 10.32\u00b10.022 10.65\u00b10.017 07.66\u00b10.012 02.79\u00b10.006 07.66\u00b10.009 164.35\u00b11.152 73.46\u00b10.309 27.64\u00b10.021\nDeepSeek R1-7B 07.17\u00b10.029 06.92\u00b10.018 08.44\u00b10.013 03.78\u00b10.007 01.16\u00b10.005 03.77\u00b10.008 60.70\u00b11.236 69.78\u00b10.284 23.20\u00b10.025NQAGPT-4o mini 29.37\u00b10.024 28.29\u00b10.022 32.61\u00b10.018 19.49\u00b10.014 11.12\u00b10.009 19.90\u00b10.007 53.16\u00b11.145 10.98\u00b10.025 09.56\u00b10.015\nLlama3.1-8B 37.43\u00b10.020 35.65\u00b10.028 33.30\u00b10.022 35.24\u00b10.017 21.57\u00b10.014 34.74\u00b10.009 170.36\u00b11.851 10.82\u00b10.018 08.82\u00b10.014\nDeepSeek R1-7B 21.16\u00b10.015 20.10\u00b10.020 22.50\u00b10.016 12.93\u00b10.009 06.29\u00b10.004 12.43\u00b10.008 46.83\u00b11.136 17.16\u00b10.025 14.48\u00b10.015\nTable 6: Downstream RAG Generation Performance of various LLMs, utilizing JINA-Large as\nthe retrieval model. Results underlined indicate superiority over RAG generation based on Oracle\ncontext, while bold results denote the best performance.\nand discrete reasoning Ran et al. [2019], it presents a greater retrieval challenge. The NDCG metric\nreflects that the basic retrieval model performs reasonably well in relevance and ranking accuracy on\nHotpotQA and NaturalQA datasets. Ignoring ranking accuracy, a high Hit@10 score (>0.8) indicates\na substantial likelihood of retrieving semantically relevant document blocks. The Hit@1 metric\nmeasures the ability to return the correct answer as the top-ranked result, making it suitable for tasks\nwith a single retrieval target (e.g., the golden single passage in the NaturalQA dataset). However,\nit is less effective for scenarios requiring multiple equally prioritized context targets, as seen in the\nHotpotQA dataset. Overall, the retrieval system performs well on HotpotQA and NaturalQA but\nfaces challenges on DropQA, possibly due to the complexity of the answer (Low Hit@1 scores reflect\ndifficulty in retrieving the most relevant results.). Compared to the basic retriever in Table 3 and the\nadvanced retriever in Table 4, the highlighted results indicate that incorporating additional matching\ninformation can enhance retrieval effectiveness. For example, RRFusion combines dense vectors\nwith keyword retrieval, and the sentence window includes the context of chunk nodes in the retrieval\nobjects. Additionally, it is observed that reranking methods can markedly improve performance,\nparticularly for challenging retrieval tasks such as DQA. For instance, both Both BGE-RRK and\nJina-RRK outperform the basic retriever.\nGeneration Quality In examining the question-and-answer (Q&A) component, it is essential to\ncollectively consider Tables 5, 6, and 7. Table 5 provides the test results when the human-annotated\nretrieval output is directly entered into the large model for Q&A purposes. Table 6 illustrates the\noutcomes when the retriever employs the JINA-Large model to identify relevant sentences, which are\nsubsequently fed into the large model. Similarly, Table 7 demonstrates the Q&A performance when\nthe retriever uses the BGE-Large model to acquire pertinent sentences for the large model.\nOur analysis reveals that the experimental outcomes in Tables 6, and 7 generally surpass those in\nTable 5. This finding suggests that directly supplying the large model with golden context does\nnot yield optimal results for these datasets. As depicted in Figure 2, the golden context typically\ncomprises a single text passage (DQA and NQA) or two passages (HQA), which may provide\ninadequate information for the large model. Additionally, the model may encounter difficulties\nin analyzing results from such concise texts, particularly for questions necessitating numerical\n7Datasets & Methods Conventional Generation Evaluation (ConG Evaluator)\nRetriever: BGE-Large ChrF ChrF++ METEOR R1 R2 RL PPL\u25e6\n\u2193CER\u25e6\n\u2193WER\u25e6\n\u2193HQAGPT-4o mini 28.23\u00b10.030 26.85\u00b10.025 33.07\u00b10.020 17.96\u00b10.015 09.51\u00b10.010 17.86\u00b10.005 90.51\u00b11.145 13.60\u00b10.025 11.02\u00b10.010\nLlama3.1-8B 40.63\u00b10.010 38.34\u00b10.018 36.24\u00b10.022 36.33\u00b10.013 21.31\u00b10.012 36.33\u00b10.007 181.71\u00b11.155 10.40\u00b10.007 08.10\u00b10.012\nDeepSeek R1-7B 20.02\u00b10.022 18.76\u00b10.018 23.71\u00b10.012 11.40\u00b10.008 05.19\u00b10.004 11.33\u00b10.009 81.34\u00b11.235 21.46\u00b10.025 16.47\u00b10.015DQAGPT-4o mini 08.89\u00b10.025 08.48\u00b10.018 10.38\u00b10.012 05.04\u00b10.008 02.11\u00b10.004 05.03\u00b10.006 48.22\u00b11.228 76.53\u00b10.035 27.95\u00b10.025\nLlama3.1-8B 10.85\u00b10.028 10.27\u00b10.022 11.59\u00b10.017 07.58\u00b10.012 03.08\u00b10.006 07.53\u00b10.009 148.43\u00b11.072 78.57\u00b10.019 30.62\u00b10.026\nDeepSeek R1-7B 07.49\u00b10.029 07.20\u00b10.018 08.63\u00b10.013 03.18\u00b10.007 00.93\u00b10.005 03.16\u00b10.008 56.18\u00b11.091 63.03\u00b10.028 23.02\u00b10.010NQAGPT-4o mini 29.37\u00b10.024 28.29\u00b10.022 32.61\u00b10.018 19.49\u00b10.014 11.12\u00b10.009 18.99\u00b10.007 53.16\u00b11.222 10.98\u00b10.025 09.56\u00b10.004\nLlama3.1-8B 37.28\u00b10.020 35.50\u00b10.028 32.73\u00b10.022 34.72\u00b10.017 20.88\u00b10.014 34.15\u00b10.009 158.86\u00b11.450 10.60\u00b10.018 08.84\u00b10.014\nDeepSeek R1-7B 23.51\u00b10.015 22.50\u00b10.020 26.39\u00b10.016 14.48\u00b10.009 07.64\u00b10.004 14.18\u00b10.008 50.35\u00b11.230 17.33\u00b10.025 14.15\u00b10.0009\nTable 7: Downstream RAG Generation Performance of various LLMs, utilizing BGE-Large as the\nretrieval model.\n50 100 150 200 2500.00.20.40.60.81.0F1\n(a) F1\n50 100 150 200 2500.00.20.40.60.81.0MRR (b) MRR\n50 100 150 200 2500.00.20.40.60.81.0Hit@1 (c) Hit@1\n50 100 150 200 2500.00.20.40.60.81.0Hit@10 (d) Hit@10\n50 100 150 200 2500.00.20.40.60.81.0MAP (e) MAP\n50 100 150 200 2500.00.20.40.60.81.0NDCG\n(f)NDCG\n50 100 150 200 2500.00.20.40.60.81.0Chrf (g) ChrF\n50 100 150 200 2500.00.20.40.60.81.0Chrf++ (h) ChrF++\n50 100 150 200 2500.00.20.40.60.81.0METEOR (i)METEOR\n50 100 150 200 2500.00.20.40.60.81.0R1 (j)ROUGE-1\n50 100 150 200 2500.00.20.40.60.81.0R2\n(k) ROUGE-2\n50 100 150 200 2500.00.20.40.60.81.0RL (l)ROUGE-L\n50 100 150 200 2500.20.40.60.81.0PPL (m) PPL\n50 100 150 200 2500.00.20.40.60.81.0CER (n) CER\n50 100 150 200 2500.00.20.40.60.81.0WER (o) WER\nFigure 3: Comparison of retrieval and question-answering generation effects as query length varies.\nExperiments conducted on the HQA dataset (Test).\ncomputations and logical reasoning. Our retrieval model delivers the top k (K=3) results to the LLM,\nfiltering and ranking context information to provide more comprehensive data. This observation\nimplies that for any retrieval-augmented generation (RAG) task, evaluating only the retrieval results\nmay not accurately represent the overall RAG performance; concurrent evaluation of both retrieval\nand large model Q&A generation is imperative.\nMoreover, when comparing various large models, it was observed that the DeepSeek R1-7B inference\nmodel (in its 7B distilled version) exhibited the least favorable performance. To ensure fairness, we\ndid not select the 671B parameter version of the DeepSeek model, indicating prominent potential for\nimprovement within the RAG framework for the distilled version. The most consistent performance\nwas observed with the Llama3.1-8B model, which achieved optimal Q&A results when combined\nwith either JINA-Large or BGE-Large retrieval outputs. Nonetheless, its perplexity (PPL) value\nwas the highest, with a PPL of 181.71 on the HQA dataset, for instance. It is important to note\nthat PPL and Q&A metrics assess different facets of the model: PPL emphasizes text fluency and\ngenerality, whereas ChrF and METEOR focus on the precise alignment between generated text and\nexpected answers. While Llama3.1-8B may excel in producing content closely aligned with expected\nanswers in specific tasks, it may not manage semantic coherence and text fluency as effectively as\nother models in broader generation tasks.\nWhether Longer Queries Have A Greater Impact? The illustration (as shown in Figure 3) indicates\na positive correlation between query length and the performance of several metrics, notably Hit@1 and\nMETEOR, which demonstrate an ascending trajectory with increased query length. This correlation\nimplies that the utilization of longer queries may augment the efficacy of retrieval and question-\nanswering systems. Such enhancements are likely due to the enriched contextual information that\nlonger queries provide, facilitating a more precise interpretation of user queries.\nWhether More Context Has A Greater Impact? The illustration depicted in Figure 4 demonstrates\nthat the performance of question-answering systems remains relatively stable as the number of\n80.0 0.2 0.4 0.6 0.8 1.0\nChrf++12345678910Retrieved T op-K Context\n0.32\n0.43(a) ChrF++\n0.0 0.2 0.4 0.6 0.8 1.0\nMETEOR12345678910Retrieved T op-K Context\n0.31\n0.42 (b) METEOR\n0.0 0.2 0.4 0.6 0.8 1.0\nROUGE-112345678910Retrieved T op-K Context\n0.31\n0.41 (c) ROUGE-1\n0.0 0.2 0.4 0.6 0.8 1.0\nROUGE-212345678910Retrieved T op-K Context\n0.18\n0.29 (d) ROUGE-2\n0.0 0.2 0.4 0.6 0.8 1.0\nROUGE-L12345678910Retrieved T op-K Context\n0.30\n0.40\n(e) ROUGE-L\n0\n250 500 7501000 1250 1500\nPPL12345678910Retrieved T op-K Context\n146.77\n231.89 (f)PPL\n050100 150 200\nCER12345678910Retrieved T op-K Context\n7.10\n11.54 (g) CER\n020 40 60 80100 120\nWER12345678910Retrieved T op-K Context\n10.366.00 (h) WER\nFigure 4: Ridge plots of the impact of varying numbers of retrieved contexts on Q&A performance in\nthe HQA dataset (Test). The chart shows maximum and minimum values.\nretrieval contexts increases. This observation suggests that augmenting the quantity of retrieval\ncontexts may not significantly enhance model performance. Upon analyzing the mean value ranges, it\nis evident that for instance, in the ROUGE-1 metric, the mean value only moderately increases from\n0.31 to 0.41 with the augmentation of retrieval contexts. This modest variation implies that the impact\nof increasing the number of contexts on performance improvement is limited. Furthermore, the\nobservation of maximum values reaching 1.0 in multiple figures indicates that the key to enhancing\nperformance may lie in improving the precision of retrieval. This involves identifying a few contexts\nmost relevant to the query, rather than merely increasing the number of retrieved contexts. The ChrF++\nand METEOR metrics exhibit relatively stable performance across varying numbers of retrieval\ncontexts. This suggests that these metrics are not particularly sensitive to variations in the number of\ncontexts. Consequently, this underscores the importance of focusing on enhancing the relevance and\nprecision of retrieval results in the optimization of the retrieval module within Retrieval-Augmented\nGeneration (RAG) systems, rather than solely increasing the quantity of retrieved contexts.\n4 Conclusion\nThis work introduces XRAG, an open-source framework for benchmarking advanced RAG systems\nthat facilitates exhaustive evaluation of foundational components across pre-retrieval, retrieval, post-\nretrieval, and generation phases. It introduces a modular RAG process, unified benchmark datasets,\ncomprehensive testing methodologies, and strategies for identifying and mitigating RAG failure\npoints. The framework supports a range of evaluation metrics, including conventional retrieval and\ngeneration assessments and cognitive LLM evaluations. We believe XRAG will empower researchers\nto construct and evaluate RAG modules, streamlining workflows efficiently.\n5 Broader Impacts & Limitations\nXRAG offers researchers a standardized RAG framework and evaluation tools, covering around 50\nretrieval and generation metrics. It systematically analyzes the four core stages of RAG systems: pre-\nretrieval, retrieval, post-retrieval, and generation. Furthermore, XRAG supports both API-based and\nlocalized deployment methods for industrial applications. Our XRAG currently has several limitations\nwe intend to address in future updates. i. Although XRAG incorporates core RAG modules, time cost\nand compatibility constraints prevent comprehensive coverage of all RAG advancements, which may\nrequire more open-source contributions. ii. The toolkit does not support RAG component training;\nhowever, due to the variety of methods and the presence of specialized repositories, it was not included.\nWe may offer supplementary scripts for training in the future. iii. Currently, our toolkit includes\ndatasets focused on multi-hop question answering, numerical reasoning, and logical reasoning. Future\nupdates could expand to encompass OpenQA Mallen et al. [2023], Joshi et al. [2017], long-form\nQ&A Stelmakh et al. [2022], Min et al. [2023], and multiple-choice Q&A Hendrycks et al. [2021],\nHuang et al. [2022]. We encourage the open-source community to overcome these limitations. Our\ngoal is to persistently refine the XRAG framework by delivering a more efficient and dependable\nplatform with comprehensive evaluation and development tools.\n9", "RAG Analysis with CogL Evaluation": "In Tables 8 and 9, the values for Dp-ARel across the three datasets are notably high, at 0.9367, 0.9900,\nand 0.9100, respectively. These high values indicate that the retrieved contexts are semantically\nhighly relevant to the queries, thereby providing a solid foundation for subsequent answer generation.\nWhile various LLM evaluation metrics reveal that both retrieval and generation performance are\ngenerally strong, the values for Dp-RMch across the three datasets are relatively low. This suggests\nthat the text generated by LLMs does not fully align with the golden (ideal) responses. This\ndiscrepancy highlights the difference between \u2018soft\u2019 LLM metrics and \u2018strict\u2019 rule-based metrics.\nFor the challenging DQA dataset, the values for Dp-RCnc and Dp-RVal are 0.9609 and 0.9867,\nrespectively. These values indicate that the model\u2019s generated answers for the DQA dataset exhibit\ngood conciseness, which is heuristically attributed to the predominance of numerical calculations\nin this type of question. Additionally, these values suggest that the model\u2019s generated answers are\nhighly effective, demonstrating that the model possesses sufficient knowledge to address the queries.\nInstances of ineffective responses, such as \"I\u2019m sorry, I do not have information on this,\" are rare.\nA.5", "Systematic Diagnostics of RAG Failures": "Table 10: Experimental setup for RAG failure analysis. \u2663denotes the pre-retrieval method, \u25bc\nrepresents the post-processor method, and \u2660represents the advanced retriever.\nRAG Failures Optimization Strategies Dataset Settings Evaluation Metrics\nNegative Refusal\u25bcPrompt Engineering\n\u25bcTwo-step ReasoningRandomly sampled queries\nwith unrelated contextRejection Rate\nRanking Confusion\u25bcRe-ranking\n\u2660Hybrid Retrieval\n\u2660\u25bcHybrid Retrieval & Re-rankingSamples with lower F1 scoresF1, Hit@1, EM, MRR\nMAP, DCG, IDCG, NDCG\nAnswer Absence\u25bcSimple Summarize\n\u25bcRefine\n\u25bcCompact\n\u25bcCompact AccumulateSamples with missing answers Factual Accuracy (Up-FAcc)\nResponse Consistency (Up-RCns)\nContext Utilization (Up-CUti)\nResponse Completeness (Up-RCmp)\nResponse Matching (Up-RMch)Noise Impact \u25bcRe-rankingRandom samples with varying\nnumbers of noisy context\nComplex Reasoning\u2663Query Rewriting\n\u2663Query Decomposition\n\u25bcFew-shot PromptingRandom samples from HotpotQA Hard\nRAG systems hold great potential for delivering accurate, context-aware responses but face reliability\nchallenges Chen et al. [2024], Barnett et al. [2024]. These challenges include the tendency of models\nto generate deceptive responses under uncertainty, improper ranking of retrieval results, incomplete\nanswers, sensitivity to noise, and limitations in handling complex reasoning tasks, as shown in\nFigure 6. Understanding these issues is crucial for recognizing the current boundaries of RAG\ntechnology and identifying areas where further research and development are needed. Testing RAG\nfailures, we randomly sample the complete test set Enotimes and Enois 3 in our experiments. For\nthe evaluation of RAG failures, we set Espto 20, considering the test quantity to be adequate, given\nthat we have specifically curated datasets to investigate failures (Appendix A.6).\nGolden Case . In the context of RAG, a golden case is defined as a scenario where retrieval is both\nentirely accurate and complete. When the retrieved information is subsequently fed into the large\nmodel, it facilitates the generation of a correct response.\nNegative Refusal . The challenge of negative refusal in RAG systems, where models tend to produce\ndeceptive responses instead of acknowledging uncertainty, will severely erode user trust. The issue of\nnegative refusal often stems from the model\u2019s lack of awareness of its knowledge boundaries. When\nconfronted with a query that lacks sufficient information, the model may generate a factually incorrect\nor misleading response rather than transparently admitting the absence of relevant knowledge.\n151 1 c l a s s RAGConfig :\n2 2 \" \" \"RAG C o n f i g u r a t i o n C l a s s \" \" \"\n3 3\n4 4 c l a s s APIKeys :\n5 5 \" \" \" Api c o n f i g \" \" \"\n6 6 d e f __ i n i t __ ( s e l f ) :\n7 7 s e l f . o p e n a i _ key = \" sk - xxx \" # OpenAI c r e d e n t i a l s\n8 8 s e l f . hf _ key = \" hf _ yyy \" # HuggingFace a c c e s s\n9 9 s e l f . o p e n a i _ ba s e = \" h t t p s : / / \" # OpenAI s e r v i c e URL\n10 10\n11 11 c l a s s P a r a m e t e r s :\n12 12 \" \" \" Core r u n t i m e params \" \" \"\n13 13 d e f __ i n i t __ ( s e l f ) :\n14 14 s e l f . chunk _ s i z e = \" 128 \" # Text chunk l e n g t h\n15 15 s e l f . o v e r l a p = \" 20 \" # Chunk o v e r l a p\n16 16 s e l f . embed_dim = \" 768 \" # V e c t o r dimension\n17 17 s e l f . c o n t e x t _ l e n g t h = \" 4096 \" # C o n t e x t window\n18 18 . . . . . .\n19 18\n20 19 c l a s s Index :\n21 20 \" \" \" V e c t o r i n d e x c o n f i g \" \" \"\n22 21 d e f __ i n i t __ ( s e l f ) :\n23 22 s e l f . i n d e x = \" v e c t o r \" # V e c t o r s t o r e\n24 23 s e l f . m e t r i c = \" c o s i n e \" # S i m i l a r i t y m e t r i c\n25 24 . . . . . . .\n26 25\n27 26 c l a s s S t r a t e g i e s :\n28 27 \" \" \" O p e r a t i o n s t r a t e g i e s \" \" \"\n29 28 d e f __ i n i t __ ( s e l f ) :\n30 29 # R e t r i e v a l c o n f i g\n31 30 s e l f . t o p _k = \" 3 \" # Number of r e t r i e v e d nodes\n32 31 s e l f . r e r a n k = \" True \" # Enable r e r a n k i n g\n33 32 # G e n e r a t i o n c o n f i g\n34 33 s e l f . t o k e n s = \" 1024 \" # Max g e n e r a t i o n\n35 34 . . . . . .\n36 35\n37 36 c l a s s M e t r i c s :\n38 37 \" \" \" E v a l u a t i o n m e t r i c s \" \" \"\n39 38 d e f __ i n i t __ ( s e l f ) :\n40 39 s e l f . h i t _1 = \" 0 . 7 \" # Hit@1\n41 40 s e l f . f1 = \" 0 . 6 6 \" # F1 s c o r e\n42 41 s e l f . rouge1 = \" 0 . 5 6 \" # Rouge\n43 42 s e l f . mrr = \" 0 . 8 5 \" # MRR t h r e s h o l d\n44 43 s e l f . p p l = \" 50 \" # P e r p l e x i t y window\n45 44 . . . . . .\nListing 1: XRAG supports flexible system configuration through parameter settings in\nconfig files while also providing an integrated front-end user interface (UI) that enables\ninteractive configuration through visual operations. This dual-configuration approach\nallows users to either 1) Directly modify configuration parameters via text-based config\nfiles for precise control, or 2) Utilize the graphical UI to dynamically adjust settings\nthrough intuitive form inputs and real-time previews. The system architecture ensures\nparameter synchronization between both configuration modes, maintaining consistency\nacross different operation methods. This design enhances accessibility for novice users\nthrough its visual interface while preserving expert-friendly text-based configuration\ncapabilities, achieving an optimal balance between usability and customization depth.\nRanking Confusion . Existing studies have shown that LLMs are more attentive to the earlier\nparts of input sequences. This characteristic poses a significant issue for RAG systems: even if\nthe retrieval module successfully locates the correct document segments, the system may still be\naffected if the most relevant segments do not appear early in the input sequence. In such cases, the\ngeneration module will initially encounter suboptimal or erroneous information, ultimately impacting\nthe accuracy and quality of the RAG system\u2019s output.\nAnswer Absence . The RAG system is designed to integrate information retrieved by the retrieval\nmodule with the inference capabilities of the generative model. However, a common challenge is\nthat LLMs may overlook relevant details during the answer generation phase, even when all related\ncontexts have been correctly retrieved. This issue often arises from the limitations in the reasoning\n16Expected Answer :\n\u2026because salts from rock \nweathering enter the \nocean, and evaporation \nleaves the salt behind\u2026\ud835\udc02\ud835\udfcf\u2026comes \nfrom rain \ndissolving salt \nfrom rocks...0.82\n\ud835\udc02\ud835\udfd0... also release \nminerals and \nsalts...0.79\n\ud835\udc02\ud835\udfd1The water \nturns into \nvapor, but the \nsalt \nremains\u20260.78\u6b63\u5e38\u95ee\u7b54\nActual Response: \nSalts from rocks dissolve in the \nocean, and the salt stays as water \nevaporates.Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcf\u2026comes \nfrom rain \ndissolving salt \nfrom rocks...1.00\n\ud835\udc02\ud835\udfd0... also release \nminerals and \nsalts...1.00\n\ud835\udc02\ud835\udfd1The water \nturns into \nvapor, but the \nsalt \nremains\u20261.00\nWe expect the RAG system to \nprecisely search for relevant \ninformation and combine the \ngenerative model's capabilities to \nproduce accurate answers.User query: Why is seawater salty?(a) Golden Case\nExpected Answer :\nBased on the context \ninformation provided, \nthere is no mention\u2026\ud835\udc02\ud835\udfd4James Henry \nMiller... is a \nAustralian \nrules \nfootballer0.76\n\ud835\udc02\ud835\udfd7June Miller  \nwas the \nsecond wife \nof Henry \nMiller.0.74\n\ud835\udc02\ud835\udfd5James Miller\u2019s \nnotable \nworks \ninclude\u2026 0.59\n\u5426\u5b9a\u62d2\u7edd\nActual Response: \nAustralian.Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcfJames Henry \nMiller...activis\nt, actor, poet, \nplaywright \nand ...1.00\n\ud835\udc02\ud835\udfd0Margaret \nPeggy Seeger. \nShe is \u2026was \nmarried to \nthe singer\u20261.00\nThe RAG system should clearly say \n\"Sorry, I don't know,\" but it may \nprovide incorrect answers, stemming \nfrom misinterpretation or over -\nspeculation of the information.User query: What nationality was James Henry Miller's wife? (b) Negative Refusal\nExpected Answer :\n\u2026an object will remain at \nrest or in uniform motion \nunless acted upon by an \nexternal force.\ud835\udc02\ud835\udfd1...every action \nhas an equal \nand opposite \nreaction.0.82\n\ud835\udc02\ud835\udfd0\u2026depends on \nthe force \napplied to it \nand its mass.0.79\n\ud835\udc02\ud835\udfcf\u2026stays at \nrest or moves \nsteadily \nunless a force \nacts on it.0.59\nRanking errors occur when the \nretrieval algorithm misjudges \ninformation relevance, leading the \ngenerative model to use less relevant \ncontent.\nActual Response: \n\u2026every action has an equal and \nopposite reaction.Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcf\u2026stays at \nrest or moves \nsteadily \nunless a force \nacts on it.1.00\n\ud835\udc02\ud835\udfd0\u2026depends on \nthe force \napplied to it \nand its mass.1.00\n\ud835\udc02\ud835\udfd1...every action \nhas an equal \nand opposite \nreaction.1.00\u6392\u540d\u6df7\u6dc6User query: What is Newton's First Law? (c) Ranking Confusion\nExpected Answer :\n\u2026composed of nitrogen \n(78%), oxygen (21%), and \nsmall amounts of carbon \ndioxide and argon.\ud835\udc02\ud835\udfcf\u2026primarily \ncomposed of \nnitrogen, \nmaking up \nabout 78%.0.82\n\ud835\udc02\ud835\udfd0\u2026oxygen in \nEarth's \natmosphere \nis about 21%.0.81\n\ud835\udc02\ud835\udfd1\u2026gases on \nEarth include \ncarbon \ndioxides.0.79\nThe answer omission issue in a RAG \nsystem occurs when the retriever \nprovides relevant information, but \nthe generative model fails to \nproduce a correct response.\nActual Response: \nThe main gaseous component of \nEarth's atmosphere is nitrogen.Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcf\u2026primarily \ncomposed of \nnitrogen, \nmaking up \nabout 78%.1.00\n\ud835\udc02\ud835\udfd0\u2026oxygen in \nEarth's \natmosphere \nis about 21%.1.00\n\ud835\udc02\ud835\udfd1\u2026gases on \nEarth include \ncarbon \ndioxides.1.00\u7b54\u6848\u7f3a\u5931User prompt: What are the main gaseous components of \nEarth's atmosphere?\n(d) Answer Absence\nExpected Answer :\n\u2026acts as a tumor \nsuppressor, preventing \nthe formation of cancer \nby regulating the cell \u2026\ud835\udc02\ud835\udfcf\ud835\udfd5While p300 \nitself is not \ndirectly \ninvolved in \ntumor \nsuppression \nlike p530.82\n\ud835\udc02\ud835\udfd0\ud835\udfd0For instance, \np300's \u2026supp\nressor genes, \nthus \ninfluencing \ncancer \ndevelopment.0.81\nThe noise issue in a RAG system \narises when the retriever fails to \neffectively filter out irrelevant \ninformation during the retrieval \nphase.\nActual Response: \nis involved in gene regulation \nand plays a role in cell \ndifferentiation \u2026Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcfp53 is often \nreferred to as \nthe 'guardian \nof the \ngenome' 1.00\n\ud835\udc02\ud835\udfd0\u2026common \ngenetic \nalterations \nfound in \nhuman \ncancers1.00\n\ud835\udc02\ud835\udfd1Restoring p53 \nfunction is1.00\u566a\u58f0\u5f71\u54cdUser query: What is the role of the protein p53 in \npreventing cancer? (e) Noise Impact\nExpected Answer :\nGPS satellites move fast, \nmaking their clocks tick \nslower than Earth's.\ud835\udc02\ud835\udfcfGPS satellites \ncommunicate \nwith \nreceivers on \nEarth via ...0.92\n\ud835\udc02\ud835\udfd0According to \nspecial \nrelativity, \ntime moves \nslower ...0.71\n\ud835\udc02\ud835\udfd1... time \npasses more \nslowly in \nstronger\u20260.71\nThe complex reasoning issue in a \nRAG system arises when retrieved \ncorrect information, but the \ngenerative model fails to integrate \nand reason it effectively.\nActual Response: \nGPS systems rely on radio signals \nand multiple satellites to \ndetermine location.Retrieval context \uff1a Golden context\n\u590d\u6742\u63a8\u7406User query: How does Einstein's theory of relativity affect \nthe accuracy of the GPS?\n\ud835\udc02\ud835\udfcfGPS satellites \ncommunicate \nwith \nreceivers on \nEarth via ...1.00\n\ud835\udc02\ud835\udfd0According to \nspecial \nrelativity, \ntime moves \nslower ...1.00\n\ud835\udc02\ud835\udfd1... time \npasses more \nslowly in \nstronger\u20261.00 (f) Complex Reasoning\nFigure 6: Golden case and typical RAG failures.\ncapabilities of these models and problems associated with the format or order in which the context\nis presented. For instance, the existence of multiple methods for inputting context to LLMs in\nLlamaIndex can result in disparate response generation mechanisms, potentially impeding the RAG\nsystem\u2019s capacity to access and utilize critical contextual information effectively.\nNoise Impact . The robustness of RAG systems against noise is another critical aspect of their\nperformance, especially when dealing with sources containing irrelevant or misleading information.\nDue to variations in the retrieval model\u2019s accuracy, the way users formulate their queries, and stylistic\ndifferences, the document chunks retrieved often contain some degree of irrelevant content, which, if\nfed into the LLMs, can significantly affect their reasoning performance and the final response.\nComplex Reasoning . In practical tasks, handling complex reasoning scenarios that require inte-\ngrating information from multiple documents is often necessary. In these scenarios, answering user\nqueries depends on relevant clues scattered across different documents, necessitating cross-document\ninformation retrieval and integrated reasoning. However, RAG systems may struggle to fully grasp\nthe complexity and diverse implicit information needs of such tasks, affecting the efficiency of the\nretrieval module and leading to the failure to identify all relevant cross-document segments. Coupled\nwith the limitations in the reasoning capabilities of LLMs when faced with complex document inputs,\nthis can ultimately impact the accuracy of the reasoning outcomes.\nA.6", "Optimization Strategies & Evaluation of RAG Failures": "In the previous section, we summarized five types of failures in RAG systems. These issues affect the\nperformance and reliability of RAG systems and limit their effectiveness in practical applications. To\naddress these challenges, we have proposed various optimization strategies, constructed evaluation\n17datasets, and utilized detailed evaluation metrics to assess the effectiveness of the improvements, as\nshown in Table 10.\nA.6.1", "Negative Refusal": "\u25bcPrompt Engineering\n\u25bcTwo-step ReasoningRandomly sampled queries\nwith unrelated contextRejection Rate", "Ranking Confusion": "\u25bcRe-ranking\n\u2660Hybrid Retrieval\n\u2660\u25bcHybrid Retrieval & Re-rankingSamples with lower F1 scoresF1, Hit@1, EM, MRR\nMAP, DCG, IDCG, NDCG", "Answer Absence": "\u25bcSimple Summarize\n\u25bcRefine\n\u25bcCompact\n\u25bcCompact AccumulateSamples with missing answers Factual Accuracy (Up-FAcc)\nResponse Consistency (Up-RCns)\nContext Utilization (Up-CUti)\nResponse Completeness (Up-RCmp)\nResponse Matching (Up-RMch)", "Noise Impact": "\u25bcRe-rankingRandom samples with varying\nnumbers of noisy context", "Complex Reasoning": "\u2663Query Rewriting\n\u2663Query Decomposition\n\u25bcFew-shot PromptingRandom samples from HotpotQA Hard\nRAG systems hold great potential for delivering accurate, context-aware responses but face reliability\nchallenges Chen et al. [2024], Barnett et al. [2024]. These challenges include the tendency of models\nto generate deceptive responses under uncertainty, improper ranking of retrieval results, incomplete\nanswers, sensitivity to noise, and limitations in handling complex reasoning tasks, as shown in\nFigure 6. Understanding these issues is crucial for recognizing the current boundaries of RAG\ntechnology and identifying areas where further research and development are needed. Testing RAG\nfailures, we randomly sample the complete test set Enotimes and Enois 3 in our experiments. For\nthe evaluation of RAG failures, we set Espto 20, considering the test quantity to be adequate, given\nthat we have specifically curated datasets to investigate failures (Appendix A.6).\nGolden Case . In the context of RAG, a golden case is defined as a scenario where retrieval is both\nentirely accurate and complete. When the retrieved information is subsequently fed into the large\nmodel, it facilitates the generation of a correct response.\nNegative Refusal . The challenge of negative refusal in RAG systems, where models tend to produce\ndeceptive responses instead of acknowledging uncertainty, will severely erode user trust. The issue of\nnegative refusal often stems from the model\u2019s lack of awareness of its knowledge boundaries. When\nconfronted with a query that lacks sufficient information, the model may generate a factually incorrect\nor misleading response rather than transparently admitting the absence of relevant knowledge.\n151 1 c l a s s RAGConfig :\n2 2 \" \" \"RAG C o n f i g u r a t i o n C l a s s \" \" \"\n3 3\n4 4 c l a s s APIKeys :\n5 5 \" \" \" Api c o n f i g \" \" \"\n6 6 d e f __ i n i t __ ( s e l f ) :\n7 7 s e l f . o p e n a i _ key = \" sk - xxx \" # OpenAI c r e d e n t i a l s\n8 8 s e l f . hf _ key = \" hf _ yyy \" # HuggingFace a c c e s s\n9 9 s e l f . o p e n a i _ ba s e = \" h t t p s : / / \" # OpenAI s e r v i c e URL\n10 10\n11 11 c l a s s P a r a m e t e r s :\n12 12 \" \" \" Core r u n t i m e params \" \" \"\n13 13 d e f __ i n i t __ ( s e l f ) :\n14 14 s e l f . chunk _ s i z e = \" 128 \" # Text chunk l e n g t h\n15 15 s e l f . o v e r l a p = \" 20 \" # Chunk o v e r l a p\n16 16 s e l f . embed_dim = \" 768 \" # V e c t o r dimension\n17 17 s e l f . c o n t e x t _ l e n g t h = \" 4096 \" # C o n t e x t window\n18 18 . . . . . .\n19 18\n20 19 c l a s s Index :\n21 20 \" \" \" V e c t o r i n d e x c o n f i g \" \" \"\n22 21 d e f __ i n i t __ ( s e l f ) :\n23 22 s e l f . i n d e x = \" v e c t o r \" # V e c t o r s t o r e\n24 23 s e l f . m e t r i c = \" c o s i n e \" # S i m i l a r i t y m e t r i c\n25 24 . . . . . . .\n26 25\n27 26 c l a s s S t r a t e g i e s :\n28 27 \" \" \" O p e r a t i o n s t r a t e g i e s \" \" \"\n29 28 d e f __ i n i t __ ( s e l f ) :\n30 29 # R e t r i e v a l c o n f i g\n31 30 s e l f . t o p _k = \" 3 \" # Number of r e t r i e v e d nodes\n32 31 s e l f . r e r a n k = \" True \" # Enable r e r a n k i n g\n33 32 # G e n e r a t i o n c o n f i g\n34 33 s e l f . t o k e n s = \" 1024 \" # Max g e n e r a t i o n\n35 34 . . . . . .\n36 35\n37 36 c l a s s M e t r i c s :\n38 37 \" \" \" E v a l u a t i o n m e t r i c s \" \" \"\n39 38 d e f __ i n i t __ ( s e l f ) :\n40 39 s e l f . h i t _1 = \" 0 . 7 \" # Hit@1\n41 40 s e l f . f1 = \" 0 . 6 6 \" # F1 s c o r e\n42 41 s e l f . rouge1 = \" 0 . 5 6 \" # Rouge\n43 42 s e l f . mrr = \" 0 . 8 5 \" # MRR t h r e s h o l d\n44 43 s e l f . p p l = \" 50 \" # P e r p l e x i t y window\n45 44 . . . . . .\nListing 1: XRAG supports flexible system configuration through parameter settings in\nconfig files while also providing an integrated front-end user interface (UI) that enables\ninteractive configuration through visual operations. This dual-configuration approach\nallows users to either 1) Directly modify configuration parameters via text-based config\nfiles for precise control, or 2) Utilize the graphical UI to dynamically adjust settings\nthrough intuitive form inputs and real-time previews. The system architecture ensures\nparameter synchronization between both configuration modes, maintaining consistency\nacross different operation methods. This design enhances accessibility for novice users\nthrough its visual interface while preserving expert-friendly text-based configuration\ncapabilities, achieving an optimal balance between usability and customization depth.\nRanking Confusion . Existing studies have shown that LLMs are more attentive to the earlier\nparts of input sequences. This characteristic poses a significant issue for RAG systems: even if\nthe retrieval module successfully locates the correct document segments, the system may still be\naffected if the most relevant segments do not appear early in the input sequence. In such cases, the\ngeneration module will initially encounter suboptimal or erroneous information, ultimately impacting\nthe accuracy and quality of the RAG system\u2019s output.\nAnswer Absence . The RAG system is designed to integrate information retrieved by the retrieval\nmodule with the inference capabilities of the generative model. However, a common challenge is\nthat LLMs may overlook relevant details during the answer generation phase, even when all related\ncontexts have been correctly retrieved. This issue often arises from the limitations in the reasoning\n16Expected Answer :\n\u2026because salts from rock \nweathering enter the \nocean, and evaporation \nleaves the salt behind\u2026\ud835\udc02\ud835\udfcf\u2026comes \nfrom rain \ndissolving salt \nfrom rocks...0.82\n\ud835\udc02\ud835\udfd0... also release \nminerals and \nsalts...0.79\n\ud835\udc02\ud835\udfd1The water \nturns into \nvapor, but the \nsalt \nremains\u20260.78\u6b63\u5e38\u95ee\u7b54\nActual Response: \nSalts from rocks dissolve in the \nocean, and the salt stays as water \nevaporates.Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcf\u2026comes \nfrom rain \ndissolving salt \nfrom rocks...1.00\n\ud835\udc02\ud835\udfd0... also release \nminerals and \nsalts...1.00\n\ud835\udc02\ud835\udfd1The water \nturns into \nvapor, but the \nsalt \nremains\u20261.00\nWe expect the RAG system to \nprecisely search for relevant \ninformation and combine the \ngenerative model's capabilities to \nproduce accurate answers.User query: Why is seawater salty?(a) Golden Case\nExpected Answer :\nBased on the context \ninformation provided, \nthere is no mention\u2026\ud835\udc02\ud835\udfd4James Henry \nMiller... is a \nAustralian \nrules \nfootballer0.76\n\ud835\udc02\ud835\udfd7June Miller  \nwas the \nsecond wife \nof Henry \nMiller.0.74\n\ud835\udc02\ud835\udfd5James Miller\u2019s \nnotable \nworks \ninclude\u2026 0.59\n\u5426\u5b9a\u62d2\u7edd\nActual Response: \nAustralian.Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcfJames Henry \nMiller...activis\nt, actor, poet, \nplaywright \nand ...1.00\n\ud835\udc02\ud835\udfd0Margaret \nPeggy Seeger. \nShe is \u2026was \nmarried to \nthe singer\u20261.00\nThe RAG system should clearly say \n\"Sorry, I don't know,\" but it may \nprovide incorrect answers, stemming \nfrom misinterpretation or over -\nspeculation of the information.User query: What nationality was James Henry Miller's wife? (b) Negative Refusal\nExpected Answer :\n\u2026an object will remain at \nrest or in uniform motion \nunless acted upon by an \nexternal force.\ud835\udc02\ud835\udfd1...every action \nhas an equal \nand opposite \nreaction.0.82\n\ud835\udc02\ud835\udfd0\u2026depends on \nthe force \napplied to it \nand its mass.0.79\n\ud835\udc02\ud835\udfcf\u2026stays at \nrest or moves \nsteadily \nunless a force \nacts on it.0.59\nRanking errors occur when the \nretrieval algorithm misjudges \ninformation relevance, leading the \ngenerative model to use less relevant \ncontent.\nActual Response: \n\u2026every action has an equal and \nopposite reaction.Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcf\u2026stays at \nrest or moves \nsteadily \nunless a force \nacts on it.1.00\n\ud835\udc02\ud835\udfd0\u2026depends on \nthe force \napplied to it \nand its mass.1.00\n\ud835\udc02\ud835\udfd1...every action \nhas an equal \nand opposite \nreaction.1.00\u6392\u540d\u6df7\u6dc6User query: What is Newton's First Law? (c) Ranking Confusion\nExpected Answer :\n\u2026composed of nitrogen \n(78%), oxygen (21%), and \nsmall amounts of carbon \ndioxide and argon.\ud835\udc02\ud835\udfcf\u2026primarily \ncomposed of \nnitrogen, \nmaking up \nabout 78%.0.82\n\ud835\udc02\ud835\udfd0\u2026oxygen in \nEarth's \natmosphere \nis about 21%.0.81\n\ud835\udc02\ud835\udfd1\u2026gases on \nEarth include \ncarbon \ndioxides.0.79\nThe answer omission issue in a RAG \nsystem occurs when the retriever \nprovides relevant information, but \nthe generative model fails to \nproduce a correct response.\nActual Response: \nThe main gaseous component of \nEarth's atmosphere is nitrogen.Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcf\u2026primarily \ncomposed of \nnitrogen, \nmaking up \nabout 78%.1.00\n\ud835\udc02\ud835\udfd0\u2026oxygen in \nEarth's \natmosphere \nis about 21%.1.00\n\ud835\udc02\ud835\udfd1\u2026gases on \nEarth include \ncarbon \ndioxides.1.00\u7b54\u6848\u7f3a\u5931User prompt: What are the main gaseous components of \nEarth's atmosphere?\n(d) Answer Absence\nExpected Answer :\n\u2026acts as a tumor \nsuppressor, preventing \nthe formation of cancer \nby regulating the cell \u2026\ud835\udc02\ud835\udfcf\ud835\udfd5While p300 \nitself is not \ndirectly \ninvolved in \ntumor \nsuppression \nlike p530.82\n\ud835\udc02\ud835\udfd0\ud835\udfd0For instance, \np300's \u2026supp\nressor genes, \nthus \ninfluencing \ncancer \ndevelopment.0.81\nThe noise issue in a RAG system \narises when the retriever fails to \neffectively filter out irrelevant \ninformation during the retrieval \nphase.\nActual Response: \nis involved in gene regulation \nand plays a role in cell \ndifferentiation \u2026Retrieval context \uff1a Golden context\n\ud835\udc02\ud835\udfcfp53 is often \nreferred to as \nthe 'guardian \nof the \ngenome' 1.00\n\ud835\udc02\ud835\udfd0\u2026common \ngenetic \nalterations \nfound in \nhuman \ncancers1.00\n\ud835\udc02\ud835\udfd1Restoring p53 \nfunction is1.00\u566a\u58f0\u5f71\u54cdUser query: What is the role of the protein p53 in \npreventing cancer? (e) Noise Impact\nExpected Answer :\nGPS satellites move fast, \nmaking their clocks tick \nslower than Earth's.\ud835\udc02\ud835\udfcfGPS satellites \ncommunicate \nwith \nreceivers on \nEarth via ...0.92\n\ud835\udc02\ud835\udfd0According to \nspecial \nrelativity, \ntime moves \nslower ...0.71\n\ud835\udc02\ud835\udfd1... time \npasses more \nslowly in \nstronger\u20260.71\nThe complex reasoning issue in a \nRAG system arises when retrieved \ncorrect information, but the \ngenerative model fails to integrate \nand reason it effectively.\nActual Response: \nGPS systems rely on radio signals \nand multiple satellites to \ndetermine location.Retrieval context \uff1a Golden context\n\u590d\u6742\u63a8\u7406User query: How does Einstein's theory of relativity affect \nthe accuracy of the GPS?\n\ud835\udc02\ud835\udfcfGPS satellites \ncommunicate \nwith \nreceivers on \nEarth via ...1.00\n\ud835\udc02\ud835\udfd0According to \nspecial \nrelativity, \ntime moves \nslower ...1.00\n\ud835\udc02\ud835\udfd1... time \npasses more \nslowly in \nstronger\u20261.00 (f) Complex Reasoning\nFigure 6: Golden case and typical RAG failures.\ncapabilities of these models and problems associated with the format or order in which the context\nis presented. For instance, the existence of multiple methods for inputting context to LLMs in\nLlamaIndex can result in disparate response generation mechanisms, potentially impeding the RAG\nsystem\u2019s capacity to access and utilize critical contextual information effectively.\nNoise Impact . The robustness of RAG systems against noise is another critical aspect of their\nperformance, especially when dealing with sources containing irrelevant or misleading information.\nDue to variations in the retrieval model\u2019s accuracy, the way users formulate their queries, and stylistic\ndifferences, the document chunks retrieved often contain some degree of irrelevant content, which, if\nfed into the LLMs, can significantly affect their reasoning performance and the final response.\nComplex Reasoning . In practical tasks, handling complex reasoning scenarios that require inte-\ngrating information from multiple documents is often necessary. In these scenarios, answering user\nqueries depends on relevant clues scattered across different documents, necessitating cross-document\ninformation retrieval and integrated reasoning. However, RAG systems may struggle to fully grasp\nthe complexity and diverse implicit information needs of such tasks, affecting the efficiency of the\nretrieval module and leading to the failure to identify all relevant cross-document segments. Coupled\nwith the limitations in the reasoning capabilities of LLMs when faced with complex document inputs,\nthis can ultimately impact the accuracy of the reasoning outcomes.\nA.6 Optimization Strategies & Evaluation of RAG Failures\nIn the previous section, we summarized five types of failures in RAG systems. These issues affect the\nperformance and reliability of RAG systems and limit their effectiveness in practical applications. To\naddress these challenges, we have proposed various optimization strategies, constructed evaluation\n17datasets, and utilized detailed evaluation metrics to assess the effectiveness of the improvements, as\nshown in Table 10.\nA.6.1 Negative Refusal\nOptimization Strategies .\n\u2022Prompt Engineering : Explicit prompts can encourage LLMs to engage in more thoughtful and\njudgmental reasoning, aiming to elicit negative responses at appropriate times. As an effective\nmeans commonly employed in the domain of LLMs to address a wide range of problems, prompt\nengineering represents the simplest solution to the challenge of negative refusal.\n\u2022Two-step Reasoning : The two-step reasoning process leverages the capabilities of LLMs first to\nassess their ability to answer a given query. The model only proceeds to the second step to provide\na specific answer when it has enough information, completing the LLM reasoning. Conversely, if\nthe initial assessment concludes that there is insufficient information to answer, the second step of\nreasoning is declined, thereby preventing the generation of hallucinatory responses.\nExperimental Settings . To quantify the system\u2019s capacity for negative refusal, we first constructed a\ndataset comprising randomly sampled questions, each paired with an unrelated context to simulate\nscenarios where the model cannot access necessary information. We adopted the rejection rate as\nan evaluation metric to measure the system\u2019s ability to recognize and acknowledge situations that\ncannot provide a valid response. A higher rejection rate indicates a higher level of self-awareness\nand reliability in the system\u2019s responses. Without any optimizations, we established a baseline by\nmeasuring the system\u2019s rejection rate, then applied each optimization method, recording the rejection\nrates under new conditions to reveal the feasibility of each common strategy in enhancing the model\u2019s\nknowledge awareness and proper negation.\nTable 11: Evaluation of negative refusal strategies.\nStrategies Rejection Rate\nWrong ContextPrompt Engineering 1.000\nTwo-step Reasoning 1.000\nCorrect ContextPrompt Engineering 0.600\nTwo-step Reasoning 0.100\nEvaluation & Results . Regarding the problem of negative refusal, employing prompt engineering\nto explicitly require the LLMs to possess the ability to refuse, along with the two-step reasoning\napproach\u2014where the LLMs first evaluate whether the available information is sufficient to respond\nto the user\u2019s query\u2014both effectively increase the rejection rate of the RAG system, as shown in Table\n11. However, the experimental results further indicate that, in scenarios with the correct context,\nprompt engineering leads to a much higher probability (up to 60%) of still refusing to provide an\nanswer compared to the two-step reasoning method, severely undermining the usability of the RAG\nsystem. This highlights the importance of adequately designing system prompts and suggests that\nfixed prompt content might lack flexibility when dealing with diverse real-world situations.\nA.6.2 Ranking Confusion\nOptimization Strategies .\n\u2022Re-ranking : Employing a re-ranking model for more refined similarity calculations, although\ncomputationally more expensive than initial retrieval and typically applied only to smaller sample\nsizes, can significantly enhance the relevance and precision of retrieval results. In this section, we\nopt for the commonly used ColBERTv2 (Cohere rerank model) as the re-ranking model to perform\nfiner-grained matching on the coarse retrieval outcomes.\n\u2022Hybrid Retrieval : Integrating multiple retrieval methods can effectively broaden the diversity\nof retrieval results, allowing for the identification of relevant document segments from various\nperspectives, which could improve ranking accuracy. Specifically, we implement a hybrid retrieval\napproach combining BM25 with vector-based retrieval to construct the RAG system\u2019s hybrid\nretriever.\n18\u2022Hybrid Retrieval and Re-ranking : After implementing the hybrid retrieval strategy, further\napplication of the re-ranking method combines the dual advantages of diversified retrieval and\nprecise similarity calculation. This approach aims to more effectively identify and rank the most\nrelevant document segments, thereby enhancing the overall performance of the RAG system.\nExperimental Settings . For the verification, we selected samples with lower F1 scores under standard\nRAG retrieval conditions to facilitate the validation of the effectiveness of various methods. In terms\nof evaluation metrics, we use traditional measures such as F1 and EM to assess retrieval accuracy,\nalong with additional metrics like Hit@1, MRR, MAP, DCG, IDCG, and NDCG to precisely evaluate\nthe extent to which the aforementioned optimization strategies improve the correct ordering of\nretrieval results.\nTable 12: Results of ranking confusion strategies (RR: Re-ranking, HR: Hybrid Retrieval) .\nEvaluation MetricsStrategies\nF1 (\u2191) EM ( \u2191) MRR ( \u2191) HIT@1 ( \u2191) MAP ( \u2191) DCG\u25e6(\u2191) IDCG\u25e6(\u2191) NDCG ( \u2191)\nBasic-RAG 0.7400 0.0000 0.6670 0.9000 0.6800 1.2100 1.5300 0.7800\nw/ RR 0.8000 0.0000 0.7500 1.0000 0.7917 1.6309 1.3809 0.8467\nw/ HR 0.9250 0.8500 0.9250 0.9000 0.9125 1.4800 1.5360 0.9450\nw/ HR + RR 0.9750 0.9500 0.9750 1.0000 1.0000 1.6000 1.6309 0.9816\nEvaluation & Results . Regarding ranking confusion, re-ranking and hybrid retrieval strategies\nsignificantly improve the retrieval performance of RAG systems. As shown in Table 12, the gain\nfrom hybrid retrieval is noticeably superior to re-ranking. This advantage may be attributed to the\nhybrid retrieval\u2019s capability to perceive a broader range of relevant information snippets and its\ncomprehensive integration of multi-source information, leading to enhanced accuracy and better\nranking quality. Notably, the simultaneous application of both techniques achieves the most optimal\nperformance improvement.\nA.6.3 Answer Absence\nOptimization Strategies .\n\u2022Simple Summarize : All retrieved document chunks are concatenated into a single text block and\nfed into the LLMs in one go.\n\u2022Refine : Each retrieved document chunk undergoes a separate Q&A session with the LLMs, with\ninputs including the original query, the answer from the previous round, and the current document\nchunk. A predefined prompt is used to refine each answer to elicit more detailed and precise\nresponses.\n\u2022Compact : Document chunks are first merged into longer blocks as much as possible before\napplying the Refine method, reducing the number of calls to LLMs.\n\u2022Compact Accumulate : Document chunks are similarly merged into longer blocks, but each\nchunk undergoes independent Q&A sessions with inputs being the original query and the current\ndocument chunk, without the answer from the previous round. All results are then combined to\nform the final response.\nExperimental Settings . To conduct our experiments, we randomly sampled a batch of questions,\nproviding the questions and their golden contexts to the LLMs to obtain responses for each question.\nSubsequently, through manual screening, we selected questions and their corresponding golden\ncontexts that exhibited missing answers to serve as the evaluation dataset for this section. To validate\nthe effectiveness of various methods, we adopt Factual Accuracy, Response Consistency, Context\nUtilization, Response Completeness, and Response Matching as evaluation metrics.\nEvaluation & Results . Concerning the answer absence, we tested various methods for inputting\ndocument chunks into the LLMs. The results show that different input methods affect the correctness\nand comprehensiveness of the model\u2019s responses, as illustrated in Table 13. Among these, indepen-\ndently querying each document chunk produced the best results, whereas the more complex iterative\nresponse generation methods, such as Refine and Compact modes, performed poorly. This indicates\nthat simply increasing the complexity of the RAG system sometimes has counterproductive effects.\n19Table 13: Evaluation of answer absence strategies. All metrics are discrete values in [0, 1].\nStrategiesEvaluation Metrics\nUp-FAcc ( \u2191) Up-RCns ( \u2191) Up-CUti ( \u2191) Up-RCmp ( \u2191) Up-RMch ( \u2191)\nSimple Summarize 0.0250 1.0000 0.8250 0.8500 0.2790\nRefine 0.9400 1.0000 0.6750 0.5750 0.2050\nCompact 0.8750 1.0000 0.7000 0.4250 0.2930\nCompact Accumulate 0.8191 0.9650 0.9750 0.8500 0.3840\nA.6.4 Noise Impact\nOptimization Strategies .\n\u2022Re-ranking : Employing a re-ranking model for more refined similarity calculations not only serves\nas a potential solution to the ranking confusion problem but can also help filter out irrelevant\ndocuments through threshold filtering or quantity filtering, thereby reducing the amount of noise\ninput to LLMs and improving the performance of the RAG system.\nExperimental Settings . We randomly sampled query instances and combined them with their\ncorresponding golden contexts and varying numbers of irrelevant document chunks to form the\nevaluation dataset. This allows us to explore the changes in RAG system performance under different\nproportions of noisy documents, as well as the enhancement of system capability after incorporating a\npost-processing re-ranking module. We adopt the same set of metrics used in A.2.3 for the evaluation\nmetrics.\nTable 14: Evaluation of noise impact strategies. All metrics are discrete values in [0, 1].\nNoise number StrategiesEvaluation metrics\nUp-FAcc ( \u2191) Up-RCns ( \u2191) Up-CUti ( \u2191) Up-RCmp ( \u2191) Up-RMch ( \u2191)\n0 w/o Re-ranking 0.8950 1.0000 0.6750 0.5000 0.3920\n1w/o Re-ranking 0.9000 0.9950 0.8000 0.4500 0.4700\nw/ Re-ranking 0.6790 0.7976 0.8571 0.5714 0.2357\n2w/o Re-ranking 0.8650 0.9500 0.7500 0.4000 0.3760\nw/ Re-ranking 0.5883 1.0000 0.6500 0.5500 0.4000\n3w/o Re-ranking 0.7925 0.9500 0.7000 0.4000 0.2975\nw/ Re-ranking 0.8017 1.0000 0.7000 0.7000 0.4800\nEvaluation & Results . When the document chunks within retrieval results contain noise, the output\naccuracy of the RAG system deteriorates as the amount of noise increases, as seen in Table 14.\nPost-processing the retrieval results using re-ranking methods can somewhat mitigate this issue, and\nthe improvement becomes more pronounced as the number of noisy document chunks increases.\nA.6.5 Complex Reasoning\nOptimization Strategies .\n\u2022Query Rewriting : By rewriting user queries using an LLM to add and introduce more information,\nthis approach aims to guide the retrieval module to access more relevant documents explicitly.\n\u2022Query Decomposition : Breaking down complex questions into simpler sub-questions that focus\non individual aspects simplifies the reasoning process, enabling a comprehensive search for all\nrequired information.\n\u2022Few-shot Prompting : By prepending a few complex reasoning examples to the input sequence of\nthe LLM, this method aims to guide and stimulate the model\u2019s latent reasoning abilities, thereby\nenhancing the effectiveness of complex reasoning.\nExperimental Settings . For verification of complex reasoning, we randomly sampled items from the\nHard subset of HotpotQA to serve as the task set for complex logic. Similarly, we adopted the same\nmetrics used in A.2.3 to fully assess the improvement effects of each solution.\nEvaluation & Results . When the RAG system faces complex reasoning tasks, our experimental\nresults demonstrate that query rewriting significantly enhances the system\u2019s performance in addressing\n20Table 15: Evaluation of complex reasoning strategies. All metrics are discrete values in [0, 1].\nStrategiesEvaluation Metrics\nUp-FAcc ( \u2191) Up-RCns ( \u2191) Up-CUti ( \u2191) Up-RCmp ( \u2191) Up-RMch ( \u2191)\nBasic-RAG 0.5700 1.0000 0.7500 0.4000 0.3830\nQuery Rewriting 0.6750 0.9900 0.7750 0.5500 0.4430\nQuery Decomposition 0.6250 0.9500 0.5500 0.3250 0.2790\nFew-shot Prompting 0.6250 0.8900 0.5250 0.2500 0.2410\nsuch challenges, as evidenced in Table 15. This suggests that unclear or insufficiently detailed user\nqueries may be a critical factor limiting system performance in complex scenarios. Additionally,\nneither query decomposition nor few-shot prompting positively impacted this challenge; instead, they\nled to a further decline in system performance. This reaffirms that increasing the complexity of the\nRAG system may introduce potential performance risks and that system prompts need to be carefully\nand thoroughly tested rather than made more complex and detailed.\nA.7", "Prompt for XRAG Instructions": "Prompt for XRAG suitable for HotpotQA datasets\nContext information is below.\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n{context_str}\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nGiven the context information and no prior knowledge, answer the question:\n{query_str}\nWe have the opportunity to refine the original answer.\n(only if needed) with some more context below.\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n{context_msg}\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nGiven the new context, refine the original answer to better.\nAnswer the question: query_str\nIf the context isn\u2019t proper, output the original answer again.\nOriginal Answer: existing_answer\nPrompt for XRAG suitable for DropQA and NaturalQA datasets\nContext information is below.\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n{context_str}\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nGiven the context information and no prior knowledge. Please provide a brief, shortest possible answer,\nideally just one word for the following question:\nQuestion: who has sold more records Oasis or Coldplay?\nExpected Answer: Oasis\nWe have the opportunity to refine the original answer.\n(only if needed) with some more context below.\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\n{context_msg}\n\u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7\nGiven the new context, refine the original answer to better.\nAnswer the question: query_str\nIf the context isn\u2019t proper, output the original answer again.\nOriginal Answer: existing_answer\n21A.8", "Detail of Cognitive LLM Evaluation": "\u2018Ex.\u2019 provides descriptions or explanations of the metrics, while \u2018Pa.u\u2019 specifies their standardized\ninput parameters (all parameters are shown in Section 2.2). We utilize GPT-4 Turbo for the Cognitive\nLLM Evaluation.\n\u2022Up-CRel [Retrieval ]:Uptrain-Context-Relevance\n\u2013Ex .:Context relevance measures if the retrieved context has enough information to answer the question\nbeing asked.\n\u2013Pa .u:<Question ,Retrieval-Context >\n\u2022Up-CCns [Retrieval ]:Uptrain-Context-Conciseness\n\u2013Ex .:Context conciseness refers to the quality of a reference context generated from the retrieved context\nin terms of being clear, brief, and to the point.\n\u2013Pa .u:<Question ,Golden-Context ,Retrieval-Context >\n\u2022Dp-ARel [Generation ]:DeepEval-Response-Relevancy\n\u2013Ex .:Response relevancy measures how relevant the actual response is compared to the provided input.\n\u2013Pa .u:<Question ,Actual-Response >\n\u2022Up-RCmp [Generation ]:Uptrain-Response-Completeness\n\u2013Ex .:Response completeness measures if the generated response adequately answers all aspects of the\nquestion being asked.\n\u2013Pa .u:<Question ,Actual-Response >\n\u2022Up-RCnc [Generation ]:Uptrain-Response-Conciseness\n\u2013Ex .:Response conciseness measures whether the generated response contains any additional information\nirrelevant to the question asked.\n\u2013Pa .u:<Question ,Actual-Response >\n\u2022Up-RRel [Generation ]:Uptrain-Response-Relevance\n\u2013Ex .:Response conciseness measures whether the LLM-generated text contains any additional information\nirrelevant to the question asked.\n\u2013Pa .u:<Question ,Actual-Response >\n\u2022Up-RVal [Generation ]:Uptrain-Response-Valid\n\u2013Ex .:In some cases, an LLM might fail to generate a response due to limited knowledge or unclear\nquestions. A higher Response Validity Score is better. This score is used to measure whether the response\ngenerated by the model is effective.\n\u2013Pa .u:<Question ,Actual-Response >\n\u2022Up-RMch [Generation ]:Uptrain-Response-Matching\n\u2013Ex .:Response matching compares the LLM-generated text with the gold (ideal) response using the defined\nscore metric.\n\u2013Pa .u:<Question ,Actual-Response ,Expected-Answer >\n\u2022Dp-CPre [Merge Retrieval & Response ]:DeepEval-Context-Precision\n\u2013Ex .:Contextual precision measures your RAG pipeline\u2019s retriever by evaluating whether nodes in your\nretrieval context relevant to the given input are ranked higher than irrelevant ones.\n\u2013Pa .u:<Question ,Actual-Response ,Expected-Answer ,Retrieval-Context >\n\u2022Dp-CRec [Merge Retrieval & Response ]:DeepEval-Context-Recall\n\u2013Ex .:Contextual recall metric measures the quality of your RAG pipeline\u2019s retriever by evaluating how\nmuch the retrieval context aligns with the expected output.\n\u2013Pa .u:<Question ,Actual-Response ,Expected-Answer ,Retrieval-Context >\n\u2022Dp-CRel [Merge Retrieval & Response ]:DeepEval-Context-Relevance\n\u2013Ex .:Contextual relevancy metric measures the quality of your RAG pipeline\u2019s retriever by evaluating the\noverall relevance of the information presented in your retrieval context for a given input.\n\u2013Pa .u:<Question ,Actual-Response ,Retrieval-Context >\n\u2022Up-RCns [Merge Retrieval & Response ]:Uptrain-Context-Consistency\n22\u2013Ex .:Response Consistency measures how well the generated response aligns with both the question asked\nand the context provided.\n\u2013Pa .u:<Question ,Actual-Response ,Retrieval-Context >\n\u2022Up-CUti [Merge Retrieval & Response ]:Uptrain-Context-Utilization\n\u2013Ex .:Context Utilization score measures if the generated response has sufficiently used the retrieved context\nto answer the question being asked.\n\u2013Pa .u:<Question ,Actual-Response ,Retrieval-Context >\n\u2022Up-FAcc [Merge Retrieval & Response ]:Uptrain-Factual-Accuracy\n\u2013Ex .:Factual accuracy measures the degree to which a claim made in the response is true according to the\ncontext provided.\n\u2013Pa .u:<Question ,Actual-Response ,Retrieval-Context >\n\u2022Dp-Fath [Merge Retrieval & Response ]:DeepEval-Context-Faithfulness\n\u2013Ex .:The response faithfulness measures whether the actual output factually aligns with the contents of\nyour retrieval context.\n\u2013Pa .u:<Question ,Actual-Response ,Retrieval-Context >\n\u2022Dp-Hall [Merge Retrieval & Response ]:DeepEval-Context-Hallucination\n\u2013Ex .:The response hallucination measures whether your LLM generates factually correct information by\ncomparing the output to the provided context.\n\u2013Pa .u:<Question ,Actual-Response ,Golden-Context >\nA.8.1", "Development Web UI of XRAG": "The Web UI of XRAG enables developers to construct tailored action chains involving multiple\ninference steps. XRAG offers an intuitive workflow for users in Figure 7, It includes an interactive\nconfiguration tool that allows users to build datasets and vector databases, configure LLMs and\nretrievers, set evaluation methods, and view test results.\nWe proceed to elaborate on the specific components of the modules depicted in Figure 1, which\nconstitute the core architectural framework of the system. These modules form the foundation of our\nintegration of the Web UI of XRAG.\n\u2022Web Interface : This module provides an intuitive browser-based interaction platform\nfeaturing a responsive graphical user interface.\n\u2022Easy Configuration : All system parameters are unified in structured YAML configuration\nfiles (an example is shown in Listing A.4), simplifying setup procedures while maintaining\noperations through interaction with the web UI.\n\u2022Monitorable Results : Our architecture enables real-time monitoring of the Retrieval-\nAugmented Generation (RAG) pipeline through interactive dashboards that display interme-\ndiate results at each processing stage.\n\u2022Runable Demos : Pre-built demonstrative applications are included, offering executable\ntemplates that users can directly deploy or adapt for custom RAG implementations.\n\u2022Failure Managements : A comprehensive failure management system is integrated, provid-\ning automated diagnostic tools and recovery protocols for exception scenarios.\n23Figure 7: A screenshot of the Development Web UI of XRAG.\nNeurIPS Paper Checklist\n1.Claims\nQuestion: Do the main claims made in the abstract and introduction accurately reflect the\npaper\u2019s contributions and scope?\nAnswer: [Yes] .\nJustification: in Section Abstract and Section 1.\nGuidelines:\n\u2022The answer NA means that the abstract and introduction do not include the claims\nmade in the paper.\n\u2022The abstract and/or introduction should clearly state the claims made, including the\ncontributions made in the paper and important assumptions and limitations. A No or\nNA answer to this question will not be perceived well by the reviewers.\n\u2022The claims made should match theoretical and experimental results, and reflect how\nmuch the results can be expected to generalize to other settings.\n\u2022It is fine to include aspirational goals as motivation as long as it is clear that these goals\nare not attained by the paper.\n2.Limitations\nQuestion: Does the paper discuss the limitations of the work performed by the authors?\nAnswer: [Yes] .\nJustification: in Section 5.\n24Guidelines:\n\u2022The answer NA means that the paper has no limitation while the answer No means that\nthe paper has limitations, but those are not discussed in the paper.\n\u2022 The authors are encouraged to create a separate \"Limitations\" section in their paper.\n\u2022The paper should point out any strong assumptions and how robust the results are to\nviolations of these assumptions (e.g., independence assumptions, noiseless settings,\nmodel well-specification, asymptotic approximations only holding locally). The authors\nshould reflect on how these assumptions might be violated in practice and what the\nimplications would be.\n\u2022The authors should reflect on the scope of the claims made, e.g., if the approach was\nonly tested on a few datasets or with a few runs. In general, empirical results often\ndepend on implicit assumptions, which should be articulated.\n\u2022The authors should reflect on the factors that influence the performance of the approach.\nFor example, a facial recognition algorithm may perform poorly when image resolution\nis low or images are taken in low lighting. Or a speech-to-text system might not be\nused reliably to provide closed captions for online lectures because it fails to handle\ntechnical jargon.\n\u2022The authors should discuss the computational efficiency of the proposed algorithms\nand how they scale with dataset size.\n\u2022If applicable, the authors should discuss possible limitations of their approach to\naddress problems of privacy and fairness.\n\u2022While the authors might fear that complete honesty about limitations might be used by\nreviewers as grounds for rejection, a worse outcome might be that reviewers discover\nlimitations that aren\u2019t acknowledged in the paper. The authors should use their best\njudgment and recognize that individual actions in favor of transparency play an impor-\ntant role in developing norms that preserve the integrity of the community. Reviewers\nwill be specifically instructed to not penalize honesty concerning limitations.\n3.Theory assumptions and proofs\nQuestion: For each theoretical result, does the paper provide the full set of assumptions and\na complete (and correct) proof?\nAnswer: [NA] .\nJustification: Not involving any theoretical results.\nGuidelines:\n\u2022 The answer NA means that the paper does not include theoretical results.\n\u2022All the theorems, formulas, and proofs in the paper should be numbered and cross-\nreferenced.\n\u2022All assumptions should be clearly stated or referenced in the statement of any theorems.\n\u2022The proofs can either appear in the main paper or the supplemental material, but if\nthey appear in the supplemental material, the authors are encouraged to provide a short\nproof sketch to provide intuition.\n\u2022Inversely, any informal proof provided in the core of the paper should be complemented\nby formal proofs provided in appendix or supplemental material.\n\u2022 Theorems and Lemmas that the proof relies upon should be properly referenced.\n4.Experimental result reproducibility\nQuestion: Does the paper fully disclose all the information needed to reproduce the main ex-\nperimental results of the paper to the extent that it affects the main claims and/or conclusions\nof the paper (regardless of whether the code and data are provided or not)?\nAnswer: [Yes] .\nJustification: The models evaluated in the experiments are all public, it is quite easy to\nreproduce the results. Besides, in Section 3.1, Listing A.4 and Appendix A.7, we provide\nfull details about unified settings of different models.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n25\u2022If the paper includes experiments, a No answer to this question will not be perceived\nwell by the reviewers: Making the paper reproducible is important, regardless of\nwhether the code and data are provided or not.\n\u2022If the contribution is a dataset and/or model, the authors should describe the steps taken\nto make their results reproducible or verifiable.\n\u2022Depending on the contribution, reproducibility can be accomplished in various ways.\nFor example, if the contribution is a novel architecture, describing the architecture fully\nmight suffice, or if the contribution is a specific model and empirical evaluation, it may\nbe necessary to either make it possible for others to replicate the model with the same\ndataset, or provide access to the model. In general. releasing code and data is often\none good way to accomplish this, but reproducibility can also be provided via detailed\ninstructions for how to replicate the results, access to a hosted model (e.g., in the case\nof a large language model), releasing of a model checkpoint, or other means that are\nappropriate to the research performed.\n\u2022While NeurIPS does not require releasing code, the conference does require all submis-\nsions to provide some reasonable avenue for reproducibility, which may depend on the\nnature of the contribution. For example\n(a)If the contribution is primarily a new algorithm, the paper should make it clear how\nto reproduce that algorithm.\n(b)If the contribution is primarily a new model architecture, the paper should describe\nthe architecture clearly and fully.\n(c)If the contribution is a new model (e.g., a large language model), then there should\neither be a way to access this model for reproducing the results or a way to reproduce\nthe model (e.g., with an open-source dataset or instructions for how to construct\nthe dataset).\n(d)We recognize that reproducibility may be tricky in some cases, in which case\nauthors are welcome to describe the particular way they provide for reproducibility.\nIn the case of closed-source models, it may be that access to the model is limited in\nsome way (e.g., to registered users), but it should be possible for other researchers\nto have some path to reproducing or verifying the results.\n5.Open access to data and code\nQuestion: Does the paper provide open access to the data and code, with sufficient instruc-\ntions to faithfully reproduce the main experimental results, as described in supplemental\nmaterial?\nAnswer: [Yes] .\nJustification: The data set and corpus are uploaded to HuggingFace (anonymous links are\nprovided in Appendix A.1). The system code is uploaded to GitHub (anonymous links are\nprovided in the Section 1).\nGuidelines:\n\u2022 The answer NA means that paper does not include experiments requiring code.\n\u2022Please see the NeurIPS code and data submission guidelines ( https://nips.cc/\npublic/guides/CodeSubmissionPolicy ) for more details.\n\u2022While we encourage the release of code and data, we understand that this might not be\npossible, so \u201cNo\u201d is an acceptable answer. Papers cannot be rejected simply for not\nincluding code, unless this is central to the contribution (e.g., for a new open-source\nbenchmark).\n\u2022The instructions should contain the exact command and environment needed to run to\nreproduce the results. See the NeurIPS code and data submission guidelines ( https:\n//nips.cc/public/guides/CodeSubmissionPolicy ) for more details.\n\u2022The authors should provide instructions on data access and preparation, including how\nto access the raw data, preprocessed data, intermediate data, and generated data, etc.\n\u2022The authors should provide scripts to reproduce all experimental results for the new\nproposed method and baselines. If only a subset of experiments are reproducible, they\nshould state which ones are omitted from the script and why.\n\u2022At submission time, to preserve anonymity, the authors should release anonymized\nversions (if applicable).\n26\u2022Providing as much information as possible in supplemental material (appended to the\npaper) is recommended, but including URLs to data and code is permitted.\n6.Experimental setting/details\nQuestion: Does the paper specify all the training and test details (e.g., data splits, hyper-\nparameters, how they were chosen, type of optimizer, etc.) necessary to understand the\nresults?\nAnswer: [Yes] .\nJustification: The system code is uploaded to GitHub (anonymous links are provided in the\nSection 1). Besides, in Section 3.1, Listing A.4 and Appendix A.7, we provide full details\nabout experimental settings.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022The experimental setting should be presented in the core of the paper to a level of detail\nthat is necessary to appreciate the results and make sense of them.\n\u2022The full details can be provided either with the code, in appendix, or as supplemental\nmaterial.\n7.Experiment statistical significance\nQuestion: Does the paper report error bars suitably and correctly defined or other appropriate\ninformation about the statistical significance of the experiments?\nAnswer: [Yes] .\nJustification: The error value is the average of three trials of the generation, with a constant\nLLM Temperature of 0 in Table 5, Table 6, Table 7, Table 8 and Table 9. No error bars in\nretrieval because the test dataset and corpus are identical, and the retriever (BGE-LARGE)\nhas fixed algorithms and parameters, resulting in deterministic inference.\nGuidelines:\n\u2022 The answer NA means that the paper does not include experiments.\n\u2022The authors should answer \"Yes\" if the results are accompanied by error bars, confi-\ndence intervals, or statistical significance tests, at least for the experiments that support\nthe main claims of the paper.\n\u2022The factors of variability that the error bars are capturing should be clearly stated (for\nexample, train/test split, initialization, random drawing of some parameter, or overall\nrun with given experimental conditions).\n\u2022The method for calculating the error bars should be explained (closed form formula,\ncall to a library function, bootstrap, etc.)\n\u2022 The assumptions made should be given (e.g., Normally distributed errors).\n\u2022It should be clear whether the error bar is the standard deviation or the standard error\nof the mean.\n\u2022It is OK to report 1-sigma error bars, but one should state it. The authors should\npreferably report a 2-sigma error bar than state that they have a 96% CI, if the hypothesis\nof Normality of errors is not verified.\n\u2022For asymmetric distributions, the authors should be careful not to show in tables or\nfigures symmetric error bars that would yield results that are out of range (e.g. negative\nerror rates).\n\u2022If error bars are reported in tables or plots, The authors should explain in the text how\nthey were calculated and reference the corresponding figures or tables in the text.\n8.Experiments compute resources\nQuestion: For each experiment, does the paper provide sufficient information on the com-\nputer resources (type of compute workers, memory, time of execution) needed to reproduce\nthe experiments?\nAnswer: [Yes] .\nJustification: in Appendix D.\nGuidelines:\n27\u2022 The answer NA means that the paper does not include experiments.\n\u2022The paper should indicate the type of compute workers CPU or GPU, internal cluster,\nor cloud provider, including relevant memory and storage.\n\u2022The paper should provide the amount of compute required for each of the individual\nexperimental runs as well as estimate the total compute.\n\u2022The paper should disclose whether the full research project required more compute\nthan the experiments reported in the paper (e.g., preliminary or failed experiments that\ndidn\u2019t make it into the paper).\n9.Code of ethics\nQuestion: Does the research conducted in the paper conform, in every respect, with the\nNeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines ?\nAnswer: [Yes] .\nJustification: Reviewed and Confrmed.\nGuidelines:\n\u2022The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics.\n\u2022If the authors answer No, they should explain the special circumstances that require a\ndeviation from the Code of Ethics.\n\u2022The authors should make sure to preserve anonymity (e.g., if there is a special consid-\neration due to laws or regulations in their jurisdiction).\n10.Broader impacts\nQuestion: Does the paper discuss both potential positive societal impacts and negative\nsocietal impacts of the work performed?\nAnswer: [Yes] .\nJustification: in Section 5.\nGuidelines:\n\u2022 The answer NA means that there is no societal impact of the work performed.\n\u2022If the authors answer NA or No, they should explain why their work has no societal\nimpact or why the paper does not address societal impact.\n\u2022Examples of negative societal impacts include potential malicious or unintended uses\n(e.g., disinformation, generating fake profiles, surveillance), fairness considerations\n(e.g., deployment of technologies that could make decisions that unfairly impact specific\ngroups), privacy considerations, and security considerations.\n\u2022The conference expects that many papers will be foundational research and not tied\nto particular applications, let alone deployments. However, if there is a direct path to\nany negative applications, the authors should point it out. For example, it is legitimate\nto point out that an improvement in the quality of generative models could be used to\ngenerate deepfakes for disinformation. On the other hand, it is not needed to point out\nthat a generic algorithm for optimizing neural networks could enable people to train\nmodels that generate Deepfakes faster.\n\u2022The authors should consider possible harms that could arise when the technology is\nbeing used as intended and functioning correctly, harms that could arise when the\ntechnology is being used as intended but gives incorrect results, and harms following\nfrom (intentional or unintentional) misuse of the technology.\n\u2022If there are negative societal impacts, the authors could also discuss possible mitigation\nstrategies (e.g., gated release of models, providing defenses in addition to attacks,\nmechanisms for monitoring misuse, mechanisms to monitor how a system learns from\nfeedback over time, improving the efficiency and accessibility of ML).\n11.Safeguards\nQuestion: Does the paper describe safeguards that have been put in place for responsible\nrelease of data or models that have a high risk for misuse (e.g., pretrained language models,\nimage generators, or scraped datasets)?\nAnswer: [NA] .\n28Justification: All the data and model we use is publicly available. There is no high risk for\nmisuse.\nGuidelines:\n\u2022 The answer NA means that the paper poses no such risks.\n\u2022Released models that have a high risk for misuse or dual-use should be released with\nnecessary safeguards to allow for controlled use of the model, for example by requiring\nthat users adhere to usage guidelines or restrictions to access the model or implementing\nsafety filters.\n\u2022Datasets that have been scraped from the Internet could pose safety risks. The authors\nshould describe how they avoided releasing unsafe images.\n\u2022We recognize that providing effective safeguards is challenging, and many papers do\nnot require this, but we encourage authors to take this into account and make a best\nfaith effort.\n12.Licenses for existing assets\nQuestion: Are the creators or original owners of assets (e.g., code, data, models), used in\nthe paper, properly credited and are the license and terms of use explicitly mentioned and\nproperly respected?\nAnswer:[Yes] .\nJustification: We have cited the original paper or attached the link to the existing assets used\nin this paper.\nGuidelines:\n\u2022 The answer NA means that the paper does not use existing assets.\n\u2022 The authors should cite the original paper that produced the code package or dataset.\n\u2022The authors should state which version of the asset is used and, if possible, include a\nURL.\n\u2022 The name of the license (e.g., CC-BY 4.0) should be included for each asset.\n\u2022For scraped data from a particular source (e.g., website), the copyright and terms of\nservice of that source should be provided.\n\u2022If assets are released, the license, copyright information, and terms of use in the\npackage should be provided. For popular datasets, paperswithcode.com/datasets\nhas curated licenses for some datasets. Their licensing guide can help determine the\nlicense of a dataset.\n\u2022For existing datasets that are re-packaged, both the original license and the license of\nthe derived asset (if it has changed) should be provided.\n\u2022If this information is not available online, the authors are encouraged to reach out to\nthe asset\u2019s creators.\n13.New assets\nQuestion: Are new assets introduced in the paper well documented and is the documentation\nprovided alongside the assets?\nAnswer: [Yes] .\nJustification: We have stated the detailed versionand license of each asset. See the Ap-\npendix A.1.\nGuidelines:\n\u2022 The answer NA means that the paper does not release new assets.\n\u2022Researchers should communicate the details of the dataset/code/model as part of their\nsubmissions via structured templates. This includes details about training, license,\nlimitations, etc.\n\u2022The paper should discuss whether and how consent was obtained from people whose\nasset is used.\n\u2022At submission time, remember to anonymize your assets (if applicable). You can either\ncreate an anonymized URL or include an anonymized zip file.\n14.Crowdsourcing and research with human subjects\n29Question: For crowdsourcing experiments and research with human subjects, does the paper\ninclude the full text of instructions given to participants and screenshots, if applicable, as\nwell as details about compensation (if any)?\nAnswer: [NA] .\nJustification: : No crowdsourcing nor research with human subjects with used in this paper.\nGuidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022Including this information in the supplemental material is fine, but if the main contribu-\ntion of the paper involves human subjects, then as much detail as possible should be\nincluded in the main paper.\n\u2022According to the NeurIPS Code of Ethics, workers involved in data collection, curation,\nor other labor should be paid at least the minimum wage in the country of the data\ncollector.\n15.Institutional review board (IRB) approvals or equivalent for research with human\nsubjects\nQuestion: Does the paper describe potential risks incurred by study participants, whether\nsuch risks were disclosed to the subjects, and whether Institutional Review Board (IRB)\napprovals (or an equivalent approval/review based on the requirements of your country or\ninstitution) were obtained?\nAnswer:[NA] .\nJustification: : No crowdsourcing nor research with human subjects with used in this paper.\nGuidelines:\n\u2022The answer NA means that the paper does not involve crowdsourcing nor research with\nhuman subjects.\n\u2022Depending on the country in which research is conducted, IRB approval (or equivalent)\nmay be required for any human subjects research. If you obtained IRB approval, you\nshould clearly state this in the paper.\n\u2022We recognize that the procedures for this may vary significantly between institutions\nand locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the\nguidelines for their institution.\n\u2022For initial submissions, do not include any information that would break anonymity (if\napplicable), such as the institution conducting the review.\n16.Declaration of LLM usage\nQuestion: Does the paper describe the usage of LLMs if it is an important, original, or\nnon-standard component of the core methods in this research? Note that if the LLM is used\nonly for writing, editing, or formatting purposes and does not impact the core methodology,\nscientific rigorousness, or originality of the research, declaration is not required.\nAnswer: [NA] .\nJustification: Just used for editing (e.g., grammar, spelling, word choice).\nGuidelines:\n\u2022The answer NA means that the core method development in this research does not\ninvolve LLMs as any important, original, or non-standard components.\n\u2022Please refer to our LLM policy ( https://neurips.cc/Conferences/2025/LLM )\nfor what should or should not be described.\n30"}, "A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods": {"Declaration": "of Authorship\nI, Stefanos Gkikas, declare that this thesis titled, \u201cA Pain Assessment Framework based on multi-\nmodal data and Deep Machine Learning methods, \u201d and the work presented in it are my own. I\nconfirm that:\n\u2022 This work was done wholly or mainly while in candidature for a research degree at this University.\n\u2022 Where any part of this thesis has previously been submitted for a degree or any other qualification\nat this University or any other institution, this has been clearly stated.\n\u2022 Where I have consulted the published work of others, this is always clearly attributed.\n\u2022 Where I have quoted from the work of others, the source is always given. With the exception of\nsuch quotations, this thesis is entirely my own work.\n\u2022 I have acknowledged all main sources of help.\n\u2022 Where the thesis is based on work done by myself jointly with others, I have made clear exactly\nwhat was done by others and what I have contributed myself.\nSigned:\n \nDate: 26/10/2024\nvvi\u201cDo not go gentle into that good night,\nOld age should burn and rave at close of day;\nRage, rage against the dying of the light.\nThough wise men at their end know dark is right,\nBecause their words had forked no lightning they\nDo not go gentle into that good night.\nGood men, the last wave by, crying how bright\nTheir frail deeds might have danced in a green bay,\nRage, rage against the dying of the light.\nWild men who caught and sang the sun in flight,\nAnd learn, too late, they grieved it on its way,\nDo not go gentle into that good night.\nGrave men, near death, who see with blinding sight\nBlind eyes could blaze like meteors and be gay,\nRage, rage against the dying of the light.\nAnd you, my father, there on the sad height,\nCurse, bless, me now with your fierce tears, I pray.\nDo not go gentle into that good night.\nRage, rage against the dying of the light. \u201d\n\u2013 Dylan Thomas, 1947", "Acknowledgments": "First and foremost, I want to express my deepest gratitude to my supervisor, Manolis Tsik-\nnakis, for his invaluable support and guidance throughout my Ph.D. journey. The opportu-\nnities he provided, particularly my involvement in research projects and international sci-\nentific conferences, have profoundly shaped my perspective and worldview. Additionally,\nhe established a supportive and flexible framework that enabled me to work effectively and\nautonomously, significantly enriching my research experience and personal development \u2014\nI am sincerely thankful.\nI want to thank my family for their support throughout my life\u2014my father, Dimitris, my\nmother, Katerina, and my brother, Alexandros. I never say it to them in person, but I can\nwrite it here; maybe they\u2019ll read it someday, though probably not.\nLastly, I would like to acknowledge that the success of this Ph.D. project, both in terms of\nthe quantity and quality (whatever they may be) of the scientific work produced, was made\npossible by my dedication, ambition, and an almost ascetic lifestyle that allowed me to fi-\nnancially and mentally support myself.\nTo my daim \u00b4onion ( \u03b4\u03b1\u03b9\u03bc\u03cc\u03bd\u03b9\u03bf\u03bd )...\nTo Sarah ( \u03a3\u03ac\u03c1\u03b1 )...\nTo Piki ( \u03a0\u03af\u03ba\u03b9)...\nix", "Abstract": "Pain is a manifold condition that affects a large portion of the population. Accurate and\nreliable pain evaluation is essential for creating effective pain management strategies and\nprotocols. In cases where patients cannot communicate their pain, clinicians rely on ob-\nserving behavioral cues and monitoring vital signs. However, this approach is subjective\nand deficient in terms of continuous monitoring. Moreover, even in circumstances where\npatients are able to communicate, numerous challenges persist. Factors such as gender and\nage significantly influence how pain is perceived and expressed. Individuals with psycho-\nlogical disorders like depression and anxiety exhibit altered pain-related behaviors, further\ncomplicating the assessment. Human biases and beliefs, including racial and cultural dif-\nferences, also shape pain judgment and interpretation. Additionally, social dynamics and\npersonal relationships between the observer and the person experiencing pain can heavily\ndistort the assessment, further intensifying the challenges of accurate pain evaluation. Au-\ntomatic pain assessment can alleviate these challenges by providing continuous monitoring\nthrough computational systems that utilize designed algorithms to recognize pain indicators\nfrom input modalities, including videos and biosignals. In recent years, researchers have\nincreasingly adopted deep learning algorithms to capture and encode the multidimensional\naspects of pain into meaningful data attributes.\nThis thesis initially aims to study the pain assessment process from a clinical-theoretical\nperspective while exploring and examining existing automatic approaches. Building on this\nfoundation, the primary objective of this Ph.D. project is to develop innovative computational\nmethods for automatic pain assessment that achieve high performance and are applicable in\nreal clinical settings. A primary goal is to thoroughly investigate and assess significant fac-\ntors, including demographic elements that impact pain perception, as recognized in pain\nresearch, through a computational standpoint. Within the limits of the available data in this\nresearch area, our goal was to design, develop, propose, and offer automatic pain assessment\npipelines for unimodal and multimodal configurations that are applicable to the specific re-\nquirements of different scenarios. The studies published in this Ph.D. thesis showcased the\neffectiveness of the proposed methods, achieving state-of-the-art results. Additionally, they\npaved the way for exploring new approaches in artificial intelligence, foundation models,\nand generative artificial intelligence.\nxi\u03a0\u03b5\u03c1\u03af\u03bb\u03b7\u03c8\u03b7\n\u039f \u03c0\u03cc\u03bd\u03bf\u03c2 \u03b5\u03af\u03bd\u03b1\u03b9 \u03bc\u03b9\u03b1 \u03c0\u03bf\u03bb\u03c5\u03b5\u03c0\u03af\u03c0\u03b5\u03b4\u03b7 \u03ba\u03b1\u03c4\u03ac\u03c3\u03c4\u03b1\u03c3\u03b7 \u03c0\u03bf\u03c5 \u03b5\u03c0\u03b7\u03c1\u03b5\u03ac\u03b6\u03b5\u03b9 \u03bc\u03b5\u03b3\u03ac\u03bb\u03bf \u03bc\u03ad\u03c1\u03bf\u03c2 \u03c4\u03bf\u03c5 \u03c0\u03bb\u03b7\u03b8\u03c5\u03c3\u03bc\u03bf\u03cd. \u0397 \u03b1\u03ba\u03c1\u03b9\u03b2\u03ae\u03c2 \u03ba\u03b1\u03b9\n\u03b1\u03be\u03b9\u03cc\u03c0\u03b9\u03c3\u03c4\u03b7 \u03b1\u03be\u03b9\u03bf\u03bb\u03cc\u03b3\u03b7\u03c3\u03b7 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5 \u03b5\u03af\u03bd\u03b1\u03b9 \u03bf\u03c5\u03c3\u03b9\u03ce\u03b4\u03b7\u03c2 \u03b3\u03b9\u03b1 \u03c4\u03b7 \u03b4\u03b7\u03bc\u03b9\u03bf\u03c5\u03c1\u03b3\u03af\u03b1 \u03b1\u03c0\u03bf\u03c4\u03b5\u03bb\u03b5\u03c3\u03bc\u03b1\u03c4\u03b9\u03ba\u03ce\u03bd \u03c3\u03c4\u03c1\u03b1\u03c4\u03b7\u03b3\u03b9\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9\n\u03c0\u03c1\u03c9\u03c4\u03bf\u03ba\u03cc\u03bb\u03bb\u03c9\u03bd \u03b4\u03b9\u03b1\u03c7\u03b5\u03af\u03c1\u03b9\u03c3\u03b7\u03c2 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5. \u03a3\u03b5 \u03c0\u03b5\u03c1\u03b9\u03c0\u03c4\u03ce\u03c3\u03b5\u03b9\u03c2 \u03c0\u03bf\u03c5 \u03bf\u03b9 \u03b1\u03c3\u03b8\u03b5\u03bd\u03b5\u03af\u03c2 \u03b4\u03b5\u03bd \u03bc\u03c0\u03bf\u03c1\u03bf\u03cd\u03bd \u03bd\u03b1 \u03b5\u03c0\u03b9\u03ba\u03bf\u03b9\u03bd\u03c9\u03bd\u03ae\u03c3\u03bf\u03c5\u03bd \u03c4\u03bf\u03bd\n\u03c0\u03cc\u03bd\u03bf \u03c4\u03bf\u03c5\u03c2, \u03bf\u03b9 \u03ba\u03bb\u03b9\u03bd\u03b9\u03ba\u03bf\u03af \u03b2\u03b1\u03c3\u03af\u03b6\u03bf\u03bd\u03c4\u03b1\u03b9 \u03c3\u03c4\u03b7\u03bd \u03c0\u03b1\u03c1\u03b1\u03c4\u03ae\u03c1\u03b7\u03c3\u03b7 \u03c3\u03c5\u03bc\u03c0\u03b5\u03c1\u03b9\u03c6\u03bf\u03c1\u03b9\u03ba\u03ce\u03bd \u03b5\u03bd\u03b4\u03b5\u03af\u03be\u03b5\u03c9\u03bd \u03ba\u03b1\u03b9 \u03c3\u03c4\u03b7\u03bd \u03c0\u03b1\u03c1\u03b1\u03ba\u03bf\u03bb\u03bf\u03cd\u03b8\u03b7\u03c3\u03b7\n\u03b6\u03c9\u03c4\u03b9\u03ba\u03ce\u03bd \u03c3\u03b7\u03bc\u03b5\u03af\u03c9\u03bd. \u03a9\u03c3\u03c4\u03cc\u03c3\u03bf, \u03b1\u03c5\u03c4\u03ae \u03b7 \u03c0\u03c1\u03bf\u03c3\u03ad\u03b3\u03b3\u03b9\u03c3\u03b7 \u03b5\u03af\u03bd\u03b1\u03b9 \u03c5\u03c0\u03bf\u03ba\u03b5\u03b9\u03bc\u03b5\u03bd\u03b9\u03ba\u03ae \u03ba\u03b1\u03b9 \u03b5\u03bb\u03bb\u03b9\u03c0\u03ae\u03c2 \u03cc\u03c3\u03bf\u03bd \u03b1\u03c6\u03bf\u03c1\u03ac \u03c4\u03b7 \u03c3\u03c5\u03bd\u03b5\u03c7\u03ae\n\u03c0\u03b1\u03c1\u03b1\u03ba\u03bf\u03bb\u03bf\u03cd\u03b8\u03b7\u03c3\u03b7. \u0395\u03c0\u03b9\u03c0\u03bb\u03ad\u03bf\u03bd, \u03b1\u03ba\u03cc\u03bc\u03b7 \u03ba\u03b1\u03b9 \u03c3\u03b5 \u03c0\u03b5\u03c1\u03b9\u03c0\u03c4\u03ce\u03c3\u03b5\u03b9\u03c2 \u03cc\u03c0\u03bf\u03c5 \u03bf\u03b9 \u03b1\u03c3\u03b8\u03b5\u03bd\u03b5\u03af\u03c2 \u03bc\u03c0\u03bf\u03c1\u03bf\u03cd\u03bd \u03bd\u03b1 \u03b5\u03c0\u03b9\u03ba\u03bf\u03b9\u03bd\u03c9\u03bd\u03ae\u03c3\u03bf\u03c5\u03bd,\n\u03c5\u03c0\u03ac\u03c1\u03c7\u03bf\u03c5\u03bd \u03c0\u03bf\u03bb\u03bb\u03ad\u03c2 \u03c0\u03c1\u03bf\u03ba\u03bb\u03ae\u03c3\u03b5\u03b9\u03c2. \u03a0\u03b1\u03c1\u03ac\u03b3\u03bf\u03bd\u03c4\u03b5\u03c2 \u03cc\u03c0\u03c9\u03c2 \u03c4\u03bf \u03c6\u03cd\u03bb\u03bf \u03ba\u03b1\u03b9 \u03b7 \u03b7\u03bb\u03b9\u03ba\u03af\u03b1 \u03b5\u03c0\u03b7\u03c1\u03b5\u03ac\u03b6\u03bf\u03c5\u03bd \u03c3\u03b7\u03bc\u03b1\u03bd\u03c4\u03b9\u03ba\u03ac \u03c4\u03bf\u03bd \u03c4\u03c1\u03cc\u03c0\u03bf\n\u03c0\u03bf\u03c5 \u03bf \u03c0\u03cc\u03bd\u03bf\u03c2 \u03b1\u03bd\u03c4\u03b9\u03bb\u03b1\u03bc\u03b2\u03ac\u03bd\u03b5\u03c4\u03b1\u03b9 \u03ba\u03b1\u03b9 \u03b5\u03ba\u03c6\u03c1\u03ac\u03b6\u03b5\u03c4\u03b1\u03b9. \u0384\u0391\u03c4\u03bf\u03bc\u03b1 \u03bc\u03b5 \u03c8\u03c5\u03c7\u03bf\u03bb\u03bf\u03b3\u03b9\u03ba\u03ad\u03c2 \u03b4\u03b9\u03b1\u03c4\u03b1\u03c1\u03b1\u03c7\u03ad\u03c2 \u03cc\u03c0\u03c9\u03c2 \u03b7 \u03ba\u03b1\u03c4\u03ac\u03b8\u03bb\u03b9\u03c8\u03b7 \u03ba\u03b1\u03b9\n\u03b7 \u03b1\u03b3\u03c7\u03ce\u03b4\u03b7\u03c2 \u03b4\u03b9\u03b1\u03c4\u03b1\u03c1\u03b1\u03c7\u03ae \u03b5\u03ba\u03b4\u03b7\u03bb\u03ce\u03bd\u03bf\u03c5\u03bd \u03c4\u03c1\u03bf\u03c0\u03bf\u03c0\u03bf\u03b9\u03b7\u03bc\u03ad\u03bd\u03b5\u03c2 \u03c3\u03c5\u03bc\u03c0\u03b5\u03c1\u03b9\u03c6\u03bf\u03c1\u03ad\u03c2 \u03c3\u03c7\u03b5\u03c4\u03b9\u03ba\u03ad\u03c2 \u03bc\u03b5 \u03c4\u03bf\u03bd \u03c0\u03cc\u03bd\u03bf, \u03b4\u03c5\u03c3\u03c7\u03b5\u03c1\u03b1\u03af\u03bd\u03bf\u03bd\u03c4\u03b1\u03c2\n\u03c0\u03b5\u03c1\u03b1\u03b9\u03c4\u03ad\u03c1\u03c9 \u03c4\u03b7\u03bd \u03b1\u03be\u03b9\u03bf\u03bb\u03cc\u03b3\u03b7\u03c3\u03b7. \u039f\u03b9 \u03b1\u03bd\u03b8\u03c1\u03ce\u03c0\u03b9\u03bd\u03b5\u03c2 \u03c0\u03c1\u03bf\u03ba\u03b1\u03c4\u03b1\u03bb\u03ae\u03c8\u03b5\u03b9\u03c2 \u03ba\u03b1\u03b9 \u03c0\u03b5\u03c0\u03bf\u03b9\u03b8\u03ae\u03c3\u03b5\u03b9\u03c2, \u03c0\u03b5\u03c1\u03b9\u03bb\u03b1\u03bc\u03b2\u03b1\u03bd\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd \u03c4\u03c9\u03bd \u03c6\u03c5\u03bb\u03b5-\n\u03c4\u03b9\u03ba\u03ce\u03bd \u03ba\u03b1\u03b9 \u03c0\u03bf\u03bb\u03b9\u03c4\u03b9\u03c3\u03bc\u03b9\u03ba\u03ce\u03bd \u03b4\u03b9\u03b1\u03c6\u03bf\u03c1\u03ce\u03bd, \u03b5\u03c0\u03af\u03c3\u03b7\u03c2 \u03b4\u03b9\u03b1\u03bc\u03bf\u03c1\u03c6\u03ce\u03bd\u03bf\u03c5\u03bd \u03c4\u03b7\u03bd \u03ba\u03c1\u03af\u03c3\u03b7 \u03ba\u03b1\u03b9 \u03c4\u03b7\u03bd \u03b5\u03c1\u03bc\u03b7\u03bd\u03b5\u03af\u03b1 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5. \u0395\u03c0\u03b9\u03c0\u03bb\u03ad\u03bf\u03bd,\n\u03bf\u03b9 \u03ba\u03bf\u03b9\u03bd\u03c9\u03bd\u03b9\u03ba\u03ad\u03c2 \u03b4\u03c5\u03bd\u03b1\u03bc\u03b9\u03ba\u03ad\u03c2 \u03ba\u03b1\u03b9 \u03bf\u03b9 \u03c0\u03c1\u03bf\u03c3\u03c9\u03c0\u03b9\u03ba\u03ad\u03c2 \u03c3\u03c7\u03ad\u03c3\u03b5\u03b9\u03c2 \u03bc\u03b5\u03c4\u03b1\u03be\u03cd \u03c4\u03bf\u03c5 \u03c0\u03b1\u03c1\u03b1\u03c4\u03b7\u03c1\u03b7\u03c4\u03ae \u03ba\u03b1\u03b9 \u03c4\u03bf\u03c5 \u03b1\u03c4\u03cc\u03bc\u03bf\u03c5 \u03c0\u03bf\u03c5 \u03b2\u03b9\u03ce\u03bd\u03b5\u03b9\n\u03c4\u03bf\u03bd \u03c0\u03cc\u03bd\u03bf \u03bc\u03c0\u03bf\u03c1\u03bf\u03cd\u03bd \u03bd\u03b1 \u03b4\u03b9\u03b1\u03c3\u03c4\u03c1\u03b5\u03b2\u03bb\u03ce\u03c3\u03bf\u03c5\u03bd \u03c3\u03b7\u03bc\u03b1\u03bd\u03c4\u03b9\u03ba\u03ac \u03c4\u03b7\u03bd \u03b1\u03be\u03b9\u03bf\u03bb\u03cc\u03b3\u03b7\u03c3\u03b7, \u03b5\u03bd\u03c4\u03b5\u03af\u03bd\u03bf\u03bd\u03c4\u03b1\u03c2 \u03c0\u03b5\u03c1\u03b1\u03b9\u03c4\u03ad\u03c1\u03c9 \u03c4\u03b9\u03c2 \u03c0\u03c1\u03bf\u03ba\u03bb\u03ae\u03c3\u03b5\u03b9\u03c2\n\u03c4\u03b7\u03c2 \u03b1\u03ba\u03c1\u03b9\u03b2\u03bf\u03cd\u03c2 \u03b1\u03be\u03b9\u03bf\u03bb\u03cc\u03b3\u03b7\u03c3\u03b7\u03c2 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5. \u0397 \u03b1\u03c5\u03c4\u03cc\u03bc\u03b1\u03c4\u03b7 \u03b1\u03be\u03b9\u03bf\u03bb\u03cc\u03b3\u03b7\u03c3\u03b7 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5 \u03bc\u03c0\u03bf\u03c1\u03b5\u03af \u03bd\u03b1 \u03b1\u03bc\u03b2\u03bb\u03cd\u03bd\u03b5\u03b9 \u03b1\u03c5\u03c4\u03ad\u03c2 \u03c4\u03b9\u03c2\n\u03c0\u03c1\u03bf\u03ba\u03bb\u03ae\u03c3\u03b5\u03b9\u03c2 \u03c0\u03b1\u03c1\u03ad\u03c7\u03bf\u03bd\u03c4\u03b1\u03c2 \u03c3\u03c5\u03bd\u03b5\u03c7\u03ae \u03c0\u03b1\u03c1\u03b1\u03ba\u03bf\u03bb\u03bf\u03cd\u03b8\u03b7\u03c3\u03b7 \u03bc\u03ad\u03c3\u03c9 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03b9\u03ba\u03ce\u03bd \u03c3\u03c5\u03c3\u03c4\u03b7\u03bc\u03ac\u03c4\u03c9\u03bd \u03c0\u03bf\u03c5 \u03c7\u03c1\u03b7\u03c3\u03b9\u03bc\u03bf\u03c0\u03bf\u03b9\u03bf\u03cd\u03bd\n\u03b5\u03b9\u03b4\u03b9\u03ba\u03ac \u03c3\u03c7\u03b5\u03b4\u03b9\u03b1\u03c3\u03bc\u03ad\u03bd\u03bf\u03c5\u03c2 \u03b1\u03bb\u03b3\u03cc\u03c1\u03b9\u03b8\u03bc\u03bf\u03c5\u03c2 \u03b3\u03b9\u03b1 \u03bd\u03b1 \u03b1\u03bd\u03b1\u03b3\u03bd\u03c9\u03c1\u03af\u03c3\u03bf\u03c5\u03bd \u03b5\u03bd\u03b4\u03b5\u03af\u03be\u03b5\u03b9\u03c2 \u03c0\u03cc\u03bd\u03bf\u03c5 \u03b1\u03c0\u03cc \u03b4\u03b9\u03b1\u03c6\u03cc\u03c1\u03bf\u03c5\u03c2 \u03c4\u03cd\u03c0\u03bf\u03c5\u03c2 \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad-\n\u03bd\u03c9\u03bd, \u03c3\u03c5\u03bc\u03c0\u03b5\u03c1\u03b9\u03bb\u03b1\u03bc\u03b2\u03b1\u03bd\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd \u03c4\u03c9\u03bd \u03b2\u03af\u03bd\u03c4\u03b5\u03bf \u03ba\u03b1\u03b9 \u03c4\u03c9\u03bd \u03b2\u03b9\u03bf\u03c3\u03b7\u03bc\u03ac\u03c4\u03c9\u03bd. \u03a4\u03b1 \u03c4\u03b5\u03bb\u03b5\u03c5\u03c4\u03b1\u03af\u03b1 \u03c7\u03c1\u03cc\u03bd\u03b9\u03b1, \u03bf\u03b9 \u03b5\u03c1\u03b5\u03c5\u03bd\u03b7\u03c4\u03ad\u03c2 \u03ad\u03c7\u03bf\u03c5\u03bd\n\u03c5\u03b9\u03bf\u03b8\u03b5\u03c4\u03ae\u03c3\u03b5\u03b9 \u03bf\u03bb\u03bf\u03ad\u03bd\u03b1 \u03ba\u03b1\u03b9 \u03c0\u03b5\u03c1\u03b9\u03c3\u03c3\u03cc\u03c4\u03b5\u03c1\u03bf \u03b1\u03bb\u03b3\u03cc\u03c1\u03b9\u03b8\u03bc\u03bf\u03c5\u03c2 \u03b2\u03b1\u03b8\u03b9\u03ac\u03c2 \u03bc\u03ac\u03b8\u03b7\u03c3\u03b7\u03c2 \u03b3\u03b9\u03b1 \u03bd\u03b1 \u03b1\u03c0\u03bf\u03c4\u03c5\u03c0\u03ce\u03c3\u03bf\u03c5\u03bd \u03ba\u03b1\u03b9 \u03bd\u03b1 \u03ba\u03c9\u03b4\u03b9\u03ba\u03bf\u03c0\u03bf\u03b9-\n\u03ae\u03c3\u03bf\u03c5\u03bd \u03c4\u03b9\u03c2 \u03c0\u03bf\u03bb\u03c5\u03b4\u03b9\u03ac\u03c3\u03c4\u03b1\u03c4\u03b5\u03c2 \u03c0\u03c4\u03c5\u03c7\u03ad\u03c2 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5 \u03c3\u03b5 \u03c7\u03c1\u03ae\u03c3\u03b9\u03bc\u03b1 \u03b4\u03b5\u03b4\u03bf\u03bc\u03b5\u03bd\u03b9\u03ba\u03ac \u03c7\u03b1\u03c1\u03b1\u03ba\u03c4\u03b7\u03c1\u03b9\u03c3\u03c4\u03b9\u03ba\u03ac.\n\u0397 \u03c0\u03b1\u03c1\u03bf\u03cd\u03c3\u03b1 \u03b4\u03b9\u03b4\u03b1\u03ba\u03c4\u03bf\u03c1\u03b9\u03ba\u03ae \u03b4\u03b9\u03b1\u03c4\u03c1\u03b9\u03b2\u03ae \u03b1\u03c1\u03c7\u03b9\u03ba\u03ac \u03c3\u03c4\u03bf\u03c7\u03b5\u03cd\u03b5\u03b9 \u03bd\u03b1 \u03bc\u03b5\u03bb\u03b5\u03c4\u03ae\u03c3\u03b5\u03b9 \u03c4\u03b7 \u03b4\u03b9\u03b1\u03b4\u03b9\u03ba\u03b1\u03c3\u03af\u03b1 \u03b1\u03be\u03b9\u03bf\u03bb\u03cc\u03b3\u03b7\u03c3\u03b7\u03c2 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5\n\u03b1\u03c0\u03cc \u03bc\u03b9\u03b1 \u03ba\u03bb\u03b9\u03bd\u03b9\u03ba\u03bf-\u03b8\u03b5\u03c9\u03c1\u03b7\u03c4\u03b9\u03ba\u03ae \u03c0\u03c1\u03bf\u03bf\u03c0\u03c4\u03b9\u03ba\u03ae \u03b5\u03bd\u03ce \u03b4\u03b9\u03b5\u03c1\u03b5\u03c5\u03bd\u03ac \u03ba\u03b1\u03b9 \u03b5\u03be\u03b5\u03c4\u03ac\u03b6\u03b5\u03b9 \u03c4\u03b9\u03c2 \u03c5\u03c0\u03ac\u03c1\u03c7\u03bf\u03c5\u03c3\u03b5\u03c2 \u03b1\u03c5\u03c4\u03cc\u03bc\u03b1\u03c4\u03b5\u03c2 \u03c0\u03c1\u03bf\u03c3\u03b5\u03b3\u03b3\u03af\u03c3\u03b5\u03b9\u03c2.\n\u03a3\u03c4\u03b7\u03c1\u03b9\u03b6\u03cc\u03bc\u03b5\u03bd\u03b7 \u03c3\u03b5 \u03b1\u03c5\u03c4\u03ae \u03c4\u03b7 \u03b2\u03ac\u03c3\u03b7, \u03bf \u03ba\u03cd\u03c1\u03b9\u03bf\u03c2 \u03c3\u03c4\u03cc\u03c7\u03bf\u03c2 \u03b1\u03c5\u03c4\u03bf\u03cd \u03c4\u03bf\u03c5 \u03b4\u03b9\u03b4\u03b1\u03ba\u03c4\u03bf\u03c1\u03b9\u03ba\u03bf\u03cd \u03ad\u03c1\u03b3\u03bf\u03c5 \u03b5\u03af\u03bd\u03b1\u03b9 \u03bd\u03b1 \u03b1\u03bd\u03b1\u03c0\u03c4\u03cd\u03be\u03b5\u03b9 \u03ba\u03b1\u03b9\u03bd\u03bf-\n\u03c4\u03cc\u03bc\u03b5\u03c2 \u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03b9\u03ba\u03ad\u03c2 \u03bc\u03b5\u03b8\u03cc\u03b4\u03bf\u03c5\u03c2 \u03b3\u03b9\u03b1 \u03b1\u03c5\u03c4\u03cc\u03bc\u03b1\u03c4\u03b7 \u03b1\u03be\u03b9\u03bf\u03bb\u03cc\u03b3\u03b7\u03c3\u03b7 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5 \u03c0\u03bf\u03c5 \u03b5\u03c0\u03b9\u03c4\u03c5\u03b3\u03c7\u03ac\u03bd\u03bf\u03c5\u03bd \u03c5\u03c8\u03b7\u03bb\u03ae \u03b1\u03c0\u03cc\u03b4\u03bf\u03c3\u03b7\n\u03ba\u03b1\u03b9 \u03b5\u03af\u03bd\u03b1\u03b9 \u03b5\u03c6\u03b1\u03c1\u03bc\u03cc\u03c3\u03b9\u03bc\u03b5\u03c2 \u03c3\u03b5 \u03c0\u03c1\u03b1\u03b3\u03bc\u03b1\u03c4\u03b9\u03ba\u03ac \u03ba\u03bb\u03b9\u03bd\u03b9\u03ba\u03ac \u03c0\u03b5\u03c1\u03b9\u03b2\u03ac\u03bb\u03bb\u03bf\u03bd\u03c4\u03b1. \u0384\u0395\u03bd\u03b1\u03c2 \u03c0\u03c1\u03c9\u03c4\u03b1\u03c1\u03c7\u03b9\u03ba\u03cc\u03c2 \u03c3\u03c4\u03cc\u03c7\u03bf\u03c2 \u03b5\u03af\u03bd\u03b1\u03b9 \u03bd\u03b1 \u03b4\u03b9\u03b5\u03c1\u03b5\u03c5-\n\u03bd\u03ae\u03c3\u03b5\u03b9 \u03ba\u03b1\u03b9 \u03bd\u03b1 \u03b1\u03be\u03b9\u03bf\u03bb\u03bf\u03b3\u03ae\u03c3\u03b5\u03b9 \u03c3\u03b5 \u03b2\u03ac\u03b8\u03bf\u03c2 \u03c3\u03b7\u03bc\u03b1\u03bd\u03c4\u03b9\u03ba\u03bf\u03cd\u03c2 \u03c0\u03b1\u03c1\u03ac\u03b3\u03bf\u03bd\u03c4\u03b5\u03c2, \u03c3\u03c5\u03bc\u03c0\u03b5\u03c1\u03b9\u03bb\u03b1\u03bc\u03b2\u03b1\u03bd\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd \u03c4\u03c9\u03bd \u03b4\u03b7\u03bc\u03bf\u03b3\u03c1\u03b1\u03c6\u03b9\u03ba\u03ce\u03bd\n\u03c3\u03c4\u03bf\u03b9\u03c7\u03b5\u03af\u03c9\u03bd \u03c0\u03bf\u03c5 \u03b5\u03c0\u03b7\u03c1\u03b5\u03ac\u03b6\u03bf\u03c5\u03bd \u03c4\u03b7\u03bd \u03b1\u03bd\u03c4\u03af\u03bb\u03b7\u03c8\u03b7 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5, \u03cc\u03c0\u03c9\u03c2 \u03b1\u03bd\u03b1\u03b3\u03bd\u03c9\u03c1\u03af\u03b6\u03bf\u03bd\u03c4\u03b1\u03b9 \u03c3\u03c4\u03b7\u03bd \u03ad\u03c1\u03b5\u03c5\u03bd\u03b1 \u03c0\u03cc\u03bd\u03bf\u03c5, \u03b1\u03c0\u03cc \u03bc\u03b9\u03b1\n\u03c5\u03c0\u03bf\u03bb\u03bf\u03b3\u03b9\u03c3\u03c4\u03b9\u03ba\u03ae \u03c3\u03ba\u03bf\u03c0\u03b9\u03ac. \u0395\u03bd\u03c4\u03cc\u03c2 \u03c4\u03c9\u03bd \u03bf\u03c1\u03af\u03c9\u03bd \u03c4\u03c9\u03bd \u03b4\u03b9\u03b1\u03b8\u03ad\u03c3\u03b9\u03bc\u03c9\u03bd \u03b4\u03b5\u03b4\u03bf\u03bc\u03ad\u03bd\u03c9\u03bd \u03c3\u03b5 \u03b1\u03c5\u03c4\u03cc \u03c4\u03bf\u03bd \u03c4\u03bf\u03bc\u03ad\u03b1 \u03ad\u03c1\u03b5\u03c5\u03bd\u03b1\u03c2, \u03bf \u03c3\u03c4\u03cc-\n\u03c7\u03bf\u03c2 \u03bc\u03b1\u03c2 \u03ae\u03c4\u03b1\u03bd \u03bd\u03b1 \u03c3\u03c7\u03b5\u03b4\u03b9\u03ac\u03c3\u03bf\u03c5\u03bc\u03b5, \u03bd\u03b1 \u03b1\u03bd\u03b1\u03c0\u03c4\u03cd\u03be\u03bf\u03c5\u03bc\u03b5, \u03bd\u03b1 \u03c0\u03c1\u03bf\u03c4\u03b5\u03af\u03bd\u03bf\u03c5\u03bc\u03b5 \u03ba\u03b1\u03b9 \u03bd\u03b1 \u03c0\u03c1\u03bf\u03c3\u03c6\u03ad\u03c1\u03bf\u03c5\u03bc\u03b5 \u03b1\u03c5\u03c4\u03cc\u03bc\u03b1\u03c4\u03b5\u03c2 \u03b1\u03bb\u03c5\u03c3\u03af\u03b4\u03b5\u03c2\n\u03b5\u03c0\u03b5\u03be\u03b5\u03c1\u03b3\u03b1\u03c3\u03af\u03b1\u03c2 \u03b1\u03be\u03b9\u03bf\u03bb\u03cc\u03b3\u03b7\u03c3\u03b7\u03c2 \u03c4\u03bf\u03c5 \u03c0\u03cc\u03bd\u03bf\u03c5 \u03b3\u03b9\u03b1 \u03bc\u03bf\u03bd\u03bf\u03bc\u03bf\u03c1\u03c6\u03b9\u03ba\u03ad\u03c2 \u03ba\u03b1\u03b9 \u03c0\u03bf\u03bb\u03c5\u03bc\u03bf\u03c1\u03c6\u03b9\u03ba\u03ad\u03c2 \u03b4\u03b9\u03b1\u03bc\u03bf\u03c1\u03c6\u03ce\u03c3\u03b5\u03b9\u03c2 \u03c0\u03bf\u03c5 \u03b5\u03af\u03bd\u03b1\u03b9 \u03b5\u03c6\u03b1\u03c1-\n\u03bc\u03cc\u03c3\u03b9\u03bc\u03b5\u03c2 \u03c3\u03c4\u03b9\u03c2 \u03c3\u03c5\u03b3\u03ba\u03b5\u03ba\u03c1\u03b9\u03bc\u03ad\u03bd\u03b5\u03c2 \u03b1\u03c0\u03b1\u03b9\u03c4\u03ae\u03c3\u03b5\u03b9\u03c2 \u03b4\u03b9\u03b1\u03c6\u03bf\u03c1\u03b5\u03c4\u03b9\u03ba\u03ce\u03bd \u03c3\u03b5\u03bd\u03b1\u03c1\u03af\u03c9\u03bd. \u039f\u03b9 \u03bc\u03b5\u03bb\u03ad\u03c4\u03b5\u03c2 \u03c0\u03bf\u03c5 \u03b4\u03b7\u03bc\u03bf\u03c3\u03b9\u03b5\u03cd\u03c4\u03b7\u03ba\u03b1\u03bd \u03c3\u03b5 \u03b1\u03c5\u03c4\u03ae\n\u03c4\u03b7 \u03b4\u03b9\u03b4\u03b1\u03ba\u03c4\u03bf\u03c1\u03b9\u03ba\u03ae \u03b4\u03b9\u03b1\u03c4\u03c1\u03b9\u03b2\u03ae \u03c0\u03b1\u03c1\u03bf\u03c5\u03c3\u03af\u03b1\u03c3\u03b1\u03bd \u03c4\u03b7\u03bd \u03b1\u03c0\u03bf\u03b4\u03bf\u03c4\u03b9\u03ba\u03cc\u03c4\u03b7\u03c4\u03b1 \u03c4\u03c9\u03bd \u03c0\u03c1\u03bf\u03c4\u03b5\u03b9\u03bd\u03cc\u03bc\u03b5\u03bd\u03c9\u03bd \u03bc\u03b5\u03b8\u03cc\u03b4\u03c9\u03bd, \u03b5\u03c0\u03b9\u03c4\u03c5\u03b3\u03c7\u03ac\u03bd\u03bf\u03bd\u03c4\u03b1\u03c2\n\u03c0\u03c1\u03c9\u03c4\u03bf\u03c0\u03cc\u03c1\u03b1 \u03b5\u03c0\u03b9\u03c4\u03b5\u03cd\u03b3\u03bc\u03b1\u03c4\u03b1. \u0395\u03c0\u03b9\u03c0\u03bb\u03ad\u03bf\u03bd, \u03ba\u03b1\u03b9\u03bd\u03bf\u03c4\u03cc\u03bc\u03b7\u03c3\u03b1\u03bd \u03c3\u03c4\u03b7\u03bd \u03b5\u03be\u03b5\u03c1\u03b5\u03cd\u03bd\u03b7\u03c3\u03b7 \u03bd\u03ad\u03c9\u03bd \u03c0\u03c1\u03bf\u03c3\u03b5\u03b3\u03b3\u03af\u03c3\u03b5\u03c9\u03bd \u03c3\u03c4\u03b7\u03bd \u03c4\u03b5\u03c7\u03bd\u03b7\u03c4\u03ae\n\u03bd\u03bf\u03b7\u03bc\u03bf\u03c3\u03cd\u03bd\u03b7, \u03c4\u03b1 \u03bc\u03bf\u03bd\u03c4\u03ad\u03bb\u03b1 \u03b8\u03b5\u03bc\u03b5\u03bb\u03af\u03c9\u03c3\u03b7\u03c2 \u03ba\u03b1\u03b9 \u03c4\u03b7\u03bd \u03b3\u03b5\u03bd\u03b5\u03c4\u03b9\u03ba\u03ae \u03c4\u03b5\u03c7\u03bd\u03b7\u03c4\u03ae \u03bd\u03bf\u03b7\u03bc\u03bf\u03c3\u03cd\u03bd\u03b7.\nxiiiContents\nDeclaration v\nAcknowledgments ix\nAbstract xi", "Abstract in Greek": "xiii", "Table of Contents": "xv", "List of Figures": "xix", "List of Tables": "xxiii\n1", "Introduction": "1\n1.1", "Context and Motivation": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2", "Scope and Challenges": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3", "Contributions \u2013 Peer-review Publications": ". . . . . . . . . . . . . . . . . . . 5\n1.4", "Thesis Outline": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n2", "Clinical Pain Assessment": "9\n2.1", "Chapter Overview": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nxv2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . 20\n3 Automatic Pain Assessment\u2013A Literature Review 21\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2", "Biology of Pain": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3", "Classification and Characteristics of Pain": ". . . . . . . . . . . . . . . . . . . 11\n2.4", "Pain Indicators": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1", "Behavioral Indicators": ". . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2", "Physiological Indicators": ". . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3", "Biochemical Indicators": ". . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5", "Sociodemographic and Psychological Variables": ". . . . . . . . . . . . . . . 15\n2.5.1", "Sex and Gender": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2", "Age": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3", "Psychological Factors": ". . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4", "Race and Culture": ". . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nxv2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . 20\n3 Automatic Pain Assessment\u2013A Literature Review 21\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain\nDatabase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . 49\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research\nDirections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4 Demographic Variables: Their Role and Impact 53\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5 Optimization: Balancing Efficiency and Performance 75\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n6 Synthetic Data: The Role of Thermal Imaging 103\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7 General-Purpose Models 117\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n8 Conclusions, Perspectives and Future Work 157\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nBibliography 163\nAppendix 201\nAcronyms 209List of Figures\n2.1 The spinothalamic tract (STT) [43]. Pain, temperature, and some touch affer-\nents end in the posterior horn, where second-order fibers cross the midline\nto form the spinothalamic tract, ascending to the thalamus and projecting to\nvarious cortical areas. Along the way, collaterals connect to the reticular for-\nmation. Due to the rostral inclination of fibers in Lissauer\u2019s tract, cordotomy\nmust be performed several segments above the pain level for effective relief. 12\n2.2 Pain classification [48]: (A)Nociceptive pain , which results from detecting\npotentially harmful stimuli and serves a protective function. (B)Inflamma-\ntory pain is linked to tissue damage and immune cell infiltration, increas-\ning pain sensitivity during healing. (C)Pathological pain is a disease state\ncaused by either nervous system damage (neuropathic) or abnormal nervous\nsystem function (dysfunctional). . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 The number of studies utilizing these specific datasets. Note that various\nstudies used multiple datasets to conduct their experiments. . . . . . . . . . 25\n4.1 The PQRST waveform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure. . 55\n4.3 The signal preprocessing using the Pan-Tompkins algorithm. . . . . . . . . 57\n4.4 Results for the Gender Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Results for the Age Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.6 Results for the Gender-Age Scheme . . . . . . . . . . . . . . . . . . . . . . 64\n4.7 The proposed MTL network: The sizes of the extracted vectors for the net-\nwork are as follows: for the Pain classifier, n\u02c61, where nis the number of\npain estimation tasks ( e.g.,2for binary classification, 5for multi-class clas-\nsification); for the Age classifier, 36\u02c61, where 36represents the possible\nage values of the subjects; for the Gender classifier, 2\u02c61, corresponding to\nthe two possible gender categories ( i.e., males and females). . . . . . . . . 66\n4.8 Results for the proposed Schemes. . . . . . . . . . . . . . . . . . . . . . . 69\n4.9 Comparison of performances utilizing various neural networks approaches. 72\nxix5.1 The application of face alignment illustrates landmarks in 2D (left) and 3D\n(right) space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 An overview of our proposed transformer-based framework for automatic\npain assessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 The impact of the number of input frames on accuracy (left) and on runtime\nin milliseconds (right). Runtime calculated during inference on a NVIDIA\nRTX-3090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Relevance Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5 Outline of the proposed framework. . . . . . . . . . . . . . . . . . . . . . 86\n5.6 Comparison of mean accuracy and inference period for unimodal and multi-\nmodal strategies across NP versus P 4and MC tasks. The diagram adopts a\ndual-y-axis configuration\u2014accuracy measurements on the left and time met-\nrics on the right\u2014to outline the balance between performance efficacy and\ncomputational load, categorizing the methodologies along the x-axis. . . . . 98\n5.7 Regions highlighted in yellow and red denote areas of significant attention.\n(a) (1strow) Sequence of original frames. (2ndrow) Derived from the\nSpatial-Module after initial stage pretraining. (3rdrow) Derived from the\nSpatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module trained on the BioVid dataset. (b) (1strow) Derived from\ntheTemporal-Module incorporating video embeddings. (2ndrow) Derived\nfrom the Temporal-Module with heart rate embeddings. (3rdrow) Derived\nfrom the Temporal-Module using a combined embedding of video and heart\nrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.1 Illustration of the procedure for creating thermal images, featuring the archi-\ntecture of the Generator G(Encoder, mid-stage ResNet, Decoder), and the\nDiscriminator D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2 Representation of the proposed framework, illustrating its components and\ntheir main functions: (a)The Vision-MLP module, tasked with extracting\nfeature embeddings from video frames. (b)TheToken-Mixer , an important\nsub-module of Vision-MLP , generates the wave representation for the tokens.\n(c)The Channel-Mixer , a crucial sub-module within Vision-MLP .(d)The\nMLP, a core component of the Channel-Mixer .(e)The fusion procedure\nthat combines RGB and synthetic thermal embeddings, succeeded by the\nTransformer module, which conducts the final pain assessment. . . . . . . . 107\n6.3 Gradual blurring of RGB and synthetic thermal facial images: a series dis-\nplaying varying levels of Gaussian blur applied, with kernel sizes gradually\nincreased from k\u201c0(no blur) to k\u201c191(extensively blurred). . . . . . . 1136.4 Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain)\nclasses in RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191\n(heavily blurred). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1 PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer hav-\ning varying depths, showcasing how token resolution decreases at each stage;\n(b)Composition of the Token-Mixer module, featuring elements like depth-\nwise convolution (DWConv) and batch normalization; (c)Architecture of the\nFeed-Forward Network (FFN) within the Token-Mixer ;(d)The Cascaded\nAttention mechanism implemented across multiple heads, illustrating how\noutputs from preceding heads are incorporated to refine the self-attention\nprocess, culminating in the final output projection; (e)Configuration of the\nproposed multimodal pipeline, employing videos and fNIRS. The embed-\ndings from PainViT\u20131 are represented as waveform diagrams, which are\nmerged into a single diagram that illustrates both modalities before entering\nPainViT\u20132 for final pain evaluation. . . . . . . . . . . . . . . . . . . . . . . 151\n7.2 Waveform illustrations for various data types: (a)original fNIRS signal,\n(b)video embedding derived from PainViT\u20131 , and (c)fNIRS embedding\nobtained from PainViT\u20131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n7.3 Attention maps from the PainViT\u20132 . . . . . . . . . . . . . . . . . . . . . . 152\n7.4 Overview of primary models and their components outlined in this research:\n(a)PainFormer is structured hierarchically into four stages, incorporating\nSpectral andSelf-Attention Layers to extract embeddings from the inputs;\n(b)The Spectral Layer , a key element of PainFormer , uses FFT to ana-\nlyze frequency-specific data along with a learnable filter Kto highlight\ncritical frequencies; (c)The Self-Attention Layer , crucial for PainFormer ,\nenables parallel processing of features and their interconnections; (d)The\nEmbedding-Mixer , employing both cross and self-attention mechanisms, func-\ntions as the component for the final classification of embeddings in pain as-\nsessment; (e)TheVideo-Encoder , designed for compact and efficient encod-\ning, compresses video data into a reduced dimensional form; (f)TheMLP-1\nis part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)TheMLP-3 configuration is integrated into the Embedding-Mixer\nandVideo-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n7.5 Examples of different vision modalities in frame samples: (a)RGB frame,\n(b)synthetic thermal frame, and (c)depth estimation frame. . . . . . . . . 153\n7.6 Examples of different visual representations for biosignals: (a)waveform ,\n(b)spectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD . . 1547.7 An overview of the presented framework. PainFormer , the foundational\nmodel, excels in deriving high-quality embeddings from a diverse array of\nbehavioral and physiological modalities. The evaluation of RGB, thermal,\nand depth videos, alongside various representations of ECG, EMG, GSR,\nand fNIRS such as waveforms and spectrograms, underscores the rich infor-\nmation captured within these embeddings. Leveraging the embeddings from\nPainFormer facilitates the creation of various and diverse unimodal and mul-\ntimodal pipelines designed for the pain assessment task. Each pipeline can\nbe customized to suit the specific modalities involved, dataset characteristics,\nand the demands of the intended application or clinical setting. Our assess-\nments included the development and implementation of several pipelines\nin both unimodal and multimodal contexts, achieving leading-edge results\nacross various modalities and data representations. . . . . . . . . . . . . . 154\n7.8 Attention maps from the PainFormer :(a)(1strow) frames from RGB, ther-\nmal, and depth video modalities; (a)(2ndrow) corresponding attention maps;\n(b)(1strow) attention maps for ECG and EMG; (b)(2ndrow) attention maps\nfor EDA and fNIRS modalities. . . . . . . . . . . . . . . . . . . . . . . . . 155\n1 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n2 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n3 Additional attention maps from the PainViT\u20132 (refer to Section 7.2). . . . . 207List of Tables\n3.1 Most commonly utilized pain databases. . . . . . . . . . . . . . . . . . . . 24\n3.2 Vision-based studies with static analysis. . . . . . . . . . . . . . . . . . . . 32\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 33\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 34\n3.3 Vision-based studies with temporal utilization. . . . . . . . . . . . . . . . . 39\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 40\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 41\n3.4 Touch sensor-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Audio-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6 Multimodal-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.7 Interpretation approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1 Results for the Basic Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Results for the Gender Scheme (1). . . . . . . . . . . . . . . . . . . . . . . 60\n4.3 Results for the Age Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Results for the Gender-Age Scheme (Males) (1). . . . . . . . . . . . . . . . 62\n4.5 Results for the Gender-Age Scheme (Females) (1). . . . . . . . . . . . . . . 63\n4.6 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (1). 63\n4.7 Hyper-parameters used in our approach. . . . . . . . . . . . . . . . . . . . 65\n4.8 Results for the Basic Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . 68\n4.9 Results for the Gender Scheme (2). . . . . . . . . . . . . . . . . . . . . . . 68\n4.10 Results for the Age Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.11 Results for the Gender-Age Scheme (2). . . . . . . . . . . . . . . . . . . . 68\n4.12 Comparison of results adopting the feature augmentation approach. . . . . . 70\n4.13 Comparison of results adopting the MT-NN approach. . . . . . . . . . . . . 71\n4.14 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2). 72\n5.1 Training details for the automatic pain assessment. . . . . . . . . . . . . . 79\n5.2 Results on the pain estimation tasks. . . . . . . . . . . . . . . . . . . . . . 81\n5.3 Results for the pain estimation tasks using various numbers of input frames. 82\n5.4 Comparison of studies utilizing BioVid , RGB videos, and LOSO validation. 84\n5.5 Datasets utilized for the pre-training process of the framework. . . . . . . . 91\nxxiii5.6 Training details for the automatic pain assessment. . . . . . . . . . . . . . 92\n5.7 Results utilizing the video modality. . . . . . . . . . . . . . . . . . . . . . 93\n5.8 Results utilizing the heart rate modality. . . . . . . . . . . . . . . . . . . . 95\n5.9 Results utilizing the video &the heart rate modality. . . . . . . . . . . . . . 95\n5.10 Comparison of studies utilizing BioVid &LOSO validation, reported on ac-\ncuracy %. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.11 Module parameters and computational cost in FLOPS for the proposed frame-\nwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.1 Datasets utilized for the pretraining process of the framework. . . . . . . . 110\n6.2 Training specifications, and number of parameters and FLOPS for each mod-\nule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.3 Results utilizing the RGB video. . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Results utilizing the synthetic thermal video. . . . . . . . . . . . . . . . . . 112\n6.5 Results utilizing the RGB &the synthetic thermal video. . . . . . . . . . . . 113\n6.6 Results utilizing the fusion of RGB &synthetic thermal video. . . . . . . . 115\n6.7 Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.116\n6.8 Comparison with the MIntPAIN dataset. . . . . . . . . . . . . . . . . . . . 116\n7.1 Number of parameters and FLOPS for the components of the proposed Twins-\nPainViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.2 Datasets utilized for the pretraining process of the framework. . . . . . . . 123\n7.3 Training details for the automatic pain assessment. . . . . . . . . . . . . . 124\n7.4 Results utilizing the video modality & Addition method. . . . . . . . . . . . 125\n7.5 Results utilizing the video modality & Concatenation method. . . . . . . . . 125\n7.6 Results utilizing the HbR & Addition method. . . . . . . . . . . . . . . . . 126\n7.7 Results utilizing the HbR & Concatenation method. . . . . . . . . . . . . . 126\n7.8 Results utilizing the HbO & Addition method. . . . . . . . . . . . . . . . . 126\n7.9 Results utilizing the HbO & Concatenation method. . . . . . . . . . . . . . 127\n7.10 Results utilizing the HbR, HbO & Addition method. . . . . . . . . . . . . . 127\n7.11 Results utilizing the videos, HbO & Addition method. . . . . . . . . . . . . 127\n7.12 Results utilizing the videos, HbO & Single Diagram method. . . . . . . . . 128\n7.13 Comparison with the validation baseline provided by the AI4PAIN challenge\norganizers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.14 Number of parameters and FLOPS for the modules of the proposed framework.130\n7.15 Details of the PainFormer\u2019s architecture. . . . . . . . . . . . . . . . . . . . 132\n7.16 Datasets utilized for the multitask learning-based pretraining process of the\nframework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347.17 Training details of the proposed framework. . . . . . . . . . . . . . . . . . 135\n7.18 Results utilizing the video modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.19 Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.20 Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.21 Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.22 Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and\nF1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.23 Results on the validation set of AI4Pain dataset, multilevel classification task,\nreported on accuracy, recall and F1 score. . . . . . . . . . . . . . . . . . . 144\n7.24 Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.25 Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4\ntask and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.26 Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs.\nP4task and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.27 Comparison of studies on the testing set of AI4Pain dataset. . . . . . . . . . 148\n1 Results utilizing the video modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n2 Results utilizing the heart rate modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n3 Results utilizing the video &the heart rate modality reported on precision,\nrecall and F1 score (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . 202\n4 Results utilizing the RGB video modality, reported on recall and F1 score\n(refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n5 Results utilizing the synthetic thermal video modality, reported on recall and\nF1 score (refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . 203\n6 Results utilizing the fusion of RGB &synthetic thermal video modality, re-\nported on recall and F1 score (refer to Section 6.2). . . . . . . . . . . . . . 204\n7 Results of the proposed approaches, reported on macro-averaged precision,\nrecall and F1 score (refer to Section 7.2). . . . . . . . . . . . . . . . . . . . 204Chapter 1\nIntroduction\nContents\n1.1 Context and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Scope and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions \u2013 Peer-review Publications . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.1 Context and Motivation\nPain is a complex and deeply personal experience that is subjective by nature. Traditionally,\nit has been described in terms of its sensory dimension [1]. However, extensive research\nhas highlighted the importance of affective, cognitive and social aspects in shaping this ex-\nperience [2]. Studies have explored physiological, psychological, and socio-environmental\nfactors that contribute to the experience of pain. It is understood as a result of biological evo-\nlution and as influenced by psychological and social factors. As Ridell et al. [3] noted, \u201cPain\nis a synthesis\u2013a sum that is greater than its parts. \u201d The brain\u2019s ability to alter the perception\nof sensory inputs through the interplay of emotion, cognition, and social processes is signifi-\ncant. Although natural systems establish the initial biological framework for pain perception,\nthis structure is highly adaptable, particularly in humans. Throughout a person\u2019s life, both\nbiological developments and personal experiences significantly reshape this framework.\nA key question driving pain research across biological, psychological, and computational\nfields is why this topic of pain is meaningful and important. This question also forms the\nbasis for initiating this thesis, highlighting the broader relevance of studying pain. Williams\nand Kappesser [4] provide a compelling explanation, stating, \u201cWe care because we are wired\nto care: to attend to other people\u2019s expression of pain and to understand its meaning; to feel\ndistress in relation to their distress; and to be motivated to reduce their distress, and ours,\nif we are able to do so. \u201d This highlights the intrinsic human response to empathize and\n12 CHAPTER 1. INTRODUCTION\nalleviate pain, underlining the fundamental importance of this research area. Indeed, from a\nDarwinian perspective, pain serves a crucial role. The manifestation of pain in humans and\nthe reactions it elicits are examined through an evolutionary lens. Pain facilitates recovery by\npromoting responses to harmful stimuli and behaviors that demonstrate the adverse nature\nof painful experiences, common among animals. Specifically, the facial expression of pain,\nwhich communicates discomfort directly to those nearby, is universally recognized across\ndifferent ages, ethnicities, roles, and relationships. Evidence from healed major fractures\n[5, 6] suggests that injured members of hominid groups were not left to fend for themselves\nbut were supported through their recovery, indicating the fundamental importance of pain\nexpression in our evolutionary history.\nPain is a widespread health concern globally, affecting up to 30% of the adult popula-\ntion [7] and between 83% and93% of elderly adults in residential care [8]. The Global Bur-\nden of Disease (GBD) study identifies pain as the primary cause of years lived with disability\n(YLD) [9], with major contributors including chronic back pain, musculoskeletal disorders,\nand neck pain [10]. Pain impacts individuals and poses significant clinical, economic, and\nsocial challenges. In the United States, the economic and healthcare costs related to pain\ndue to reduced work productivity range from $560 to$635 billion annually, surpassing the\ncosts associated with heart disease, cancer, and diabetes combined [11]. In Europe, chronic\npain\u2019s direct healthcare costs and indirect socioeconomic impacts account for 3%to10% of\nthe GDP [12]. In Australia, the average annual cost for individuals among the 15.4%living\nwith chronic pain ranges from AU $22,588to AU $42,979, including non-financial costs [13].\nBeyond direct effects on health, pain contributes to a range of adverse outcomes, such as opi-\noid dependency, drug overuse, addiction, declining social relationships, and psychological\ndisorders [14]. In the last two decades, prescription opioid use has surged in the United\nStates, where overdose deaths have increased more than fourfold from 1999 to2016 [15].\nAdditionally, side effects from these opioids, like lethargy, depression, anxiety, and nausea,\nseverely impact workforce productivity and overall life quality [16].\nAccurate pain assessment is crucial for early diagnosis, disease progression monitoring,\nand treatment effectiveness evaluation, particularly in managing chronic pain [17]. This criti-\ncal role has resulted in pain being recognized as \u201cthe fifth vital sign\u201d in nursing literature [18].\nPain assessment is also fundamental in physiotherapy, where therapists apply external stim-\nuli and need to gauge the patient\u2019s pain levels accurately [19]. Objective evaluation of pain is\nessential to provide appropriate care, especially for vulnerable populations who may not be\nable to communicate their pain effectively, such as infants, young children, individuals with\nmental health issues, and the elderly. Various methods are used for pain assessment, with\nself-reporting\u2013where individuals describe their pain experiences\u2013considered the gold stan-\ndard [20]. Pain evaluation methods in clinical environments include quantifiable measures\nlike the Numeric Pain Rating Scale (NPRS), Visual Analogue Scale (V AS), and quantitative1.1. CONTEXT AND MOTIVATION 3\nsensory testing techniques such as the pressure pain detection threshold (PPDT) [21]. Behav-\nioral indicators are also crucial and include facial expressions ( e.g., grimacing, open mouth,\nlifted eyebrows), vocalizations (like crying, moaning, or screaming), and movements of the\nbody and head [22]. Physiological measures such as electrocardiography (ECG), electromyo-\ngraphy (EMG), galvanic skin responses (GSR), and respiration rates further contribute to\nunderstanding pain\u2019s physiological aspects [17]. Additionally, brain monitoring techniques\nlike near-infrared spectroscopy (fNIRS) have effectively detected changes in hemodynamic\nactivity associated with pain stimuli [23].\nCaregivers and family members often determine the presence or absence of pain in pa-\ntients by observing their behavioral or physiological responses [17]. However, accurately\nassessing pain poses a significant challenge for clinicians [24], especially with nonverbal\npatients such as the elderly, who may have reduced expressive abilities or may be reluctant\nto communicate pain [25]. Extensive research indicates that pain manifestations vary signif-\nicantly across different genders and ages, adding to the complexity of its assessment [26].\nFurther complicating the assessment process are the heightened workload and fatigue ex-\nperienced by nursing staff due to the demands of patient monitoring [27]. Technological\nsolutions are necessary for continuous patient monitoring. Nevertheless, concerns remain\nabout the objectivity and accuracy of these observations, as inadequately trained or biased\nobservers may struggle to assess pain [28] accurately. Even among trained observers, in-\nterpretations of behaviors can vary [22], and social and interpersonal dynamics can signif-\nicantly affect the pain assessment process, influencing both the evaluators\u2019 judgments and\nthe patients\u2019 expressions of pain [29]. Additionally, the presence of an observer can lead pa-\ntients to modify their behavior [30], and expressing pain through scales and measurements\ncan be challenging [31]. While self-reporting is used because pain is inherently subjective,\nrelying solely on a one-dimensional pain score fails to capture this complex phenomenon,\noften leading to inadequate pain management [32].\nGiven the challenges described above, scientific computing (SC) researchers have fo-\ncused on developing models and algorithms to enhance automatic pain recognition systems\nover the last two decades. Their goal is to accurately determine the presence and intensity\nof pain by analyzing physiological and behavioral indicators. Adopting deep learning and\nartificial intelligence (AI) techniques has expanded these automatic methods, designed to\ninterpret the complex and varied nature of pain [17]. Numerous studies have underscored\nthe effectiveness of automated systems that utilize behavioral or physiological modalities\nfor pain assessment [33]. Sario et al. [34] have shown the capability of these systems to\naccurately recognize pain through facial expressions, proving their utility in clinical envi-\nronments. Multimodal sensing has shown particular promise, offering enhanced accuracy\nin pain detection systems [22]. Furthermore, including temporal aspects in these modalities\nhas proven to significantly improve the accuracy of pain assessments [17].4 CHAPTER 1. INTRODUCTION\n1.2 Scope and Challenges\nAlthough considerable research has been conducted on automatic pain assessment, studies\nhave yet to explore factors like demographics and social aspects from a computational angle.\nFurthermore, despite the existence of deep learning-based methods, the approaches we ob-\nserve are often outdated and repeatedly recycled. For these reasons, we aimed to address two\nissues by (i)attempting to evaluate the social or demographic context, which significantly\nimpacts and influences pain sensation and perception, and (ii)introducing innovative deep\nlearning methods inspired by the latest developments in AI and generative AI literature. We\nbelieve these approaches can forge new paths in pain research, enhance the accuracy of rec-\nognizing this complex phenomenon, and, ultimately, be adopted in real-world scenarios to\nassist those in need. Additionally, (iii)recognizing the skepticism towards new technologies\namong clinicians and the general public, especially regarding the limited understanding of\nhow deep learning models function, we have devoted a portion of our research to interpret-\ning these models to offer some level of explanation and help the adoption process of them in\nclinical settings.\nNevertheless, this thesis initially faced challenges related to our objectives and goals as\nthe research progressed. The availability of pain datasets (to be discussed in the next chapter)\nis limited. Only a few datasets are available, and crucially, they are limited in size. This re-\nstriction poses a significant challenge for developing deep learning models, which typically\nrequire a large volume of data. In automatic pain assessment, researchers who develop deep\nlearning methods typically confront a decision: either train their models from scratch, which\ncan introduce performance limitations, or employ pre-trained models. These pre-trained\nmodels are generally trained on broadly available image datasets that include a variety of\nsubjects like animals and objects, or they rely on older architectures that were trained explic-\nitly on facial datasets. In this thesis, we addressed these issues by independently pre-training\nour deep-learning models using diverse datasets related explicitly to human facial images\nand biosignals. This strategy allowed us to design specific architectures to meet our unique\nneeds for each scenario, free from the constraints of relying on models developed and trained\nby others. Furthermore, we explored and evaluated several pre-training techniques to assess\ntheir effectiveness in pain assessment applications.\nRegarding, our objective to explore methods that utilize various modalities individually\nand in combination in a multimodal manner further constrains our dataset options. More-\nover, as previously outlined, our interest in the sociodemographic aspects of pain necessitates\ndatasets that include this information type, intensifying our challenges. For these reasons,\nthis thesis focuses specifically on examining the impact of age and gender on pain. In addi-\ntion, led us to utilize two pain datasets that most closely match the characteristics necessary\nfor our research, particularly in terms of demographic elements and multimodality.1.3. CONTRIBUTIONS \u2013 PEER-REVIEW PUBLICATIONS 5\n1.3 Contributions \u2013 Peer-review Publications\nThis section outlines the publications and projects produced during the Ph.D. research on\nautomatic pain assessment, where I was the first author.\n1.Automatic assessment of pain based on deep learning methods: A systematic re-\nview [17]\nThis systematic literature review (SLR) was conducted at the start of this Ph.D. re-\nsearch. This paper aims to explore the surge in recent years of deep learning algorithms\nadopted by researchers to encode the multidimensional nature of pain into meaning-\nful features. Specifically, this systematic review examines the models, methods, and\ndata types used to establish the foundation for deep learning-based automatic pain\nassessment systems. It identified relevant original studies from digital libraries such\nasScopus ,IEEE Xplore , and ACM Digital Library , following defined inclusion and\nexclusion criteria for studies published until December 2021 . The findings highlight\nthe critical role of multimodal approaches in automatic pain estimation, particularly\nin clinical environments, and emphasize the substantial gains observed with the inclu-\nsion of temporal exploitation of modalities. The review also recommends selecting\nhigh-performing deep learning architectures and methods, encouraging the adoption\nof robust evaluation protocols and interpretability techniques to deliver reliable and\nunderstandable outcomes. Additionally, it underscores the current limitations of exist-\ning pain databases in adequately supporting the development, validation, and practical\napplication of deep learning models as decision-support tools in real-world settings.\nFurthermore, we believe this paper is valuable not only for this Ph.D. project but also\nfor other practitioners and researchers in the field.\n2.Automatic Pain Intensity Estimation based on Electrocardiogram and Demographic\nFactors [35]\nThis study investigated the relationship between gender, age, and pain sensation and\ntheir effects on the automatic pain assessment process. By analyzing physiological\nsignals, particularly electrocardiography (ECG), we estimated pain intensity and ex-\namined the influence of these demographic factors. Utilizing the Pan-Tompkins algo-\nrithm for feature extraction and applying well-established classification methods, we\nexplored the correlation between gender, age, and pain manifestation.\n3.Multi-task Neural Networks for Pain Intensity Estimation Using Electrocardiogram\nand Demographic Factors [36]\nInspired by the previous study, this research further explored the influence of gender\nand age on pain perception. In this work, we analyze electrocardiography signals\nto uncover variations in pain perception across different demographic groups. We6 CHAPTER 1. INTRODUCTION\nleveraged these insights by developing a novel multi-task neural network for automatic\npain estimation, incorporating age and gender data for each individual. The study\ndemonstrated the advantages of this approach compared to other existing methods.\n4.A Full Transformer-based Framework for Automatic Pain Estimation using Videos\n[37]\nThis study introduced an innovative full transformer-based framework featuring a Trans-\nformer in Transformer (TNT) model combined with cross-attention and self-attention\nblocks. We achieved state-of-the-art performance using video data from the BioVid\ndatabase, demonstrating the model\u2019s effectiveness, efficiency, and strong generaliza-\ntion across primary pain estimation tasks.\n5.Multimodal automatic assessment of acute pain through facial videos and heart rate\nsignals utilizing transformer-based architectures [38]\nThis study presented a multimodal automatic acute pain assessment framework, inte-\ngrating video and heart rate signals. The framework consists of four key modules:\ntheSpatial Module , which extracts embeddings from videos; the Heart Rate Encoder ,\nwhich maps heart rate signals into a higher-dimensional space; the AugmNet , which\ngenerates learning-based augmentations in the latent space; and the Temporal Mod-\nule, which leverages the video and heart rate embeddings for the final assessment.\nThe Spatial Module undergoes a two-stage pre-training process: first, it learns uni-\nversal facial features through face recognition, followed by emotion recognition in a\nmultitask learning approach, enabling high-quality embeddings for pain assessment.\nExperiments with facial videos and heart rate data extracted from electrocardiograms\nin the BioVid database, alongside direct comparisons to 29studies, demonstrate state-\nof-the-art performance in unimodal and multimodal settings while maintaining high\nefficiency. In the multimodal setting, the framework achieved 82.74% accuracy for bi-\nnary pain classification and 39.77% for multi-level pain classification, using only 9.62\nmillion parameters across the entire framework.\n6.Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-\nMLP Architecture [39]\nThis paper introduced synthetic thermal videos generated by Generative Adversarial\nNetworks , which are integrated into the pain recognition process to assess their effec-\ntiveness. The framework employs a Vision-MLP andTransformer -based module, lever-\naging RBG and synthetic thermal videos in unimodal and multimodal settings. Exper-\niments conducted using facial videos from the BioVid database highlighted synthetic\nthermal videos\u2019 effectiveness and showcased their potential benefits in pain recognition\ntasks.1.4. THESIS OUTLINE 7\n7.Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for\nMultimodal Automatic Pain Assessment using Facial Videos and fNIRS [40]\nThis study was submitted to the First Multimodal Sensing Grand Challenge for Next-\nGen Pain Assessment (AI4PAIN) . The proposed multimodal framework leverages fa-\ncial videos and fNIRS, offering a modality-agnostic approach that eliminates the need\nfor domain-specific models. Utilizing a dual ViT configuration and waveform repre-\nsentations for both fNIRS and the extracted embeddings from the two modalities, the\nmethod demonstrates its effectiveness, achieving an accuracy of 46.76% in the multi-\nlevel pain assessment task.\n8.PainFormer: a Vision Foundation Model for Automatic Pain Assessment [41]1\nThis study introduces PainFormer , a vision foundation model built on multi-task learn-\ning principles and trained across 14distinct tasks and datasets comprising 10.9million\nsamples. As an embedding extractor for various input modalities, PainFormer provides\nfeature representations to the Embedding-Mixer , a transformer-based module respon-\nsible for conducting the final pain assessment. Extensive experimentation using both\nbehavioral modalities\u2013including RGB, synthetic thermal, and estimated depth videos\u2013\nand physiological modalities like ECG, EMG, GSR, and fNIRS revealed PainFormer \u2019s\nability to extract high-quality embeddings from diverse inputs. Tested on the BioVid\nandAI4Pain datasets and compared to more than 60existing methods, the framework\ndemonstrated state-of-the-art performance in unimodal and multimodal settings, posi-\ntioning itself as a step toward developing general-purpose models for automated pain\nevaluation.\n1.4 Thesis Outline\nThe dissertation is organized into the following chapters:\nChapter 2 introduces the foundational concepts of pain from biological, psychological, and\nclinical perspectives.\nChapter 3 reviews existing literature on automatic pain assessment using deep learning\nmethods and details the pain datasets used.\nChapter 4 outlines and proposes methods for evaluating demographic variables, their uti-\nlization, and their integration into an automatic pain assessment framework.\nChapter 5 discusses methods that utilize video and wearable device data, exploring the\ntrade-offs between efficiency and accuracy. It also proposes efficient, fast, effective models\nsuitable for real-world applications.\nChapter 6 explores synthetic data in pain assessment and introduces synthetic thermal im-\n1Under Review8 CHAPTER 1. INTRODUCTION\nagery techniques to enhance performance in automatic pain recognition.\nChapter 7 discusses general-purpose models, introduces a modality-agnostic framework,\nand presents the first foundation model used in automatic pain assessment.\nChapter 8 concludes the thesis with a final discussion, offering perspectives and ideas for\nfuture research in automatic pain assessment.Chapter 2\nClinical Pain Assessment\nContents\n2.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . . . . 20\n2.1 Chapter Overview\nThis chapter provides an anatomical and physiological overview of pain, focusing on the\nmechanisms responsible for generating, transmitting, processing, and interpreting pain sig-\nnals. It examines the various types of pain and explores the actions and expressions typically\nassociated with pain. Additionally, it reviews current pain assessment methods used in clin-\nical settings for adults, children, and newborns. The chapter also discusses developing and\nvalidating existing clinical pain assessment tools. This foundational knowledge is essen-\ntial for understanding the development and validation of computer-assisted pain assessment\n910 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nmethods discussed in later chapters. Finally, it highlights the challenges faced in clinical\npain assessment and underscores the need for automated pain assessment techniques.\n2.2 Biology of Pain\nPain, according to the International Association for the Study of Pain (IASP) [42], is \u201can\nunpleasant sensory and emotional experience associated with actual or potential tissue dam-\nage, or described in terms of such damage\u201d. Biologically, pain is an undesirable sensation\noriginating from the peripheral nervous system. Its fundamental function is to engage sen-\nsory neurons, notifying the organism of potential harm and playing a vital role in recognizing\nand responding to threats [43].\nThe transmission of a noxious stimulus from the periphery to the central nervous sys-\ntem involves a complex pathway through the spinal cord, resulting in the physical sensation\nof pain and a corresponding emotional response and memory. This process culminates in\nthe perception of pain. The initial stage of pain processing occurs when a stimulus at noci-\nceptive sensory fibers in the periphery is converted into an action potential. A nerve signal\nis generated if the stimulus is strong enough to surpass the action potential threshold [44].\nThis signal travels along the primary afferent fiber toward the central nervous system. As the\nstimulus intensity grows, more nerve fibers and areas of the nervous system are engaged [44].\nDue to their branching nature, primary afferent fibers typically relay information from sev-\neral pain receptors. These fibers and their receptors comprise a sensory unit, which gathers\ndata from a specific receptive field [44]. When receptive fields are larger and overlap with\nnearby fields, it becomes more challenging for the sensory system to locate the source of pain\naccurately. The primary afferent neuron is a pseudounipolar neuron that splits into a periph-\neral and central axon. The cell bodies of these neurons are located in the peripheral nervous\nsystem, within the posterior or cranial root ganglia. The peripheral axon extends to the skin,\nmuscles, tendons, or joints, branching into terminal fibers that connect with somatosensory\nreceptors. In contrast, the central axon leads to the central nervous system [45].\nPeripheral somatosensory fibers are categorized into three main groups. The first group\nincludes A\u00b4\u03b1,A\u00b4\u03b2,A\u00b4\u03b3fibers, large, myelinated fibers that rapidly conduct sig-\nnals [46]. These fibers involve touch and proprioception but are not associated with pain\nperception. The second group consists of A\u00b4\u03b4fibers, which are smaller and slower con-\nducting. Certain A\u00b4\u03b4fibers play a key role in pain sensation, with some responding only\nto intense mechanical stimuli and others reacting to noxious and non-noxious heat. The\nthird group comprises Cfibers, which are small, unmyelinated, and conduct signals very\nslowly. Most Cfibers are polymodal for pain perception, responding to various noxious\nmechanical, thermal, and chemical stimuli. These fibers are mainly linked to burning pain\nsensations [43]. The sensation of pain, known as nociception, is primarily facilitated by2.3. CLASSIFICATION AND CHARACTERISTICS OF PAIN 11\nvarious intracellular and extracellular molecular messengers. When activated by a specific\nstimulus, nociceptors relay information through glutamate, an excitatory neurotransmitter.\nAdditionally, inflammatory mediators are released at the injury site, further stimulating no-\nciceptor activation by releasing chemicals such as neurotransmitters ( e.g., serotonin), lipids\n(e.g., prostaglandins), peptides ( e.g., bradykinin), and neurotrophins ( e.g., nerve growth fac-\ntor) [46]. There are ascending tracts responsible for transmitting sensory information from\nthe periphery to the central nervous system. Fibers that convey two-point discrimination, tac-\ntile information, pressure, vibration, and proprioception ascend via the dorsal column of the\nspinal cord, forming the gracile and cuneate fasciculi. Fibers transmitting pain, temperature,\nand crude touch from somatic and visceral structures travel through the lateral spinothalamic\ntract. The anterior spinothalamic tract also transmits pain, temperature, and touch informa-\ntion to the brainstem and diencephalon (Figure 2.1) [47].\n2.3 Classification and Characteristics of Pain\nAccording to neurobiologist Clifford Woolf [48], pain can be classified into three categories\nbased on its function and characteristics: nociceptive ,inflammatory , and pathological pain.\nThese classes and their respective functions are illustrated in Figure 2.2.\nNociceptive pain (refer to Figure. 2.2(A)), arising from tissue damage, is a high-threshold\npain that activates only in response to intense stimuli [49], serving as a vital warning signal\nto the body. The neurobiological system responsible for nociceptive pain evolved from the\nability of even the most primitive nervous systems to detect impending or actual tissue dam-\nage caused by external stimuli. Its protective role requires immediate attention and action,\nachieved through the withdrawal reflex it initiates, the unpleasant sensation it produces, and\nthe emotional distress it triggers. Nociceptive pain demands avoidance in the present mo-\nment, and when activated, it overrides most other neural processes [48].\nInflammatory pain (refer to Figure. 2.2(B)) is also protective and adaptive, increasing\nsensory sensitivity following tissue damage to aid healing by discouraging movement and\ncontact with the injured area. This heightened sensitivity, or tenderness, helps prevent further\nharm and supports recovery, as seen after surgical wounds or inflamed joints where normally\nnon-painful stimuli now cause pain. It is triggered by immune system activation in response\nto tissue injury or infection. Despite its adaptive role, this pain often needs to be alleviated in\npatients with persistent inflammation, such as in rheumatoid arthritis or severe injuries [48].\nPathological pain (Figure. 2.2(C)) is maladaptive, arising from abnormal nervous sys-\ntem functioning and not serving a protective role. Unlike nociceptive and inflammatory pain,\npathological pain is a disease state of the nervous system itself. It may occur following\nnerve damage (neuropathic pain) or in conditions without apparent damage or inflammation\n(dysfunction l pain). Examples of dysfunctional pain include fibromyalgia, irritable bowel12 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFIGURE\n 2: Spinothalamic tract.\nPain, temperature, and some touch and pressure afferents end in the posterior horn. Second- or\nhigher-order fibers cross the midline, form the spinothalamic tract, and ascend to the ventral\nposterolateral (VPL) nucleus of the thalamus (and also to other thalamic nuclei not shown).\nThalamic cells then project to the somatosensory cortex of the postcentral gyrus, to the insula,\nand to other cortical areas (also not shown). Along their course through the brainstem,\nspinothalamic fibers give off many collaterals to the reticular formation (RF). The inset to the left\nshows the lamination of fibers in the posterior columns and the spinothalamic tract in a leg-\nlower trunk-upper trunk-arm sequence. The inset to the right shows the longitudinal formation\nof the spinothalamic tract. Primary afferents ascend several segments in Lissauer\u02bcs tract before\nall their branches terminate; fibers crossing to join the spinothalamic tract do so with a rostral\ninclination. As a result, a cordotomy incision at any given level would spare most of the\ninformation entering the contralateral side of the spinal cord at that level, and to be effective,\nthe incision must be made several segments rostral to the highest dermatomal level of pain.\n2017 Khalid et al. Cureus 9(10): e1754. DOI 10.7759/cureus.1754\n5\n of \n14\nFigure 2.1: The spinothalamic tract (STT) [43]. Pain, temperature, and some touch afferents\nend in the posterior horn, where second-order fibers cross the midline to form the\nspinothalamic tract, ascending to the thalamus and projecting to various cortical\nareas. Along the way, collaterals connect to the reticular formation. Due to the ros-\ntral inclination of fibers in Lissauer\u2019s tract, cordotomy must be performed several\nsegments above the pain level for effective relief.\nsyndrome, tension headaches, and temporomandibular joint disease, where significant pain\nexists without an apparent noxious stimulus or peripheral pathology. Pathological pain, a\nlow-threshold pain primarily driven by amplified sensory signals in the central nervous sys-\ntem, is the clinical pain syndrome with the greatest unmet need. To analogize, while nocicep-\ntive pain acts as a fire alarm for intense heat, and inflammatory pain reacts to warm tempera-\ntures, pathological pain is a false alarm triggered by a system malfunction. Thus, treatment\nmust specifically target the underlying mechanisms causing each type of pain [48].\nPain from a time-duration perspective can be categorized by duration into acute and2.4. PAIN INDICATORS 13\nchronic , with chronic pain persisting or recurring for more than three months [50]. Acute\npain is typically related to identifiable physiological damage from injury, surgery, illness,\ntrauma, or medical procedures and generally subsides once the underlying cause is resolved.\nHowever, if untreated, it may develop into chronic pain. Acute pain is further classified\nintoprocedural pain, caused by medical interventions such as muscular injections [51], and\npostoperative pain, which occurs after surgery and is a significant concern for both patients\nand healthcare providers. Effective management is crucial to aid recovery and prevent the\ntransition to chronic pain [52]. Chronic pain manifests in various forms, including chronic-\nrecurrent pain, like migraine headaches, and chronic-continuous pain, such as persistent low\nback pain [53].\n2.4 Pain Indicators\nPain can manifest in numerous ways and is often shaped by individual characteristics and\nenvironmental influences. Various human expressions, actions, and bodily responses have\nbeen linked to pain, serving both communicative and coping purposes. These pain indicators\nare generally categorized into three primary groups: (i)behavioral, (ii)physiological, and\n(iii)biochemical. While these indicators are universally present, certain expressions are more\nprominent in specific groups. For instance, crying is a common pain response across all age\ngroups but is more frequently observed in younger infants. This may be due to contextual\nfactors\u2014such as culture, social status, age, and ego\u2014influencing how pain is expressed\nover time. Adults, for example, may suppress crying in favor of other vocalizations, such as\ngroans and moans, as crying could be perceived as inappropriate in certain contexts. These\nmediating factors are often considered when interpreting pain indicators. The following\nsections will delve into each of these three categories [51].\n2.4.1 Behavioral Indicators\nBehavioral indicators such as facial expressions ( e.g., grimacing, open mouth, raised eye-\nbrows), vocalizations ( e.g., crying, moaning, screaming), and various bodily movements\n(e.g., changes in posture, signs of tension) are vital markers used in assessing pain [22].\nFacial expressions and limb movements in response to acute pain are typically rapid and\ninvoluntary. Facial reactions include brow bulging, eye squeezing, nasolabial furrow forma-\ntion [54], grimacing, clenched teeth, jaw-dropping, and tightened lips [55]. Body movements\nassociated with pain include bracing (gripping an object or the affected area during move-\nment), rubbing (massaging the painful area), restlessness (constant shifting of position) [55],\nand knee flexion [56]. Non-verbal vocalizations such as groaning, moaning, sighing, crying,\nand gasping [57] also indicate pain. Verbal expressions like \u201couch\u201d ,\u201cstop\u201d ,\u201cthat hurts\u201d ,14 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFigure 2.2: Pain classification [48]: (A)Nociceptive pain , which results from detecting po-\ntentially harmful stimuli and serves a protective function. (B)Inflammatory pain is\nlinked to tissue damage and immune cell infiltration, increasing pain sensitivity dur-\ning healing. (C)Pathological pain is a disease state caused by either nervous sys-\ntem damage (neuropathic) or abnormal nervous system function (dysfunctional).\n\u201cthat is enough\u201d , and even cursing [55] also serve as pain indicators. Interestingly, swearing\nhas been found to significantly alleviate pain, although its effect diminishes with frequent\nuse over a short period [58, 59].2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 15\n2.4.2 Physiological Indicators\nVital signs can reflect the state of the central nervous system, and since pain is mediated\nthrough this system, trends in vital signs can provide insights into pain levels. Clinical stud-\nies [60, 61] have examined physiological changes in response to pain and established em-\npirical solid evidence linking pain to vital sign alterations. However, as vital signs can also\nchange due to other non-pain-related pathological conditions, it is recommended that they be\nassessed alongside behavioral pain indicators for accuracy. Physiological pain responses are\nconsidered more reliable than behavioral signals, as they cannot be consciously controlled\nor altered. Physiological measurements such as electrocardiography (ECG), electromyogra-\nphy (EMG), galvanic skin responses (GSR), and respiration rate provide critical insights into\nthe body\u2019s reaction to pain [17]. In addition, brain monitoring techniques like near-infrared\nspectroscopy (fNIRS) have demonstrated the ability to detect pain-related hemodynamic\nchanges [23]. At the same time, functional magnetic resonance imaging (fMRI) has been\nexplored for assessing pain in both normal and pathological conditions [62].\n2.4.3 Biochemical Indicators\nCompared to other pain indicators, biochemical changes are the most precise and sensitive\nreactions to pain. However, their routine use in pain assessment is restricted due to the\ninvasive nature of measurement techniques [63]. These biochemical responses are most\nevident during surgical procedures with limited anesthesia, leading to increased levels of\nendorphins, norepinephrine, cortisol, growth hormones, renin, glucagon, aldosterone, and\ncatecholamines, along with a decrease in insulin levels [60].\n2.5 Sociodemographic and Psychological Variables\nIn1965 , Melzack and Wall [64] introduced the \u201cGate Control Theory\u201d , which interprets pain\nfrom two perspectives. The first involves the mechanisms of nociceptive signal transmission\nand modulation, while the second emphasizes pain as a psychophysiological phenomenon\narising from the interaction between physiological and psychological factors [53]. Observa-\ntions, empirical research, and theoretical models increasingly suggest that a comprehensive\nunderstanding of pain requires a biopsychological approach. It is also becoming apparent\nthat, although pain is often regarded as private and subjective, it is also fundamentally a so-\ncial experience [53]. Pain is not solely explained by biomedical components ( e.g., muscle\ndamage) but also involves psychological ( e.g., cognitive, affective) and social factors ( e.g.,\nfriends, family, health professionals), leading to what is known as a biopsychosocial sensa-\ntion [65]. Numerous factors contribute to how painful experiences are expressed and per-\nceived, varying wildly due to social and personal biases. These factors prompted Williams16 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nand Craig [2] to define pain as \u201ca distressing experience associated with actual or potential\ntissue damage with sensory, emotional, cognitive, and social components. \u201d\n2.5.1 Sex and Gender\nSeveral studies have explored the relationship between gender and pain expressiveness, as\nwell as variations in pain reporting. Research indicates that women generally exhibit a lower\npain threshold compared to men. A meta-analysis by Boerner et al. [66] on gender differ-\nences in children and adolescents found that girls over the age of 12reported higher pain\nintensity in response to cold-induced pain than boys. Furthermore, multiple studies suggest\nthat women tend to describe a greater degree of pain compared to men. In addition to bi-\nological differences, psychological aspects linked to gender also play a role. For instance,\nindividuals with a masculine identity may be less inclined to express or report their pain or\nseek assistance [67].\nMoreover, the manifestation of pain is not only influenced by the individual\u2019s gender but\nalso by dyadic interactions between people of different sexes. Levine and Desimone [68]\nconducted one of the initial studies on this phenomenon, showing that male participants in\na cold pressure experiment reported lower pain intensity when a female experimenter was\npresent. Similarly, McClelland and McCubbin [69] found that female participants expressed\nand reported higher pain levels when accompanied by a female friend. This dynamic also\nextends to patient-healthcare provider interactions. In studying health records, Vigil and\nAlcock [70] discovered that when the pain intensity was reported as high, the patients ( i.e.,\nmen and women) were examined by a female doctor or nurse. Additionally, studies exam-\nining gender differences among physicians in pain treatment options revealed that female\npatients were more likely to receive prescriptions for more potent drugs, such as analgesics,\nand female physicians were more likely to prescribe medications. Extensive research has\nalso shown that both lay observers and healthcare professionals tend to estimate higher pain\nlevels for female patients compared to male patients [71]. Hooper et al. [72] further noted\nthat clinicians communicate more effectively with female patients, often displaying greater\nempathy. Gender roles, beliefs, and expectations play a significant role in understanding\nhow social factors influence the differences in pain perception and experience between men\nand women [73].\n2.5.2 Age\nAge plays a crucial role in pain assessment and management. At the same time, there are\nsignificant challenges, limitations, and biases related to the patient\u2019s age group. Two of the\nmost vulnerable groups, albeit for different reasons, are the elderly and infants.\nPain recognition and interpretation among the elderly, particularly by caregivers, often2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 17\npresent unique challenges. Older adults frequently exhibit stoicism and reluctance to ex-\npress their pain, while healthcare providers struggle to accurately assess the patient\u2019s pain,\nleading to inappropriate pain management decisions [74]. McPherson et al. [75] noted that\ndespite caregivers\u2019 accommodating and empathetic relationships with elderly patients, con-\nflicts still arise. Older patients may resist acknowledging their weaknesses and accepting\nhelp, which can cause them to conceal their pain. The situation becomes even more complex\nwhen dealing with dementia, a disorder encompassing a range of conditions ( e.g., Parkin-\nson\u2019s, Alzheimer\u2019s, Vascular dementia), characterized by abnormal brain changes that im-\npair cognitive and linguistic abilities. A person with dementia may find it challenging to\ncommunicate their pain verbally. However, non-verbal pain expressions remain intact even\nin moderate dementia, although such reactions can be exaggerated [76]. However, aggres-\nsive behavior and disturbances in dementia patients, often caused by pain, are frequently\nmisinterpreted as psychiatric symptoms, leading to improper medication that can have life-\nthreatening consequences [77]. Caregivers of dementia patients face additional challenges,\nnot only related to pain management but also in addressing dementia\u2019s impact on language\nand memory. Particularly in the later stages of dementia, patients encounter severe pain\ncommunication difficulties due to cognitive decline, necessitating that caregivers recognize\nbehavioral and contextual indicators of pain [74]. Age is also known to cause changes in\nskin characteristics, such as texture, rigidity, and elasticity, which impact the performance of\nemotional face recognition tasks [78].\nInfants represent another vulnerable age group where pain assessment requires special-\nized attention, particularly when they experience painful events. The first challenge is obvi-\nously their limited reporting ability to express their pain through language. Although crying\nmight appear to signal pain, this is an oversimplified and unreliable method, as crying can in-\ndicate a variety of situations, such as discomfort, hunger, or pain. Accurately discerning the\ntype of cry is only one part of the challenge; assessing pain in infants is far more complex and\ninfluenced by numerous factors, including the interpersonal relationships within their envi-\nronment. Riddell and Racine [79] found that through various distressing experiences, infants\ncan learn that specific signaling behaviors can prompt their caregiver\u2019s proximity. This at-\ntachment dynamic suggests that, to some extent, infants may consciously utilize pain-related\nbehaviors to elicit responses from their caregivers. Similarly, the context affects older chil-\ndren as well; for example, self-reports of pain tend to be significantly lower when a parent is\npresent compared to when the child is alone [80].\n2.5.3 Psychological Factors\nMultiple studies have revealed that several psychological factors are consistently linked with\npain-related behavior, including depression, pain-related fear, and catastrophizing. Research18 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nfocusing on the impact of depression and anxiety on pain-related behavior has been con-\nducted mainly on patient populations. These studies have shown that depressed individ-\nuals exhibit more pronounced protective and communicative behaviors compared to non-\ndepressed patients [81]. Similarly, numerous studies suggest that patients with higher levels\nof anxiety demonstrate more pain-related behaviors than those with lower anxiety levels [82].\nDespite the frequent coexistence of pain with psychological conditions, research indicates\nthat these patients often experience underestimation of their pain. For instance, De Ruddere\net al. [83] found that patients dealing with psychological stressors such as anxiety, depres-\nsion, and daily life challenges are often perceived by physiotherapists as experiencing less\nsevere pain, illustrating the influence of psychosocial factors on the patient\u2019s pain experience.\n2.5.4 Race and Culture\nPain expression is generally understood across ethnicities and cultures, though differences\nexist in how it is conveyed [4]. However, cultural variations and the nuances of facial ex-\npressions related to emotion are complex and necessitate deeper study. Additionally, racial\nand cultural biases significantly influence pain assessment, judgment, and interpretation.\nExtensive research highlights the impact of a patient\u2019s race as a sociodemographic factor\non observer responses. The most examined topic relates to the different responses toward\nCaucasian versus non-Caucasian individuals, particularly African Americans, who are more\nlikely to have their pain underestimated and undertreated by healthcare providers [84].\nEthnocultural factors are crucial in shaping how individuals perceive and express pain.\nFor example, Western cultures often emphasize conservative expressions and self-control,\nleading to restrained responses in personal pain experiences and in perceiving others\u2019 pain\n[3]. Differences also arise in coping mechanisms; African Americans, for instance, are more\nprone to catastrophizing pain events compared to European Americans [85]. Furthermore,\nevidence shows racial biases in pain treatment across various racial groups, with certain\ngroups being more sensitive to pain but receiving lower-quality treatment [86]. For exam-\nple, Cleeland et al. [87] found that minority cancer patients, mainly Black and Hispanic\nindividuals, were more likely to experience inadequate analgesia compared to non-minority\npatients.\n2.5.5 Observer\u2019s Impact on Pain\nThe variability in pain management stems from the interplay of various elements, including\nsociocultural, biomedical, and psychosocial factors, especially in cases of chronic pain [88].\nWhen it comes to the observer responsible for assessing a patient\u2019s pain, several characteris-\ntics directly influence the objectivity of their evaluation. The first and perhaps most critical\nfactor is the observer\u2019s experience level. One would expect that more experience leads to2.6. IMPACT OF INADEQUATE PAIN MANAGEMENT 19\nbetter and more accurate assessments, but studies show that even experienced healthcare\nproviders consistently underestimate pain, much like laypersons [28]. The greater the expe-\nrience, the more pronounced the underestimation tends to be. This may be due to desensitiza-\ntion caused by repeated exposure to pain events, as seen in the differences between internists\nand surgeons in their evaluation of postoperative pain, with surgeons often encountering se-\nvere pain more regularly [89]. Another significant factor is the observer\u2019s knowledge and\nbeliefs about pain. For example, [83] found that laypersons and healthcare professionals\nwithout physical signs of pain might view the patient\u2019s complaints less seriously. Proper\ntraining is also essential for adequate pain assessment, which is why the Department of\nHealth and Human Services (DHHS) initiated a strategic program to improve healthcare\nproviders\u2019 education and knowledge regarding pain management, following evidence of in-\nadequate training in the field [90].\n2.6 Impact of Inadequate Pain Management\nThe experience of pain, particularly persistent pain, can have detrimental effects on the indi-\nvidual and their surrounding environment. Thoughts about severe pain often lead to grief and\nfear, causing individuals to perceive pain as a threat and feel incapable of managing it. This\ncan prompt avoidance behaviors aimed at escaping perceived harm [91]. Studies have shown\nthat children with a catastrophizing mindset about pain struggle with daily activities, while\nadolescents with chronic pain tend to have fewer friends and may miss out on social and\nentertainment opportunities, putting them at greater risk of victimization [92]. These adoles-\ncents often feel isolated and lonely compared to their healthy peers, and they may experience\nanxiety in social interactions [93]. Parental reactions to their children\u2019s pain can further com-\nplicate the situation, as parents with catastrophic tendencies tend to engage in overprotective\nbehaviors that hinder the child\u2019s functioning and psychosocial development [94]. Addition-\nally, the family\u2019s overall dynamic is affected, with the patient\u2019s sadness, sleep disorders, and\nchanges in leisure activities impacting the household [74].\nOn a biological level, pain, particularly when experienced early and severely, can alter\nthe brain and nervous system. These early pain experiences can disrupt neurobiological\ndevelopment and affect how pain is processed later in life [95]. A growing body of research\nlinks chronic pain to changes in the medial prefrontal cortex, a region crucial to emotional\nprocessing. Chronic pain is associated with structural and biochemical alterations in this\nbrain area, suggesting that these changes play a role in the pathophysiology of chronic pain\n[96].20 CHAPTER 2. CLINICAL PAIN ASSESSMENT\n2.7 Pain Measurement Scales and Metrics\nIn clinical settings, self-reporting remains the gold standard for assessing pain, allowing in-\ndividuals to describe their pain\u2019s intensity and location. Various self-report scales have been\ndeveloped for different age groups, such as the visual analog scale (V AS) [97] and the verbal\nrating scale (VRS) [98]. Additionally, observation-based scales, where a third party eval-\nuates the pain\u2019s severity, include tools like the Prkachin and Solomon pain intensity scale\n(PSPI) [99] and the neonatal/infant pain scale (NIPS) [100]. However, some studies suggest\nthat patients may exaggerate their pain severity to prompt more aggressive treatment inter-\nventions [101], raising concerns about the accuracy of self-reported symptoms. Therefore,\nobjective pain measurement remains clinically crucial.Chapter 3\nAutomatic Pain Assessment\u2013A Literature\nReview\nContents\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database . . . . 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 49\n2122 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research Di-\nrections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.1 Chapter Overview\nThis chapter corresponds to the publication [17], a systematic literature review (SLR) con-\nducted at the start of this Ph.D. research. This review facilitated an understanding of au-\ntomatic pain assessment methods, particularly those based on deep learning, and the tech-\nniques and strategies employed. It enabled the identification and proposal of new approaches\nthat could enhance the effectiveness of pain recognition.\nAdditionally, it allowed for identifying gaps in the literature from other reviews con-\nducted on this specific research topic. Every existing systematic review on pain assessment\nwas identified and assessed, revealing several insights. The first review on automatic pain\nassessment, published by Prkachin in 2009 [102], did not cover papers on deep learning,\nas the practical implementations of deep architectures only began around 2012. Zamzmi et\nal.[103] focused their review exclusively on infants, omitting deep learning methods. In\n2018, Chen et al. [104] reviewed automated pain detection methods using the Facial Ac-\ntion Coding System (FACS), noting only three publications that employed deep learning\ntechniques. In 2019, Hassan et al. [105] included only seven papers that used deep learn-\ning methods in their review. Similarly, Werner et al. [106], also in 2019, discussed pain\nassessment without restrictions on modalities or age groups, finding fewer than ten papers\nthat reported on deep learning methods. In 2020, Al-Eidan et al. [107] published the first\nsystematic literature review titled \u201cDeep-Learning-Based Models for Pain Recognition: A\nSystematic Review\u201d, which included fifteen papers but was critiqued for having significant\nlimitations and incorrect information. It was noted that some papers analyzed might not be\nrelevant, and there was confusion between \u201cneural networks\u201d and \u201cdeep learning\u201d. For in-\nstance, while study [105] mentioned using neural network approaches, they did not provide\nevidence of using deep learning methods. Moreover, in the study [104], the authors devel-\noped a neural network with only two layers combined with handcrafted features, which does\nnot qualify as a deep learning method. Additionally, studies [103, 107] focused on detect-\ning protective movement behaviors in chronic pain patients, which deviates from the central\ntopic of automatic pain assessment. Several reviews and SLRs on automatic pain assessment\nhave been published, but none exclusively or adequately focus on deep learning methods.\nThis SLR aims to bridge this gap by thoroughly reviewing deep learning techniques used for\nautomatic pain assessment.3.2. MODALITIES AND HARDWARE FOR AUTOMATIC PAIN ASSESSMENT 23\n3.2 Modalities and Hardware for Automatic Pain Assessment\nCreating an automatic pain assessment system hinges on capturing the necessary input data\nthrough various information channels, referred to as modalities. These modalities are cat-\negorized into behavioral and physiological types. A system utilizing only one modality is\ntermed unimodal, whereas a multimodal system incorporates multiple modalities.\nKey behavioral modalities encompass facial expressions, body movements, gestures, and\nauditory signals. Researchers use a range of optical and light sensors to record images or\nvideo sequences of facial and body movements. Commonly, researchers employ color RGB\ncameras, but depth and thermal sensors are also used to enhance visual data. Motion capture\nsensors are also employed to track movements, and microphones are frequently employed\nto capture sound. On the physiological front, modalities often involve biosignals that detect\nelectrical activities from various tissues and organs. Techniques such as electrocardiogra-\nphy (ECG), electromyography (EMG), electrodermal activity (EDA), photoplethysmogra-\nphy (PPG), blood oxygen saturation (SpO2), near-infrared spectroscopy (NIRS), respiration\nrate, and skin temperature are commonly used to gauge pain. Multiple sensors can mea-\nsure several modalities simultaneously \u2014 for instance, strain sensors and cameras can track\nrespiration rates.\nBesides the sensors that gather input data, the computational hardware is crucial. Deep\nlearning-based systems operate in two phases: training and inference. The training phase is\nparticularly resource-intensive, necessitating a graphics processing unit (GPU). The trained\nmodel makes predictions on new data during inference, typically processed on a central\nprocessing unit (CPU). The choice of hardware depends on various factors, especially in\nreal-time scenarios where low latency is crucial, compared to offline settings where data\nprocessing can be deferred. Additionally, characteristics of the model, such as floating point\noperations per second (FLOPS) and total computational operations, are significant consider-\nations.\n3.3 Pain Databases\nAccess to data is crucial for evaluating methods and algorithms in automatic pain assess-\nment. However, only a few databases have explicitly been developed for automatic pain\nrecognition based on human behavioral and physiological changes. Unlike the extensive\ndata found in most facial expression databases, publicly accessible pain datasets often offer\nlimited samples and suffer from significant class imbalance. This primarily stems from the\nethical concerns associated with collecting pain data. Table 3.1 lists the principal databases\nreviewed in the studies. Figure 3.1 shows how frequently each database was used. Most\nresearch utilized publicly available datasets, with some studies exploring multiple datasets.24 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nFew studies used private datasets, mainly those aimed at detecting pain in neonates. The\nUNBC-McMaster Shoulder Pain Archive Database [108] is the most utilized, followed by\nTheBioVid Heat Pain Database [109]. The former contains 200facial videos of 25individ-\nuals with shoulder pain. At the same time, the latter includes facial videos and biopotentials\nof90healthy participants subjected to experimentally induced heat pain at four intensity\nlevels. The following subsections provide a brief description of some of these datasets.\nTable 3.1: Most commonly utilized pain databases.\nDatabase Modality Population Annotation\nGranularityAnnotation Labels\nUNBC-McMaster\nShoulder PainA[108]RGB video of face 25 adults with shoulder painFrame level\nSequence levelFACS\nV AS, OPI\nBioVidA[109] RGB video of face, EDA, ECG,\nEMG87 healthy adults Sequence level stimulus\n(calibrated per person)\nMIntPAINA[110] RGB-Depth-Thermal video of\nface20 healthy adults Sequence level stimulus\n(calibrated per person),\nV AS\niCOPEA[111] RGB photographs of face 26 healthy neonates Frame level pain, cry, rest, air puff,\nfriction\niCOPEvidA[112] Grayscale video of face 49 neonates Sequence level pain, no pain\nNPAD-IA[113] RGB video of face & body, HR,\nSpO2, BP, NIRS36 healthy neonates & 9 neonates\nwith tissue injured by surgerySequence level NIPS, N-PASS\nAPN-dbA[114] RGB video of face 112 healthy neonates Sequence level NFLAPS, NIPS, NFCS\nEmoPainN[115] video, audio, EMG, MoCap 22 adults with chronic pack pain &\n28 healthy adultsSequence level self-report, naive OPI\nSenseEmotionN\n[116]video of face, audio, EDA, ECG,\nEMG, RSP45 healthy adults Sequence level stimulus\n(calibrated per person)\nX-ITEN[117] RGB-Thermal video of face,\nRGB-Depth video of body, au-\ndio, EDA, ECG, EMG134 healthy adults Sequence level stimulus\n(calibrated per person)\nA: Publicly available by request, complete or part of the dataset N: Not yet available Modality: HR: heart rate SpO2: oxygen saturation rate BP:\nblood pressure NIRS: near-infrared spectroscopy MoCap: motion capture RSP: respiration rate EDA: electrodermal activity ECG: electrocardiogram EMG:\nelectromyogram Annotation Labels: FACS: Facial Action Coding System V AS: visual analogue scale OPI: observer pain intensity NIPS: neonatal infant\nscale N-PASS: neonatal pain, agitation and sedation scale NFLAPS: neonatal face and limb acute pain scale NFCS: neonatal facial coding system\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database\nTheUNBC McMaster Shoulder Pain Database [108] comprises 200video sequences show-\ning the facial expressions of 25subjects undergoing motion tests, including arm abduction\nand external and internal rotations. The data collection utilized both active and passive ap-\nproaches: in the active mode, subjects moved their affected arms to their bearable limit,\nwhile in the passive mode, a physiotherapist moved the subjects\u2019 arms. Each video sequence\ncontains about 60to700frames, totaling 48,398, with 82.71% of frames scoring a pain\nrating of zero, indicating a significant imbalance in the data. All frames are FACS-coded\nfor pain-related action units (AUs)\u2014AU4, AU6, AU7, AU9, AU10, AU12, AU20, AU25,\nAU26, AU27, and AU43\u2014with each AU coded for intensity from A to E, 0, or5, except\nfor AU43 (closed eyes), which is coded as either present or absent. Pain scores are assigned\nusing the PSPI metric based on the intensity of the AUs present. Additionally, the database3.3. PAIN DATABASES 25\n0815233038455360\nUNBC-McMasterBioVidEmoPainSenseEmotionX-ITEMIntPAINiCOPEiCOPEvidNPAD-IAPN-dbother\nTable 1Category AUNBC-McMaster59BioVid21EmoPain7SenseEmotion5X-ITE2MIntPAIN4iCOPE3iCOPEvid1NPAD-I5APN-db1other21\n1\nFigure 3.1: The number of studies utilizing these specific datasets. Note that various studies\nused multiple datasets to conduct their experiments.\nincludes 66facial landmarks per frame, determined by an active appearance model. Pain as-\nsessments also include self-reports using two Likert scales with 15options each and a visual\nanalog scale (V AS) from 1(no pain) to 10(extreme pain). One scale measures the sen-\nsory intensity from \u201cextremely weak\u201d to \u201cextremely intense\u201d , while the other assesses the\naffective-motivation aspect of pain from \u201cbearable\u201d to \u201cextremely excruciating\u201d. Indepen-\ndent observer pain intensity (OPI) ratings use a 6-point scale from 0(no pain) to 5(intense\npain). The UNBC database is currently the most extensively utilized dataset for automatic\npain recognition among publicly available resources.\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database\nTheBioVid dataset [109] is a prominent resource in pain research, comprising facial videos,\nelectrocardiograms, electromyograms, and galvanic skin response data from eighty-seven\npn=87qhealthy participants ( 44males and 43females, aged 20to65). The pain was in-\nduced using a thermode on the participants\u2019 right arm, with pain and tolerance thresholds\nestablished before data collection. These thresholds defined the range of pain from No Pain\n(NP) to Very Severe Pain (P 4), encompassing five levels of pain intensity. The temperatures\nfor the pain inductions ranged from P 1to P 4and did not exceed 50.5\u02ddC. Each participant\nunderwent 20inductions at each of four pain levels, with each induction lasting 4sfollowed\nby a recovery period of 8to12s. In addition, 20baseline measurements were taken at 32\u02ddC\n(NP), totaling 100stimulations per participant, randomly administered. Data processing\nsegmented these into 5.5sdurations starting 1safter the target temperature was reached, re-26 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsulting in 8,700samples across the five pain intensity classes, equally distributed among all\nmodalities for each participant. Video recordings were made at a frame rate of 25FPS, and\nbiosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\n3.3.3 The EmoPain Database\nTheEmoPain [115] dataset encompasses various pain indicators, including body movements,\naudio, biosignals, and postural and facial expressions. It features video and audio recordings\nof22patients ( 7male, 15female) exhibiting natural pain expressions while engaging in\nphysiotherapy-like exercises. These exercises, performed at regular and challenging levels,\ninclude a sitting-standing sequence, balancing on one leg for five minutes, and reaching for-\nward while standing. The video signals are captured in high resolution ( 1024\u02c61024 pixels)\nusing eight cameras positioned at various angles, enhanced by specialized lighting condi-\ntions. Audio is recorded with two microphones: an AKG C-1000S MKIII placed near the\ncameras and an AKG HC 577 L worn by the patients, both operating at a 48kHz sampling\nrate with bit Pulse Code Modulation. Body movements and postures are tracked using a mo-\ntion capture suit with 18sensors distributed across the body. Biosignals are monitored with\nfour sEMG sensors attached to the trapezius and lumbar para-spinal muscles. Additionally,\nthe dataset provides continuous frame-wise pain ratings for facial expressions by eight naive\nannotators and binary frame-wise annotations for protective behaviors by four experts, along\nwith coordinates from 26body nodes. Six annotated protective behaviors include stiffness,\nbracing, hesitation, limping, rubbing, and abrupt actions. Audio and EMG signals from the\neight activities per subject also contribute to multimodal pain recognition. Like the UNBC\ndatabase, EmoPain faces significant challenges due to data sparsity and imbalance\u2014only\n11.4%of frames show facial expressions of pain, and 8.6%show protective behaviors. This\nscarcity complicates pain recognition research, necessitating the development of methods\nthat efficiently utilize limited data to achieve optimal performance.\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database\nTheX-ITE [117] dataset is one of the largest pain datasets but is not publicly available. It\ninvolved 134healthy adults ( 67men and 67women) aged between 18and50. The aver-\nage age was 31.4years (SD = 9.7), with men averaging 33.4years (SD = 9.3) and women\n32.9years (SD = 10.2). Participants had no chronic pain, depression, psychiatric disorders,\nneurological conditions, headache syndromes, or cardiovascular disease, nor had they taken\npain medication or painkillers before the experiment. Pain stimuli were stimulated using the\nMedoc PATHWAY Model ATS for heat pain on the forearm and the Digitimer DS7A for elec-\ntrical pain on the index and middle fingers. Both modalities featured phasic stimuli (short,\n5seconds) and tonic stimuli (long, 60seconds), each in three intensities. After calibration,3.4. UNIMODAL STUDIES 27\nparticipants underwent a 90-minute stimulation phase where phasic stimuli were repeated\n30times in a randomized sequence with 8-12-second pauses. The tonic stimuli were applied\nonce per intensity, totaling six per participant, each followed by a five-minute pause. The\nhighest intensity tonic stimuli for heat and electrical pain were induced at the experiment\u2019s\nending, with the other stimuli randomly interspersed during the phasic period. Simultane-\nous to the pain stimulation, various sensors collected multimodal pain response data: frontal\nand side view RGB videos for facial expression and head pose analysis, audio for paralin-\nguistic response analysis, electrocardiogram (ECG) to monitor heart rate variability, surface\nelectromyography (EMG) to assess muscle activity in the trapezius, corrugator supercilii,\nand zygomaticus major, electrodermal activity (EDA) to measure sweating, video for body\nmovement analysis, and thermal video for facial temperature changes.\n3.3.5 The AI4Pain Database\nThe AI4Pain Grand Challenge 2024 [118] dataset is a recent contribution to the pain re-\nsearch field, tailored for sophisticated pain recognition tasks using fNIRS and facial video\ndata. This dataset involves sixty-five volunteers pn=65q, including 23females, with ages\nranging from 17to52years (mean age of 29.06years and a standard deviation of 8.28years).\nAlthough it captures physiological signals such as photoplethysmography (PPG), electroder-\nmal activity (EDA), and respiration (RESP), these signals are not publicly available yet. The\ndataset is segmented into three parts: training ( 41volunteers), validation ( 12volunteers),\nand testing ( 12volunteers). The experimental setup includes fNIRS data recorded with an\nArtinis device, measuring changes in oxygenated and deoxygenated haemoglobin concentra-\ntions across 24channels targeting the prefrontal cortex. The optodes configuration includes\n10sources and 8detectors spaced 30mm apart, using near-infrared light at 760nm and 840\nnm, sampled at 50Hz. Additionally, facial movements are captured by a Logitech Stream-\nCam at30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low Pain ,\nandHigh Pain . It features 65instances of No Pain (each lasting 60s),780instances of Low\nPain (each lasting 10s), and 780instances of High Pain (each lasting 10s). The No Pain\ninstances, recorded during baseline, serve as control data. The Low Pain instances reflect\nmild pain responses, and the High Pain instances capture significant pain, both derived from\na pain tolerance test and reflected in the corresponding neurological and behavioral data\nrecorded.\n3.4 Unimodal studies\nThis section presents the studies that utilized only one information channel to estimate the\nsubject\u2019s pain condition.28 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.1 Vision-based: Static Analysis\nThe first publicly available pain database that significantly contributed to the development\nof automatic pain assessment methods was the UNBC-McMaster Shoulder Pain Database .\nNumerous studies have employed this dataset. Pedersen [119] implemented the first deep\nlearning approach in 2015 to address the pain assessment problem, utilizing a 4-layer contrac-\ntive autoencoder. He combined the encoded representations with a support vector machine\n(SVM), achieving high performance in frame-level pain detection. A significant advance-\nment in vision-based pain recognition methods was the EmoPain challenge in 2020, which\nbecame the first international competition to compare machine learning methods for chronic\npain assessment. Egede et al. [120] presented the EMOPAIN 2020 Challenge , utilizing a\ndataset composed of features extracted via both handcrafted methods and deep-learned mod-\nels. They utilized facial landmarks, histogram of oriented gradients (HOG), and deep vectors\nfrom VGG-16 [121] and ResNet-50 [122], both pre-trained on the Aff-Wild dataset1. The au-\nthors report that combining hand-engineered features with deep learning cues led to the best\nperformance. Similarly, Yang et al. [123] extracted both low- and high-level features from lo-\ncal descriptors and the pre-trained VGG-16 CNN, combining them through weighted coeffi-\ncients. Semwal and Londhe [124] demonstrated that fusing deep-learned features with facial\nlandmarks is beneficial for multi-class pain estimation. Lakshminarayan et al. [125] com-\nbined deep-learned features with handcrafted ones\u2014namely features from VGG-16 [121]\nandResNet-50 [122], HOG, action unit occurrence and intensity, facial landmarks, and head\npose\u2014through a fully connected network. Their study found that combining VGG-16 with\nhandcrafted features lowered regression error, whereas [126] achieved maximal performance\nusing only VGG-16 features with a fully connected network.\nConversely, Semwal and Londhe [127] noted the limitations of traditional handcrafted\nfeature engineering and the computational expense of deep neural networks. As a solution,\nthey proposed a relatively shallow 4-layer CNN, which reduces computational costs due to\nfewer parameters while achieving performance comparable to deeper models. A different\napproach came from [128], where the authors focused on representing facial expressions\nas compact binary codes for pain intensity classification. Feature extraction was conducted\nusing a pre-trained model [129], with a fully connected network used to generate the binary\ncodes.\nSeveral studies utilized CNN ensemble designs with varying architectures to exploit fea-\nture diversity. Semwal and Londhe [130] combined predictions from three compact CNNs\u2014\nVGG-16 ,M-MobileNet [131], and GoogleNet [132]\u2014using the average ensemble rule, re-\nsulting in improved classification performance. Kharghanian et al. [133] developed a con-\nvolutional deep belief network (CDBN) using unsupervised feature learning. An SVM used\n1https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge3.4. UNIMODAL STUDIES 29\nthe extracted features to differentiate between two states for binary pain classification ( i.e.,\npain vs. no pain). Later, [134] added two layers to the CDBN, though the results were not\ndirectly comparable due to differing evaluation methods.\nSeveral papers suggest that because pain is predominantly expressed in specific facial\nregions, focusing on these areas rather than the whole face could improve model accuracy by\nreducing noise. Huang et al. [135] initially identified the left eye, right eye, nose, and mouth\nas key regions and utilized a multi-stream CNN for feature extraction, assigning learned\nweights to enhance attention on these regions. Xin et al. [136] employed a 9-layer CNN\nwith an attention mechanism to assign different weights to face regions, resulting in more\naccurate attention face maps and boosting prediction accuracy by up to 19%. Cui and Huang\n[137] introduced a multi-scale regional attention network (MSRAN), which uses multiple\ncropping regions from video frames. The framework includes self-attention and relation-\nattention modules to highlight pain-relevant regions and explore interrelationships. Li et\nal.[138] extended this concept by integrating contrastive and multi-task training through an\nautoencoder, building on the work of [139].\nOne challenge in pain intensity estimation is that individual facial features, such as face\nshape, can introduce significant variability in how pain is expressed. This makes it difficult\nto distinguish between adjacent intensity levels. To address this, Peng et al. [140] examined\nfacial shape information and developed a deep multi-task network to account for the rela-\ntionship between pain recognition and shape, which improved pain estimation performance.\nSimilarly, Xin et al. [141] proposed a novel multi-task framework that combines a CNN\nfeature learning module with an autoencoder attention component, also estimating subject\nidentity, as individual differences in pain manifestation are key. Their experiments achieved\nstate-of-the-art results on publicly available datasets.\nMost studies report results obtained from controlled laboratory settings, which typically\nfeature proper lighting, minimal head pose variability, and no occlusions. However, such\nconditions do not represent typical hospital environments. Semwal and Londhe [142] ad-\ndressed this by focusing on pain assessment in uncontrolled settings, developing a shallow\nCNN with three convolutional layers that performed comparably to deeper pre-trained mod-\nels. In a subsequent study [143], they introduced a more complex framework comprising\nthree modules that leveraged high-level spatial descriptors with both local and global geomet-\nric cues, achieving results comparable to models like GoogleNet [144] and VGG [121]. Lee\nand Wang [145] explored pain assessment in intensive care unit (ICU) settings, where par-\ntially occluded faces frequently complicate facial analysis. They developed a 4-layer CNN\ncombined with an extreme learning machine (ELM) for final estimation. Virrey and Cae-\nsarendra [146] used CNNs to classify sections of frames where pain was triggered, peaked,\nand subsided. Nugroho et al. [147] tackled pain detection in smart home-care settings, par-\nticularly for elderly patients, using relatively low-power mobile devices. They modified the30 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nOpenFace2library, based on pre-trained FaceNets [148], and showed that transfer learning\ncould enable real-time binary classification ( pain vs.no pain ), even on low-powered hard-\nware.\nResearchers like Dai et al. [149] and Menchetti et al. [150] have noted that most models,\nwhether deep or shallow, are trained on dataset-specific features rather than actual pain-\nrelated features. Moreover, most studies employ validation methods using the same dataset,\nwhile cross-dataset performance is rarely addressed, limiting real-world applicability. To\ntackle these issues, Dai et al. [149] combined pain and emotion detection datasets to develop\na real-time pain assessment system with better generalization capabilities. They emphasized\nthe importance of cross-corpus evaluation, real-time testing, and the need for well-balanced,\necologically valid pain datasets [151].\nSeveral studies have explored combining pain scales to improve prediction objectivity\nand reliability. Liu et al. [152] developed a two-stage personalized model trained using active\nappearance model (AAM) facial landmarks and multi-task learning, with visual analog scale\n(V AS) and observed pain index (OPI) as ground truth. Xu et al. [153] similarly reduced\nmean square error (MSE) by incorporating various pain scales with the VGG-Face model.\nHowever, Casti et al. [154] pointed out the limitations of original ground truth data due to\nsubjectivity and annotation inconsistencies. To address this, they re-annotated their dataset\nwith judgments from multiple experts, using multidimensional scaling to map frames to\nillumination-invariant 3D space, which they then fed into a pre-trained AlexNet [155].\nCelona and Manoni [156] investigated neonatal facial expressions to detect pain, achiev-\ning the highest accuracy when utilizing two pre-trained models: VGG-Face [157] and mapped\nLBP+CNN (MBPCNN) [158]. Similarly, Lu and Hao [159] found that pre-trained models\nwere crucial for small datasets like neonates, as training from scratch led to overfitting. They\nachieved optimal classification performance by fine-tuning the entire VGG-16 model [122].\nHowever, Zamzmi et al. [160] argue that most face recognition methods are tailored for\nadults and thus less applicable to infants. They developed a lightweight 2D CNN trained\nend-to-end and achieved high pain detection accuracy, but external validation on a different\nneonatal dataset revealed challenges with generalizability. In 2019, Brahnam et al. [112]\nintroduced the iCOPEvid neonatal video dataset, a significant contribution since the only\npublicly available neonatal pain dataset [111] previously contained only static images. Their\nexperiments showed that local descriptors based on the bag-of-features (BoF) approach out-\nperformed deep learning models like VGG-Face andResNet . Combining handcrafted and\ndeep-learned features offered only a marginal improvement in performance. In contrast, Za-\nmzmi et al. [161] found that the most effective approach for binary classification (pain vs.\nno pain) was the fusion of high-level features from VGG [162] and optical flow strains, with\n2http://cmusatyalab.github.io/openface3.4. UNIMODAL STUDIES 31\nnaive Bayes serving as the classifier. Celona and Brahnam [163] applied a Wasserstein gen-\nerative adversarial network with gradient penalty (WGAN-GP) [164], demonstrating that\ntraining set augmentation with synthetic samples improved classification performance. Ta-\nble 3.2 summarizes the vision-based studies focusing exclusively on the spatial dimension.32 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [112]F (RGB) texture\ndescriptors- FF 2D CNN`SVM SL C P O 49 k-fold iCOPEvid 79.80 AUC\n'15 [119]F (RGB) - - - AE SVM SeSL,\nSLC P PS 25 LOSO UNBC 86.10 ACC,\n96.50 AUC\n'20 [120]F (RGB) - - FF 2D CNN`NN SL R IC O 36 hold-out EmoPain 0.91 MAE;\n'18 [123]F (RGB) HOG,\nstatistics- FF 2D CNN`SVR SL R IC PS 25 LOSO UNBC 1.44 MSE;\n'21 [130]F (RGB) - - DF 2D CNN`- SL C ID PS 25 k-fold UNBC 93.87 ACC;\n'16 [133]F (RGB) - - - CDBN SVM UL C P PS 25 LOSO:UNBC 87.20 ACC;\n'21 [134]F (RGB) - - - CDBN SVM SL C P PS 25 LOSO UNBC 93.16 AUC\n'19 [135]F (RGB) - - FF 2D CNN - SL C ID1, IC PS 25 LOSO UNBC 88.191ACC\n'20 [136]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 51.10 ACC;\n'20 [140]F (RGB) - - FF 2D CNN`- SL R ID S 25 ? UNBC 79.94 ACC;\n'21 [142]F (RGB) - - - 2D CNN - SL C ID O 8 k-fold other 97.48 ACC;\n'19 [145]F (RGB) - - - 2D CNN ELM SL R IC PS 25 k-fold UNBC\u201a1.22 MSE;\n'19 [146]F (RGB) - - - 2D CNN - SL C TR, CL, DI PS 25 k-fold UNBC 60.00 ACC\n'19 [150]F (RGB) - - - 2D CNN`- SL C AUs-D PS 25, 43 k-fold UNBC & CK+1,\nWilkie97.701ACC;\n'17 [152]F (RGB) statistics - - NN GPM WSL R IC O, S 25 k-fold UNBC 2.18 MAE\n'20 [153]F (RGB) statistics - FF 2D CNN`NN SL R IC S 25 k-fold UNBC 1.95 MAE;\n'19 [154]F (RGB) LBP, MDS - - 2D CNN`- SL C ID O 25 hold-out UNBC 80.00 ACC\n'18 [159]F (RGB) - - - 2D CNN`- SL C ID O ? hold-out other 78.30 ACC\n`: Pre-trained model -:Not exist &: in Dataset indicates the utilization of cross-database training/validation ?: Not found :: The authors provide additional experiments with other validation methods \u201a: The authors\nutilized occluded facial images ;: The authors provide additional metrics Modality: F: face region Non deep features: LBP: local binary pattern MDS: multidimensional scaling Fusion: M: fusion of modalities E:\nfusion of deep learned features or hand-crafted features Deep models: AE: autoencoder RCNN: recurrent convolutional neural network CDBN: convolutional deep belief network CNN: convolutional neural network\nNN: neural network WGAN-GP: Wasserstein generative adversarial model with gradient penalty Non deep model: SVM: support vector machine GPM: Gaussian process regression model kNN: k-nearest neighbors\nNB: naive Bayes ELM: extreme learning machine Learning Method: SL: supervised learning SeSL: semi-supervised learning UL: unsupervised learning WSL: weakly supervised learning Classific./Regres.: C:\nclassification R: regression Objective: P: presence of pain ID: intensity in discrete scale IC: intensity in continuous scale TR: trigger CL: climax DI: diminishing AUs-D: Action Units detection GT: ground truth\nPS: Prkachin and Solomon S: self-report O: observer rating ST: stimulus Validation Method: LOSO: leave one subject out Metrics: AUC: Area Under the ROC Curve ACC: accuracy PPV: precision MSE: mean\nsquared error MAE: mean absolute error3.4. UNIMODAL STUDIES 33Table 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [124]F (RGB) facial landmarks - FF 2D CNN NN SL C, R ID, IC1P 25 LOSO:UNBC 0.171MSE;\n'20 [125]F (RGB) HOG, head pose,\nAUs intensity/\noccurrence, facial\nlandmarksFF - 2D CNN`NN SL R IC O 36 hold-out EmoPain 5.48 RMSE;\n'20 [126]F (RGB) - - - 2D CNN`NN SL R IC O 36 hold-out EmoPain 1.49 RMSE;\n'18 [127]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 92.00 ACC;\n'18 [128]F (RGB) statistics, distance\nmetrics- FF 2D CNN`- SL C, R ID, IC PS 25 LOSO UNBC 0.81 PCC,\n0.69 MSE\n'21 [137]F (RGB) - - FF 2D CNN`- SL C, R ID, IC P 25 LOSO UNBC 91.13 ACC,\n0.78 PCC,\n0.46 MSE\n'18 [138]F (RGB) - - - AE`- SL R IC PS 25 k-fold UNBC 0.33 MAE;\n'21 [141]F (RGB) - - FF [AE, 2D CNN]Y- SL C, R ID1, IC2,\nP3P, ST 25, 87 LOSO UNBC1,\nBioVid (A)289.1711ACC,\n0.8121PCC,\n85.6532ACC,\n40.4012ACC\n'21 [143]F (RGB) entropy texture\ndescriptors- - 2D CNN`- SL C ID O 8 k-fold other 0.92 PPV;\n'18 [147]F (RGB) - - - 2D CNN`- SL C P PS 14 k-fold UNBC 93.00 ACC\n'19 [149]F (RGB) - - - 2D CNN - SL C P PS 25, 20 k-fold UNBC &\nBioVid (A)\u02db56.75 ACC\n'17 [156]F (RGB) HOG, LBP - FF 2D CNN`SVM SL C P O 26 LOSO iCOPE 73.78 ACC\n'19 [160]F (RGB) - - - 2D CNN - SL C P O 31 LOSO NPAD1,\niCOPE296.981ACC;,\n89.802ACC\n'21 [165]F (RGB) - - - 2D CNN`- FL C P PS 25 LOSO UNBC 76.00 ACC;\n'21 [166]F (RGB) - - - 2D CNN`- SL C P O 25 hold-out UNBC 75.49 ACC\n'21 [167]F (RGB) - - - 2D CNN`SVR SL R IC P 25 LOSO UNBC 0.34 MSE\n'21 [168]F (RGB) - - - 2D R-CNN - SL C P O ? hold-out other 87.80 PPV\nY: The authors combined the deep models into a unified framework \u02db: The authors experimented with additional datasets combinations Non deep features: AUs: actions units HOG: histogram of oriented gradients Non\ndeep model: SVR: support vector regression Learning Method: FL: federated learning Metrics: RMSE: root mean squared error34 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [161]F (RGB) optical flow - FF 2D CNN`SVM,\nkNN, NBSL C P O 31 k-fold other 92.71 ACC,\n94.80 AUR\n'19 [163]F (RGB) - - - WGAN-GP - SL C P O 26 LOSO iCOPE 93.38 ACC\n'17 [169]F (RGB) - - - 2D CNN`- SL R IC PS 25 LOSO UNBC 0.99 MAE;\n'20 [170]F (RGB) - - - 2D CNN - SL C ID ST 87 hold-out BioVid (A) 36.60 ACC\n'20 [171]F (RGB) - - - 2D CNN - SL C P PS 25 hold-out UNBC 97.00 PPV;\n'21 [172]F (RGB) - - - 2D CNN - SL C ID P 28 LOSO:UNBC 90.30 ACC\n'19 [173]F (RGB) - - - 2D CNN - SL C P O 31 hold-out NPAD1,\niCOPE291.001ACC;,\n84.502ACC;\n'21 [174]F (RGB) - - - 2D CNN`- SL C P O 26, 30 hold-out iCOPE &\nUNIFESP89.90 ACC;\n'21 [175]F (RGB) - - - 2D CNN - SL C AUs-D P 10 hold-out Pain-ICU 77.00 ACC;3.4. UNIMODAL STUDIES 35\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach)\nPain assessment is particularly challenging due to its complex and dynamic nature. Rely-\ning on static, individual frames to assess pain fails to capture the phenomenon\u2019s temporal\nprogression and often leads to inaccurate estimations. Additionally, many studies highlight\nthe difficulties of applying deep learning techniques to small datasets, with one proposed\nsolution being the combination of deep learning and traditional feature extraction methods.\nEgede et al. [176] addressed this by extracting deep features from a pre-trained CNN, explic-\nitly targeting the eyes and mouth regions. Using a relevance vector regressor (RVR), they\ndemonstrated that combining deep and hand-crafted features led to optimal performance. De-\nspite the valuable insights the UNBC-McMaster database provides, its imbalanced sample\ndistribution\u2014particularly the limited number of frames showing pain\u2014poses a significant\nchallenge for deep learning models. In response, Egede and Valstar [177] devised a method\nbased on the observation that neighboring pain level classes share many common features.\nThis approach allowed them to avoid extracting all possible features for classes with fewer\nsamples, as certain features had already been utilized from other related classes. The study\nalso showed that combining deep and hand-crafted features improved performance. How-\never, in a later study [178], the authors applied a similar approach, using only deep-learned\nfeatures to address data imbalance, but could not replicate the same high-performance levels.\nTavakolian et al. [179] took a different approach, focusing on the detection of genuine\nversus acted pain through facial expressions, a technique with important applications in both\nmedical and forensic contexts. They developed a residual GAN (R-GAN) to capture subtle\nfacial changes and the dynamic nature of expressions, using a weighted spatio-temporal pool-\ning (WSP) method. In a subsequent study [180], the authors suggested that self-supervised\nlearning could reduce the time and effort needed for data labeling, as it does not require\ncomplete dataset annotation. They introduced a new similarity function for learning general-\nized representations with a Siamese network. They also employed statistical spatio-temporal\ndistillation (SSD) based on the Gaussian scale mixture (GSM) to improve computational effi-\nciency. This technique encodes spatiotemporal variations in facial videos into a single RGB\nimage, simplifying the model while maintaining effectiveness.\nOther studies also aim to capture the dynamic aspects of pain. For instance, [181] com-\nbined a random forest classifier with the pre-trained MobileNetV2 model [182], encoding\nvideos by selecting and merging three frames from different time points into a single image.\nOthman et al. [183] emphasized the importance of using diverse datasets\u2014including vary-\ning age, gender, pose, occlusion, and lighting conditions\u2014to improve model generalization.\nThey used multiple data combinations and a reduced version of MobileNetV2 , showing that\ncross-dataset training is essential for achieving better generalizability.36 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.3 Vision-based: Implicit Temporal Utilization\nSeveral studies have explored the application of 3D CNNs for pain assessment. Tavakolian\nand Hadid [184] developed a 3D CNN to capture dynamic facial representations from videos.\nThey noted that researchers often use fixed temporal kernel depths when employing 3D\nconvolution techniques, which limits the ability to capture short, mid, and long temporal\nranges simultaneously. To address this, they designed a model with parallel 3D convolutional\nlayers featuring variable temporal depths, allowing the capture of temporal dependencies\nfrom 32consecutive frames. Similarly, Wang and Sun [185] applied 3D convolutions based\non the architecture proposed in [186], consisting of 8convolutional layers with 3\u02c63\u02c63\nfilters. While they reported high performance, the authors acknowledged that extracting deep\nfeatures from small datasets posed a challenge for model generalization. In a related study,\nHuang et al. [187] developed a framework that integrated 3D, 2D, and 1D CNNs to extract\nspatio-temporal, spatial, and geometric features. For the 3D CNN component, they modified\nthe architecture from [188] by using discrete kernels of 1\u02c63\u02c63and3\u02c61\u02c61rather than the\ntraditional 3\u02c63\u02c63kernel. Other researchers have also proposed 3D deep CNNs with varying\ntemporal depths to capture short, mid, and long-range facial expression variations [189].\nRecognizing the difficulty and time consumption involved in training a deep 3D CNN from\nscratch, they introduced a cross-architecture knowledge transfer learning technique, utilizing\na pre-trained 2D CNN to assist in the training of the 3D CNN. In studies by Praveen et\nal.[190] and [191], the authors employed weakly-supervised domain adaptation, where the\nsource domain focused on human affective expressions and the target domain was explicitly\nrelated to pain expressions. Their framework featured an inflated 3D-CNN (I3D) [192],\nincorporating 3convolutional layers and 3inception modules [132] to capture both spatial\nand temporal information from video data.\nBargshady et al. [193] opted to use the HSV color space instead of RGB, arguing that it\nbetter reflects human visual perception for tasks such as skin pixel detection and multi-face\ndetection. They employed the pre-trained VGG-Face [157] for feature extraction, followed\nby a temporal convolutional network (TCN) using dilated causal convolutional operations to\nleverage temporal dependencies. Rezaei et al. [194] tackled the challenge of pain detection\nin people with dementia, a difficult task due to insufficient pain-related images or videos\nof elderly subjects in existing datasets. They developed a 10-layer 2D CNN that processed\npairs of pain and no-pain images, analyzing frame-to-frame changes and employing con-\ntrastive training methods [195]. The model demonstrated high performance in both healthy\nindividuals and people with dementia. In another study, Pandit and Schmitt [196] explored\nthe potential of using shallow 1D CNN architectures for real-time pain recognition. They ex-\ntracted facial action units from each frame using the OpenFace 2.03toolkit, with promising\n3https://github.com/TadasBaltrusaitis/OpenFace3.4. UNIMODAL STUDIES 37\nresults for pain detection in real-time settings.\n3.4.4 Vision-based: Explicit Temporal Utilization\nSeveral efforts have focused on addressing the limitations of static frames by developing\ndedicated temporal modules. Zhou et al. [197] tackled this issue using a regression frame-\nwork based on a 4-layer recurrent convolutional neural network (RCNN), each with a se-\nquence length of 3time steps. Rodriguez et al. [198] leveraged dynamic information by\ndesigning an LSTM model fed with feature vectors extracted from VGG-16 [122]. Simi-\nlarly, Bellantonio et al. [199] emphasized that facial expressions evolve, making it essential\nto analyze the spatio-temporal dimension of pain. They improved estimation performance\nusing a fine-tuned 16-layer CNN model [157], an LSTM processing 16frames as a time\nwindow, and super-resolution techniques. In another study, Bargshady et al. [200] com-\nbined the VGG-Face CNN [157] with a 3-layer LSTM to extract spatio-temporal features\nfrom grayscale images, applying zero-phase component analysis (ZCA). In [201], principal\ncomponent analysis (PCA) was used to reduce dimensionality. Mauricio et al. [202] also\nemployed VGG-Face but replaced LSTM with a 2-layer gated recurrent unit (GRU) to cap-\nture temporal dependencies. Thuseethan et al. [203] used a conventional 2D CNN and two\nRCNNs to extract temporal features from previous and subsequent frames, enhancing the\ntime dimension of expression analysis.\nA similar approach was followed by Bargshady et al. [204], who employed ensemble\nlearning with three distinct CNN-biLSTM modules, merging their outputs for the final pre-\ndiction. Salekin et al. [205] used a bilinear CNN (B-CNN) based on the VGG architecture\n[121], pre-trained on VGGFace24andImageNet5datasets, along with an LSTM to capture\ntemporal dependencies in image sequences. Kalischek et al. [206] explored deep domain\nadaptation for facial expression and pain detection, utilizing the self-ensembling approach\n[207] with a long-term recurrent convolutional network (LRCN). While they achieved state-\nof-the-art results for facial expression recognition, performance was lower for pain detection,\nlikely due to the subtle nature of pain-related expressions.\nDespite the availability of additional information in pain datasets, multi-task approaches\nremain limited. Martinez et al. [208] proposed a personalized multi-task learning method\nbased on individual physiological and behavioral pain responses. They extracted AAM fa-\ncial landmarks, processed them through a biLSTM to produce PSPI scores, and predicted the\nfinal V AS score. Erekat et al. [209] combined AlexNet [155] with 2 GRU layers to capture\ntemporal dependencies, using both self and observer-reported pain intensity as ground truth.\nVuet al. [210] developed a multi-task framework to estimate pain levels while reconstruct-\n4https://www.robots.ox.ac.uk/ \u02dcvgg/data/vgg_face\n5https://www.image-net.org38 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\ning heatmaps of action unit locations, improving model generalization with a CNN-LSTM\ncombination to capture micro facial movements.\nHuang et al. [211] noted that specific frames within a video sequence exhibit more pro-\nnounced pain expressions, requiring special handling. They developed a novel framework\nusing attention saliency maps with a VGG-16 model, GRUs and learned weights for each\nframe\u2019s contribution to pain intensity estimation. The study demonstrated that dynamic and\nsalient features can significantly improve performance. Similarly, Yu et al. [212] used VGG-\n11 (configuration A) and an LSTM to create an attention mechanism, predicting pain in-\ntensity from 16consecutive frames. Xu and Liu [213] adopted a ResNet-50 model with an\nattention mechanism to extract spatial features, followed by a transformer encoder to capture\ntemporal sequences, achieving promising results.\nIn other studies, Ragolta et al. [214] used extracted action units to train a 2-layer LSTM\npredicting pain on an 11-point scale, employing curriculum learning. Guo et al. [215] devel-\noped a convolutional LSTM (C-LSTM) to extract both spatial and temporal features from\nvideos, showing that temporal models outperform non-temporal models for pain estimation\naccuracy. Rasipuram et al. [216] utilized in-the-wild video data for pain detection, gener-\nating a 3D morphable model without relying on facial landmarks and combining it with an\nLSTM. Zhi and Wan [217] introduced sparse coding with LSTM (SLTM), using the iterative\nhard thresholding algorithm (ISTA) [218] to capture dynamic facial expressions. Although\nSLTM did not achieve high performance, it offers speed and efficiency for specific applica-\ntions. Finally, Thiam et al. [219] developed a method combining motion history and optical\nflow images with a 10-layer CNN and 2-layer biLSTM, showing that weighted score aggre-\ngation improves performance. Table 3.3 summarizes studies incorporating the modalities\u2019\ntemporal dimensions.3.4. UNIMODAL STUDIES 39Table 3.3: Vision-based studies with temporal utilization.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'17 [176] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVR SL R IC PS 25 LOSO UNBC 0.99 RMSE,\n0.67 PCC\n'17 [177] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVM SL R IC PS 25 LOSO UNBC 1.04 RMSE,\n0.64 PCC\n\u201918 [178] F (RGB) - - - NL 2D CNN - SL R IC PS 25 LOSO UNBC 1.20 RMSE,\n0.47 PCC\n'18 [184] F (RGB) - - - I 3D CNN - SL R IC PS 25 LOSO UNBC 0.53 MSE,\n0.84 PCC;\n'18 [185] F (RGB) HOG,\ngeometric\ndifference- DF I 3D CNN SVR SL R IC PS 25 LOSO UNBC 0.94 RMSE,\n0.67 PCC\n'20 [191] F (RGB) - - - I 3D CNN`- WSL R IC PS 24, ? LOSO UNBC\n& RECOLA0.64 MAE,\n0.82 PCC;\n'16 [197] F (RGB) - - FF E RCNN - SL R IC PS 25 LOSO UNBC 1.54 MSE,\n0.65 PCC\n'17 [198] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C, R P, IC1PS 25 LOSO UNBC 0.741MSE,\n0.781PCC;\n'17 [199] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 61.90 ACC\n'19 [200] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 75.20 ACC\n'20 [201] F (RGB) PCA - DF E [2D CNN`,\n1D CNN, biLSTM]Y- SL C ID PS 25 LOSO:UNBC 85.00 ACC;\n'19 [202] F (RGB) - - - E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 85.40 ACC,\n0.62 MSE;\n'19 [203] F (RGB) - - FF E [2D CNN, RCNN]Y- SL R IC PS 25 LOSO UNBC 1.29 MSE,\n0.73 PCC\n'17 [208] F (RGB) - - FF E biLSTM HCRF,\nFCSL C IC O,\nS25 hold-out UNBC 2.46 MAE;\nNon deep features: PCA: principal component analysis Temporal Exploitation: NL: non-machine learning method I: implicit method E: explicit method Deep models: RCNN: recurrent convolutional neural network\nLSTM: long short memory networks biLSTM: bidirectional neural network GRU: gated recurrent unit Non deep models: SVM: support vector machine RVM: relevance vector machine GPM: Gaussian process regression\nmodel HCRF: hidden conditional random fields FC: fully connected SVR: support vector regression Objective: I2: intensity in binary pairs Metrics: PCC: Pearson correlation coefficient40 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [114]F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN RVR SL R IC O 13 LOSO APN-DB 1.71 MAE;\n'19 [179]F (RGB) - - - NL R-GAN - UL C genuine\nvs posedPS,\nST25,\n34,\n87,\n87? UNBC\n& STOIC\n& BioVid (A)\n& BioVid (D)90.97 ACC\n'20 [180]F (RGB) - - FF NL 2D CNN`- SSL C IC P,\nST25\n87LOSO UNBC1,\nBioVid (A)2\u20180.781PCC;,\n71.022AUC;\n'21 [181]F (RGB) AUs\nintensity- H NL 2D CNN`RF SL C ID ST 127 k-fold X-ITE 25.00 ACC\n'19 [183]F (RGB) - - - NL 2D CNN - SL C P ST 87\n134k-fold BioVid (A)\n& X-ITE\u201867.90 ACC\n'20 [209]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC O,\nS25 k-fold UNBC 2.34 MAE\n'20 [211]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC PS 19 LOSO UNBC 0.21 MSE,\n0.89 PCC\n'19 [212]F (RGB) - - FF E [2D CNN, LSTM]Y- SL R IC PS 24 LOSO UNBC 1.22 MSE;,\n0.40 PCC;\n'20 [214]F (RGB) AUs\nintensity- - E LSTM - SL R IC O 36 hold-out EmoPain 2.12 RMSE,\n1.60 MAE;\n'20 [216]F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C P O ? k-fold UNBC 78.20 ACC;\n'20 [219]F (RGB) - - DF E [2D CNN, biLSTM,\nNN]Y- SL C P ST 87\n40LOSO BioVid (A)1,\nSenseEmotion269.251ACC,\n64.352ACC\n'20 [220]F (RGB) - - FF E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 0.84 ACC,\n0.69 PCC;\n\u2018: The authors provide experiments with cross-dataset settings Fusion: H: hybrid Non deep models: RF: random forest classifier3.4. UNIMODAL STUDIES 41Table 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [187]F (RGB) facial\nlandmarks- DF I [3D CNN`,\n2D CNN`,\n1D CNN, FC]Y- SL R IC PS 25 LOSO UNBC 0.76 MSE,\n0.82 PCC;\n'19 [189]F (RGB) - - - I [2D CNN`,\n3D CNN]Y- UL,\nSLC, R IC1, P2P,\nST25, 87 LOSO UNBC1,\nBioVid (A)20.9211PCC;,\n86.0222AUC\n'20 [190]F (RGB) - - - I 3D CNN`- WSL R IC PS 24,?,\n87, 18LOSO UNBC1\n& RECOLA\n& BioVid (A)2\u20180.741PCC,\n0.342PCC\n'20 [193]F (RGB) PCA - FF I [2D CNN`,\nTCN]Y- SL C ID P,\nST25, 20 LOSO:UNBC1,\nMIntPAIN292.441ACC;,\n89.002ACC;\n'20 [194]F (RGB) - - - I 2D CNN - SL C, R IC, P1P 95, 25 k-fold UofR & UNBC182.0011PCC;\n'20 [196]F (RGB) AUs\noccurrence- FF I 1D CNN - SL R IC P 24, 87 hold-\noutUNBC1,\nBioVid (A)0.801CCC\n'20 [204]F (RGB) PCA - DF E [2D CNN`, 1D\nCNN, biLSTM]Y- SL C ID PS,\nST25, 20 k-fold UNBC1,\nMIntPAIN286.001ACC;\n92.262ACC;\n'20 [205]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- SL R P, IC1O 45 LOSO NPAD 3.991MSE,\n1.552MAE\n'19 [206]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- UL C P ST 40 LOSO SenseEmotion 60.61 ACC\n'21 [210]F (RGB) - - - E [2D CNN`,\nLSTM]Y- SL R IC P 25, 27 LOSO UNBC1,\nDISFA\u20180.60`MSE,\n0.82`PCC;\n'21 [213]F (RGB) - - - E [2D CNN`,\nTransformer]Y- SL R IC P 25 LOSO UNBC 0.40 MSE,\n0.76 PCC;\n'21 [215]F (RGB) - - - E 2D C-LSTM - SL C ID S 29 hold-\noutother 69.58 F1\n'19 [217]F (RGB) - - FF E SLSTM - SL C P1, ID2ST 85 LOSO BioVid (A) 61.701ACC\n29.702ACC\n'21 [221]F (RGB) - - - I 3D CNN`- SL R IC S 25 k-fold UNBC 0.66 ICC;\nFusion: H: hybrid Deep models: TCN: temporal convolutional neural network C-LSTM: convolutional-LSTM SLTM: sparse long short memory network Learning Method: SSL: self-supervised learning Metrics: F1:\nF1 score CCC: concordance correlation coefficient42 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.5 Touch sensor-based\nTouch (contact) sensors provide a viable alternative for pain assessment, often outperforming\nvision-based methods. Table 3.4 highlights studies that utilized contact sensor data to evalu-\nate pain. Yu et al. [222] analyzed three categories of pain-no pain, moderate pain, and severe\npain\u2014using EEG signals. They extracted several bands from the biosignals, including al-\npha, beta, and gamma, and applied a convolutional module. The study found that combining\nthese bands yielded better results than evaluating them independently. Similarly, [223] used\nEEG potentials with an autoencoder to compress the raw data and applied a logistic regressor\nfor classification.\nOther researchers, such as Rojas et al. [224], utilized functional near-infrared spec-\ntroscopy (fNIRS) for pain detection. They developed three models\u2014multilayer perceptron\n(MLP), LSTM, and biLSTM\u2014with biLSTM demonstrating superior accuracy. Addition-\nally, [225] focused on PPG signals, extracting hand-crafted features from the time and fre-\nquency domains, which were then combined with a deep belief network (DBN) to achieve\nover65% accuracy in a 4-class pain assessment task. Hu et al. [226] used kinematic data\nto compare healthy individuals with those suffering from low back pain (LBP). Their ap-\nproach, which employed two stacked LSTM layers, reached over 97% accuracy in binary\nclassification using raw motion data. Lastly, Mamontov et al. [227] were the first to apply\nevolutionary algorithms in the design of an optimized recurrent neural network (RNN) for\npain estimation, achieving 91.94% accuracy using EDA signals.\n3.4.6 Audio-based\nA few studies have explored using audio information for pain detection and intensity esti-\nmation, as outlined in Table 3.5. These methods are especially relevant for neonates, where\nfrequent facial and body occlusions make analyzing cries a more effective approach for pain\ndetection. Chang and Li [228] concentrated on infant cries to differentiate between hunger,\npain, and sleepiness. They transformed the audio signals into 2D spectrograms using a fast\nFourier transform (FFT) and trained a 2D CNN for feature extraction. Similarly, [229] uti-\nlized spectrograms generated from recorded sounds, employing a model identical to that\nused in [160]. Thiam and Schwenker [230] focused on detecting adult pain by analyzing\nbreathing sounds. They leveraged deep-learned features from spectrograms with Mel-scaled\nshort-time Fourier transform, combined with various handcrafted cues. A CNN followed by\na biLSTM was used to capture spatial and temporal dependencies, integrating both low- and\nhigh-level features. In a different approach, Tsai et al. [231] examined pain events during\nemergency triage. They developed an LSTM autoencoder framework to extract temporal\nfeatures from verbal behavior, reporting encouraging results.3.4. UNIMODAL STUDIES 43Table 3.4: Touch sensor-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'20 [222]EEG - - FF I 1D TCN - S C ID S 32 k-fold other 97.30 ACC;\n'20 [223]EEG - - - I AE (TCN) LR UL, S C P S 29 LOSO other 74.60 ACC\n'21 [224]fNIRS - - - E biLSTM - SL C ID S 18 k-fold other 90.60 ACC;\n'19 [225]PPG - - - NL DBN SBM U, SL C P1, ID2S 100 k-fold other 86.791ACC,\n65.572ACC\n'18 [226]kinematatics - - FF E LSTM - SL C P LBP 44 LOSO other 97.20 ACC;\n'19 [227]EDA - - FF E [RNN, LSTM,\nGRU, NN]YSelfCGA,\nselfCGP,\nPSOPBSL C P ST 40 LOSO Sense-\nEmotion81.94 ACC\n'21 [232]EDA - - - I NN - SL C P1, I2 ST 87,\n55LOSO BioVid (A)1,\nPainMonit284.2211ACC;,\n86.5012ACC;\nModality: PPG: photoplethysmogram fNIRS: functional near-infrared spectroscopy EEG: electroencephalography EDA: electrodermal activity Deep models: DBN: Deep belief network RNN: recurrent neural network\nNon deep models: SBM: selective bagging model LR: Logistic Regression SelfCGA: Self-Configuring Genetic Algorithm SelfCGP: Self-Configuring Genetic Programming PSOPB: Particle Swarm Optimisation with\nparasitic behaviour GT: LBP: low back pain vs healthy population\nTable 3.5: Audio-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'16 [228]audio (cry) - - - - 2D CNN - SL C P O ? k-fold other 78.50 ACC\n'19 [229]audio (cry) - - - - 2D CNN - SL C P O 31 LOSO:NPAD 96.77 ACC;\n'19 [230]audio\n(breathing)MFCCs,\nRASTA-\nPLP,\nDTD- FF E [2D CNN,\nLSTM]YRFc SL C P ST 40 LOSO Sense-\nEmotion64.39 ACC\n'17 [231]audio\n(voice)prosodic-\nspectral\nfeatures,\nSF- FF E LSTM`SVM UL,\nSLC P1, ID2S 63 LOSO other 72.301UAR,\n54.202UAR\nNon deep features: MFCCs: Mel Frequency Cepstral Coefficients RASTA-PLT: Relative Spectral Perceptual Linear Predictive DTD: descriptors from temporal domain SF: statistical features44 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.5 Multimodal studies\nSince pain is a multidimensional phenomenon, combining multiple modalities in a multi-\nmodal system offers a promising approach. Heterogeneous information sources can com-\nplement one another, enhancing specificity and sensitivity. As reported in [106], when in-\ndividual modalities demonstrate good predictive performance, their fusion tends to yield\nimproved outcomes. Moreover, integrating cues from various channels may be helpful and\nnecessary, especially in clinical settings where specific modalities may become unavailable\n(for instance, if the patient turns and their face is occluded). The information channels can\noriginate from (1) the same hardware sensor but focus on different regions of interest, such\nas RGB facial images and RGB body images [233], (2) different hardware sensors but the\nsame region of interest, like RGB facial images and thermal facial images [110], or (3)\ndifferent hardware sensors and information sources, such as RGB facial images and ECG\nsignals [234]. Table 3.6 lists the studies utilizing multimodal approaches.\n3.5.1 Static Analysis\nA commonly used biosignal combination is those of EDA, EMG, and ECG, as these channels\nare found in all main pain reference databases. Thiam et al. [235] applied an early fusion\nmethod by merging these signals into a 2D representation and inputting it into a 9-layer 2D\nCNN. Their results showed a strong correlation between EDA and pain intensity, and com-\nbining all three modalities did not outperform using EDA alone. Al-Qerem et al. [236] used\nleast generative adversarial networks (LSGANs) to enhance EMG, EDA, and ECG samples,\nreporting a notable improvement in classification when using an SVM on the augmented\ndataset. Haque et al. [110] introduced the MIntPAIN dataset, which includes RGB, depth,\nand thermal videos for multi-class ( 5levels) pain recognition. They combined these three vi-\nsual modalities into a 5D matrix (RGB+D+T) and used it to train the pre-trained VGG-Face\nmodel [157], leading to better classification performance in their experiments.\n3.5.2 Temporal Utilization\nZhiet al. [237] proposed a multimodal stream-integrated neural network that leverages video\nand biosignal data. They combined raw facial video frames with optical flow images to cap-\nture spatio-temporal dependencies via 3D CNNs, integrating these with biosignal features\nextracted using LSTMs. The entire network was trained end-to-end, achieving superior re-\nsults compared to their unimodal methods. Beyond facial analysis, Salekin et al. [233]\nfocused on assessing neonatal pain through body movements in videos. After identifying\nrelevant body regions, video frames were fed into a pre-trained VGG-16 [121], connected to\nan LSTM to capture temporal dynamics. In a follow-up study, Salekin et al. [238] fused three3.5. MULTIMODAL STUDIES 45\nmodalities\u2014facial expressions, body movements, and crying sounds\u2013demonstrating that this\nmultimodal approach outperformed unimodal techniques. Similarly, Wang et al. [239] ex-\nplored combining EMG, EDA, and ECG biosignals with handcrafted and learned features\nfrom a biLSTM model. They applied the minimum relevance method (MRMR) to reduce\nthe number of features, resulting in notable outcomes.\nIn addition to EDA, EMG, and ECG, other biosignal combinations have been explored.\nZhao et al. [240] integrated PPG, EDA, and temperature signals, using 2D convolutions for\nspatial feature extraction and time windows for capturing temporal information. Yuan et\nal.[241] successfully estimated pain using whole-body MoCap sensors and EMG, utilizing\nLSTM layers with an attention mechanism in an autoencoder, which reduced training time\nby leveraging latent space representations of raw data. Similarly, Li et al. [242] employed\nMoCap and EMG as data sources and tested various LSTM configurations to predict pain\nintensity, achieving the best performance with a 3-layer vanilla LSTM combined with a 3-\nlayer fully connected network.46 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.6: Multimodal-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [110]F (RGB,\nthermal,\ndepth)- RF - - 2D CNN`- SL C ID S 20 k-fold MIntPAIN 36.55 ACC\n'19 [233]F, B (RGB) - FF - E [2D CNN`,\nLSTM]Y- SL C P O 31 LOSO other 92.48 ACC;\n'19 [234]F (RGB),\nECG, EDAbiosignals\u2019\nfeaturesmFF FF - 2D CNN`RFc SL C I2 S 85 k-fold BioVid (A) 74.00 ACC\n'19 [235]EDA,\nEMG,\nECG- RF - - 2D CNN - SL C P1I2,\nID2S 87,\n86LOSO BioVid (A)1\nBioVid (B)84.4011ACC;,\n36.5412ACC;\n'20 [236]EDA,\nEMG,\nECGBoruta\nfeaturesFF - - LSGAN SVM UL,\nSLC I2, ID1S 85 hold-\noutBioVid (A) 82.801ACC\n'21 [237]F (RGB),\nEDA,\nEMG, ECGoptical\nflowFF FF NL,\nE, I[3D CNN,\nLSTM]Y- SL C, R P1, I2,\nID2S 87,\n40k-fold:BioVid (A)1,\nMIntPain68.2011ACC;,\n28.1021ACC\n'21 [238]F, B (RGB),\nsound- DF - E [2D CNN`,\nLSTM]Y- SL C P O 45 LOSO NPAD 78.95 ACC;\n'20 [239]EDA,\nEMG,\nECGMRMR,\nbiosig-\nnals\u2019\nfeaturesRF\nFFE biLSTM NN SL C P1, I2 S 87 LOSO BioVid (A) 83.301ACC\n'20 [243]EDA,\nEMG,\nECG- FF - I [DDCAE,\nNN]Y- UL,\nSLC P1, I2 S 87 LOSO BioVid (A) 83.991ACC;\n'21 [244]EDA,\nEMG,\nECG, RSP- FF - I [DDCAE,\nNN]Y- UL,\nSL,\nSSLC, R P1, ID2,\nICS 87,\n40LOSO BioVid (A)1,\nSense-\nEmotion84.2511ACC;,\n35.4421ACC;\n'21 [245]EDA, ECG - FF - E 1D CNN,\nLSTM- UL C P1, I2 S 67 hold-\noutBioVid (A) 81.711ACC\n'20 [240]PPG, EDA,\ntemperature- RF - I 2D CNN - SL R\u02ddP1, ID2S 21 k-fold other 96.301ACC,\n95.232ACC\n'20 [241]MoCap,\nEMG- RF - E AE, LSTM - UL,\nSLC ID O 23 LOSO:EmoPain 52.60 ACC;\n'20 [242]MoCap,\nEMG- RF - E LSTM, NN - UL C ID O 30 hold-\noutEmoPain 80.00 ACC;\n'21 [246]MoCap,\nEMG- RF - E LSTM, NN - SL C ID O 30 LOSO:EmoPain 54.60 ACC;\nm: Not specifically described \u02dd: Ordinal Modality F: face region B: body region EMG: electromyography Non deep features: MRMR: Minimum Redundancy Maximum Relevance method Deep models: LSGAN:\nLeast Square Generative Adversarial Networks3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 47\n3.6 Summary of Automatic Pain Assessment Methods\nThis section presents an analysis of the reviewed studies, summarizing the main conclusions\non current methods for automatic pain assessment, their advantages, and corresponding lim-\nitations. Additionally, it offers recommendations for future research directions that could\nadvance the field of pain research from a computational perspective.\n3.6.1 Input\nFirst, we observe a clear imbalance between unimodal and multimodal approaches in pain\nassessment studies. More than 86% of the reported research focuses on unimodal methods,\neven though the databases often contain multiple information channels. Notably, contact\nsensor-based and audio-based approaches are underrepresented, with only seven and four\nstudies, respectively, compared to 84studies that utilize a vision-based approach.\nMultimodal approaches are even less explored, with only 15studies falling into this\ncategory, making it difficult to draw strong conclusions about the effectiveness of specific\nmodality combinations. However, there are indications that EDA sensor data is particularly\nvaluable compared to other biopotentials. Researchers have primarily focused on visual data,\nlikely due to the complexity of implementing multimodal frameworks or the impracticality\nof contact sensors in non-laboratory settings. Further exploration of diverse modality com-\nbinations is necessary to evaluate their potential for pain assessment fully\u2014additionally, 28\nstudies employed non-deep features to enhance deep-learned representations.\nFinally, we identified three primary strategies in examining the approaches that utilize\ntemporal information: non-machine learning-based, machine learning-based (implicit), and\nmachine learning-based (explicit). Non-machine learning-based methods, such as motion\nhistory images [219] or temporal distillation [180], rely on traditional computer vision tech-\nniques. These methods tend to be more straightforward but are generally less sophisti-\ncated. In contrast, machine learning-based approaches [190] [217] offer richer temporal\ninformation and the flexibility to adapt to specific requirements, such as emphasizing certain\nvideo frames. Among the studies reviewed, 55% employed temporal features, with explicit\nmethods\u2014most commonly LSTM models\u2014being the predominant choice. Given that many\nstudies report superior performance when temporal information is incorporated, compared\nto non-temporal methods, it is evident that further emphasis on temporal approaches is war-\nranted.\n3.6.2 Processing\nRegarding machine learning approaches, various models and techniques have been employed\nfor pain estimation. CNN models remain the most widely used, with more than 75% of stud-48 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nies utilizing 1D, 2D, or 3D filters, highlighting the central role of convolution operations\nin deep learning. Sequential models, such as RNNs, GRUs, LSTMs, and biLSTMs, follow\nclosely behind in popularity. Almost half of the studies used pre-trained models to achieve\ntheir desired performance. This suggests that existing pain databases may not be adequate\nfor training deep-learning models from scratch. Non-deep learning models have also been\nemployed in 26studies as auxiliary decision components, with SVMs and shallow neural net-\nworks being the most common choices. There seems to be significant potential for adopting\nnewer deep learning architectures, especially transformer-based models, which have demon-\nstrated state-of-the-art results in various AI research fields and are particularly suited for\nexploiting temporal modality information [247].\nThe predominant learning method used across studies is supervised learning. How-\never, 16papers explored or adopted alternative methods such as unsupervised learning [119,\n133, 179, 189, 206, 223, 225, 231, 236, 241, 243], self-supervised [180, 244], self-supervised\nlearning [180, 244], semi-supervised learning [119], weakly supervised learning [190, 191],\nand federated learning [165]. Given the limited availability of pain data resources, self-\nsupervised learning appears to be the most appropriate method for future research and should\nbe further embraced by the community.\nLastly, it is notable that most studies\u2014approximately 70%\u2014treat pain assessment as a\nclassification problem rather than a regression problem. However, we believe that regres-\nsion more closely reflects the continuous nature of pain and is better suited to capturing the\ncomplexity of pain sensation.\n3.6.3 Evaluation\nThe primary objectives of the reviewed studies were (i)to estimate pain intensity on a dis-\ncrete scale (multi-class classification), (ii)to measure pain intensity on a continuous scale,\nand(iii)to determine the presence or absence of pain (binary classification). Notably, 25\nstudies focused on pain detection rather than pain intensity estimation, which, from a clin-\nical standpoint, is less informative as it does not provide sufficient data for effective pain\nmanagement. From an engineering perspective, detecting the presence or absence of pain is\nalso a more straightforward and less demanding task.\nA small subset of studies took a different approach to pain estimation. For instance, one\nstudy [179] sought to differentiate genuine pain from acted pain. Another [231] explored\npain events in emergency triage settings rather than controlled laboratory environments,\nwhile [234] examined the feasibility of real-time pain detection on IoT devices. Addition-\nally, [142] and [143] aimed to address the issue of occluded faces in pain estimation. So-\nciodemographic and psychological factors were also considered, as seen in studies like [245],\nwhich explored gender differences, and [194], which focused on pain assessment in elderly3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 49\npatients with dementia. The limited exploration of pain estimation in real-world settings\nor unconventional contexts suggests that current approaches may not be fully applicable in\npractical environments like clinics and hospitals.\nVarious annotation types are used regarding ground truth, such as self-reported ratings,\nFACS, and observer scales. Temporal features are critical for accurately estimating pain\nintensity, making the temporal granularity of the ground truth equally important. Several\nstudies have questioned the objectivity of PSPI scores, as noted in [248], which highlights\nthat PSPI scores can be zero even when pain is present or that there may be no visible facial\nexpressions in low-intensity pain. Pain expressions not captured by the FACS system, such\nas raising eyebrows or opening the mouth, further challenge the use of PSPI [249]. Addi-\ntionally, PSPI does not account for pain-related head and body movements, which are par-\nticularly valuable in newborn assessments [250]. For these reasons, we recommend moving\naway from PSPI as ground truth in favor of self-reports and observer scales at the video-\nsegment level.\nAround 54% of the studies employed the leave-one-subject-out (LOSO) validation method,\nwhich is widely regarded as more objective and better for assessing the generalizability of\nmodels. However, LOSO can be less practical due to the increased model size and longer\ntraining times. When researchers use other validation methods, such as k-fold or hold-out,\nit is essential to ensure that consecutive, highly correlated frames from the same subject do\nnot skew the training and validation results, leading to flawed estimations. Moreover, when\nresearchers define their own validation or testing sets, comparing results across studies\u2014\nespecially between classification and regression models\u2014becomes nearly impossible. We\nbelieve standardized evaluation protocols should be developed for each publicly available\ndatabase for these reasons.\n3.6.4 Pain Databases for Evaluation\nThe availability of suitable public databases is arguably the most crucial factor in addressing\nthe challenge of automatic pain assessment. Several aspects must be considered in evaluating\nthese datasets, including the number of subjects and their characteristics, such as age, sex,\nhealth status, and race. Moreover, the ground truth must be objective and offer meaningful\ninsights into the subject\u2019s pain experience [154].\nFig. 3.1 illustrates the number of papers corresponding to the pain database utilized in\neach study. It is clear from this figure that the UNBC andBioVid databases were the most\ncommonly used public datasets. However, the UNBC dataset does not record the subjects\u2019\nages, despite age being a known factor in pain expression [35,66]. While the BioVid dataset\ndoes document age, the oldest participants are only 65years old, which is notable since pain\nand its management are critical issues among individuals aged 65and older [251]. Simi-50 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nlar limitations are found in other pain datasets, such as X-ITE [117], EmoPain [115], and\nSenseEmotion [116].\nIt is well known that aging causes skin changes, including texture, rigidity, and elastic-\nity alterations, which can impact facial emotion recognition tasks [78]. Additionally, race-\nrelated factors can lead to inaccurate pain assessments due to variations in how pain is ex-\npressed [252]. Notably, one study by Nerella et al. [175] reported lower performance when\ntheir model was tested on African American patients. Furthermore, only one study [194]\nwas found that specifically addressed pain estimation in elderly individuals with dementia.\nIn summary, developing objective, automated, and generalizable deep learning-based\npain assessment systems will only be possible if balanced and representative datasets are\navailable for training and external validation.\n3.6.5 Interpretation of Results\nRecent advancements in AI have shown state-of-the-art performance across nearly every\nscientific discipline, often surpassing human accuracy in specific diagnostic tasks [253].\nHowever, a significant drawback of AI solutions, particularly deep neural networks, is their\nlack of transparency, commonly called \u201cblack box AI\u201d. This term highlights how these\nmodels learn intricate functions that are opaque and frequently incomprehensible to hu-\nmans [254]. This opacity is a primary reason for the criticism directed toward deep learning\ntechniques [255]. Various techniques, such as visualizations and gradients-backpropagation\nfocusing on specific units, have been developed to offer insights into how these models func-\ntion. For further reading, refer to the comprehensive review on explanatory techniques in\ndeep learning [256].\nTable 3.7 outlines the different approaches used to interpret model decisions. Only a\nsmall fraction of the reviewed studies\u2014 20out of 110\u2014implemented methods to explain\nhow their models work and which features or elements they focus on. It is important to\nnote that interpretable machine learning can be broadly defined as the \u201cextraction of rele-\nvant knowledge from a machine-learning model concerning relationships either contained\nin data or learned by the model\u201d [257]. To summarize: (i)18% of the reviewed studies\nprovided an approach to enhance the interpretability of the model\u2019s decision, (ii)all of these\nmethods were applied to studies using facial images as the input modality, and (iii)around\nhalf of these studies were conducted by just three specific research groups. These findings\nsuggest that the issue of interpretability and explainability within deep learning remains un-\nderexplored, particularly in the context of automatically classifying pain severity levels.3.7. CHALLENGES AND FUTURE DIRECTIONS 51\nTable 3.7: Interpretation approaches.\nPaper Year Modality Method\n[124] 2021 F (RGB) visualization (saliency maps)\n[128] 2018 F (RGB) visualization (heat maps)\n[130] 2021 F (RGB) visualization (saliency map)\n[133] 2016 F (RGB) visualization (learned filters)\n[134] 2021 F (RGB) visualization (learned filters)\n[135] 2019 F (RGB) visualization (heat maps),\nvalues of learned weights\n[138] 2018 F (RGB) visualization (saliency maps)\n[141] 2021 F (RGB) visualization (attention maps)\n[142] 2021 F (RGB) visualization (saliency map)\n[143] 2021 F (RGB) visualization (activation maps)\n[153] 2020 F (RGB) visualization (pixels contributions)\n[177] 2017 F (RGB) visualization (average saliency map)\n[179] 2019 F (RGB) visualization\n(generated intermediate representation)\n[194] 2020 F (RGB) visualization (saliency maps)\n[196] 2020 F (RGB) weights per AU (contribution of AUs)\n[173] 2019 F (RGB) visualization (feature maps)\n[174] 2021 F (RGB) visualization (integrated gradients)\n[210] 2021 F (RGB) visualization (heatmaps)\n[211] 2020 F (RGB) visualization (attention maps),\nvalues of learned weights\n[212] 2019 F (RGB) visualization (attention maps)\n3.7 Challenges and Future Directions\nThis section discusses the existing challenges in automatic pain assessment and proposes\nfuture research directions to further progress in the field.\n3.7.1 Current Challenges in Automatic Pain Assessment & Future Research Direc-\ntions\nSeveral limitations exist in the current pain databases. Important demographic factors such\nas sex, gender, and age are often missing, and there is an apparent lack of racial diversity\namong subjects. For example, facial structures and emotional expressions vary across Cau-\ncasian, Asian, and African populations [258]. Moreover, social interactions, such as the\npresence of a partner during assessments, could influence pain manifestation and should\nbe included in future datasets [69]. Estimating the location of pain, particularly for infants\nor individuals with communication impairments, is another vital aspect of pain assessment52 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsystems, which current databases largely overlook. Future datasets should incorporate stim-\nuli targeting various body locations. Furthermore, the videos in existing visual databases\noften have low to medium resolution and frame rates, which are inadequate for capturing\nfacial micro-expressions. Audio data is also sparsely represented, though it holds potential\nas a valuable modality. From an audio perspective, integrating natural language processing\n(NLP) methods to extract linguistic features and create multimodal systems is a promising\ndirection, as shown in affective computing research [259]. Finally, specific validation proto-\ncols should be provided with present and future datasets to ensure objective and consistent\ncomparisons across studies.\nFrom an engineering perspective, several issues must be addressed to advance automatic\npain assessment. Developing multimodal approaches is essential for creating robust systems\nwith enhanced capabilities. Not only do multimodal methods demonstrate better perfor-\nmance than unimodal ones, but they are also crucial in real-world scenarios where a specific\nmodality may become unavailable. Additionally, it is essential to exploit each modality\u2019s\ntemporal aspects fully. We encourage using machine learning models or other techniques\nthat can accommodate the dynamic nature of pain. More work is needed to improve the accu-\nracy of multi-level and low-intensity pain estimation. Another area of research involves the\nrelationship between pain and other affective states, such as negative emotions, which often\ncoexist during painful events. Detecting these emotions could improve pain assessment. Ad-\ndressing challenges like occlusions or poor lighting conditions in vision-based systems also\nrequires attention. Researchers should explore these scenarios, even if current databases do\nnot account for them. Real-time application of pain assessment systems is another critical\nfactor, so future studies should measure throughput, such as the number of images processed\nper second during inference. Generalization is another crucial concern for AI systems, and\nevaluating trained models across different pain databases could be valuable. Finally, to facil-\nitate the clinical adoption of AI-based pain assessment systems, the models\u2019 decisions need\ngreater explainability. Developing or adopting methods that improve interpretability will\nenhance their clinical viability.Chapter 4\nDemographic Variables: Their Role and\nImpact\nContents\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.1 Chapter Overview: Introduction & Related Work\nThis chapter includes the findings published in [35, 36]. As discussed in Section 2, research\nhas demonstrated that biological and psychological differences can lead to variations in how\npain is perceived between men and women. Regarding age, it is known that infants who\ncannot express themselves directly and older adults with health issues require specific care\ndue to their unique needs. However, a significant question remains unanswered in pain\nresearch, both from clinical and biological perspectives: Does the sensation of pain change\nas individuals age? Specifically, does a person in pain perceive their situation differently\n5354 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nas they grow older than others in different age groups? Is the sensation of pain evolving\nthroughout life? To the best of our knowledge, this question has not yet been definitively\nexplored thoroughly. This chapter investigates the differences among males and females, as\nwell as age groups, using ECG signals. In addition, it proposes a computational framework\nthat utilizes these two demographic elements to improve pain assessment performance. This\nchapter analyzes ECG signals to explore the differences in pain perception between males\nand females and various age groups. Additionally, it introduces a computational framework\nincorporating these two demographic factors to enhance the accuracy of pain assessments.\nFrom a computational standpoint, the literature concerning the use of demographic fac-\ntors in pain assessment is scarce. The study in [260] utilized a range of biosignals, including\nEDA, respiration rate, diastolic blood pressure, and facial action units, to demonstrate dif-\nferences in pain perception between males and females. Similarly, in research [245], the\nauthors utilized a hybrid CNN-LSTM model that processed ECG and EDA data, highlight-\ning gender-based variations in pain response. Following the publications of our research,\nanother study by Ricken et al. [261] was released, which explored the differences in adap-\ntation and habituation between men and women. This study extracted handcrafted features\nfrom various biosignals (including ECG and EDA) and employed random forest classifiers\nto analyze the data.\n4.2 ECG Analysis with Classical Machine Learning\nWe explore a pain estimation process using ECG signals and examine variations across dif-\nferent demographic groups, focusing on gender and age. Specifically, we analyze how pain\nmanifestation differs between males and females, investigate variations in pain perception\nacross different age groups, and consider the combined effects of age and gender on pain\nperception.\n4.2.1 Methodology\nThis section will describe the electrocardiography processing algorithm and the methods\nused for feature extraction and classification algorithms.\nECG signal Processing and Analysis\nAn ECG signal captures the heart\u2019s electrical activity over time. Typically, a normal ECG\ndisplays a sequence of waves, identified as P, Q, R, S, T, and occasionally U. These waves\nand their intervals provide crucial insights into heart function. The P wave indicates atrial\ndepolarization, the QRS complex signifies ventricular depolarization and contraction, and\nthe T wave corresponds to the repolarization of the ventricles. Each heartbeat is depicted4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 55\nQR\nST\nPQRS  \nComplex\nPR\nInterval\nQT Interval\nFigure 4.1: The PQRST waveform.\nLow Pass Filter ECG  High Pass Filter Differentiation\nAdaptive ThresholdsMoving W indow\nIntegrationSquaringQRS\nComplexBand-Pass Filter\nLow Pass Filter ECG  High Pass Filter\nDifferentiation\nAdaptive\nThresholdsMoving W indow\nIntegrationSquaring\nQRSBand-Pass Filter\nFigure 4.2: The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure.\nthrough the PQRST complex (refer to Figure 1). Accurately detecting the R wave within the\nQRS complex is especially critical as it is the most pronounced peak in the complex. Precise\ndetection of the R wave allows for the calculation of heart rate (HR) and heart rate variability\n(HRV), the latter of which measures the time intervals between successive R waves, known\nas the R-R or Interbeat interval. The Pan-Tompkins algorithm, developed in 1985, is one of\nthe most extensively used real-time QRS detection algorithms [262]. Over the years, both the\noriginal algorithm and its variations have been rigorously tested, consistently proving their\neffectiveness even with noisy and low-quality data [263,264]. The Pan-Tompkins Algorithm\nis frequently cited as a benchmark in the field due to its robust performance, making it a stan-\ndard against which new QRS detection methods are compared [265]. Our research employed\nthe original Pan-Tompkins Algorithm to identify the QRS complex. We integrated the algo-\nrithm in two primary phases: preprocessing and decision-making. The preprocessing stage\nis crucial for conditioning the ECG by eliminating noise and artifacts, smoothing the signal,\nand enhancing the QRS slope. The preprocessing steps of the Pan-Tompkins algorithm are\ndepicted in the flow diagram shown in Figure 4.2.56 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nFeature Extraction\nThe subsequent phase involves extracting specific features based on the inter-beat intervals\n(IBIs). In our study, we calculated several metrics, including the mean of IBIs, the root mean\nsquare of successive differences (RMSSD), the standard deviation of IBIs (SDNN), the slope\nof the linear regression of IBIs, the ratio of SDNN to RMSSD, and the heart rate, as outlined\nbelow:\n1. Mean of IBIs\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (4.1)\nwhere RRrepresents consecutive Rpeaks.\n2. Root mean square of successive differences\nRMSSD\u201cgffe1\nn\u00b41n\u00b41\u00ff\ni\u201c1pRRi`1\u00b4RRiq2 (4.2)\n3. Standard deviation of IBIs\nSDNN\u201cd\n1\nn\u00b41n\u00ff\ni\u201c1pRRi\u00b4\u00b5q2 (4.3)\n4. Slope of the linear regression of IBIs\nATAx\u201cATb, (4.4)\nwhere is calculated using the least-square approximation, where bis the vector of RR\npeak intervals and Ais the corresponding time series.\n5. Ratio of SDNN to RMSSD\nSR\u201cSDNN\nRMSSD(4.5)\n6. Heartbeat rate\nHR\u201c60\u00a8FS\n\u00b5, (4.6)\nwhere FSis the sampling frequency of the ECG recording, typically 512Hz. Figure\n4.3 illustrates the raw ECG signal and the algorithm\u2019s stages.4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 57\n0 500 1000 1500 2000 2500\nTime (ms)Raw Signal\n0 500 1000 1500 2000 2500\nTime (ms)Band Pass Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Derivative Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Squared\n500 1000 1500 2000 2500\nTime (ms)Moving Window Averaged\nSignal\nQRS\nNoise Level\nSignal Level\nAdaptive Threshold\nFigure 4.3: The signal preprocessing using the Pan-Tompkins algorithm.\nClassification Methods\nFor the classification phase, three widely recognized classifiers were utilized: Linear Dis-\ncriminant Analysis (LDA), Support Vector Machine (SVM) with a linear kernel, and SVM\nwith a Radial Basis Function (RBF) kernel.\n1. Linear Discriminant Analysis\nPpX|y\u201ckq\u201cexp\u00b4\n\u00b41\n2pX\u00b4\u00b5kqt\u03a3\u00b41\nkpX\u00b4\u00b5kqt\u00af\np2\u03c0qd{2|\u03a3k|1{2, (4.7)\nwhere Pdenotes the probability density function for the feature set X, conditional on\nthe target class y\u201ck.\n2. SVM with linear kernel\nKpx1, x2q\u201cxT\n1x2, (4.8)\nwhere x1andx2represent feature vectors from two separate classes.58 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n3. SVM with Radial Basis Function (RBF) kernel\nKpx1, x2q\u201cexp\u02dc\n\u00b4||x1\u00b4x2||2\n2\u03c32\u00b8\n, (4.9)\nwhere \u03c3is the parameter defining the width of the RBF kernel.\nDataset Details\nIn this study, we utilized the publicly available \u201cBioVid Heat Pain Database\u201d [109], which\ncontains facial videos and biosignals (ECG, EMG, EDA) from 87participants ( 44males and\n43females, aged 20\u00b465). This dataset is unique because it is the only publicly accessible\nresource that includes the subjects\u2019 age and gender. The data collection involved applying\na heat stimulus to the right arm of each participant using a thermode. Prior to recording,\nthe pain threshold (the temperature at which the participant first perceives heat as pain) and\npain tolerance (the temperature at which the pain becomes intolerable) were established for\neach participant. The study defined specific thresholds as the temperatures for the lowest\nand highest pain levels. Also, it included two intermediate levels, resulting in five pain\nconditions: No pain (NP), mild pain (P 1), moderate pain (P 2), severe pain (P 3), and very\nsevere pain (P 4). Each participant was exposed to 20stimulations for each intensity level,\ngenerating 100samples across the four modalities.\n4.2.2 Experiments\nIn the following experiments, we specifically used Part A of the BioVid , which includes\npre-processed ECG samples filtered through a Butterworth band-pass filter, totaling 8700\nsamples ( 87\u02c6100\u201c8700 ). All experiments were conducted in triplicate under identical\nconditions, using a distinct classifier for each iteration to compare their effectiveness. This\nwas based on the leave-one-subject-out (LOSO) cross-validation method, utilizing all avail-\nable subjects and ECG samples. The performance of each classifier was evaluated based on\naccuracy.\nUsing the previously mentioned classification algorithms, we conducted experiments to\nrecognize pain and its relationship with demographic factors. The classification tasks were\nstructured around the pain conditions in multi-class and binary classification formats. Specif-\nically, five distinct experiments were executed: (i)multi-class pain classification, (ii)NP vs.\nP1,(iii)NP vs. P 2,(iv)NP vs. P 3,(v)NP vs. P 4. In experiment (i), the goal was to cate-\ngorize an ECG signal into one of the five pain conditions, while experiments (ii)-(v) aimed\nto classify signals into one of two pain conditions, either no pain or the specified pain level.\nFurthermore, considering the gender and age of the subjects, we developed four different4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 59\nTable 4.1: Results for the Basic Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)AllMC 23.72 23.79 22.77\nNP vs P 1 50.97 52.38 49.97\nNP vs P 2 52.55 52.78 52.70\nNP vs P 3 55.20 55.37 53.87\nNP vs P 4 58.62 58.39 57.41\nMC: multi-classification NP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain\nP4: very severe pain LDA: Linear Discriminant Analysis LN:Linear RBF: Radial Basis\nFunction\nexperimental schemes: (i)theBasic Scheme , utilizing the entire dataset, (ii)theGender\nScheme , where data were segmented by the gender of the subjects into males and females,\n(iii) the Age Scheme , which grouped subjects into three age categories: \u201820-35\u2019 ,\u201836-50\u2019 ,\n\u201851-65\u2019 , and (iv) the Gender-Age Scheme , which combined both demographic factors, result-\ning in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\n\u2018Males 51-65\u2019 ,\u2018Females 51-65\u2019 . The most successful classification results are displayed\nin Figures 4.4-4.5 for each task and classification method, while Tables 4.1-4.5 detail the\noutcomes of each individual experiment.\n4.2.3 Results\nTable 4.1 shows the results from the entire dataset, where the multi-class pain classification\nachieved a 23.79% accuracy, and performance scores generally increased with pain intensity,\npeaking at 58.62% for NP vs. P 4. This progression highlights the difficulty in detecting\nlower levels of pain severity. Regarding the classification algorithms, SVM (linear) was\nmore effective, except for the highest pain level task, where SVM (RBF) was less successful.\nIn the Gender Scheme (see Table 4.2), notable differences were observed between males\nand females. Overall, females showed a 1.12% higher accuracy variation than males, with\nfemales achieving 60.69% in NP vs. P 4over males\u2019 56.07%. This 4.62% increase suggests\nthat females are more sensitive to higher levels of pain than males. Interestingly, in NP vs. P 1\nand NP vs. P 2, males outperformed females by 1.16% and1.78%, respectively. Consistent\nwith the first scheme, SVM (linear) yielded better results in most tasks. Figure 4.4 illustrates\nthe gender differences in classification accuracy.\nIn the Age-Scheme (refer to Table 4.3), the \u201820-35\u2019 age group achieved 25.06% accuracy\nin multi-level classification, compared to 23.27% and22.35% for the \u201836-50\u2019 and\u201851-65\u2019\ngroups, respectively, indicating that age significantly affects pain perception. The nearly 9%60 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.2: Results for the Gender Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)MalesMC 22.13 22.25 20.70\nNP vs P 1 51.53 52.61 47.72\nNP vs P 2 53.12 53.69 52.15\nNP vs P 3 54.94 54.71 51.36\nNP vs P 4 55.28 56.07 51.36FemalesMC 25.11 24.41 23.41\nNP vs P 1 50.23 51.45 49.06\nNP vs P 2 51.62 51.86 51.91\nNP vs P 3 55.98 55.87 55.29\nNP vs P 4 60.17 60.69 59.82\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.4: Results for the Gender Scheme .\ndifference in the NP vs P 4task between the youngest and oldest groups was particularly no-\ntable. Similar to the gender-based results, minor differences in low pain intensities among\nthe age groups became more pronounced as pain intensity increased. Specifically, the vari-\nance ( \u03c32) between the groups in NP vs. P 1was1.38%. At the same time, in the other tasks,\nit increased to 2.44%,6.35%, and 20.42%, respectively, showing that high pain intensities\nare necessary to discern perceptual differences among age groups. Regarding classification\naccuracy, the \u201820-35\u2019 group showed the highest sensitivity, followed by \u201836-50\u2019 and\u201851-\n65\u2019. Regarding classification methods, the SVM (RBF) performed best in the \u201851-65\u2019 group\nacross almost all tasks, while it underperformed in the \u201820-35\u2019 group, suggesting it is better\nsuited for more challenging, separable classes. Figure 4.5 displays the results from the age\nscheme.\nIn the final analysis, we examined the subjects more closely to gain deeper insights into\nthe relationship between pain and the demographic factors of gender and age. As shown4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 61\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.5: Results for the Age Scheme .\nin Tables 4.4-4.5, the \u2018Females 20-35\u2019 group achieved the highest accuracy in multi-class\npain classification at 24.80%, while \u2018Females 51-65\u2019 led in NP vs. P 1with 55.38%, again\nindicating higher pain sensitivity in females. Moreover, \u2018Females 51-65\u2019 followed by \u2018Males\n51-65\u2019 topped the performance in NP vs. P 2, and in NP vs. P 3,\u2018Females 36-50\u2019 surpassed\nthe next best group, \u2018Males 20-35\u2019 , by3.5%. In the final NP vs. P 4task, \u2018Females 20-\n35\u2019excelled with 67% accuracy, whereas \u2018Males 51-65\u2019 had the lowest at 54.50%, marking\nthem as the most and least pain-sensitive groups, respectively. It is noted that sometimes\nclassification accuracy decreases despite increased pain levels ( e.g.,\u2018Females 36-50\u2019 ). This\nmight be attributed to the subjects becoming accustomed to the stimulus over time during\nthe biosignal recording.\nFigure 4.6 illustrates the classification performance of the six groups in the Gender-Age\nScheme . Additionally, Table 7 compares our results with other studies that used ECG signals\nfrom the BioVid database and followed the same evaluation protocol, ensuring an objective\ncomparison. Our study achieved the best classification performance in both the multi-class\nsetting and the NP vs. P 1and NP vs. P 2tasks, with acceptable results in the remaining binary\nclassification tasks.\n4.2.4 Discussion\nWe analyzed ECG biosignals using the Pan-Tompkins algorithm to detect QRS complexes\nand extracted features about inter-beat intervals. We also evaluated three machine learning\ntechniques, assessing their performance in multi-class and binary pain classification across\nvarious pain intensities. We also examined the influence of gender and age on pain percep-\ntion, discovering significant differences: males generally showed lower sensitivity to high\npain levels. Regarding the age factor, significant variations suggest that pain sensitivity tends\nto diminish with age, potentially increasing the risk of further injury. In certain demographic\ngroups, the difference in pain perception exceeded 12%, underscoring the variability of pain\nsensation among individuals.62 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.3: Results for the Age Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)20-35MC 25.06 24.73 21.96\nNP vs P 1 52.83 52.83 49.90\nNP vs P 2 54.33 53.75 52.75\nNP vs P 3 55.58 56.16 54.66\nNP vs P 4 63.83 63.41 60.7536-50MC 23.27 22.06 23.03\nNP vs P 1 50.34 48.36 50.68\nNP vs P 2 49.13 51.20 50.17\nNP vs P 3 58.10 58.70 58.27\nNP vs P 4 58.10 57.75 55.9451-65MC 21.89 22.07 22.35\nNP vs P 1 52.23 51.87 52.58\nNP vs P 2 52.14 51.69 52.76\nNP vs P 3 53.66 53.39 54.10\nNP vs P 4 54.46 54.19 54.91\nTable 4.4: Results for the Gender-Age Scheme (Males) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Males 20-35MC 23.13 23.20 18.73\nNP vs P 1 52.50 52.83 45.83\nNP vs P 2 54.00 53.50 53.16\nNP vs P 3 56.33 56.50 54.83\nNP vs P 4 60.00 59.00 53.66Males 36-50MC 23.21 22.21 20.92\nNP vs P 1 50.53 50.53 46.42\nNP vs P 2 50.00 51.78 47.50\nNP vs P 3 54.64 56.25 47.32\nNP vs P 4 55.53 56.25 51.96Males 51-65MC 20.06 21.60 19.60\nNP vs P 1 52.66 51.66 50.66\nNP vs P 2 54.00 54.66 51.50\nNP vs P 3 53.00 54.66 51.50\nNP vs P 4 53.33 54.50 49.834.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 63\nTable 4.5: Results for the Gender-Age Scheme (Females) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Females 20-35MC 24.73 24.80 23.26\nNP vs P 1 49.83 51.50 52.00\nNP vs P 2 54.50 53.66 46.50\nNP vs P 3 53.50 52.83 49.00\nNP vs P 4 65.83 67.00 62.16Females 36-50MC 23.06 22.73 21.93\nNP vs P 1 48.16 49.33 48.33\nNP vs P 2 48.66 49.83 47.83\nNP vs P 3 57.50 60.00 55.00\nNP vs P 4 59.00 58.83 56.16Females 51-65MC 21.23 21.84 23.92\nNP vs P 1 48.84 49.80 55.38\nNP vs P 2 51.15 48.65 55.96\nNP vs P 3 53.07 53.07 50.96\nNP vs P 4 52.69 55.00 56.34\nTable 4.6:\nComparison of studies utilizing BioVid , ECG signals\nand LOSO validation (1).\nMethod Task Results\nMartinez and Picard [266] NP vs P 4 57.69\nWerner et al. [267]NP vs P 1 48.70\nNP vs P 2 51.60\nNP vs P 3 56.50\nNP vs P 4 62.00\nThiam et al. [235]MC 23.23\nNP vs P 1 49.71\nNP vs P 2 50.72\nNP vs P 3 52.87\nNP vs P 4 57.04\nOursMC 23.79\nNP vs P 1 52.38\nNP vs P 2 52.78\nNP vs P 3 55.37\nNP vs P 4 58.6264 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65MC232523232224BL vs P1535251495355BL vs P2545552505556BL vs P3575456605553BL vs P4606756595456018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65\n1\nFigure 4.6: Results for the Gender-Age Scheme .\n4.3 ECG Analysis with Multitask Neural Networks\nIn this section, we build on previous analysis 4.2 that explored variations in pain manifesta-\ntion across different demographic groups using ECG signals. It expands this investigation\nby implementing neural networks as the primary machine learning model and introduces a\nnovel multi-task learning (MTL) neural network. This network leverages demographic infor-\nmation to estimate age and gender in addition to pain levels, aiming to enhance the automatic\npain estimation system.\n4.3.1 Methodology\nThe following method we developed is a neural network-based approach. The feature extrac-\ntion process remains the same as previously described in 4.2.1, utilizing the Pan-Tompkins\nalgorithm for ECG signal processing.\nNeural Network\nThe proposed neural network was designed and trained using two distinct approaches: single-\ntask learning (STL) and multi-task learning (MTL). In the multi-task learning framework, the\nnetwork is simultaneous training for pain estimation and predicting age and/or gender.\nSingle-Task Neural Network: The proposed neural network comprises two components:\nthe encoder, which maps the original feature vectors into a higher dimensional space, and\nthe task-specific classifier. In our design, both the encoder and the classifier utilize fully-4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 65\nTable 4.7: Hyper-parameters used in our approach.\nEpochs Optimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsLabel\nsmoothEMA\n300 AdamW 1e-3 cosine 0.1 50 0.1 \u2713\nconnected (FC) layers, which are defined as follows:\nzipsq\u201cbi`nin\u00ff\nj\u201c1Wijsj for i\u201c1, .., n out, (4.10)\nwhere zirepresents the result of linearly combining the incoming inputs sj, with each input\nbeing weighted by Wijand adjusted by a bias bi. Each layer in the encoder is followed by a\nnonlinear activation function, specifically the rectified linear unit (ReLU), which is defined\nas:\n\u03c3pzq\u201c#\n1, z\u011b0\n0, z\u01030(4.11)\nThe classifier\u2019s layers are connected without nonlinearity, the encoder comprises four fully\nconnected (FC) layers with 256,512,1024 , and 1024 neurons respectively. The classifier\nincludes 2layers with 1024 andnneurons, where nrepresents the number of distinct pain\nclasses being classified. The hyperparameters of the network are detailed in Table 4.7.\nMulti-task neural network: This proposed machine learning method is based on the princi-\nple of sharing representations across related tasks, which helps the model better generalize\nto the primary task of pain estimation in this case. We kept the same encoder and pain\nclassifier in this configuration but introduced two additional auxiliary networks for age and\ngender estimation. The architecture of the proposed multi-task learning (MTL) neural net-\nwork is illustrated in Fig. 4.7. The objective of this network is to simultaneously minimize\nthree different losses. We adopt and expand upon the framework suggested by [268] for the\nmulti-task learning loss, where learned weights are applied to each loss function based on\nthe homoscedastic uncertainty of each task:\nLtotal\u201crew1LPain`w1sc1`rew2LAge`w2sc2`rew3LGender`w3sc3. (4.12)\nHere, Lrepresents the corresponding loss, wdenotes the weights, and care the coefficients\nthat modulate the losses LAgeandLGender to prioritize learning in the pain estimation task.\nIt should be noted that all tasks are treated as classification problems, utilizing cross-entropy66 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nL1\nL2\nL3 L4Feature\nvector\nPainAge\nGender6 x 1256 x 1512 x 11024 x 1 1024 x 1\n1024 x 1n x 1512 x 1256 x 1\nL1L2L1L2L336 x 1\n512 x 1256 x 1\nL1L2L32 x 1\nEncoderMTL LossPain\nEstimation\nFigure 4.7: The proposed MTL network: The sizes of the extracted vectors for the network are\nas follows: for the Pain classifier, n\u02c61, where nis the number of pain estimation\ntasks ( e.g.,2for binary classification, 5for multi-class classification); for the Age\nclassifier, 36\u02c61, where 36represents the possible age values of the subjects; for\nthe Gender classifier, 2\u02c61, corresponding to the two possible gender categories\n(i.e., males and females).\nloss with label smoothing:\nLD\u201c\u00b4\u00ff\n\u03b4PDnout\u00ff\ni\u201c1ppi|x\u03b4qlogrqpi|x\u03b4qs. (4.13)\nHere, Ddenotes the pain database, ppi|x\u03b4q \u201c1\u00b4\u03f5represents the probability of the true\nclass igiven the input x\u03b4, and ppi\u2030i\u03b4|x\u03b4q \u201c\u03f5{pnout\u00b41qis the probability distribution\nacross the other classes. This formulation spreads a small portion \u03f5of the probability across\nclasses other than the true class to implement label smoothing. Furthermore, qpi|x\u03b4qis the\nprobability distribution over the classes ias predicted by the network\u2019s output.\n4.3.2 Experiments\nSimilar to 4.2.2, we utilized the BioVid database, specifically focusing on its ECG signals.\nEmploying a single-task neural network (ST-NN), we conducted an initial series of experi-\nments to assess the impact of demographic factors. Building on the concept we proposed\nin the previous section, we devised five experimental schemes: (i)theBasic Scheme , which\nincluded all subjects from the database; (ii)theGender Scheme , which segregated subjects4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 67\ninto male and female groups; (iii)theAge Scheme , which categorized subjects into three age\ngroups\u2014 \u201820-35\u2019 ,\u201836-50\u2019 , and \u201851-65\u2019 ; and (iv)the\u2018Gender-Age Scheme\u2019 , which combined\nboth demographic factors, resulting in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\n\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\u2018Males 51-65\u2019 , and \u2018Females 51-65\u2019 . All experiments were\nconducted in both binary and multi-class classification formats. Specifically, the binary clas-\nsification tasks were (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3, and (4) NP vs. P 4, and the\nmulti-class task utilized all available pain classifications from the database.\n4.3.3 Results\nDemographic Groups\nTable 4.8 presents the classification results of the Basic Scheme , which utilized all subjects\nin the database. For the multi-class pain classification, we achieved an accuracy of 29.43%,\nwith NP vs. P 1scoring 61.15% and NP vs. P 4reaching 68.82%. These results indicate that\nas pain intensity increases, so performs, highlighting the difficulty in recognizing less severe\npain. According to the Gender Scheme (refer to Table 4.9), notable differences emerge\nbetween males and females, particularly at higher pain intensities. Specifically, in NP vs.\nP4, females achieved an accuracy of 69.48% compared to 66.48% for males, with an overall\nvariance of 1.63% between genders, suggesting that females exhibit higher pain sensitivity.\nFigure 4.8a illustrates these gender-based classification disparities. In the Age Scheme (see\nTable 4.10), the \u201820-35\u2019 age group outperformed the \u201836-50\u2019 and\u201851-65\u2019 groups in NP vs. P 4,\nwith accuracies of 72.58%,66.29%, and 64.91%, respectively. While the differences are less\npronounced at lower pain intensities, this scheme still shows that age significantly impacts\npain perception, particularly among the older population. Figure 4.8b shows the results from\nthe age scheme.\nIn the final scheme, by dividing subjects into more specific groups, we can analyze them\nmore precisely and gain better insights into the relationship between gender, age, and pain\nperception. Table 4.11 reveals that in the NP vs. P 4task, the group \u2018Females 20-35\u2019 reached\nthe highest accuracy of 71.67%, significantly outperforming the \u2018Males 51-65\u2019 group, which\nscored the lowest at 60.67%, marking them as the least sensitive group. This pattern is\nconsistent across the multi-class classification and other pain tasks, with \u2018Females 20-35\u2019\nand\u2018Males 51-65\u2019 exhibiting the highest and lowest accuracies, respectively. This supports\nthat females generally experience more pronounced pain responses, while older males have\na reduced pain sensation. It is noted that in some instances, such as with \u2018Males 20-35\u2019\nand\u2018Males 36-50\u2019 , higher pain levels do not necessarily correlate with higher classification\naccuracy, a phenomenon also noted in our previous experiments, in Section 4.2.3. A possible\nexplanation could be the subjects\u2019 habituation to pain stimuli, especially at lower intensities.\nFigure 4.8c visualizes the performance outcomes of the Gender-Age Scheme .68 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.8: Results for the Basic Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN 61.15 62.87 65.14 68.82 29.43\nST-NN: single-task neural network NP: no pain P1: mild pain P2: moderate pain P3: severe pain P4: very severe\npain MC: multi-classification\nTable 4.9: Results for the Gender Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales ST-NN 60.40 63.24 63.18 66.48 28.61\nFemales ST-NN 60.87 62.15 66.98 69.48 30.59\nTable 4.10: Results for the Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n20-35 ST-NN 61.58 64.08 66.08 72.58 31.07\n36-50 ST-NN 60.52 61.38 64.05 66.29 29.59\n51-65 ST-NN 61.70 60.80 62.50 64.91 27.82\nTable 4.11: Results for the Gender-Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales 20-35 ST-NN 62.83 62.33 65.50 71.33 29.73\nMales 36-50 ST-NN 61.79 60.00 59.64 64.11 27.14\nMales 51-65 ST-NN 59.50 58.67 57.33 60.67 26.07\nFemales 20-35 ST-NN 63.17 63.17 66.83 71.67 31.53\nFemales 36-50 ST-NN 59.50 61.00 65.83 67.00 29.13\nFemales 51-65 ST-NN 60.96 60.96 59.23 63.27 27.69\nAugmentation of Feature Vectors\nBuilding on the findings from the previous experiments about the impact of demographic\nfactors on pain perception, we explored the practical use of subjects\u2019 demographic data.\nExperiments were conducted using the Single-Task Neural Network (ST-NN) and feature4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 69\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(a) Gender\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(b) Age\nAge\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-Age\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\nBL vs P1 63 63 62 60 60 61\nBL vs P2 62 63 60 61 59 61\nBL vs P3 66 67 60 66 57 59\nBL vs P4 71 72 64 67 61 63\nMC 30 32 27 29 26 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales 20-35\n Females 20-35\n Males 36-50\n Females 36-50\n Males 51-65\n Females 51-65\nGender\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales\n Females\nAge-1\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-1\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales\n Females\n020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\n20-35 36-50 51-65018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales Females020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\n1\n(c) Gender-Age\nFigure 4.8: Results for the proposed Schemes.\nvectors enhanced with demographic attributes. Initially, the feature vectors, which originally\nconsisted of six features (see 4.3.1), were augmented by adding either one additional fea-\nture ( i.e., the subject\u2019s gender or age) or two additional features (both the subject\u2019s gender\nand age). We conducted the same pain estimation tasks using this enhanced set of features.\nAs shown in Table 4.12, the results demonstrate improved performance with the augmented\nfeature vectors. Specifically, the most effective augmentation involved combining gender70 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.12: Comparison of results adopting the feature augmentation approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAux: Auxiliary information -: original feature vectors F(G): feature vectors with the additional feature of gender F(A): feature\nvectors with the additional feature of age F(GA): feature vectors with the additional features of gender and age\nand age features, which increased the average pain estimation performance by 0.55%. Us-\ning these demographic features individually also improved classification accuracy, albeit\npartially.\nMulti-Task Neural Network\nThe final experiments utilized a multi-task learning framework with the proposed Multi-Task\nNeural Network (MT-NN) outlined in 4.3.1. The classification results for MT-NN, incorpo-\nrating additional tasks of (1) gender estimation, (2) age estimation, and (3) simultaneous\ngender and age estimation, are detailed in Table 4.13. For comparison, the results from ear-\nlier experiments using the Single-Task Neural Network (ST-NN) method are also included\nin Table 4.13. We noted that the task of gender estimation alone performed less effectively\nthan the other tasks. In contrast, the combined gender and age estimation delivered the high-\nest performance across four tasks. Specifically, in the multi-class classification, it achieved\n30.24%, and in NP vs. P 1, it reached 62.8%, marking the best results of any method pre-\nsented in this study. In NP vs. P 3and NP vs. P 4, the combined tasks outperformed the\nindividual gender and age tasks but were slightly less effective than the ST-NN approaches\nusing augmented features. Interestingly, in NP vs P 2, the age estimation task alone excelled,\nachieving 63.97%, the highest result recorded in this study.\nComparing the overall performances of MT-NN with the ST-NN approaches (using both\noriginal and augmented feature vectors), there is a noticeable improvement of 0.71% and\n0.39%, respectively, in average pain estimation accuracy across all tasks. Figure 4.9 visually\ncompare each neural network approach used in this study, encompassing multi-class and\nbinary classification tasks.4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 71\nTable 4.13: Comparison of results adopting the MT-NN approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAll MT-NN T(G) 61.72 63.39 65.95 68.99 30.00\nAll MT-NN T(A) 62.72 63.97 65.40 69.28 29.79\nAll MT-NN T(GA) 62.82 63.68 66.12 69.40 30.24\nT(G): MT-NN with the additional task of gender estimation T(A): MT-NN with the additional task of age estimation T(GA):\nMT-NN with the additional task of gender and age estimation\nComparison with Existing Approaches\nIn this section, we benchmark the results achieved using the Multi-Task Neural Network\n(MT-NN), which incorporated additional tasks of gender and age estimation against relevant\nstudies. These comparative studies also utilized electrocardiography signals from Part A of\ntheBioVid database with all 87participants. To ensure a fair comparison, they followed the\nsame evaluation protocol, specifically the leave-one-subject-out (LOSO) cross-validation.\nThe comparative results are detailed in Table 4.14 and include research that employed hand-\ncrafted features with traditional machine learning algorithms [35] [267], end-to-end deep\nlearning models [269] [235], and finally, hybrid approaches combine hand-crafted features\nwith deep learning classifiers [266]. Our approach, which leverages hand-crafted engineered\nECG features and a high-dimensional mapping from the encoder in combination with multi-\ntask learning neural networks, demonstrated superior performance across all pain estimation\ntasks, whether in binary or multi-class classification settings.\n4.3.4 Discussion\nWe explored multi-task learning neural networks for automatic pain estimation from electro-\ncardiography signals. By implementing the Pan-Tompkins algorithm to identify QRS com-\nplexes, we extracted features associated with inter-beat intervals (IBIs). Numerous experi-\nments were conducted to explore how gender and age influence pain perception, highlighting\ntheir significant impact. Additionally, we introduced two approaches to enhance pain esti-\nmation results by leveraging demographic information. Firstly, we augmented the original\nfeature vectors by incorporating the subjects\u2019 demographic data, improving classification\naccuracy. Secondly, we employed a multi-task learning neural network that combined the72 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(a) Binary classification\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(b) Multi-class classification\nFigure 4.9: Comparison of performances utilizing various neural networks approaches.\nTable 4.14: Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2).\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nGkikas et al. [35]:52.38 52.78 55.37 58.62 23.79\nHuang et al. [269]\u2039d- - - 65.00 28.50\nMartinez and Picard [270]\u00b8- - - 57.69 -\nThiam et al. [235]\u203949.71 50.72 52.87 57.04 23.23\nWernel et al. [267]:48.70 51.60 56.50 62.00 -\nThis study\u00b862.82 63.68 66.12 69.40 30.24\n::hand-crafted features and classic machine learning \u2039: end-to-end deep learning \u00b8: hand-crafted features with deep\nlearning classification algorithms d: pseudo heart rate gain extracted from visual modality\ntasks of pain, gender, and age estimation. This approach yielded superior results compared\nto methods previously discussed in this chapter and other related research. These findings\nindicate that domain-specific features can achieve excellent outcomes when combined with\nwell-designed deep-learning architectures and demographic factors.4.4. SUMMARY 73\n4.4 Summary\nIn this chapter, we examine the impact of age and gender on pain perception using ECG\nsignals to extract relevant features. Our study involved a series of experiments where subjects\nwere categorized into different groups based on gender (males and females) and age (20-35,\n36-50, and 51-65 years). Additionally, we created combined groups that segregated age\ngroups within each gender. Our findings from both approaches provided strong evidence\nof significant differences in pain perception among these groups. Notably, we observed\na12.5%disparity in pain sensitivity between young females and older males. Generally,\nour results confirm that females exhibit higher pain sensitivity than males, aligning with\nfindings from other studies in pain research. A critical discovery from our study is that pain\nsensitivity appears to decrease with age, which may increase the risk of unnoticed injuries.\nWe presented two methods of incorporating demographic information into our models from a\ncomputational perspective. First, we augmented the feature vectors derived from ECGs with\ndemographic data. Second, we utilized a multi-task neural network approach to estimate\npain, gender, and age simultaneously. Both methods demonstrated improved performance\ncompared to the standard approach, indicating that integrating demographic information can\nenhance the accuracy of automatic pain assessment systems.\nWe recommend that clinical pain assessment tools be designed for specific demographic\ngroups to account for the distinct ways pain manifests across different populations. Ad-\nditionally, we emphasize to researchers developing new pain databases the importance of\nincluding demographic factors and information on the social context and psychological con-\nditions of subjects to enhance the quality and applicability of the data collected. Our study\nfocused on analyzing pain sensation through biosignals, specifically ECGs. We propose that\nfurther research should explore pain expressivity through visual mediums such as video. As\npreviously discussed in Section 2, the expression of pain is a crucial and complex issue. Peo-\nple vary in expressiveness; for various reasons, they might exaggerate or even feign pain,\nmaking accurate assessment challenging.74Chapter 5\nOptimization: Balancing Efficiency and Per-\nformance\nContents\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the studies published in [37, 38]. As outlined in\n3.6.3, research in automatic pain assessment has rarely considered real-world situations. For\nexample, [231] implemented their study in an emergency triage setting, while [234] tested\ntheir approach within IoT devices. Additionally, we highlighted that the scarcity of stud-\nies exploring pain estimation in real-world settings or unconventional contexts suggests that\ncurrent methodologies might not be entirely suitable for practical environments like clinics\nor hospitals due to issues with generalization or operational factors such as efficiency and\ninference time. For these reasons, this chapter\u2019s objective is to explore approaches that (i)\n7576 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nutilize modalities readily available and applicable in the market and (ii)examine the impact\nof model size and computational cost on performance. In this context, our methodologies\nand experiments exclusively utilize RGB videos, a universally available modality, particu-\nlarly on mobile devices. Additionally, we incorporate heart rate data, which is commonly\naccessible from various types of wearable technology. It is important to note that although\nwe employ established pain datasets in our experiments, the videos and heart rate data ex-\ntracted from ECGs are akin to those that could be obtained from smartphones and wearables,\nserving as a proof of concept for application in real-world environments. Furthermore, with\nrespect to efficiency and the speed of inference, our goal is to develop the most compact pain\nassessment frameworks possible, ensuring they maintain adequate performance levels.\nNumerous studies highlight the capabilities of automated systems that utilize behavioral\nor physiological modalities for pain assessment [271]. Sario et al. [34] demonstrate the fea-\nsibility of accurately detecting and quantifying pain through facial expressions, establishing\ntheir value in clinical settings. The use of multimodal sensing appears especially promising,\noffering increased accuracy in pain monitoring systems [22]. An important aspect of pain\nmonitoring involves wearable devices that record biopotentials to estimate pain levels. Few\nstudies have investigated the use of mainstream, wearable technology for this purpose, possi-\nbly due to a research preference for more costly, highly precise medical equipment. Leroux\net al. [32] state, \u201cThe challenge is not whether wearable devices will provide useful clinical\ninformation but rather when we will start to use them in practice to enhance the field of pain. \u201d\nAdditionally, Claret et al. [272] explore the potential of using cardiac signals from wearable\nsensors for automatic emotion recognition, confirming the effectiveness of such methods.\nIn this chapter, our deep learning approaches are founded on transformer-based archi-\ntectures. Convolutional Neural Networks (CNNs) have been the cornerstone of mainstream\nneural architectures in computer vision (CV), especially in the field of automatic pain as-\nsessment using images and videos, as we discussed in Section 3. Inspired by the success\nof transformer architecture in natural language processing (NLP), where the self-attention\nmechanism is a fundamental element [273], researchers have developed similar models for\nvisual tasks. The introduction of Vision Transformers (ViT) [274] has established a new\nparadigm in the computer vision domain. This has led to a plethora of new approaches based\non ViT, such as the Transformer in Transformer (TNT) [275], which enhances local feature\nrepresentation by subdividing image patches into smaller sub-patches. While transformer-\nbased models have shown impressive results and offer great flexibility, they tend to scale\npoorly with input size and incur higher computational costs due to the self-attention layers\nthat compute interactions between all input pairs. Efforts to mitigate these challenges in-\nclude replacing self-attention with cross-attention [276] or combining both techniques [277]\nto improve the efficiency and reduce the complexity of these architectures.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 77\nFigure 5.1: The application of face alignment illustrates landmarks in 2D (left) and 3D (right) space.\n5.2 Video Analysis with Vision Transformers\nWe introduce a framework incorporating a vision transformer as a module extracting spatial\nfeatures for individual video frames, combined with a transformer-based model equipped\nwith cross and self-attention blocks, extracting temporal features from the video feature\nsequences. This configuration enables the effective utilization of the temporal dimensions of\nvideo data to deliver more accurate and reliable estimation of the continuous nature of pain.\n5.2.1 Methodology\nThis section outlines the preprocessing methods employed, the design of our framework, the\nimplementation details concerning the training procedure, and the database used.\nPre-processing\nBefore processing videos for pain estimation, applying face detection and alignment was\ncrucial to enhance performance and computational efficiency. We utilized the well-known\nface detector MTCNN [278] in combination with the Face Alignment Network (FAN) [279],\nwhich leverages 3D landmarks. This 3D approach is critical for addressing our specific chal-\nlenges, as head movements tend to increase, particularly during instances of high-intensity\npain, which can lead to inaccurate alignments with 2D methods. Additionally, it should be\nnoted that all experiments were carried out using video frames with a resolution of 224\u02c6224\npixels. Figure 5.1 illustrates the facial alignment process applied to a video frame.\nTransformer-based Framework\nOur framework is composed of two primary components: the \u201cspatial feature extraction\nmodule\u201d , specifically a TNT (Transformer in Transformer) model, and the \u201ctemporal fea-78 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nture extraction module\u201d , which is a transformer with both cross and self-attention blocks.\nThis framework, illustrated in Figure 5.2, includes approximately 24million parameters and\nperforms operations at 4.2GFLOPS.\nSpatial feature extraction module :Each frame is initially divided into npatches represented\nasFk\u201crF1\nk, F2\nk, . . . , Fn\nksPRn\u02c6p\u02c6p\u02c63, where p\u02c6pdenotes the resolution of each patch ( i.e.,\n16\u02c616) and 3represents the number of color channels. These patches are then subdivided\nintomsub-patches to facilitate the model\u2019s learning of global and local features. Each frame\nis thus transformed into a sequence of patches and sub-patches:\nFk\u00d1rFk,n,1, Fk,n,2, . . . , F k,n,ms, (5.1)\nwhere Fk,n,mPRs\u02c6s\u02c63is the m-th sub-patch of the n-th patch of the k-th frame, with each\nsub-patch having a resolution of s\u02c6s(i.e.,4\u02c64). Following this, the patches and sub-\npatches undergo a linear projection and are transformed into embeddings ZandY. Position\nembeddings are then added to retain spatial information:\nZ0\u00d0Z0`Epatch, (5.2)\nwhere Epatchare the position encodings for the patches. Correspondingly, for each sub-patch\nwithin a patch, a position encoding is also added:\nY0\ni\u00d0Y0\ni`Esub-patch , (5.3)\nwhere Esub-patch are the sub-patch position encodings and i\u201c1,2, . . . , m denotes the index of\na sub-patch. These sub-patches are then processed through an \u201cInner Transformer Encoder\u201d ,\nwhich consists of two multi-head self-attention blocks, crucial for dot product attention. The\nattention mechanism is defined as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n, (5.4)\nwhere QPRM\u02c6D, KPRM\u02c6C,andVPRM\u02c6C(Mis the input dimension, CandDare\nchannel dimensions) are projections of the corresponding input and represent the Query, Key,\nand Value matrices. They defined as Q\u201cXW Q,K\u201cXW K, and V\u201cXW V, where W\nare the learnable weight matrices and Xis the input. The output embedding from the \u201cInner\nTransformer Encoder\u201d is then added to the patch embedding and forwarded to the \u201cOuter\nTransformer Encoder\u201d . This encoder comprises three multi-head self-attention blocks, and\nits output is a feature vector d\u201c192. The \u201cspatial feature extraction module\u201d as a whole\nencompasses a depth of 12blocks.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 79\nTable 5.1: Training details for the automatic pain assessment.\nEpochs Optimizer Learning\nRateLR decay Weight\ndecayWarmup\nepochs\n200 AdamW 1e-4 cosine 0.1 5\nLabel\nSmoothingDropPath Attention\nDropOutLoss\nfunctionAugmentation methods\n0.1 0.1 0.1 Cross\nEntropyAugMix [281] &\nTrivialAugment [282]\nDropPath applied to the \u201cspatial feature extraction module\u201d , Attention DropOut applied to the \u201ctemporal\nfeature extraction module\u201d\nTemporal feature extraction module :The extracted embeddings of each input video frame\nare concatenated into a unified vector D, representing the entire video as V\u00f1D\u201c\npd1\"d2\", . . . , dkq. This vector is then processed through the temporal module, a transformer\narchitecture consisting of 1cross-attention and 2self-attention mechanisms, each followed\nby a fully connected neural network (FCN). The introduction of cross-attention, which em-\nploys asymmetry in the attention mechanism, helps reduce computational complexity and\nincrease the model\u2019s efficiency. Specifically, rather than projecting the input with dimen-\nsions M\u02c6D, theQin cross-attention is a learned matrix with dimensions N\u02c6D, where\nN\u0103M. This module\u2019s self-attention components function as detailed in Equation 7.3, with\nthe cross and self-attention units comprising 1and8heads, respectively. In addition, we\nincorporate Fourier feature position encoding [277].\nTraining Details: Before starting the automatic pain estimation training process, we pre-\ntrained the \u201cspatial feature extraction module\u201d using the VGGFace2 dataset [280], incorpo-\nrating over three million facial images from more than nine thousand individuals. Table 7.3\ndetails the hyperparameters of our method and the applied augmentation techniques.\nDatabase Details: For this approach, we used the publicly available BioVid dataset [109],\nas described in the previous chapters.\n5.2.2 Experiments\nIn this section, we detail the experiments conducted for pain estimation. Our experiments\nwere carried out in both binary and multi-level classification formats. Specifically, we con-\nducted binary classification tasks: (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3,(iv)NP vs. P 4,\nand(v)a multi-level pain classification utilizing all available pain classes from the database.\nWe employed the leave-one-subject-out (LOSO) cross-validation method as our evaluation80 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nprotocol. Additionally, the classification metrics used in this study include micro-average ac-\ncuracy, macro-average precision, macro-average recall (sensitivity), and macro-average F1\nscore.\n5.2.3 Results\nPain Estimation\nIn evaluating pain estimation tasks, we noted the following results: For the NP vs. P 1task,\naccuracy reached 65.95%, with precision almost identical at 65.90%. The F1 score was\nFigure 5.2: An overview of our proposed transformer-based framework for automatic pain as-\nsessment.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 81\nTable 5.2: Results on the pain estimation tasks.\nMetricTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAccuracy 65.95 66.87 69.22 73.28 31.52\nPrecision 65.90 66.89 69.18 73.31 31.48\nRecall 67.85 68.34 70.84 74.75 29.94\nF1 65.04 66.19 68.54 72.75 27.82\nNP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain P 4: very severe pain MC: multi-level\nclassification\nslightly lower at 65.04%, and the recall stood out at 67.85%. In the NP vs. P 2task, accuracy\nincreased to 66.87%, and all related metrics improved, with the F1 score climbing by over\n1.15%, highlighting enhanced detection of true positives. The results were notably better\nfor the NP vs. P 3task, with an accuracy of 69.22% and a sensitivity of 70.84%. This is\nexpected as the pain at this level is considered severe, eliciting more pronounced responses\nfrom subjects. In the highest pain task, NP vs. P 4, the recall was particularly high at 74.75%,\nwith an accuracy of 73.28%, demonstrating that the detection of very severe pain is relatively\nmore straightforward due to the pain reaching tolerance thresholds, making it more visibly\nevident through subjects\u2019 facial expressions. However, in the multi-level classification task,\nperformance metrics were lower, illustrating the complexity of estimating all pain levels\nconcurrently; accuracy was only 31.52%, with a recall of 29.94%, pointing to significant\nchallenges in accurately identifying true positives across multiple pain levels.\nIt should be noted that our framework, encompassing both the architectural and procedu-\nral aspects of training, was consistent across all binary and multi-level classification tasks.\nThis was done to evaluate the generalization potential of our approach across all possible\nscenarios provided by the database, akin to real-world clinical settings. The detailed classifi-\ncation outcomes are presented in Table 5.2.\nVideo Sampling\nIn this section, we explore the impact of video frame sampling on automatic pain estimation.\nExperiments detailed in Section 5.2.3 utilized all available frames ( 138) from each video.\nSubsequent experiments employed frame sampling with strides of 2,3, and 4. Starting with\nall138frames, the video feature representation Dhas dimensions 138\u02c6192, totaling 26,496.\nA stride of 2reduces this to 69frames, with Dhaving dimensions 69\u02c6192and totaling\n13,248. With strides 3and4, the frame counts reduce to 46and35, resulting in Dsizes82 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.3: Results for the pain estimation tasks using various numbers of input frames.\nNumber of\nFramesTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n138 65.95 66.87 69.22 73.28 31.52\n69 65.76 66.74 69.15 73.25 31.29\n46 65.66 66.70 68.50 71.78 31.20\n35 65.40 66.12 68.32 72.01 30.80\nFigure 5.3: The impact of the number of input frames on accuracy (left) and on runtime in\nmilliseconds (right). Runtime calculated during inference on a NVIDIA RTX-3090 .\nof8,832and6,720, respectively. Table 5.3 displays the classification accuracies achieved\nwith these varying frame counts for each pain estimation task. Concurrently, Figure 5.3\ndemonstrates how the number of frames affects mean accuracy across the five tasks and\nmean runtime during inference. We noted a performance increase of approximately 1.38%\nwhen using 138frames compared to 35frames. Additionally, the runtime increased by a\nfactor of 3. Despite the longer runtime, each sampling rate allows for real-time automatic\npain estimation when necessary.\nInterpretation\nResearch in deep learning, particularly relevant to healthcare, increasingly focuses on model\ninterpretability to explain decision-making processes. This is crucial for enhancing the trans-\nparency of models, a key factor for their acceptance and integration into clinical settings. In\nour study, we implemented the technique described in [283] to generate relevant maps illus-\ntrating which facial areas our model\u2014the \u201cspatial feature extraction module\u201d \u2014focuses on.\nAs shown in Figure 5.4, the model\u2019s attention is distributed over \u201carbitrary\u201d areas at the on-\nset of a facial expression sequence. However, as the expression of pain intensifies, the focus\nsharpens on specific regions indicative of pain. It is important to note from our relevance5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 83\nFigure 5.4: Relevance Maps.\nmaps that no universal facial expressions are unique to pain. However, there is a noticeable\nconcentration on areas like the mouth and eyes.\nComparison with existing methods\nIn this section, we present a comparison of our results achieved using a transformer-based\nframework that utilizes all available video frames against other studies that also employed\nPart A of the BioVid database with all 87subjects, following the same leave-one-subject-out\n(LOSO) cross-validation protocol. This ensures objective and accurate comparisons, with\nresults detailed in Table 5.4. The studies compared fall into three main categories: i) those\nfocusing exclusively on pain detection (NP vs. P 4),ii) those examining both pain detection\nand multi-level pain estimation, and iii) those that cover all major pain-related tasks.\nOur method, tested across all tasks, recorded the highest performance metrics in bi-\nnary and multi-level pain estimations. Studies limited to specific aspects of pain detec-\ntion or multi-level pain estimation often yielded comparable or superior results, as indicated\nin [219], [180], and [284]. This highlights that while focused studies often show high perfor-\nmance, the broader impact lies in developing systems that perform well across all potential\nscenarios.\n5.2.4 Discussion\nThis research examined the application of transformer-based architectures for automatic pain\nestimation through video analysis. Our framework employed exclusively transformer mod-\nels, leveraging the spatial and temporal aspects of the video frames. The experiments demon-\nstrated the effectiveness of our approach in assessing pain, showing strong generalization\nacross various pain estimation tasks with notable results, particularly with low-intensity pain\nwhere facial expressions are less apparent. Additionally, the framework demonstrated high\nefficiency, suitable for real-time applications. A significant contribution of our work includes\ndeveloping relevance maps highlighting facial areas the model focuses on. We advocate for\ncontinued efforts within the affective computing field to enhance the interpretability of these\ndeep-learning methods.84 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.4: Comparison of studies utilizing BioVid , RGB videos, and LOSO validation.\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nThiam et al. [219] - 69.25 -\nTavakolian et al. [180] - - - 71.02 -\nPatania et al. [284] - - - 73.20 -\nHuang et al. [269] - - - 77.50 34.30\nXinet al. [141] - - - 86.65 40.40\nZhi and Wan [217] 56.50 57.10 59.60 61.70 29.70\nWerner et al. [248] 53.30 56.00 64.00 72.40 30.80\nOur approach 65.95 66.87 69.22 73.28 31.52\n5.3 Video & Heart Rate Analysis with Transformer Architectures\nWe introduce a proof of concept for an automatic pain assessment framework that integrates\nfacial video data captured by an RGB camera with heart rate signals. We build and extend\nour previous analysis in 5.2. Our main objectives include (1) evaluating the effectiveness and\nlimitations of video and heart rate data as standalone modalities in an unimodal setting, (2)\nexploring the efficacy of combining behavioral (video) and physiological (heart rate) markers\nto overcome challenges associated with their reliance on different sensing technologies and\ninformation representations, and (3) analyzing the performance and efficiency of recently\nintroduced transformer-based architectures.\n5.3.1 Methodology\nThis section details the preprocessing methods for video and ECG, the design of the proposed\nframework, the augmentation techniques developed, and the specifics of implementing the\npretraining process.\nPre-processing\nPreparatory processing was essential before feeding data into the pain assessment framework,\nparticularly for the raw ECG data used to compute heart rate. We focused on exploring heart\nrate as the primary feature due to its benefits: It\u2019s readily obtainable from wearable devices,\ncost-effective, and easily accessible. These advantages position heart rate as a potentially\nvaluable feature for automated pain assessment.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 85\nVideo Preprocessing: Video preprocessing included face detection to isolate the facial re-\ngion using the MTCNN face detector [278], which employs multitask cascaded convolu-\ntional networks to predict facial and landmark locations. Predicting landmarks facilitates\nface alignment, which is crucial for accurate facial analysis. However, we observed that\nface alignment reduced the expressiveness linked to head movements, a common behavioral\nindicator of pain. Consequently, face alignment was omitted from our proposed pipeline.\nAdditionally, the resolution of frames post-face detection was standardized at 448\u02c6448\npixels.\nECG preprocessing & analysis: Similar to Section 4, we utilize the Pan-Tompkins [262] al-\ngorithm to detect the QRS complex, the most prominent wave complex in an ECG signal.\nThis algorithm operates in two phases: preprocessing and decision-making. The preprocess-\ning phase focuses on noise removal, artifact elimination, signal smoothing, and enhancing\nthe QRS slope. The decision-making phase involves initial QRS detection using adaptive\nthresholds, a retrospective search to identify any missed QRS complexes, and a method for\ndistinguishing T waves. Following the accurate detection of R waves, the estimation of inter-\nbeat intervals (IBIs) was conducted, leading to the extraction of key features. Specifically,\nwe calculated the mean of the IBIs as follows:\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (5.5)\nwhere nis the total number of IBIs, and RRidenotes the consecutive R\u00b4Rintervals.\nSubsequently, the heart rate was calculated using the following formula:\nHR\u201c60\u00a8FS\n\u00b5, (5.6)\nwhere FSdenotes the sampling frequency of the ECG recording.\nFramework architecture\nThe proposed framework, as illustrated in Figure 5.5, consists of four key components: the\nSpatial-Module that extracts embeddings from video data, the Heart Rate Encoder which\nmaps heart rate signals into a higher dimensional space, the AugmNet that generates aug-\nmentations in the latent space, and the Temporal-Module performs with the final assessment\nof pain.\nSpatial-Module: The architecture for this module draws inspiration from the Transformer\nin Transformer approach as detailed by [275]. The process begins with the initial video86 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(a) Video analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(b) ECG analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(c) Fusion analysis pipeline.\nFigure 5.5: Outline of the proposed framework.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 87\nframe at a resolution of 448\u02c6448pixels, segmented into 4quadrants, each at 224\u02c6224\npixels. This tiling method, which maximizes the utilization of the frame\u2019s resolution, is\ninfluenced by approaches seen in satellite imaging analysis. Our framework incorporates the\n4tiles and the original frame\u2014resized to 224\u02c6224pixels\u2014into our analysis pipeline. Thus,\neach video frame transforms into 5distinct images, denoted as:\nFk\u201crFk,1, Fk,2, . . . , Fk,ts, (5.7)\nwhere kstands for the frame number, and tencompasses the tile count, including the resized\nfull frame. Each tile is initially split into npatches, expressed as:\nFk,t\u201crFk,t,1, Fk,t,2, . . . , Fk,t,nsPRn\u02c6p\u02c6p\u02c63, (5.8)\nwhere p\u02c6pspecifies the resolution of each patch ( 16\u02c616), and 3denotes the RGB channels.\nThese patches are further segmented into msub-patches, allowing the model to capture the\nimage\u2019s global and localized features. Each tile from a frame thus transitions into a sequence\nof patches and sub-patches, represented as\nFk,t\u201crFk,t,n, 1, Fk,t,n, 2, . . . , Fk,t,n,ms.\nConsequently, each video frame is characterized by:\nFk\u00d1\u201c\nFk,t,n,m|tPr1,5s, nPr1,196s, mPr1,16s\u2030\n, (5.9)\nwhere Fk,t,n,mPRs\u02c6s\u02c63defines the m-th sub-patch within the n-th patch of the t-th tile\nfor the k-th frame, with each sub-patch having a resolution of s\u02c6s(4\u02c64). Each frame\nconsists of 5image representations, encompassing 196patches, and each patch contains 16\nsub-patches. The patches and sub-patches are then linearly projected into embeddings Zand\nY. Positional embedding is applied to maintain spatial information, employing 1D learnable\nposition encodings:\nZ0\u00d0Z0`Epatch, (5.10)\nwhere Epatch indicates the position encoding. Each sub-patch also receives its specific posi-\ntional encoding:\nYi\n0\u00d0Yi\n0`Esub\u00b4patch, (5.11)\nwhere Esub\u00b4patch denotes the positional encodings for sub-patches, and irepresents the index\nof a sub-patch within a patch. The sub-patches are processed in the Inner Encoder , which88 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nconsists of four self-attention heads [285], utilizing dot product attention:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n. (5.12)\nThe output from the Inner Encoder integrates into the patch embedding, advancing to the\nOuter Encoder , which mimics the Inner Encoder with ten self-attention heads. The Spatial-\nModule consists of twelve parallel blocks, generating embeddings of dimensionality d\u201c100.\nFor each input video frame, 5distinct output embeddings of dimensionality 100are produced\nand combined to form a comprehensive frame representation:\nD\u201cdFullFrame`pdTile1`dTile2`dTile3`dTile4q\u00a8c, DPR100, (5.13)\nwhere cadjusts the contribution from the tile embeddings, subsequently, the embedding for\neach frame, D, is concatenated with those of other frames to construct a complete video\nrepresentation:\nVD\u201crD1}D2}. . .}Dfs, VDPRN, (5.14)\nwhere frepresents the total number of frames in the video, and Ndenotes the dimensionality\nof the final video embedding.\nHeart Rate Encoder: As mentioned in Section 5.3.1, heart rate is computed from the origi-\nnal ECG every second, producing a heart rate vector of length h\u201c\u03b8for recordings lasting\n\u03b8seconds. It should be noted that when beats per minute (BPM) fall below 60within any\n1-second segment of the ECG, making direct heart rate calculation impractical, the method\naverages heart rates from the data points immediately before and after to maintain a uniform\nset of \u03b8data points. The Heart Rate Encoder , which is part of a transformer-based archi-\ntecture similar to the Inner andOuter Encoders , utilizes one cross-attention head instead\nof self-attention followed by a fully connected neural network (FCN). This use of cross-\nattention introduces an asymmetry that reduces computational load and increases efficiency.\nUnlike traditional input projections that are M\u02c6Din dimension, as detailed in Section\n5.3.1, the Qmatrix in cross-attention is a learnable matrix sized N\u02c6Dwhere N\u0103M. The\nencoder\u2019s internal embeddings are set to a dimensionality of 512and contain only a single\nblock depth. Fourier feature position encodings [277] are also implemented to handle posi-\ntional information. The main goal of this encoder is to transform the initial heart rate vector\nhinto a more complex and richer feature space,\nhPR\u03b8\u00d1EhPR2048,5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 89\nwhere Ehrepresents the enhanced output embedding of this encoder. In the next step, the\noutput from the heart rate encoder is expanded dimensionally via a bicubic interpolation\nmodule. This process enhances the original heart rate\u2019s feature representation, allowing it to\nintegrate smoothly with the video\u2019s embedding representation through addition. The need\nfor identical dimensions in both embedding vectors is critical and is adeptly addressed by this\ninterpolation module. This non-learning-based approach proves to be efficient and effective\nfor encoding purposes. Additionally, interpolation provides the flexibility to dynamically set\nthe dimensionality of the final output embedding, unlike the fixed dimensions typically seen\nin neural network-based methods. Specifically:\nBh\u201c3\u00ff\ni\u201c03\u00ff\nj\u201c0aijpEhq\u00a8px\u00b4x0pEhqqi\u00a8py\u00b4y0pEhqqj, (5.15)\nwhere aijare the coefficients used for interpolation, and Bhis the resulting vector from the\nbicubic interpolation process. The dimension of BhisN, which is the same as that of VD.\nAugmNet: Inspired by recent developments in the augmentation literature [286], employs\na learning-based technique to identify augmentation patterns within the latent space. Un-\nlike conventional methods that perform image augmentations ( e.g., rotation, cropping) in the\npixel space, AugmNet universally applies transformations to the embeddings. This method\neliminates the necessity for specific transformations tailored individually to each modality,\ne.g., image, signal, and text. Incorporating this module within the automatic pain assessment\nframework helps to regularize the learning process and address overfitting issues. Moreover,\ncorrupting the input embeddings compels the following model, especially the Temporal-\nModel , to derive more precise and representative features, thereby improving performance\nin the pain assessment task. The modality-agnostic method effectively applies to embedding\nrepresentations from any original modality, including video and heart rate signals. AugmNet\nadopts an encoder-decoder architecture, where both the encoder and decoder consist of only\n2fully connected layers with the ELU nonlinear activation function applied after each layer.\nFor a session lasting \u03b8seconds, it produces \u03b8\u02c6frames per second frames and \u03b8\u02c6FS\ndata points for video and ECG, respectively. In the video analysis pipeline, the Spatial-\nModule constructs an embedding representation, VD(5.14), from the original video, dimen-\nsioned at d\u02c6FPS\u201cN. In the ECG analysis pipeline, a feature vector with dimension \u03b8\nis created after extracting the heart rate, one data point per second. The Heart Rate Encoder\nand bicubic interpolation then produce an embedding, Bh(5.15), with dimension N. The fu-\nsion of video and heart rate embeddings at the session level is performed, where VDandBh\nare merged by addition, integrating the data from the initial input modalities. The resulting90 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncombined embedding is then processed by AugmNet :\nAD\u00d0AugmNetpVD`Bhq (5.16)\nP\u00d0AD`pVD`Bhq, (5.17)\nwhere Prepresents the transformed embedding vector, serving as input for the final module,\ntheTemporal-Module .AugmNet is active only during training as a standard augmentation\nmethod and remains inactive during inference.\nTemporal-Module: Like the Heart Rate Encoder , this component operates on a transformer-\nbased architecture. It employs a combination of multi-head cross-attention and multi-head\nself-attention mechanisms. The architecture consists of one multi-head cross-attention block\nwith a single attention head and three subsequent multi-head self-attention blocks, each fea-\nturing eight attention heads. An FCN follows each attention block. The internal embed-\ndings in this module have a dimensionality of 128and encompass a single block in depth.\nThe position encoding method used here mirrors that of the Heart Rate Encoder , utilizing\nFourier feature position encoding. This module processes the input embedding Por the sum\npVD`BhqifAugmNet is inactive, to derive the final classification outcome. The learning\nerror is computed during this phase, and the framework undergoes further training.\nSupplementary augmentation methods\nWe also introduced additional augmentation strategies alongside the AugmNet module, which\napplies learned transformations to embeddings. The initial technique, dubbed Basic , com-\nbines polarity inversion and noise addition to manipulate the original inputs by reversing\ntheir polarity and injecting noise. Another technique, named Masking , involves nullifying el-\nements within the embeddings using randomly sized and placed masks that zero out 10\u00b420%\nof the embedding elements. These methods function within the latent space, similar to Augm-\nNet.\nPre-training\nBefore starting the training for automatic pain assessment, we individually pretrained all\nmodules except AugmNet . The Spatial-Module underwent a dual-phase pretraining process.\nInitially, it was pre-trained on VGGFace2 [280], a dataset designed for facial recognition\nto learn basic facial features. This was followed by an advanced training phase involving\nemotion recognition datasets in a multi-task learning framework. These datasets include the\nwidely used AffectNet [287], Compound Facial Expressions of Emotions Database [288],\nRAF Face Database basic [289], and RAF Face Database compound [289], enabling the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 91\nTable 5.5: Datasets utilized for the pre-training process of the framework.\nDataset # samples # classes Task\nVGGFace2 [280] 3.31M 9,131 Face\nAffectNet [287] 0.40M 8 Emotion\nCompound FEE-DB [288] 6,000 26 Emotion\nRAF-DB basic [289] 15,000 7 Emotion\nRAF-DB compound [289] 4,000 11 Emotion\nECG HBC Dataset [291] 0.45M 5 Arrhythmia\nTask: all tasks involve classification\nmodule to adapt to specific emotional expressions related to pain manifestations. During\nthis phase, the model is trained across these datasets simultaneously, employing a multi-task\nlearning loss approach as suggested by [290], where learned weights scale the individual\nlosses to account for the homoscedastic uncertainty of each task:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s`rew3LS3`w3s`rew4LS4`w4s,\nwhere LSdenotes the loss for each dataset and ware the weights adjusting the learning focus\nto optimize the overall loss Ltotal. The Temporal-Module is trained solely on the VGGFace2\ndataset. For this module, images are converted into 1D vectors prior to processing. The\nHeart Rate Encoder is pre-trained using the ECG Heartbeat Categorization Dataset [291],\nwhich includes heartbeat signals from the MIT-BIH Arrhythmia Dataset [292] and the PTB\nDiagnostic ECG Database [293] [294]. Table 5.5 provides a detailed list of the datasets used\nin our training process.\nDataset Details\nIn this study, to assess the developed framework, we utilized the BioVid Heat Pain Database\n[109], which includes facial videos, electrocardiograms, electromyograms, and skin conduc-\ntance levels from 87healthy individuals ( 44males and 43females, aged 20\u00b465). The\ndataset employs a thermode to induce varying pain levels in the participants\u2019 right arm. Ini-\ntially, each participant\u2019s pain threshold (the transition from heat sensation to pain) and pain\ntolerance (the point at which pain becomes unbearable) were determined. These thresholds\ndelineated the minimum and maximum pain levels, along with two intermediary levels, form-\ning five distinct pain intensities: No Pain (NP), Mild Pain (P 1), Moderate Pain (P 2), Severe\nPain (P 3), and Very Severe Pain (P 4). Pain stimuli temperatures ranged from P 1to P 4, capped\nat50.5\u00b0C. Each subject experienced 20stimulations at each of the four intensities (P 1to P 4).\nEach stimulation lasted 4seconds, interspersed with recovery intervals of 8to12seconds.92 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.6: Training details for the automatic pain assessment.\nOptimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsBatch\nsize\nAdamW 1e-4 cosine 0.1 50 32\nThis protocol, along with 20baseline measurements at NP ( 32\u00b0C), culminated in randomly\nordered 100stimulations per participant. The data was preprocessed to capture 5.5-second\nwindows starting 1-second after reaching the target temperature for each stimulation. This\nprocess produced 8,700samples, each 5.5seconds in duration, distributed evenly across the\nfive classes and modalities for all 87subjects.\n5.3.2 Experiments\nThe study leveraged videos and electrocardiograms from Part A of the BioVid dataset, using\nall available samples from the 87participants. The videos were recorded at a rate of 25\nframes per second (FPS), and the electrocardiogram (ECG) signals were sampled at 512\nHertz (Hz). Each recording session lasted 5.5seconds, generating 138video frames and\nECG vectors containing 2,816elements each, then converted into heart rate vectors of 5\ndata points. The complete set of frames and data points from both videos and cardiac signals\nwas utilized in the experiments. The experimental approach included iterative refinement\nof techniques, with the most effective combinations selected for extended training periods\nranging from 500to800epochs to improve feature extraction and performance outcomes.\nTable 5.6 details the training configurations for the automatic pain assessment tasks.\nPain assessment experiments were structured around binary and multi-level classification\nsetups, testing each modality individually and in combination. The binary classification task\ndifferentiated between No Pain (NP) and Very Severe Pain (P 4), whereas the multi-level\nclassification (MC) involved categorizing all pain intensities available in the dataset. The\nevaluation strategy adopted was the leave-one-subject-out (LOSO) cross-validation, and the\nassessment metrics included accuracy, precision, recall (sensitivity), and F1 score. Notably,\na consistent training regimen was applied across both the binary (NP vs. P 4) and multi-level\n(MC) classification tasks without varying the training schedule or optimization strategies.\n5.3.3 Results\nVideo modality\nExperiments related to the video modality explored the effects of pretraining on the Spatial-\nModule , the video analysis pipeline\u2019s performance, particularly the impact of tiling, and the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 93\nTable 5.7: Results utilizing the video modality.\nEpochsPretraining stage Pipeline Augmentations Task\n1st2ndFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n500 \u2713 - \u2713 - \u2713 - - 72.56 31.22\n500 - \u2713 \u2713 - \u2713 - - 74.25 33.34\n500 - \u2713 - \u2713 \u2713 - - 68.07 31.49\n500 - \u2713 \u2713 \u2713 \u2713 - - 65.11 27.84\n500 - \u2713 \u2713 \u2713c\u2713 - - 74.86 33.86\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 - 73.05 32.14\n500 - \u2713 \u2713 \u2713c\u2713 - \u2713 74.83 33.73\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 73.16 32.87\n800 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 77.10 35.39\nStage: referring to pretraining process for Spatial-Module Mask: Masking c: constant-coefficient applied exclusively to the tiles NP:\nNo Pain P 4: Very Severe Pain MC: multiclass pain level\nimplementation of new augmentation techniques. These experiments are detailed in Table\n5.7. Performance enhancements are evident when comparing the first and second pretraining\nstages of the Spatial-Module . For instance, in the NP vs. P 4task, initial pretraining alone\nachieved 72.56% accuracy, while including the second emotion-focused pretraining stage\nincreased accuracy to 74.25%. This trend is also notable in the multi-level classification,\nwhere the second stage added 1.12% to the performance, totaling 33.34%. Further experi-\nments assessed the effect of using tiles in the video representation. Initially, employing four\ntiles led to a performance decrease of over 6%in the binary classification task and 1.85% in\nthe multi-level task. This reduction likely results from the localized information in each tile,\nwhich may capture irrelevant details like non-expressive facial areas or background elements.\nIncluding the resized full-frame ( 224\u02c6224pixels) alongside tiles further decreased accuracy\nto65.11% and27.84% for binary and multi-level tasks, respectively. However, introducing\na coefficient ( c\u201c0.1) to adjust the tile embeddings restored some performance, achieving\n74.86% and33.86% in respective tasks.\nThe integration of two augmentation techniques, Masking andAugmNet , along with the\nBasic method, was then tested. Masking reduced performance by 1.81% and1.72%, and\nAugmNet showed smaller declines of 0.03% and0.13%. Using both techniques together\nresulted in better outcomes than Masking alone but did not independently surpass the per-\nformance of AugmNet . Despite these initial results, combining all augmentation methods\nproved advantageous for extended training periods. This approach addresses the risk of\noverfitting through a robust regularization strategy. Ultimately, this comprehensive strategy\nled to final accuracy rates of 77.10% and35.39% for binary and multi-level classifications,\ndemonstrating its effectiveness in an unimodal, vision-based pain assessment framework.94 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nHeart rate modality\nThe experiments concerning the heart rate modality explore the use of the encoder and vari-\nous augmentation methods. Table 5.8 details all experiments involving the heart rate modal-\nity. Initially, using the original heart rate vectors with a dimensionality of h\u201c5, we achieved\nclassification scores of 61.70% for NP vs P 4and27.60% for the multi-level task. After apply-\ning the Heart Rate Encoder to map these vectors to a higher-dimensional space of h\u201c2048 ,\nwe noted a slight improvement: a 0.23% increase for the binary task and 0.08% for the\nmulti-level task. Despite the considerable increase in embedding size, this modest enhance-\nment suggests that the intrinsic information within the limited heart rate data points does not\nsignificantly enhance the feature representation. Nonetheless, the encoder\u2019s use is vital for\nproducing larger embeddings, especially for our multimodal approach integrating video and\nheart rate data, which will be discussed in subsequent sections.\nWe also tested augmentation methods on the heart rate data. Applying Masking yielded\na slight improvement of 0.02% for the binary task and 0.05% for the multi-level task. Imple-\nmenting AugmNet further enhanced performance to 62.09% and28.11% for binary and multi-\nlevel tasks, respectively. However, combining all augmentations decreased performance to\n61.87% and27.96%. During an extended 800-epoch training period, we achieved 64.87%\naccuracy for the binary task and 29.81% accuracy for the multi-level task using all augmen-\ntations. Despite these gains, we found that augmentations pose more challenges for accurate\nheart rate classification than video. Therefore, we repeated the extended training without\nBasic andMasking , keeping only AugmNet , which improved binary task performance to\n67.04% and multi-level to 31.22%. This reduction in heart rate embedding corruption sig-\nnificantly enhanced performance. The differing effects of augmentations between heart rate\nand video modalities highlight the challenges of using a single, isolated feature in a machine\nlearning system. We infer that heart rate embeddings with limited informational content\nare more vulnerable to significant performance degradation from augmentations than richer\nvideo embeddings.\nMultimodality\nThe results of integrating video and heart rate modalities are detailed in Table 5.9. Based on\nthe insights gained from separate experiments with each modality, we extended the training\nduration to 800epochs. For this integrated approach, we utilized the tiles with a coefficient\nc\u201c0.1and applied AugmNet as the sole augmentation method. This fusion strategy re-\nsulted in a classification accuracy of 82.74% for NP vs. P 4and39.77% for the multi-level\nclassification task. These results mark a substantial enhancement, with improvements of\n5.64% and15.70% over the individual performances of the video and heart rate modalities,\nrespectively, for the binary classification. Similarly, for the multi-level classification, the in-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 95\nTable 5.8: Results utilizing the heart rate modality.\nEpochsHR\nEncoderAugmentations Task\nBasic Mask AugmNet NP vs P 4 MC\n500 - \u2713 - - 61.70 27.60\n500 \u2713 \u2713 - - 61.93 27.68\n500 \u2713 \u2713 \u2713 - 61.95 27.73\n500 \u2713 \u2713 - \u2713 62.09 28.11\n500 \u2713 \u2713 \u2713 \u2713 61.87 27.96\n800 \u2713 \u2713 \u2713 \u2713 64.84 29.81\n800 \u2713 - - \u2713 67.04 31.22\nTable 5.9: Results utilizing the video & the heart rate modality.\nEpochsHR\nEncoderPipeline Augmentations Task\nFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n800 \u2713 \u2713 \u2713c- - \u2713 82.74 39.77\ntegrated approach shows a 4.38% and8.55% increase compared to the standalone modalities.\nThe combination of these two pivotal modalities significantly boosts the efficacy of the pain\nassessment process, outperforming the results obtained by each modality on its own.\nComparison with existing methods\nIn this section, a comparative analysis is performed to evaluate the performance of our\nmethod against other existing approaches documented in the literature. This evaluation\nis based on Part A of the BioVid dataset, including all 87participants. The same evalua-\ntion protocol\u2014leave-one-subject-out (LOSO) cross-validation\u2014is adhered to for all com-\nparisons to ensure fairness and accuracy. Our method is contrasted with both unimodal and\nmultimodal approaches, divided into (1) video-based, (2) ECG-based, and (3) multimodal\nstudies regardless of the modalities involved. The outcomes are summarized in Table 5.10.\nFor video-based studies, our approach, achieving 77.10% in binary and 35.39% in multi-\nlevel classification tasks, is recognized as one of the highest-performing methods. It exceeds\nthe average results of comparative studies by about 4.7%for binary and 3.4%for multi-level\ntasks. Regarding ECG-based studies, our method shows superior performance, exceeding\nthe average by 8.5%and18.1%for binary and multi-level classifications, respectively. Re-\nmarkably, it records the highest classification accuracy in the multi-level task at 31.22%.96 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nThese findings underscore the effectiveness of using heart rate data extracted from ECG\nas a standalone feature, establishing our method\u2019s capability to assess pain accurately and\nachieve state-of-the-art results. In multimodal studies, our approach records an impressive\n82.74% accuracy in the NP vs. P 4task, making it one of the top-performing methods. It\nis slightly outperformed by studies [269] and [243], which achieved 88.10% and83.99%,\nrespectively. For multi-level tasks, comparisons are scarce; however, study [269] reached\n42.20%, and [235] reported 36.54%, positioning our method favorably within this context.\nInference time\nWe explored several methodologies in this study, including a video-based approach, video\nincorporating tiles, a heart rate-based approach, heart rate analysis with an encoder, and a\ncombined multimodal strategy. Figure 5.6 illustrates each method\u2019s inference time in sec-\nonds and the corresponding average accuracy performances across binary and multi-level\ntasks. Table 5.11 details the number of parameters and the computational cost of floating-\npoint operations (FLOPS) for each component. Inference tests were conducted on an Intel\nCore i7-8750H CPU, including the time for face detection in each frame but excluding the\nextraction of heart rate from electrocardiography, focusing on the potential use of automati-\ncally provided cardiac features from wearables.\nThe inference time for the video modality employing the standard pipeline is approxi-\nmately 26seconds. Utilizing the tile pipeline increases inference time significantly, soaring\nto about 130seconds due to processing five image representations per frame\u2014one full frame\nand four tiles\u2014compared to a single image representation in the non-tiled approach. In the\ncontext of heart rate signals, completing a pain assessment requires only 1.2seconds. With\nthe integration of the Heart Rate Encoder , the processing time remains virtually unchanged,\nshowing a negligible increase of less than half a second, highlighting this specific encoder\u2019s\nefficiency. Lastly, the comprehensive multimodal framework incorporating the tiles and the\nHeart Rate Encoder demands about 131seconds, illustrating the increased complexity and\ncomputational requirements when combining multiple modalities.\nInterpretation\nImproving model interpretability is essential for their acceptance and integration into clin-\nical settings. This study generates attention maps from both the Spatial-Module and the\nTemporal-Module , as illustrated in Figure 5.7. For the Spatial-Module , attention maps are de-\nrived from the last fully connected layer\u2019s weight contributions, which are then interpolated\nonto the images to highlight the model\u2019s focal areas. Figure 5.7a displays an original frame\nsequence along with three attention map variations: (1) post-initial pretraining, (2) after the\nsecond pretraining phase, and (3) post-training on the BioVid . Based on face recognition5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 97\nTable 5.10: Comparison of studies utilizing BioVid & LOSO validation, reported on accuracy %.\nStudy ModalityMethod Task\nFeatures Machine Learning Params (M) FLOPS (G) NP vs P 4 MC\n[295] Video optical flow RF - - 70.20 -\n[217] Video raw SLSTM - - 61.70 29.70\n[269] Video raw 3D CNN, - - 77.50 34.30\n[219] Video raw 2D CNN, biLSTM - - 69.25 -\n[180] Video raw 2D CNN 25.00\u00154.00 71.00 -\n[296] Video facial action\ndescriptorsDeep RF - - 72.40 30.80\n[296] Video facial 3D distances Deep RF - - 72.10 30.30\n[284] Video fiducial points GNN - - 73.20 -\n[135]:Video raw 2D CNN - - 71.30 37.60\n[211]:Video raw 2D CNN, GRU 150.00\u0015- 73.90 39.10\n[37] Video raw Transformer 24.00 4.20 73.28 31.52\n[267] Video facial landmarks,\n3D distancesRF 71.60 -\nOur Video raw Transformer 4.20\u00101.62 77.10 35.39\n[235] ECG raw 1D CNN 1.80\u0015- 57.04 23.23\n[270] ECG domain-specific\u00b8LR - - 57.69 -\n[35] ECG domain-specific\u00b8SVM - - 58.39 23.79\n[269] ECG heart rate\u20393D CNN - - 65.00 28.50\n[267] ECG domain-specific RF - - 62.00 -\n[36] ECG domain-specific FCN 4.09\u00120.40 69.40 30.24\n[297] ECG domain-specific\u00b8SVM - - 63.50 -\nOur ECG heart rate Transformer 6.03\u00101.25 67.04 31.22\n[235] ECG, EMG, GSR raw 2D CNN 10.00\u0015- 76.72 36.54\n[270] ECG, GSR domain-specific\u00b8SVM - - 72.20 -\n[269] Video1, ECG2raw1, heart rate2\u20393D CNN - - 88.10 42.20\n[267] ECG1, EMG1,\nGSR1domain-specific1\u00b8RF - - 74.10 -\n[267] Video1,ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 77.80 -\n[297] Video1, ECG2,\nGSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 78.90 -\n[297] Video1, ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8SVM - - 76.60 -\n[243] ECG, EMG, GSR raw DDCAE 4.00\u0015- 83.99 -\nOur Video1,ECG2raw1, heart rate2Transformer 8.60\u00102.44 82.74 39.77\nM: Millions G: Giga :: reimplemented for pain intensity estimation on BioVid by [269]\u2039: pseudo heart rate gain \u00b8: numerous\nfeatures \u0015: parameter count estimated from provided paper details \u0010:AugmNet excluded from parameter count, not used in inference\n\u0012: parameter count not mentioned in study, provided directly by authors -: missing value RF: Random Forest AE-ATT: Autoencoder\nAttention SVM: Support Vector Machines LR: Logistic Regression98 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nVideo Video\n(Tiles)Heart rate Heart rate\n(Encoder)Multimodal0102030405060Accuracy (%)Average Accuracy\n020406080100120\nInference Time (s)\nInference Time\nFigure 5.6: Comparison of mean accuracy and inference period for unimodal and multimodal\nstrategies across NP versus P 4and MC tasks. The diagram adopts a dual-y-axis\nconfiguration\u2014accuracy measurements on the left and time metrics on the right\u2014\nto outline the balance between performance efficacy and computational load, cate-\ngorizing the methodologies along the x-axis.\nTable 5.11: Module parameters and computational cost in FLOPS for the proposed framework.\nModule Params (M) FLOPS (G)\nSpatial-Module 2.57 1.19\nHeart Rate Encoder 4.40 0.82\nAugmNet 1.02 0.02\nTemporal-Module 1.63 0.43\nTotal 9.62 2.46\ntasks, the Spatial-Module produces maps focusing broadly on the facial area, particularly\nthe zygomatic, buccal, oral, mental, and nasal regions. The second stage, oriented towards\nmulti-task emotion recognition, refines the focus, sharpening attention on specific facial ar-\neas, highlighted in the first stage but with greater clarity and emphasis. After training on the\nBioVid for pain assessment, the attention maps show further refined focus on specific facial\nareas, with reduced attention to less relevant regions, ensuring concentrated focus on key\nareas. These maps consistently demonstrate the model\u2019s capability to adjust its focus based\non pain-related facial expressions.\nAttention maps from the Temporal-Module , based on input embeddings, illustrate the\nweight contributions in the module\u2019s final layer, forming easy-to-visualize rectangular pat-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 99\nterns. Figure 5.7b shows examples for three scenarios: (1) video embedding, (2) heart rate\nembedding, and (3) a combined embedding of video and heart rate. The attention maps ex-\nhibit a grid-like pattern, possibly due to the Fourier position encoding used, akin to those\nseen in perceiver-like architectures. The video embedding map shows intense attention\nacross the input. In contrast, the heart rate embedding map focuses attention more sparsely,\nwith a notable concentration in specific areas indicated by intense red coloration. The com-\nbined embedding map displays moderate intensity, consistent with the blended nature of the\ninput. These maps tend to emphasize the latter part of the session, aligning with the timing\nof pain manifestation towards the session\u2019s end, indicating the model\u2019s responsiveness to\nreal-time pain expressions.\n5.3.4 Discussion\nThis research introduced a multimodal framework that integrates video and heart rate sig-\nnals to assess pain automatically. Our innovative approach includes four main modules,\neach characterized by effectiveness and efficiency. The Spatial Module , particularly no-\ntable for its compact size of only 2.57million parameters, ranks as one of the most efficient\nvision-based models in automatic pain assessment literature. Despite limited comparative\nstudies, our model has shown it can match or exceed the performance of larger models. Its\nhigh efficiency and robust performance are primarily due to a thorough pretraining regime\non datasets related to affective responses, crucial for enhancing model capabilities in pain\nestimation tasks. The Heart Rate Encoder , with 4.40million parameters, excels at transform-\ning heart rate data into complex, high-dimensional embeddings, which integrate seamlessly\nwith video data during inference, all within under half a second. This quick processing\nunderscores the encoder\u2019s efficiency, supported by bicubic interpolation that modifies input\ndynamically to achieve variable output dimensions without predefined constraints. AugmNet ,\na novel augmentation module, learns to modify latent space representations directly, prevent-\ning the need for specific augmentation techniques designed for each data type. However, this\nmodule requires careful application to avoid overfitting and other training challenges. The\nTemporal-Module , consisting of 1.63million parameters, is crucial for final pain level as-\nsessments. It leverages a mix of cross- and self-attention mechanisms to enhance efficiency\nand accuracy, demonstrating the potential of transformers in streamlined settings contrary to\ntheir typical use in large-scale applications.\nOur experiments demonstrate that videos are invaluable for discerning individual pain ex-\nperiences by capturing diverse behavioral indicators like facial expressions, eye movements,\nand even slight color changes under stress. Utilizing video data, our methodology reached\nan accuracy of 77.10% in binary classification, effectively distinguishing between no pain\nand very severe pain scenarios. Moreover, it achieved 35.39% accuracy in a multi-level clas-100 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n(a) Attention maps from the Spatial-Module .\n(b) Attention maps from the Temporal-Module .\nFigure 5.7: Regions highlighted in yellow and red denote areas of significant attention. (a)\n(1strow) Sequence of original frames. (2ndrow) Derived from the Spatial-Module\nafter initial stage pretraining. (3rdrow) Derived from the Spatial-Module post sec-\nond stage pretraining. (4throw) Derived from the Spatial-Module trained on the\nBioVid dataset. (b) (1strow) Derived from the Temporal-Module incorporating\nvideo embeddings. (2ndrow) Derived from the Temporal-Module with heart rate\nembeddings. (3rdrow) Derived from the Temporal-Module using a combined em-\nbedding of video and heart rate.5.4. SUMMARY 101\nsification spanning five distinct pain intensities. The heart rate signal, tested as a standalone\nfeature from electrocardiography, showed that remarkable outcomes are possible with this\nsingle data feature, which is pivotal for validating the feasibility of heart rate as a viable\npain indicator. This is crucial as heart rate data is readily accessible from most wearable\ntechnologies, reducing the need for specialized algorithms to handle cardiac signals or raw\nbiosignals, thereby conserving both time and computational resources. Solely using heart\nrate, our model excelled, registering accuracies of 67.04% and31.22% for binary and multi-\nlevel classifications, respectively, among the highest reported. Incorporating video and heart\nrate data, our multimodal method yielded superior results\u2014 82.74% and39.77% for binary\nand multi-level classifications, respectively. These figures significantly enhance video-only\nresults by roughly 9%and heart rate-only outcomes by about 24%. Furthermore, with a total\nparameter count of just 9.62million, our approach stands out for its efficiency and effec-\ntiveness. Overall, this study showcases the synergy achievable by merging video and heart\nrate data, leading to a system outperforming its unimodal counterparts and emphasizing the\npotential of integrated multimodal pain assessment tools.\nUsing attention maps generated by the Spatial-Module , our framework analysis iden-\ntified crucial facial areas like the zygomatic and oral regions as significant for automatic\npain assessment. These maps demonstrated that different pretraining stages refine the focus,\nshowing more targeted attention with specialized training. Similarly, attention maps from\ntheTemporal-Module focused on the latter part of the input image, corresponding to where\npain manifestations are typically observed in the particular dataset.\n5.4 Summary\nThis chapter explores the interplay between model efficiency and performance in automatic\npain assessment tasks. We also aimed to mirror real-world conditions by leveraging read-\nily accessible and applicable modalities without relying on costly precision medical devices.\nConsequently, we utilized RGB videos with a resolution of approximately 1080x1080 and\nheart rate data. The videos utilized are of medium quality, comparable to those captured with\nmobile phone cameras, and the heart rate data simulates readings from wearable devices. It\nis important to note that wearables across various price ranges automatically provide heart\nrate information. Consequently, exploring the potential of using this readily available modal-\nity for pain assessment is crucial. This proof of concept is significant as it could enable\ncost-effective and accessible pain assessment solutions without dependence on specialized\nmedical equipment. Additionally, our study focuses on developing compact and efficient\nmodels that maintain robust performance.\nThe experiments detailed in this chapter reveal that reducing the number of frames used\nin a video-based pipeline by a factor of four minimizes the accuracy loss, under 1.5%, while102 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncutting inference time threefold, facilitating near real-time pain assessment. Furthermore,\nour initially proposed framework, which incorporates 24million parameters and operates at\n4.3GFLOPS, demonstrated exceptionally high performance. In the subsequent experiments,\nwe showcased that heart rate data alone can be effectively used for pain assessment, achiev-\ning impressive results. This finding underscores the practical utility of data available from\nwearables. Additionally, combining video data with heart rate information yielded the high-\nest accuracy in our tests, illustrating that an integrated approach using both behavioral and\nphysiological modalities can significantly enhance performance. Additionally, we demon-\nstrated that creating one of the smallest models documented, with only 9.62million param-\neters and 2.46GFLOPS, allowed us to achieve top-tier results. This highlights that large\nmodels, commonly favored in the current era of AI, are not always necessary for effective\nperformance. However, it is important to note that extensive multi-stage pretraining across\nvarious datasets greatly aided this framework\u2019s success, which was critical in achieving such\nhigh efficiency and effectiveness.\nIn this chapter, we aimed to explore the effectiveness of compact models in achieving\nhigh performance with rapid inference times. We exclusively used heart rate data, mimicking\nthe information typically available from wearable devices, though our experiments did not\nextend to real-world conditions. Instead, they relied on the sole publicly accessible dataset\nexplicitly designed for pain assessment, including facial videos and cardiac signals collected\nin highly controlled laboratory settings. Participants were positioned facing forward un-\nder optimal lighting conditions, with physiological sensors precisely affixed. Recognizing\nthe potential challenges of applying these findings to clinical settings is essential. Issues\nlike inconsistent lighting, unforeseen facial movements, occlusions, or difficulties with sen-\nsor placement must be meticulously addressed to tailor these systems for real-world use.\nMoreover, depending solely on heart rate as a cardiac feature could be restrictive in more\ndemanding scenarios, highlighting the necessity to integrate multiple extracted features or\nuutilizeraw biosignals for comprehensive assessments.Chapter 6\nSynthetic Data: The Role of Thermal Imag-\ning\nContents\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n6.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the study published in [39]. In recent years, syn-\nthetic data generation has gained traction as a viable approach to addressing data scarcity\nand privacy issues while also meeting the requirements for training AI algorithms on unbi-\nased data with adequate sample size and statistical robustness. Additionally, synthetic data\ncan increase the availability and diversity of real data, particularly in rare modalities, which\ncan be essential for training AI-driven diagnostic and predictive models. This enhance-\nment supports healthcare research and improves patient outcomes. [298]. In the literature\non automatic pain assessment, no studies have been reported concerning creating synthetic\nmodalities. This chapter introduces the generation process of synthetic thermal videos using\nGenerative Adversarial Networks (GANs).\nRegarding thermal modality, in recent years, the field of affective computing research\nhas increasingly adopted thermal imaging techniques [299]. This shift was motivated by\n103104 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nstudies showing that stress and cognitive load significantly affect skin temperature [300], at-\ntributable to the autonomic nervous system\u2019s (ANS) regulation of physiological signals such\nas heart rate, respiration rate, blood perfusion, and body temperature. These signals are vital\nindicators of human emotions and affect [299]. Moreover, muscle contractions can alter fa-\ncial temperature by transferring heat to the facial skin [301]. Consequently, thermal imaging\nhas emerged as a viable method for recording transient facial temperatures [302]. Research\nby the authors in [303] on thermal imaging and facial action units to evaluate emotions, such\nas frustration, boredom, and enjoyment, indicated that a multimodal approach yielded the\nmost accurate results. Within pain research, thermal imaging has been explored in limited\nstudies. For instance, [304] reported increased facial temperature in response to painful stim-\nuli, suggesting thermal cameras as effective tools for monitoring pain. Another study [305]\nintroduced a pain dataset consisting of RGB, thermal, and depth videos, finding that while\nthe RGB modality slightly outperformed the others, integrating all modalities provided the\nbest results. This prompted us to explore the specific modality of thermal imagery through\nthe prism of synthesis using generative deep learning.\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks\nWe present a process of creating synthetic thermal videos using GANs in unimodal and\nmultimodal settings alongside RGB video modalities. Integrating a Vision Multilayer Per-\nceptron (MLP) model with a transformer-based module is at the core of our automatic pain\nassessment framework. Key contributions of this research include (1) generating synthetic\nthermal videos to enhance pain assessment as an additional vision modality, (2) assessing\nthe effectiveness of RGB and synthetic thermal videos as standalone modalities, (3) exam-\nining the utility of thermal-related information for pain assessment, and (4) evaluating the\nperformance and implications of the newly developed Vision-MLP architectures.\n6.2.1 Methodology\nThis section outlines the generation of synthetic thermal videos, which will be utilized sub-\nsequently and incorporated into an automatic assessment pipeline.\nSynthetic Thermal Videos\nAn image-to-image translation (I2I) approach has been utilized to create synthetic thermal\nvideos. I2I generative models are designed to bridge different image domains by learning the\ndata distributions inherent to each domain. Here, the source domain comprises RGB images;\nthe target domain is thermal images. In this research, conditional generative adversarial\nnetworks (cGANs) [306] were employed and trained in a supervised manner using paired6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 105\nimages. Fig. 6.1 provides a high-level overview of the method. The generator Gproduces\nimages that appear realistic, whereas the discriminator Dworks to differentiate between\ngenuine and synthetic images through the following minimax game:\nmin\nGmax\nDLcGANpG, Dq, (6.1)\nwhere the objective function LcGANpG, Dqis defined as:\nEx,yrlogDpx, yqs`Ex,zrlogp1\u00b4Dpx, Gpx, zqqqs, (6.2)\nwithxdenoting the actual data, yindicating the target data, and zrepresenting the random\nnoise vector. Here, Gseeks to minimize the objective function, whereas Doperates in\nopposition, striving to maximize it. Additionally, we incorporated the Wasserstein gradient\npenalty (WGAN-GP) [307] to enhance the stability of training. The overall objective is\narticulated as:\nLcGANpG, Dq`\u03bbE\u02c6x,yrp}\u2207\u02c6xDp\u02c6x, yq}2\u00b41q2s, (6.3)\nwhere \u03bbis the penalty coefficient. In the architectural design of our proposed method, which\ndraws inspiration from [308], the generator Gis divided into three distinct modules: an en-\ncoder, which includes two convolutional layers that downsample the input; a middle ResNet\nmodule, featuring nine residual blocks, each equipped with two convolutional layers; and a\ndecoder that upsamples the feature maps back to the final resolution ( i.e.,256\u02c6256) for the\nsynthetic sample. The discriminator D, based on the approach outlined in [309], employs a\npixel-level PatchGAN strategy using 1\u02c61kernels and consists of two convolutional layers.\n6.3 Combination of RGB and Synthetic Thermal Videos\n6.3.1 Methodology\nThis section presents the structure of the proposed automatic pain assessment framework,\nthe augmentation techniques developed, the pre-processing methods employed, and the pre-\ntraining strategy for the modules.\nFramework Architecture\nThe proposed framework consists of two primary modules: a Vision-MLP model that acts\nas a spatial embedding extractor for individual video frames and a transformer-based model\nthat functions as a temporal module, using the embedded representations of the videos for\ntemporal analysis and final pain assessment. Fig. 6.2 displays the modules and their main\ncomponents.106 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nDecoder\n\u2026Encoder\n\u2026Residual blocks\nDiscriminator\nGenerator\nAuthentic/Synthetic?\nFigure 6.1: Illustration of the procedure for creating thermal images, featuring the architecture\nof the Generator G(Encoder, mid-stage ResNet, Decoder), and the Discriminator\nD.\nVision-MLP: MLP-like models have recently emerged as a novel class of vision models,\nproviding an alternative to traditional Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViT). These models are characterized by their straightforward architectures,\nwhich consist of fully connected layers combined with activation functions. They possess\na lower level of inductive bias and rely on basic matrix multiplication operations. Our\nmethodology is grounded in the principles outlined in [310] that introduced the Vision-\nMLP, and [311] that incorporates a wave representation for the patches (also known as\ntokens). Each video frame is initially partitioned into nnon-overlapping tokens Fm\u201c\nrfm,1, fm,2, . . . , f m,nsPRn\u02c6p\u02c6p\u02c63, where pspecifies the resolution of each token, i.e.,16\u02c616\npixels, and 3represents the number of color channels. Each token is subsequently linearly\nprojected into a dimension d\u201c768prior to entering the Vision-MLP (refer to Fig. 2a). The\nfirst principal sub-module, the Channel-Mixer (Fig. 2c), operates independently on each\ntoken fj, enabling interactions among different channels, and is formulated as:\nChannel-Mixer pfj, Wcq\u201cWcfj (6.4)\nwhere Wcdenotes the weight matrix with learnable parameters, and j\u201c1,2, . . . , n . Fol-\nlowing this, the next significant sub-module, the Token-Mixer (Fig. 2b), facilitates commu-\nnication among various tokens, aiding in the extraction of features from different spatial6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 107\nVision-MLPLinear ProjectionToken MixerChannel Mixer\nNormNorm2x\n3xStage-1Token MixerChannel Mixer\nNormNorm4xStage-2Token MixerChannel Mixer\nNormNorm18xStage-3Token MixerChannel Mixer\nNormNorm 3xStage-4\nInput Imagea\nChannel-MixerMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLPChannelsTokensSkip-connectionscMLP\nFully-connectedGELUFully-connectedd\nChannel-MixerWave-BlockWave-Block\nToken-MixerTokensChannels\n......FCNb\nTransformer\n\u2026\nVision-MLP\nWeighted Fusion\nPain Assessment\nCross-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN1x\n8x\n8x4x\n8xe\nFigure 6.2: Representation of the proposed framework, illustrating its components and their\nmain functions: (a)The Vision-MLP module, tasked with extracting feature em-\nbeddings from video frames. (b)The Token-Mixer , an important sub-module of\nVision-MLP , generates the wave representation for the tokens. (c)The Channel-\nMixer , a crucial sub-module within Vision-MLP .(d)The MLP, a core component\nof the Channel-Mixer .(e)The fusion procedure that combines RGB and synthetic\nthermal embeddings, succeeded by the Transformer module, which conducts the\nfinal pain assessment.\nlocations. Typically, in MLP-based models, the token mixers are defined as:\nToken-MixerpF, Wtqj\u201c\u00ff\nkWt\njkdfk, (6.5)\nwhere Wtis the corresponding weight matrix for the tokens, and drepresents element-\nwise multiplication. Our proposed approach modifies tokens into wave-like representations\nto dynamically adjust the interactions between tokens and weights based on their semantic\ncontent. To depict a token fjas a wave \u02dcfjvia a wave function, both amplitude and phase\ninformation is necessary:\n\u02dcfj\u201c|fj|dei\u03b8j. (6.6)\nHere, iis the imaginary unit satisfying i2\u201c \u00b41. The term|fj|indicates the amplitude of\nthe signal, while ei\u03b8jis a periodic function, with \u03b8jrepresenting the phase of the signal. The108 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nabsolute value operation is typically omitted and substituted with 6.4 for simplicity. Each to-\nken\u2019s phase \u03b8jreflects its position within the wave cycle and can, therefore, be characterized\nusing fixed parameters, which are adjustable during the training phase. As such, 6.4 is also\napplied for phase estimation. Given that 6.6 characterizes a wave within the complex domain,\nthe Euler formula is employed to embed tokens within the neural network architecture:\n\u02dcfj\u201c|fj|dcos\u03b8j`i|fj|dsin\u03b8j. (6.7)\nCombining 6.5 and 6.7, a token is represented as:\nfj\u201c\u00ff\nWt\njkfkdcos\u03b8k`Wi\njkfkdsin\u03b8k (6.8)\n\u00f9\u00f1\u00ff\nWt\njkfkdcospWcfkq`Wi\njkfkdsinpWcfkq (6.9)\nwhere Wt,Wc, and Wiare the learnable weight matrices. The described process focuses\non wave-like representations and is conducted within the Token-Mixer , particularly in the\nWave-Block . The Token-Mixer architecture consists of three blocks: two Wave-Blocks and\noneChannel-Mixer operating in parallel. The Vision-MLP module is organized into four\nstages, each featuring a sequence of a Token-Mixer and a Channel-Mixer block, preceded\nby a normalization layer. The depth of parallel blocks in each stage is 3,4,18, and 3, re-\nspectively, allowing for hierarchical embeddings extraction with corresponding dimensions\nacross stages 64,128,320, and 100.\nFusion: For each input frame, the Vision-MLP extracts an embedding of dimensionality d\u201c\n100. The embeddings from the individual frames of a specific video are then concatenated\nto form a comprehensive embedding representation of the video:\nVD\u201crd1}d2}\u00a8\u00a8\u00a8} dms,VDPRN, (6.10)\nwhere mrepresents the number of frames in the video, and Nis the dimensionality of the\nultimate embedding. Following this, the embeddings from the RGB and synthetic thermal\nvideos are combined through a weighted fusion process:\nVFused\u201cw1\u00a8VRGB`w2\u00a8VThermal ,VFusedPRN. (6.11)\nThis fusion strategy integrates the respective embeddings using learned weights w1andw2,\nwhich adjust the influence of the RGB and thermal embeddings, respectively. This weighted\nsummation achieves an effective integration, capturing the relevance of each modality in the\nresultant fused representation VFused .6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 109\nTransformer: The fused embeddings are then input into a transformer-based module that\nincludes self-attention and cross-attention blocks (Fig. 2e). The self-attention mechanism is\nexpressed as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dk\u02d9\nV. (6.12)\nIn this formulation, QPRM\u02c6C,KPRM\u02c6C, and VPRM\u02c6Care the Query, Key, and Value\nmatrices respectively, where Mindicates the input dimension, and Crepresents the channel\ndimension. The cross-attention mechanism also utilizes a dot product operation; however,\nQfor cross-attention is dimensioned N\u02c6Crather than M\u02c6C, with N\u0103Mproviding\na reduction in computational cost. Each self-attention and cross-attention block features 1\nand8attention heads, respectively, and the entire Transformer module consists of 4parallel\nblocks. The output embeddings, with a dimensionality of 340, are used to perform the final\npain assessment via a fully connected neural network.\nAugmentation Methods: Two augmentation techniques have been implemented within the\nframework. Firstly, the method known as Basic is utilized, which combines polarity in-\nversion with the addition of noise. This approach modifies the original input embedding by\ninverting the polarity of data elements and adding random noise from a Gaussian distribution,\nthus introducing variability and perturbations. Secondly, the Masking technique involves ap-\nplying zero-valued masks to the embeddings, effectively eliminating sections of the vectors.\nThe dimensions of these masks are randomly determined, ranging from 10% to 50% of the\nembedding\u2019s total dimensions, and are randomly positioned within the embeddings.\nPre-processing: The pre-processing involves face detection to isolate the facial region. The\nMTCNN face detector [278] is used, which employs multitask cascaded convolutional neural\nnetworks to identify faces and landmarks. It is essential to highlight that the face detector\nwas applied exclusively to the individual RGB frames, and the coordinates of the detected\nface were then applied to the corresponding synthetic thermal frames. The resolution of all\nframes was standardized at 224\u02c6224pixels.\nPre-training: For the I2I approach, the SpeakingFaces dataset [312] was employed to train\nthe proposed GAN model for translating RGB to synthetic thermal videos. Additionally, be-\nfore commencing the automatic pain assessment training, the Vision-MLP andTransformer\nmodules underwent pre-training. The Vision-MLP was subject to a three-stage pre-training\nstrategy: initially, it was trained on DigiFace-1M [313] to acquire basic facial features. It\nwas then trained on AffectNet [287] and RAF Face Database basic [289] to learn features\nrelated to basic emotions through multi-task learning. Lastly, the Compound Facial Expres-\nsions of Emotions Database [288] and the RAF Face Database compound [289] were used110 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.1: Datasets utilized for the pretraining process of the framework.\nDataset # samples # classes Task\nSpeakingFaces [312] 4.58M 142 Face\u0015\nDigiFace-1M [313] 1.00M 10,000 Face\u0010\nAffectNet [287] 0.40M 8 Emotion\u0010\nCompound FEE-DB [288] 6,000 26 Emotion\u0010\nRAF-DB basic [289] 15,000 7 Emotion\u0010\nRAF-DB compound [289] 4,000 11 Emotion\u0010\n\u0015: includes face image pairs for the I2I task \u0010: includes images for face or emotion\nrecognition tasks\nto learn features of compound emotions in a similar multi-task framework. The multi-task\nlearning process is represented as:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s, (6.13)\nwhere LSis the loss for the specific task associated with different datasets, and ware the\nlearned weights that guide the learning process in minimizing the collective loss Ltotal, in-\ntegrating all the individual losses. The Transformer was pre-trained solely on the DigiFace-\n1M[313], adapting the input images into 1D vectors to suit its architectural needs. Table 6.1\noutlines the datasets utilized in the pre-training phase.\n6.3.2 Experiments\nThe proposed framework was assessed using the BioVid Heat Pain Database [109], which\ncomprises facial videos, electrocardiograms, electromyograms, and skin conductance levels\nfrom 87healthy subjects. Participants underwent heat-induced pain via a thermode on their\nright arm at five different intensities: no pain (NP), mild pain (P 1), moderate pain (P 2), se-\nvere pain (P 3), and very severe pain (P 4). Each level was applied 20times to each subject,\nresulting in 100samples per modality and a total of 1740 samples per class. Experiments\nwere structured around binary and multi-level classification schemes to assess pain, analyz-\ning each modality individually and collectively. Binary classification aimed to distinguish\nbetween no pain (NP) and very severe pain (P 4), while multi-level classification (MC) was\ntasked with categorizing all levels of pain intensity present in the dataset. The leave-one-\nsubject-out (LOSO) method was utilized for validation, and accuracy served as the metric\nfor performance evaluation. Table 6.2 outlines the training details for the automatic pain\nassessment, including parameter number and the computational cost measured in floating-\npoint operations (FLOPS) for each module.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 111\nTable 6.2: Training specifications, and number of parameters and FLOPS for each module.\nTraining Details Vision-MLP Transformer\nOptimizer: AdamW Params: 7.35 M Params: 7.96 M\nLearning rate: 2e-5 FLOPS: 30.95 G FLOPS: 30.90 G\nLR decay: cosine\nWeight decay: 0.1\nWarmup epochs: 5\nBatch size: 32\nTotal Params: 15.31 Millions FLOPS: 61.85 Giga\nTable 6.3: Results utilizing the RGB video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.37 30.23\n200 \u2713 20-50 0.7 70.26 28.50\n300 \u2713 30-50 0.9 70.05 30.02\nMasking: indicates the percentage of the input embedding to which zero-value masking is applied\nP(Aug): represents the probability of applying the augmentation methods of Basic & Masking NP: No\nPain P 4: Very Severe Pain MC: multiclass pain level\n6.3.3 Results\nRGB Videos\nWithin the context of the RGB video modality, we recorded an accuracy of 69.37% for the\nbinary classification task (NP vs. P 4) and 30.23% for the multi-class classification (MC).\nThe use of the Masking augmentation method, which obscured 20\u00b450% of the input em-\nbeddings, yielded a slight increase in binary classification accuracy by 0.89%. However, it\nled to a reduction in multi-class classification accuracy. By extending the training period to\n300epochs, modifying the Masking method to cover 30\u00b450% of the embeddings, and ap-\nplying a 90% probability to both augmentation methods, the accuracies improved to 70.05%\nand30.02% for the binary and multi-class tasks, respectively. This represents an average\naccuracy gain of just under 0.5%. The classification outcomes are detailed in Table 6.3.\nSynthetic Thermal Videos\nIn the synthetic thermal modality experiments conducted under identical conditions, the ini-\ntial accuracies were 69.97% for the binary classification and 30.04% for the multi-class clas-\nsification. Enhancing the intensity of the masking method yielded modest gains in accuracy112 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.4: Results utilizing the synthetic thermal video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.97 30.04\n200 \u2713 20-50 0.7 70.20 30.50\n300 \u2713 30-50 0.9 70.69 29.60\nof0.23% for the binary classification and 0.46% for the multi-class classification. Subse-\nquent final accuracies were 70.69% for the binary classification and 29.60% for the multi-\nclass classification, reflecting an average increment of 0.28%. This discrepancy likely arises\nfrom the challenge of detecting nuanced facial changes linked to low-level pain, exacerbated\nby more aggressive augmentation that potentially reduces performance. The summarized\nresults are presented in Table 6.4.\nAdditional Analysis on RGB & Synthetic Thermal Videos\nThe results from the previous section showed a surprising similarity in performance between\nthe RGB and synthetic thermal modalities. Specifically, the RGB modality achieved maxi-\nmum accuracies of 70.26% for the NP vs. P4 task and 30.23% for the MC task. Similarly,\nthe synthetic thermal modality reached top accuracies of 70.69% and30.50% for the same\ntasks, respectively. On average, the thermal video performances were about 1%higher than\nthose for the RGB modality. This was unexpected, considering the synthetic modality was\ninitially considered inferior to the original. This prompted further investigation into why\nsynthetic modalities might perform comparably to or better than the original RGB modality.\nA key question involved the relevance and efficacy of the thermal-related data featured in the\nsynthetic videos. The theory proposed that minimizing facial expressions in thermal videos\ncould enhance the clarity of thermal data assessment.\nGaussian blurring was incrementally applied to RGB and synthetic thermal videos, as\nshown in Fig. 6.3, with kernel sizes increasing from 0to191. According to Table 6.5, at\na kernel size of k\u201c0, a performance differential of 0.47%, favoring the thermal modality,\nconfirms prior findings. This gap slightly expands to 0.49% atk\u201c41. Remarkably, at\nk\u201c91, the disparity enlarges to 2.13% and increases to 5.90% atk\u201c191, where blurring\nis most intense. The results reveal that as facial expressions become less visible through\nblurring, synthetic thermal videos consistently outperform RGB videos, with respective ac-\ncuracies of 66.24% versus 60.34%. As blurring intensifies from k\u201c0tok\u201c191, accuracy\nrates for synthetic thermal and RGB modalities decrease by 1.81% and7.13%, respectively.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 113\nFigure 6.3: Gradual blurring of RGB and synthetic thermal facial images: a series displaying\nvarying levels of Gaussian blur applied, with kernel sizes gradually increased from\nk\u201c0(no blur) to k\u201c191(extensively blurred).\nTable 6.5: Results utilizing the RGB & the synthetic thermal video.\nEpochs Modality BlurAugmentations Task\nBasic Masking P(Aug) NP vs P 4\n100 RGB 0 \u2713 10-20 0.7 67.47\n100 Thermal 0 \u2713 10-20 0.7 68.05\n100 RGB 41 \u2713 10-20 0.7 66.61\n100 Thermal 41 \u2713 10-20 0.7 67.10\n100 RGB 91 \u2713 10-20 0.7 64.80\n100 Thermal 91 \u2713 10-20 0.7 66.93\n100 RGB 191 \u2713 10-20 0.7 60.34\n100 Thermal 191 \u2713 10-20 0.7 66.24\nBlur: Gaussian blurring with kernel sizes k\nThis suggests that critical information, such as visually represented facial temperature in\nthe synthetic modality, is unaffected or slightly impacted. Fig. 6.4 displays the embedding\ndistribution for the RGB and synthetic thermal modalities at k\u201c0andk\u201c191, highlight-\ning a distinct variation in distribution patterns, albeit with ambiguous data point separation.\nFork\u201c191, while RGB embeddings tend to cluster and potentially overlap, many points\nconspicuously stray from the central mass without any distinct arrangement. Conversely, the\ndata points for the synthetic modality spread more uniformly, possibly indicating better class\ndifferentiation.114 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nRGB-0\nThermal-0 Thermal-191RGB-191\nNP\nP4\nFigure 6.4: Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain) classes\nin RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191(heavily\nblurred).\nFusion\nThree fusion strategies were assessed for multimodal analysis involving RGB and synthetic\nthermal video data. Initially, the fusion strategy referenced in 6.11 utilized learned weights\nw1andw2to scale the contributions of each modality. A second strategy incorporated an\nadditional weight, w3, modifying the formula to w3\u00a8pw1\u00a8VRGB`w2\u00a8VThermalq. The\nthird method bypassed learned weights altogether, directly combining the embedding vectors\nfrom the modalities. The results, detailed in Table 6.6, indicate that omitting learned weights\nachieved accuracies of 64.92% and26.40% for the binary and multi-class tasks, respectively.\nThe introduction of w3reduced 0.5%in accuracy for both tasks. The strategy using only\nweights w1andw2yielded the best outcomes, with accuracies of 65.08% and26.50% for\nthe binary and multi-class tasks, respectively. By maintaining the use of weights w1and\nw2and increasing the training duration from 100to300epochs, while consistent with the\naugmentation settings, accuracies improved to 69.50% and29.80% for the binary and multi-\nclass tasks, respectively. Further prolonging the training to 500epochs, with no evidence\nof overfitting, led to further improved performances, with final accuracies of 71.03% and\n30.70% for the respective tasks.6.4. SUMMARY 115\nTable 6.6: Results utilizing the fusion of RGB & synthetic thermal video.\nEpochsFusion\nweightsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n100 \u2013 \u2713 10-20 0.7 64.92 26.40\n100 W2 \u2713 10-20 0.7 65.08 26.50\n100 W3 \u2713 10-20 0.7 64.42 25.90\n300 W2 \u2713 10-20 0.7 69.50 29.80\n500 W2 \u2713 10-20 0.7 71.03 30.70\nW2: utilization of [w 1,w2] W3: utilization of [w 1,w2,w3]\nComparison with Existing Methods\nThis section compares the proposed method with other methodologies reported in the liter-\nature. We utilize Part A of the BioVid dataset, which includes all 87subjects, employing\nthe LOSO cross-validation protocol for validation. The results are displayed in Table 6.7.\nOur vision-based approach, which leverages RGB and synthetic thermal modalities, shows\nperformances comparable to or surpass those of prior studies. Relative to results in the lit-\nerature from [180, 217, 219, 295], our method achieved higher accuracy in both binary and\nmulti-level tasks. Notably, the research in [296] recorded accuracies of 72.40% and30.80%,\nmarking an improvement of 1.37% and0.10% over our method, respectively. The highest\nreported results are from [37], which utilized a transformer-based architecture.\nAdditionally, Table 6.8 compares our results with those from [305], where the authors\nintroduced the MIntPAIN dataset comprising both RGB and thermal videos for automatic\npain assessment across five intensity levels. Our analysis revealed that the accuracies of\nthe RGB and thermal modalities were closely matched at 18.55% and18.33%, respectively,\nwhich parallels our observations of similar performance between RGB and synthetic ther-\nmal modalities. By integrating these modalities, the authors in [305] reported a significant\nperformance increase of 30.77%, surpassing our modest gains. It should be emphasized that\nin [305], the performance levels of the individual modalities were below the random guess\nprediction threshold of 20%. It was only through their combination that performance was\nelevated above this threshold.\n6.4 Summary\nThis chapter investigated the creation of synthetic thermal imagery via GAN models to as-\nsess its utility in automatic pain evaluation. Additionally, a novel framework incorporating\naVision-MLP and supported by a Transformer module as the core of the assessment sys-\ntem was introduced. The experiments demonstrated the effectiveness of the synthetic ther-116 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.7: Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.\nStudy MethodTask\nNP vs P 4 MC\nWerner et al. [296] Deep RF 72.40 30.80\nWerner et al. [295] RF 70.20 \u2013\nZhiet al. [217] SLSTM 61.70 29.70\nThiam et al. [219] 2D CNN, biLSTM 69.25 \u2013\nTavakolian et al. [180] 2D CNN 71.00 \u2013\nGkikas et al. [37] Vision-Transformer 73.28 31.52\nOur Vision-MLP 71.03 30.70\nTable 6.8: Comparison with the MIntPAIN dataset.\nStudy Dataset ModalityTask\nMC\nHaque et al. [305] MIntPAINRGB 18.55\nThermal\u02dd18.33\nFusion 30.77\nOur BioVidRGB 30.02\nThermal\u203929.69\nFusion 30.70\n\u02dd:real\u2039: synthetic\nmal modality, which achieved performances comparable to or better than the original RGB\nmodality. This research also delved into the factors contributing to this effectiveness, par-\nticularly the role of temperature color representations in the analysis. Furthermore, various\nfusion techniques were used to evaluate the combination of the two vision modalities, high-\nlighting the potential for performance improvements over single-modality approaches. It\nis important to note that further enhancements and experimental work, especially with the\nmultimodal approach, could improve outcomes. The generation and integration of synthetic\nmodalities, such as thermal imagery, within an automatic pain assessment framework exhibit\nconsiderable promise, warranting additional exploration and research.Chapter 7\nGeneral-Purpose Models\nContents\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n7.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n7.1 Chapter Overview: Introduction & Related Work\nThis chapter discusses the approaches and findings from [40] and [41]. In recent years,\nthe literature on deep learning has shown a trend towards adopting general-purpose models.\nThese models are characterized by architectures not tied to specific modalities or pre-training\non datasets derived solely from a single domain. We will explore two approaches: first, a\nmodality-agnostic pipeline for automatic pain assessment, and second, the development of a\nfoundation model explicitly applied for automatic pain assessment tasks.\nRegarding modality-agnostic approaches, in 5.3, we introduced the concept specifically\nfor augmentations. These augmentations were designed not directly for the image or biosig-\nnal space but for the latent space. Thus, regardless of the original input modality, whether\n117118 CHAPTER 7. GENERAL-PURPOSE MODELS\nimages, biosignals, or others, the augmentations were applied to their feature representa-\ntions. This chapter advances our work by developing a modality-agnostic multimodal fusion\npipeline evaluated in pain assessment tasks. The literature on modality-agnostic approaches\nremains limited. In [314], the researchers pursued a novel approach by exploring modality-\nagnostic representations through knowledge distillation for semantic segmentation. Their\ngoal was to reduce the modality gap and diminish semantic ambiguity, enabling the com-\nbination of various modalities under any visual conditions. In [315], the authors addressed\nthe persistent challenges of temporal asynchrony and modality heterogeneity in multimodal\nsequence fusion, often leading to performance bottlenecks. To overcome these issues, they\ndeveloped a strategy integrating modality-exclusive and modality-agnostic representations\nfor multimodal ideo sequence fusion. This approach enabled them to capture reliable con-\ntext dynamics within individual modalities and enhance distinctive features across modality-\nexclusive spaces. Additionally, they designed a hierarchical cross-modal attention module\nto uncover significant element correlations across different modalities within the modality-\nagnostic space.\nThe literature on foundation models is considerably more extensive. With the emerging\nparadigm of building AI systems around foundation models, there has been a shift toward\ncreating more adaptable and scalable systems that generalize across various tasks and do-\nmains. A foundation model is defined as any model trained on vast datasets, often through\nextensive self-supervision, which can then be adapted\u2014such as through fine-tuning\u2014to a\nwide range of downstream tasks. Despite their reliance on traditional deep learning and\ntransfer learning techniques, the extensive scale of foundation models fosters the develop-\nment of new capabilities and enhances effectiveness across many tasks [316]. Numerous\nexamples have surfaced recently in academic literature. For instance, the SAM model [317],\na foundation model for image segmentation, was initially trained from scratch on 11million\nimages. In later studies [318, 319], researchers have adapted SAM for medical imaging by\noptimizing it for smaller, specialized datasets. Additionally, a notable paradigm shift has oc-\ncurred with the introduction of generalist models [320], a novel class of foundation models\ntrained simultaneously on various tasks under a unified learning strategy, typically super-\nvised. This approach is particularly advantageous in computer vision, enabling handling\ndiffering embedding representations across tasks and various visual modalities [321].\nTo the best of our knowledge, there are no modality-agnostic or foundation models in\nthe field of automatic pain assessment. Currently, the majority of the approaches utilize\npre-trained models; however, these models typically adhere to the traditional methodology\nof pre-training on a general, large-scale dataset and then fine-tuning for the specific task of\npain assessment. Studies such as those detailed in [305, 322] rely on transfer learning tech-\nniques derived from facial recognition datasets. The majority of these studies are reviewed\nin Chapter 3.7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 119\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment\nWe present a modality-agnostic multimodal framework that leverages both video and fNIRS\ndata. The pipeline operates on a dual Vision Transformer (ViT) format, which obviates the\nnecessity for modality-specific architectures or expansive feature engineering. It interprets\nthe inputs as unified images through 2D waveform representations.\n7.2.1 Methodology\nThis section outlines the pipeline for the proposed multi-modal automatic pain assessment\nframework, describing the model architectures, pre-processing methods, pre-training strate-\ngies, and augmentation techniques.\nFramework Architecture\nThe proposed framework, Twins-PainViT , includes two modules: PainViT\u20131 andPainViT\u20132 .\nBoth models share uniform architectures and parameters and follow to identical pre-training\nprotocol. PainViT\u20131 is tasked with processing the individual input video frames and the\nvisualized fNIRS waveforms, serving as an embedding extractor. Meanwhile, PainViT\u20132\nmanages the visual representation of these embeddings and performs the final pain classifi-\ncation task.\nPainViT: Vision Transformers (ViTs) [323] have established themselves as a leading frame-\nwork in computer vision tasks, recognized for their impressive performance. However,\ndespite their effectiveness, transformer-based models encounter scalability challenges with\nlarger input sizes, significantly increasing computational demands. This inefficiency mainly\nstems from the element-wise operations in the multi-head self-attention mechanism. Efforts\nto improve the efficiency and simplify the architecture of transformer-based models have\nincluded modifications to the self-attention module and overall structural adjustments [324]\n[325]. Our methodology builds on the principles of [326], which incorporates hierarchical\narchitectures into vision transformers, and [327], which introduces mechanisms to enhance\nefficiency and processing speed.\nPainViT\u2013block: A block consists of two key elements: the Token-Mixer and the Cascaded-\nAttention . The architecture places the Cascaded-Attention module centrally, combined with\naToken-Mixer module before and after it. For every input image I, overlapping patch em-\nbedding is utilized, generating 16\u02c616patches. Each patch is then projected into a token\nwith a dimensionality of d.120 CHAPTER 7. GENERAL-PURPOSE MODELS\nToken-Mixer: To better integrate local structural information, the token Tis processed\nthrough a depthwise convolution layer:\nYc\u201cKc\u02daTc`bc. (7.1)\nHere, Ycdenotes the output of the depthwise convolution for channel cof the token Tc.Kc\nrepresents the convolutional kernel for channel c,Tcindicates the c-th channel of the token\nT, and bcis the bias term added to the convolution output for channel c. The symbol \u02da\nindicates the convolution operation. After the depthwise convolution, batch normalization is\nthen applied to the output:\nZc\u201c\u03b3c\u02dc\nYc\u00b4\u00b5Ba\n\u03c32\nB`\u03f5\u00b8\n`\u03b2c. (7.2)\nHere, Zcrepresents the batch-normalized output for channel cof the token T. The learnable\nparameters \u03b3cand\u03b2care specific to channel c, adjusting the scale and shift of the normalized\ndata. \u00b5Bdenotes the batch mean of Yc,\u03c32\nBindicates the batch variance of Yc, and \u03f5is a\nsmall constant included for numerical stability to prevent division by zero. Subsequently, a\nfeed-forward network (FFN) is used to enhance communication between different feature\nchannels:\n\u03a6FpZcq\u201cW2\u00a8ReLUpW1\u00a8Zc`b1q`b2. (7.3)\nHere, \u03a6FpZcqrepresents the output of the feed-forward network for the input Zc. The weight\nmatrices for the first and second linear layers are denoted by W1andW2, respectively; b1\nandb2are the corresponding bias terms for these layers. The activation function employed\nhere is ReLU.\nCascaded-Attention Respecting the attention mechanism, it includes a single self-attention\nlayer. For each input embedding:\nXi`1\u201c\u03a6ApXiq. (7.4)\nTheXirefers to the entire input embedding for the i-thPainViT-block . The Cascaded-\nAttention module uses a cascaded mechanism that splits the full input embedding into smaller\nsegments, with each segment directed to a specific attention head. This method distributes\nthe computational load across the heads, improving efficiency by managing long input em-\nbeddings more effectively. The attention mechanism operates as follows:\nrXij\u201cAttnpXijWQ\nij, XijWK\nij, XijWV\nijq, (7.5)\nrXi`1\u201cConcatrrXijsj\u201c1:hWP\ni, (7.6)7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 121\nTable 7.1: Number of parameters and FLOPS for the components of the proposed Twins-PainViT .\nModule Params (M) FLOPS (G)\nPainViT\u20131 16.46 0.59\nPainViT\u20132 16.46 0.59\nTotal 32.92 1.18\nwhere each j-th head is responsible for computing the self-attention of Xi,j, thej-th segment\nof the full input embedding Xi. This embedding is organized as rXi1, Xi2, . . . , X ihs, with j\nranging from 1 to h, where his the total number of heads. The projection layers WQ\nij,WK\nij,\nandWV\nijtransform each segment of the input embedding into distinct subspaces for queries,\nkeys, and values, respectively. The process concludes with WP\ni, a linear layer that combines\nthe outputs of all heads, restoring them to the original input dimensionality. Additionally, the\ncascaded design aids in developing more complex representations for the Q,K, andVlayers.\nThis enhancement occurs as the output from each head is fed into the subsequent head,\nallowing for a progressive accumulation of information throughout the layers. Specifically:\nX1\nij\u201cXij`rXipj\u00b41q. (7.7)\nHere, X1\nijdenotes the sum of the j-th input segment Xijand the output \u02dcXipj\u00b41qfrom thepj\u00b4\n1q-th head. This summation is the new input embedding for the j-th head in the self-attention\ncomputation. Additionally, depthwise convolution is applied to each Qin every attention\nhead, enhancing the self-attention process\u2019s ability to capture both global representations\nand local details.\nThe architecture consists of three PainViT\u2013blocks with depths of 1,3, and 4. This hier-\narchical design progressively reduces the number of tokens by subsampling the resolution\nby2\u02c6at each stage, enabling the extraction of embeddings with dimensions dacross the\nblocks, particularly 192,288, and 500. Each block also features a multihead self-attention\nmechanism, employing 3,3, and 4heads, respectively. Fig. 1(a-d) depicts the PainViT archi-\ntecture and its core components, while Table 7.1 details the number of parameters and the\ncomputational costs measured in floating-point operations (FLOPS).\nEmbedding extraction & Fusion\nFor every frame in a video, V\u201crv1, v2, . . . , v ns,PainViT\u20131 generates a corresponding em-\nbedding. These embeddings are combined to produce a composite feature representation of\nthe video. Similarly, for each fNIRS signal channel, C\u201c rc1, c2, . . . , c ms,PainViT\u20131 ex-\ntracts embeddings which are then compiled to form a complete representation of the fNIRS122 CHAPTER 7. GENERAL-PURPOSE MODELS\nsignal. The following equations outline this procedure:\nEV\u00d0n\u00ff\ni\u201c1PainViT\u20131pviq, (7.8)\nEC\u00d0m\u00ff\ni\u201c1PainViT\u20131pciq, (7.9)\nwhere EVandECrepresent the embedding representations for the video and fNIRS, re-\nspectively. After these embeddings are extracted, EVandECare visualized as waveform\ndiagrams. The waveforms from both modalities\u2014video and fNIRS\u2014are then combined into\na single image with a resolution of 224\u02c6224. This integrated visual representation is input\ninto PainViT\u20132 for final pain assessment. (Fig. 1e) provides a high-level overview of the\nmultimodal proposed pipeline.\nPre-processing\nThe preprocessing steps include face detection in video frames and generating waveform\ndiagrams from the original fNIRS data. The MTCNN face detector [278], which uses a\nseries of cascaded convolutional neural networks, was employed to identify faces and facial\nlandmarks with the faces set at a resolution of 224\u02c6224pixels. All fNIRS channels are used\nto create waveform diagrams, which visually represent the signal\u2019s wave shape and form\nover time, displaying amplitude, frequency, and phase. This method offers a straightforward\napproach to visualizing a signal without requiring transformations or complex computations\ntypical of spectrograms, scalograms, or recurrence plots. Similarly, embeddings extracted\nbyPainViT\u20131 are visualized using waveform diagrams. Although these embeddings are not\nsignals per se, the 1D vectors can still be plotted in a 2D space for further analysis or use by\ndeep-learning vision models. Waveform diagrams created from fNIRS data and embeddings\nare formatted as images with a 224\u02c6224pixels resolution. Fig. 7.2 shows waveform\nrepresentations of channel-specific fNIRS signals, an embedding extracted from a video,\nand an embedding derived from a channel-specific fNIRS sample.\nPre-training\nBefore the automatic pain assessment training, the Twins-PainViT models underwent pre-\ntraining using a multi-task learning approach. This pre-training incorporated four datasets de-\nsigned for emotion recognition tasks. The AffectNet [287] and RAF-DB basic [289] datasets\nsupplied facial images to train on basic emotions, whereas the Compound FEE-DB [288]\nandRAF-DB compound [289] datasets focused on complex emotional states. In addition,\nfive datasets containing various biosignals were utilized. The EEG-BST-SZ [328] dataset in-7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 123\nTable 7.2: Datasets utilized for the pretraining process of the framework.\nDataset # samples # classes Modality\nAffectNet [287] 0.40M 8 Facial Images\nRAF-DB basic [289] 15,000 7 Facial Images\nRAF-DB compound [289] 4,000 11 Facial Images\nCompound FEE-DB [288] 6,000 26 Facial Images\nEEG-BST-SZ [328] 1.5M 2 EEG\nSilent-EMG [329] 190,816 8 EMG\nBioVid [109] 8,700 5 ECG\nBioVid [109] 8,700 5 EMG\nBioVid [109] 8,700 5 GSR\nEEG: electroencephalogram EMG: electromyogram ECG: electrocardiogram GSR: galvanic\nskin response\ncludes electroencephalograms for schizophrenia analysis, and the Silent-EMG [329] contains\nelectromyograms that help pinpoint the origin of EMG activities, such as from the throat or\nmid-jaw areas. The BioVid [109] dataset provided electrocardiogram, electromyogram, and\ngalvanic skin response samples for pain assessment. All biosignals were represented as\nwaveforms, as detailed in 7.2.1. The equation defines the multi-task learning framework:\nLtotal\u201c9\u00ff\ni\u201c1rewiLSi`wis, (7.10)\nwhere LSirepresents the loss for each specific task from the various datasets, and wiare\nthe learned weights that drive the learning process to minimize the combined loss Ltotal,\nconsidering all individual losses. Table 7.2 outlines the datasets involved in the pre-training\nphase.\nAugmentation Methods & Regularization\nSeveral augmentation methods have been employed to train the proposed framework. For the\npre-training process, RandAugment [330] and TrivialAugment [282] were used, along with\nauxiliary noise from a uniform distribution and a custom MaskOut technique that masks\nout random square sections of input images. In the automatic pain assessment task, Aug-\nMix[281] is used in addition to the previously mentioned methods. Moreover, Label Smooth-\ning[331] and DropOut [332] are implemented as regularization techniques to optimize the\ntraining outcome.124 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.3: Training details for the automatic pain assessment.\nOptimizer LR LR\ndecayWeight\ndecayEpochs Warmup\nepochsCooldown\nepochsBatch\nsize\nAdamW 2e-5 cosine 0.1 100 10 10 32\nLR: learning rate\n7.2.2 Experimental Evaluation & Results\nWe employ the dataset provided by the challenge organizers [118,333], which includes facial\nvideos and fNIRS data from 65participants. The dataset is partitioned into 41 training, 12\nvalidation, and 12 testing subjects, all recorded at the Human-Machine Interface Laboratory,\nUniversity of Canberra, Australia. Electrodes for transcutaneous electrical nerve stimula-\ntion, used as pain stimuli, were placed on the right hand\u2019s inner forearm and back. The study\nmeasures both pain threshold\u2014the lowest stimulus intensity perceived as painful\u2014and pain\ntolerance\u2014the maximum intensity of pain a participant can tolerate. For the fNIRS mea-\nsurements, 24channels were utilized for both HbO and HbR, and each video in the dataset\ncontains 30frames. This paper focuses on the results from the validation segment of the\ndataset, which are structured into a multi-level classification setting for No Pain ,Low Pain ,\nandHigh Pain . Table 7.3 details the training framework for the automatic pain assessment. It\nshould be noted that while numerous experiments were conducted across different modalities\nand their combinations, only the most successful outcomes are discussed in the subsequent\nsections and detailed in the corresponding tables.\nFacial Videos\nFor facial videos, two embedding fusion methods were implemented: the Addition tech-\nnique, which aggregates 30embeddings into a single vector of dimension d\u201c500, and the\nConcatenation approach, which merges the embeddings into a larger vector of d\u201c15,000.\nWith the Addition method, the initial accuracy reached 41.90% under basic augmentation\nand regularization settings. Enhancing the augmentation intensity and adjusting MaskOut\nimproved the accuracy incrementally, achieving a peak of 44.91%. Adjusting DropOut and\nother augmentation parameters refined the performance to 43.52%. These findings are de-\ntailed in Table 7.4. Employing the Concatenation method with initial uniform augmentation\nprobabilities resulted in a starting accuracy of 40.28%. Strategic increases in MaskOut and\nmaintaining other augmentations at moderate levels led to a gradual accuracy improvement,\nculminating in a high of 43.75% when all augmentations were maximized except MaskOut ,\npaired with high regularization settings. The results of this approach are outlined in Table\n7.5.7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 125\nTable 7.4: Results utilizing the video modality & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.1 0.1 0.1 0.1 |3 0.1 0.5 41.90\n0.5 0.5 0.5 0.7 |3 0.0 0.5 44.91\n0.5 0.5 0.5 0.7 |10 0.0 0.5 42.13\n0.5 0.5 0.5 0.7 |3 0.0 0.6 42.36\n0.9 0.9 0.9 0.7 |3 0.3 0.7 43.52\nRand :RandAugment Trivial : TrivialAugment LS:Label Smoothing MS: multiclass pain\nassessment. For Augmentation & Regularization the first number represents the probability\nof application, while in MaskOut the number followed |indicates the number of square\nsections applied.\nTable 7.5: Results utilizing the video modality & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.3 0.3 0.3 0.3 |3 0.1 0.5 40.28\n0.5 0.5 0.5 0.8 |5 0.0 0.5 41.44\n0.9 0.9 0.9 0.7 |3 0.2 0.7 42.13\n0.9 0.9 0.9 0.7 |1 0.4 0.5 41.90\n0.9 0.9 0.9 0.6 |3 0.4 0.5 43.75\nfNIRS\nSimilar to facial videos, the Addition andConcatenation methods were also applied to the\nfNIRS channels, excluding two faulty ones from the original 24. For the HbR with the\nAddition method, the initial accuracy was 39.35%, set with uniform probabilities of 0.5for\nAugMix ,Rand , and Trivial , and MaskOut adjusted to 0.6|5. Modifying MaskOut to0.7|3and\nincreasing LSslightly reduced accuracy, while subsequent adjustments in LSandDropOut\nimproved it to 41.20% (refer to Table 7.6). In the HbR with the Concatenation method, start-\ning with MaskOut at0.7|3led to an accuracy of 40.97%. Escalating all augmentations to\n0.9, while keeping MaskOut at0.7|3, achieved a peak accuracy of 42.13% (refer to Table\n7.7). For the HbO with the Addition method, accuracies started at 43.06% with uniform aug-\nmentation probabilities of 0.3andMaskOut at0.3|3. Elevating MaskOut to0.7|3with minor\nadjustments in LSandDropOut maintained similar accuracies, while optimizing MaskOut\nto0.8|3enhanced performance to 44.68% (refer to Table 7.8). The HbO with the Concate-\nnation method began with an accuracy of 42.13% under an augmentation probability of\n0.1. A balanced augmentation setup at 0.9andMaskOut at0.7|3lifted the peak accuracy\nto44.44%, demonstrating the effectiveness of increased overall augmentation coupled with\nhigh regularization. Further adjustments slightly reduced accuracy, highlighting the critical126 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.6: Results utilizing the HbR & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.6 |5 0.0 0.5 39.35\n0.5 0.5 0.5 0.7 |3 0.4 0.5 38.89\n0.9 0.9 0.9 0.7 |3 0.1 0.9 40.05\n0.9 0.9 0.9 0.7 |5 0.4 0.5 41.20\n0.5 0.5 0.5 0.7 |3 0.0 0.4 40.51\nTable 7.7: Results utilizing the HbR & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.0 0.0 0.7 0.7 |3 0.0 0.5 40.97\n0.5 0.5 0.5 0.7 |1 0.0 0.5 41.44\n0.9 0.9 0.9 0.7 |3 0.1 0.8 42.13\n0.9 0.9 0.9 0.7 |3 0.4 0.5 41.20\n0.5 0.5 0.5 0.7 |3 0.0 0.3 39.81\nTable 7.8: Results utilizing the HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.3 0.3 0.3 0.3 |3 0.1 0.5 43.06\n0.5 0.5 0.5 0.7 |3 0.2 0.5 42.82\n0.9 0.9 0.9 0.7 |3 0.4 0.8 43.29\n0.9 0.9 0.9 0.7 |9 0.4 0.5 44.44\n0.9 0.9 0.9 0.8 |3 0.4 0.5 44.68\nnature of optimal augmentation settings (refer to Table 7.9). Generally, enhanced perfor-\nmance is observed with HbO compared to HbR, as noted in other studies [334], attributed\nto its superior signal-to-noise ratio. Combining HbR and HbO using the Addition method\ninitially showed an accuracy of 42.82% with all augmentations at zero except for MaskOut\nat0.7|3. Increasing AugMix ,Rand , and Trivial to0.5while raising MaskOut to0.7|7slightly\nimproved accuracy to 43.29%. Adjustments to MaskOut back to 0.7|3with a slight increase\ninLSled to a minor reduction in accuracy to 42.59%. However, further increasing all aug-\nmentations to 0.9andLSto0.3while maintaining MaskOut at0.7|3maximized accuracy\nto43.75%. A reduction in DropOut to0.1in the final setup slightly reduced accuracy to\n43.06%, emphasizing the importance of optimizing regularization alongside augmentation\nstrategies for achieving optimal results (refer to Table 7.10).7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 127\nTable 7.9: Results utilizing the HbO & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.1 0.1 0.1 0.1 |3 0.1 0.5 42.13\n0.5 0.5 0.5 0.0 |0 0.0 0.5 43.98\n0.5 0.5 0.5 0.7 |1 0.0 0.5 42.36\n0.9 0.9 0.9 0.7 |3 0.4 0.9 44.44\n0.5 0.5 0.5 0.7 |3 0.0 0.8 43.52\nTable 7.10: Results utilizing the HbR, HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.0 0.0 0.7 0.7 |3 0.0 0.5 42.82\n0.5 0.5 0.5 0.7 |7 0.0 0.5 43.29\n0.5 0.5 0.5 0.7 |3 0.1 0.5 42.59\n0.9 0.9 0.9 0.7 |3 0.3 0.9 43.75\n0.5 0.5 0.5 0.7 |3 0.0 0.1 43.06\nTable 7.11: Results utilizing the videos, HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.4 |5 0.0 0.5 42.36\n0.5 0.5 0.5 0.7 |9 0.0 0.5 41.67\n0.9 0.9 09. 0.7 |3 0.1 0.6 42.59\n0.9 0.9 0.9 0.7 |3 0.3 0.9 43.06\n0.9 0.9 0.9 0.7 |5 0.4 0.5 43.75\nFusion\nThis section explores the fusion of facial videos and fNIRS, explicitly utilizing HbO due\nto its demonstrated superior performance over HbR. Two fusion methods were employed:\ntheAddition method, which aggregates embeddings from video frames and fNIRS channels\ninto a unified vector, and the Single-Diagram method, where aggregated embeddings from\nboth modalities are visualized simultaneously in a single image. For the Addition method,\ninitial configurations with moderate augmentation levels ( 0.5forAugMix ,Rand ,Trivial ) and\nMaskOut at0.4|5achieved an accuracy of 42.36%. Increasing the augmentation levels to 0.9\nand adjusting the regularization parameters ( LSup to 0.4andDropOut up to 0.9) enhanced\nthe accuracy, peaking at 43.75% (refer to Table 7.11). For the Single Diagram method, accu-\nracy improvements were noted, as shown in Table 7.12. Starting with lower MaskOut levels128 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.12: Results utilizing the videos, HbO & Single Diagram method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.3 |5 0.0 0.5 45.83\n0.9 0.9 0.9 0.7 |3 0.1 0.6 46.76\n0.9 0.9 0.9 0.7 |3 0.3 0.6 46.53\n0.9 0.9 0.9 0.9 |3 0.4 0.5 45.83\n0.5 0.5 0.5 0.7 |3 0.0 0.7 45.14\nTable 7.13: Comparison with the validation baseline provided by the AI4PAIN challenge organizers.\nApproachModality\nVideo fNIRS Fusion\nBaseline 40.00 43.20 40.20\nOur 44.91 44.68 46.76\nat0.3|5and standard augmentation probabilities ( 0.5), the accuracy was 45.83%. Utilizing\naugmentation probabilities to 0.9andMaskOut adjustments to 0.7|3significantly improved\nperformance, achieving a high of 46.76%.\nInterpretation & Comparison\nIn the framework\u2019s analysis, attention maps from the last layer of PainViT\u20132 were generated,\nillustrating the processed unified image that integrates both the video and HbO embedding\nwaveforms. This layer consists of 500neurons, each specifically engaging with different in-\nput aspects. Figure 7.3 displays four examples demonstrating how specific neurons predom-\ninantly focus on the video embedding waveform. In contrast, others concentrate on the HbO\nwaveform, and some attend to both, highlighting various details. Table 7.13 compares the\nproposed pipeline and the baseline results set by the challenge organizers. The video-based\napproach utilizing the Addition method exceeded the baseline by 4.91%. Implementing the\nHbO with the Addition method showed a minor improvement of 1.48%. However, the fu-\nsion of modalities through the Single Diagram method achieved a more substantial gain of\n6.56%.\n7.2.3 Discussion\nThis chapter contributes to the First Multimodal Sensing Grand Challenge for Next-Gen\nPain Assessment (AI4PAIN) , utilizing facial videos and fNIRS in a modality-agnostic frame-\nwork. The Twins-PainViT , a dual Vision Transformer configuration, was pre-trained across7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 129\nmultiple datasets using a multi-task learning approach. A key feature of our approach is the\nwaveform representation applied to the original fNIRS data and the learned embeddings, al-\nlowing for their integration into a single image diagram. This method effectively eliminates\nthe need for domain-specific models for each modality. Our experiments demonstrated high\nperformance in both unimodal and multimodal configurations, outperforming the established\nbaselines. The analysis of PainViT\u20132 through attention maps further revealed that specific\nneurons specifically target different modalities or distinct aspects within them, suggesting a\ncomprehensive analytical approach. Future research should continue to explore multimodal\nstrategies, as they have shown superior efficacy in real-world pain assessment settings. De-\nveloping interpretative methods is crucial for integrating these advanced frameworks into\nclinical practice.\n7.3 A Foundation Model for Automatic Pain Assessment\nWe introduce PainFormer , a multi-task learning vision foundation model tailored for auto-\nmatic pain assessment. This initiative is the first to develop and deploy a foundation model\nfor pain recognition, inspired by the frameworks discussed in [320]. Our method involves\ntraining across various datasets and tasks, leveraging large-scale corpora to enhance repre-\nsentation learning for pain assessment applications. This research makes three key contri-\nbutions: (1) it introduces a foundation model capable of extracting robust embeddings from\ndiverse modalities, (2) it incorporates synthetic thermal and estimated depth videos as inno-\nvative modalities, and (3) it evaluates the performance of these modalities in both unimodal\nand multimodal settings.\n7.3.1 Methodology\nThis section outlines the structure and components of the suggested framework. It covers\nthe foundation model\u2019s pretraining based on multi-task learning, the methods for augmen-\ntation, and the training configurations for pretraining and pain evaluation tasks. It also de-\nscribes how synthetic thermal and depth videos are generated and details the visualization\ntechniques for biosignal modalities employed in this research.\nFramework Architecture\nThe framework integrates three models: the PainFormer , a foundation model that extracts\nembeddings from input data; the Embedding-Mixer , which applies these embeddings, either\nsingly or in combination, to classify pain; and the Video-Encoder , which reduces video data\ninto a lower-dimensional latent space for use in the multimodal approaches that are explained\nlater. This framework operates in two separate stages: initially extracting embeddings and130 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.14: Number of parameters and FLOPS for the modules of the proposed framework.\nModule Params\n(Millions)FLOPS\n(Giga)\nPainFormer 19.60 5.82\nEmbedding-Mixer 9.85 2.94\nVideo-Encoder 3.37 0.86\nTotal 32.82 9.62\nthen deploying them according to the demands of specific modality pipelines. Table 7.14\ndetails the parameters and computational costs for each module\u2019s floating-point operations\n(FLOPS).\nPainFormer: Vision Transformers (ViT) have become increasingly widespread for various\nimage-processing tasks, demonstrating the effectiveness of their self-attention mechanisms.\nMoreover, developing Vision Multilayer Perceptron (Vision-MLP) models that use spectral\nmixing techniques\u2014substituting self-attention layers with Fourier transformation layers\u2014\nillustrates that simpler structures with fewer inductive biases can achieve similar outcomes.\nOur strategy incorporates two key concepts: hierarchical Vision Transformers (ViT) [326],\nwhich use multiple embedding extraction stages to boost performance and scalability, and\nthe Fourier transform\u2019s efficient token information mixing as shown in [335]. PainFormer\nintegrates spectral layers using the Fast Fourier Transform (FFT) with self-attention layers.\nBoth spectral and self-attention layers are initially applied, whereas later stages rely solely\non self-attention. The architecture of PainFormer is illustrated in Fig. 1(a). Each 2D input\nimage Iis segmented into nnon-overlapping patches, each patch PRn\u02c6h\u02c6w\u02c63, where hand\nware the patch resolution set at 16\u02c616, and 3represents the RGB channels. Each patch\nis linearly projected into a dimension d\u201c768, followed by positional encoding. Applying\nDiscrete Fourier Transform (DFT) to a 1D sequence of Nelements, xrns, ranging from 0to\nN\u00b41, yields:\nXrks\u201cN\u00b41\u00ff\nn\u201c0xrns\u00a8e\u00b4i2\u03c0k\nNn:\u201cN\u00b41\u00ff\nn\u201c0xrns\u00a8Wkn\nN, (7.11)\nwhere iis the imaginary unit, and WNis defined as e\u00b4i2\u03c0\nN. The sequence can be transformed\nback to the time domain by applying the inverse Discrete Fourier Transform (IDFT):\nxrns\u201c1\nNN\u00b41\u00ff\nk\u201c0Xrks\u00a8ei2\u03c0k\nNn, (7.12)7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 131\nwhere xrnsrepresents the original sequence. Furthermore, for two-dimensional inputs, xrm, ns,\nwith0\u010fm\u010fM\u00b41and0\u010fn\u010fN\u00b41, the formula extends to:\nXru, vs\u201cM\u00b41\u00ff\nm\u201c0N\u00b41\u00ff\nn\u201c0xrm, ns\u00a8e\u00b4i2\u03c0`um\nM`vn\nN\u02d8\n, (7.13)\nwhere Xru, vsis the frequency-domain representation of the input xrm, ns.\nSpectral Layer: For the tokens xfrom image I, a 2D FFT is applied across the spatial\ndimensions to transform xinto the frequency domain:\nX\u201cFrxsPCh\u02c6w\u02c6d. (7.14)\nAfter applying the FFT to extract the various frequency components of the image, we employ\na learnable filter, KPCh\u02c6w\u02c6dacts as a gate to regulate the significance of each frequency\ncomponent. This spectrum modulation allows for the identification and learning of features\nsuch as lines and edges. Specifically:\n\u02dcX\u201cKdX, (7.15)\nwhereddefines the element-wise multiplication. Afterward, the inverse Fast Fourier Trans-\nform (IFFT) is applied, which converts the spectral space back into the physical space:\nx\u00d0F\u00b41r\u02dcXs, (7.16)\nwhere the physical space is referred to as the spatial domain in this case. The final component\nof a spectrum layer is an MLP module, which enables efficient channel mixing communica-\ntion:\n\u03a6pxq\u201cW2\u00a8GELUpDWConvpW1\u00a8x`b1qq`b2, (7.17)\nwhere DWConv denotes a depthwise convolution layer. In addition, layer normalization is\nemployed before and after the FFT and IFFT processes, refer to Fig. 1(b).\nSelf-Attention Layer: In this layer, the standard self-attention mechanism characteristic\nof transformers is utilized. For a given token sequence X, the attention mechanism is defined\nas:\nAttpXq:\u201csoftmax\u02c6XW qpXW kqT\n?\nd\u02d9\nXW v, (7.18)\nwhere Att maps RN\u02c6dtoRN\u02c6d, with Nrepresenting hw. The matrices Wq,Wk, and Wvin\nRd\u02c6dcorrespond to the query, key, and value weights, respectively. Layer normalization is132 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.15: Details of the PainFormer\u2019s architecture.\nStage # Spectral\nLayers# Self-Attention\nLayers# Self-Attention\nHeadsDimension\nd\n1 2 1 2 64\n2 2 2 4 128\n3 \u2013 12 10 320\n4 \u2013 3 16 160\nd: token dimensions\napplied both before and after the attention mechanism, mirroring the approach used in the\nspectral layer. Additionally, the MLP component within this layer is expressed as:\n\u03a6pxq\u201cW2\u00a8GELUpW1\u00a8x`b1q`b2. (7.19)\nFig. 1(c) illustrates the design of this layer.\nStages: A stage-based architecture was developed to create a hierarchical representa-\ntion. PainFormer is organized into four stages, each followed by a single-layer 2D CNN that\ndownsamples the resolution by a factor of 2to reduce the token dimensions. Additionally,\neach stage incorporates a distinct combination of spectral and self-attention layers, with vary-\ning numbers of heads in the self-attention layers and different dimensions for the extracted\ntokens. Table 7.15 presents the relevant details.\nEmbedding-Mixer: The model is built on a transformer architecture, leveraging cross- and\nself-attention mechanisms. Echoing insights from prior research [277], it employs an asym-\nmetrical attention scheme using cross-attention with fewer latent variables to reduce compu-\ntational demands and boost efficiency. Cross-attention operation parallels self-attention, as\ndetailed in Eq. (7.18). Nonetheless, the dimensions for Wq,Wk, and Wvadjust to n\u02c6d\nfrom d\u02c6d, with n\u0103dandnspecifically set at 256. The Embedding-Mixer includes 2\nlayers, each equipped with 1cross-attention and 2self-attention modules. The head counts\nfor cross- and self-attention are 1and8, respectively. The final classification task utilizes an\noutput embedding of length 512, as shown in Fig. 1(d).\nVideo-Encoder: The design of this specific module mirrors the Embedding-Mixer . How-\never, it is simplified for efficiency by including only a 1layer that contains a single cross-\nattention module with a 1head. The number of latent variables, n, is maintained at 256,\nand the output embedding length is set at 40. This module functions exclusively within a7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 133\nparticular framework as part of one of the multimodal strategies, integrating video and GSR\nembeddings. The module\u2019s architecture is depicted in Fig. 1(e).\nSynthetic Thermal & Depth Videos\nThis section incorporates thermal and depth vision modalities alandGB videos into our pain\nassessment frameworks. For the thermal modality, we utilize thermal videos from our ear-\nlier research [39], described in Chapter 6, which introduced an image-to-image translation\n(I2I) method using a conditional generative adversarial network (cGAN). This network was\ndesigned and trained to map the data distribution from the RGB to the thermal domain, fa-\ncilitating the creation of synthetic thermal images from new RGB videos. For the depth\nvideos, we employ the \u201cDepth Anything\u201d technique [336], which is a pioneering model for\nmonocular depth estimation (MDE) that uses a vision transformer-based encoder-decoder\narchitecture with semi-supervised learning. Fig. 7.5 displays a frame sample from the RGB,\nsynthetic thermal, and depth modalities.\nBiosignal Visualization\nGiven that the core model in this research is vision-based, it necessitates using 2D represen-\ntations for physiological modalities. We explore four distinct visualizations: (1) waveform\ndiagrams, which outline the signal\u2019s progression over time, showcasing its amplitude, fre-\nquency, and phase characteristics; (2) spectrogram-angle , which displays the phase angles\nassociated with different frequencies; (3) spectrogram-phase , which reveals phase details\nand incorporates unwrapping to rectify discontinuities; and (4) spectrogram-PSD , which de-\nlineates the power spectral density, indicating the distribution of power across frequencies\nover time. Fig. 7.6 provides an example of each visualization type.\nFoundation Training\nPainFormer , our proposed foundation model, serves as an embedding extractor as previously\noutlined. It has undergone extensive training on 14datasets, which include a total of 10.9\nmillion samples; for further details, see Table 7.16. The training datasets cover a variety\nof human-centric data, ranging from facial recognition datasets such as VGGFace2 [280]\nandDigiFace-1M [313] to datasets aimed at recognizing basic and compound emotions,\nlike AffectNet [287] and RAF-DB [289]. Furthermore, datasets based on biosignals like\nEEG, EMG, and ECG have been incorporated. Regarding training methodology, PainFormer\nemploys a multi-task learning strategy, with each dataset representing a separate supervised\nlearning task. Architecturally, the model adheres to its initial design as specified in 7.3.1\nbut now includes additional task-specific auxiliary classifiers. These classifiers comprise\na single-layer, fully connected network with an ELU (Exponential Linear Unit) activation134 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.16: Datasets utilized for the multitask learning-based pretraining process of the framework.\nDataset # samples # classes Modality\nVGGFace2 [280] 3.31M 9,131 Facial Images\nSpeakingFaces RGB [312]\u00150.76M 142 Facial Images\nSpeakingFaces Thermal [312]\u00150.76M 142 Facial Images\nDigiFace-1M [313] 0.72M 10,000 Facial Images\nDigiFace-1M [313] 0.50M 100,000 Facial Images\nAffectNet [287] 0.40M 8 Facial Images\nSFace [337] 1.84M 10,341 Facial Images\nCACIA-WebFace [129] 0.50M 10,575 Facial Images\nRAF-DB basic [289] 15,000 7 Facial Images\nRAF-DB compound [289] 4,000 11 Facial Images\nCompound FEE-DB [288] 6,000 26 Facial Images\nEEG-BST-SZ [328]\u00121.50M 2 EEG signals\nSilent-EMG [329]\u00120.19M 8 EMG signals\nECG HBC Dataset [291]\u00120.45M 5 ECG signals\nTotal: 14 datasets\u2013tasks 10.9M\nEEG: electroencephalogram EMG: electromyography ECG \u0015: The datasets were also used for the I2I process\ndescribed in 7.3.1, in addition to the training of the PainFormer \u0012: The samples were transformed into spectrograms\nbefore being employed.\nfunction. The objective during training is to simultaneously learn from all 14datasets/tasks.\nThe following equation formalizes this approach:\nLtotal\u201c14\u00ff\ni\u201c1rewiLSi`wis, (7.20)\nwhere LSiindicates the loss linked to each dataset/task, and wiare the adaptive weights that\naim to minimize the overall loss Ltotal, encompassing all individual task losses. The model\nwas trained using this methodology over 200epochs.\nAugmentation & Regularization Methods\nVarious augmentation and regularization strategies were applied during the pre-training of\nPainFormer and in the downstream pain assessment tasks. For foundational training, Triv-\nialAugment [282] and AugMix [281] were used. A customized augmentation method that\nmodifies brightness, contrast, and saturation and involves image cropping was also imple-\nmented. The pre-training regime included adding random noise sourced from a Gaussian\ndistribution. Moreover, a technique was devised to obscure random square portions of the\ninput images. Regularization during pre-training was achieved using DropPath [338] and\nLabel Smoothing [331]. Within the pain assessment framework, two specific augmentation7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 135\nTable 7.17: Training details of the proposed framework.\nTask Optimizer LR LR decay Weight\ndecayWarmup\nepochsCooldown\nepochsBatch size\nMTL AdamW 2e-5 cosine 0.1 5 10 126\u0010\nPain AdamW 2e-5 cosine 0.1 10 10 32\nMTL: multi-task learning for pre-training the foundation model Pain: pain assessment task LR: learning rate \u0010: batch\nsize is proportionally distributed across the 14 tasks\nmethods were integrated. The first, termed Basic , involves polarity inversion and the addition\nof noise, which alters the original input embeddings by reversing data elements\u2019 polarity and\nadding random noise from a Gaussian distribution, thereby inducing variability. The second\nmethod, Masking , implements zero-valued masks on the embeddings, effectively nullify-\ning sections of the vectors. These masks are randomly sized and placed, obscuring 10%\nto20% of the embedding\u2019s total dimensions. For further regularization, techniques such as\nDropOut [332] and Label Smoothing [331] were employed. Additional specifics on the two\ntraining methodologies are detailed in Table 7.17.\nDataset Details\nTo evaluate the effectiveness and resilience of our proposed framework, we performed tests\non two specific pain datasets, BioVid [109] and AI4Pain [118]. These datasets offer a varied\nand solid foundation for validating the performance of our model in pain assessment tasks.\nBioVid Heat Pain Database: This dataset is recognized and well-established within the do-\nmain of pain research. It encompasses facial videos, electrocardiograms, electromyograms,\nand galvanic skin response measurements from eighty-seven pn=87qhealthy participants ( 44\nmales and 43females, aged between 20and65). The experiment involved applying a ther-\nmode to the participants\u2019 right arm to induce pain. Before data collection, each participant\u2019s\npain and tolerance thresholds were determined, defining the minimum and maximum levels\nof pain experienced. This setup included two additional intermediate levels, culminating in\nfive distinct pain intensities: No Pain (NP), Mild Pain (P 1), Moderate Pain (P 2), Severe Pain\n(P3), and Very Severe Pain (P 4). The temperature for inducing these pain levels ranged from\nP1to P 4but did not exceed 50.5\u02ddC. Participants underwent 20inductions at each of the four\nspecified intensity levels (P 1to P 4), with each stimulus lasting 4s, followed by a recovery\ninterval of 8to12s. Additionally, 20baseline measurements at 32\u02ddC (NP) were conducted,\nresulting in 100total stimulations per participant, administered randomly. After reaching\nthe target temperature for each induction, the data was subsequently segmented into 5.5s\nintervals starting at 1s. This segmentation generated 8,700samples, each 5.5slong, evenly\ndistributed across the five pain intensity classes for each modality, encompassing all 87sub-136 CHAPTER 7. GENERAL-PURPOSE MODELS\njects. The video recordings were captured at a frame rate of 25frames per second (FPS),\nwhile the biosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\nAI4Pain Dataset: The AI4Pain Grand Challenge 2024 dataset is a recent addition tailored\nfor advanced pain recognition tasks using fNIRS and facial video data. Sixty-five pn=65q\nvolunteers participated, including 23females, with ages ranging from 17to52years. The\ndataset additionally includes physiological signals like photoplethysmography (PPG), elec-\ntrodermal activity (EDA), and respiration (RESP), though these are not currently publicly\navailable. The dataset is segmented into three parts: training ( 41volunteers), validation\n(12volunteers), and testing ( 12volunteers). The data collection setup for this dataset in-\nvolves comprehensive fNIRS and video recording to capture both brain activity and facial\nexpressions. The fNIRS recordings were conducted using an Artinis device (Artinis Medical\nSystems, Gelderland, the Netherlands), measuring fluctuations in oxygenated haemoglobin\n(HBO2) and deoxygenated haemoglobin (HHB) concentrations (in \u00b5mol/L). This fNIRS sys-\ntem uses 24channels to cover the prefrontal cortex with optodes ( 10sources and 8detectors)\nplaced 30mm apart. It emits near-infrared light at wavelengths of 760nm and 840nm and\nsamples at a rate of 50Hz. The video data is recorded using a Logitech StreamCam at a\nframe rate of 30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low\nPain, and High Pain . It includes 65instances of No Pain (lasting 60s each), 780instances\nofLow Pain (lasting 10s each), and 780instances of High Pain (also lasting 10s each). The\nNo Pain instances represent baseline data. In contrast, Low Pain andHigh Pain are derived\nfrom the pain tolerance tests, capturing subtle and significant changes in neurological and\nbehavioral responses via fNIRS and video data.\n7.3.2 Experimental Evaluation & Results\nThis research devised various testing scenarios, including unimodal and multimodal settings,\nto assess the effectiveness of the proposed foundational model. The aim is to utilize a variety\nof behavioral and physiological modalities to ascertain the capability of PainFormer to gener-\nate and provide high-quality embeddings for pain assessment. The experimental framework\nutilizes a comprehensive set of modalities, encompassing RGB, synthetic thermal imaging,\ndepth videos, and physiological measurements such as ECG, EMG, GSR, and fNIRS, with\nwaveform and spectrogram representations. Additionally, specific pipelines were tailored to\nsingle modalities or their combinations based on each integration need. This adaptability\nis a cornerstone of our approach, as different pipelines might be required depending on the\nspecific demands, data availability, or intended application. We aim to offer robust feature\nrepresentations for any given input modality and excel in performance across all modalities\nand testing scenarios. Figure 7.7 displays a high-level view of the proposed framework. Note7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 137\nthat all images, including video frames and biosignal visual representations, are standardized\nto a resolution of 224\u02c6224pixels.\nThis research employed Part A of the BioVid dataset, focusing on pain assessment in\nbinary terms, differentiating between No Pain (NP) and Very Severe Pain (P4). Validation\nwas conducted using the leave-one-subject-out (LOSO) cross-validation technique. In the\ncase of AI4Pain , a multilevel classification scheme was applied, categorizing pain into three\nlevels: No Pain ,Low Pain , and High Pain . The challenge organizers\u2019 hold-out method\n(training, validation, testing) was utilized for validation. For both datasets, the evaluation\nmetrics included accuracy, recall (sensitivity), and F1 score. It is also important to mention\nthat all experiments used a deterministic approach, ensuring no influence from random ini-\ntializations. This practice assures that any differences in performance observed are strictly\nattributable to specific optimization parameters, modalities, and other controlled variations\nrather than any random factors.\nBioVid\nNumerous experiments were performed using the BioVid dataset. Beyond the original RGB\nvideos, synthetic thermal and depth videos were developed to provide additional visual con-\ntexts, as detailed in 7.3.1. As specified in 7.3.1, four distinct representations of ECGs, EMGs,\nand GSRs were assessed for biosignals. Combinations of these representations were also ex-\nplored.\nVideo: For behavioral modalities within the BioVid dataset, PainFormer generates an em-\nbedding of dimension d\u201c160for each video frame. These embeddings are concatenated to\ncreate a comprehensive representation of each video:\nVD\u201crd1}d2}\u00a8\u00a8\u00a8} dms, DPRN1, (7.21)\nwhere mrepresents the number of frames per video, and N1is the dimensionality of the total\nembedding, computed as m\u02c6d\u00d1138\u02c6160\u201c22,080. This unified embedding is fed\ninto the Embedding-Mixer for final pain assessment. Starting with a training duration of 200\nepochs and using augmentation only on RGB videos, an accuracy of 71.83% and a recall\nof74.52% were recorded. Thermal and depth videos achieved accuracies of 69.83% and\n69.00%, respectively. When the training was extended to 300epochs with intensified aug-\nmentations and incorporating Label Smoothing for regularization, RGB accuracy improved\nto72.50%, although recall decreased slightly by 0.46%. Thermal modality performance de-\ncreased overall, highlighting its sensitivity to augmentations and regularization techniques.\nConversely, depth modalities responded well to these changes, showing improved metrics\nwith an accuracy of 70.08%, a recall of 71.27%, and an F1 score of 69.63%. In the final138 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.18: Results utilizing the video modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1RGB200 0.5 0.5 |10-20| 0.0 0.0 71.83 74.52 70.29\n300 0.7 0.7 |15-20| 0.1 0.0 72.50 74.06 70.93\n600 0.5 0.5 |15-20| 0.1 0.5 76.29 77.56 75.56Thermal200 0.5 0.5 |10-20| 0.0 0.0 69.83 71.51 69.17\n300 0.7 0.7 |15-20| 0.1 0.0 68.83 69.77 68.41\n600 0.5 0.5 |15-20| 0.1 0.5 71.55 72.83 71.12Depth200 0.5 0.5 |10-20| 0.0 0.0 69.00 69.44 67.94\n300 0.7 0.7 |15-20| 0.1 0.0 70.08 71.27 69.63\n600 0.5 0.5 |15-20| 0.1 0.5 71.67 72.84 71.26\nLS: Label Smoothing For Augmentation and Regularization, the number denotes the probability of application,\nwhile in Masking , the number in | |indicates the size of the mask applied.\nexperimental phase, training was extended to 600epochs, employing lighter augmentations\nat a0.7probability, alongside 0.1Label Smoothing and0.5DropOut . This regimen resulted\nin the highest performance for RGB videos, achieving an accuracy of 76.29% and a recall\nof77.56%. The F1 score also significantly increased, rising over 5%to75.56%. Similar pat-\nterns were observed for the thermal and depth videos in this final experimental setup, albeit\nwith minor improvements. Accuracy for thermal videos was 71.55% and for depth videos\n71.67%, with recall rates closely matching at 72.83% and72.84%, respectively. These find-\nings demonstrate consistent enhancement across all visual modalities with refined training\nparameters and extended training durations. Table 7.18 consolidates these experimental out-\ncomes, indicating that the RGB modality consistently surpasses others, while the thermal\nand depth modalities show comparable performance levels. Moreover, although thermal and\ndepth enhancements are modest, they suggest a plateau in potential performance increases.\nECG: The training configuration used for the video data was similarly applied to the ECG\nsignals. As previously indicated, four visual representations were utilized. Each represen-\ntation corresponds to an image dimension of 224\u02c6224pixels, from which embeddings of\ndimensionality d\u201c160are extracted and then inputted into the Embedding-Mixer . Starting\nwith200epochs and employing minimal augmentation without regularization, the waveform\nrepresentation reached an accuracy of 69.58%, with recall and F1 scores of 72.67% and\n68.10%, respectively. The spectrogram-angle had lower performance in all metrics, achiev-\ning an accuracy of 65.58%. Meanwhile, the spectrogram-phase showed better accuracy,7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 139\nTable 7.19: Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 69.58 72.67 68.10\n300 0.7 0.7 |15-20| 0.1 0.0 71.08 72.74 70.22\n600 0.5 0.5 |15-20| 0.1 0.5 73.36 74.75 72.52Angle200 0.5 0.5 |10-20| 0.0 0.0 65.58 66.68 64.89\n300 0.7 0.7 |15-20| 0.1 0.0 66.33 68.22 65.22\n600 0.5 0.5 |15-20| 0.1 0.5 68.25 71.24 66.99Phase200 0.5 0.5 |10-20| 0.0 0.0 70.08 71.54 69.40\n300 0.7 0.7 |15-20| 0.1 0.0 72.33 73.73 71.69\n600 0.5 0.5 |15-20| 0.1 0.5 72.70 74.19 72.14PSD200 0.5 0.5 |10-20| 0.0 0.0 71.08 73.13 70.19\n300 0.7 0.7 |15-20| 0.1 0.0 71.50 73.14 70.18\n600 0.5 0.5 |15-20| 0.1 0.5 75.49 77.15 74.90\nsurpassing the prior two by 0.5%and4.5%, respectively. The spectrogram-PSD achieved\nthe highest results, recording 71.08% accuracy, 73.13% recall, and 70.19% F1 score. Fur-\nther improvements were seen in the 300-epoch configuration across all visual representations\nand metrics. In the ultimate experimental setup extending to 600epochs, enhancements were\nnoted universally, but the spectrogram-PSD showed the most considerable gains, nearly 4%,\nachieving 75.49% accuracy, 77.15% recall, and 74.90% F1 score. This indicates that integrat-\ning amplitude and frequency information, as the PSD representation provides, is particularly\neffective and valuable for analyzing ECG signals. Table 7.19 documents the outcomes for\nthe ECG modality.\nEMG: For EMG signals, the initial training configuration of 200 epochs demonstrated\ncomparable accuracy across the waveform ,spectrogram-phase , and spectrogram-PSD rep-\nresentations, recording scores of 68.75%,68.33%, and 69.25% respectively. However, the\nspectrogram-angle representation underperformed with an accuracy of 66.42%, mirroring\nits lower performance in the ECG modality. In subsequent training sessions with increased\nepochs and enhanced augmentation and regularization, the spectrogram-angle representa-\ntion showed a notable decline in performance across all metrics. Despite some marginal\nimprovements in the 300-epoch configuration, it still trailed behind its initial results, posting\nan accuracy of 65.32%, with recall and F1 scores of 68.15% and63.17%, respectively. This\npattern suggests that the angle representation, which lacks phase unwrapping, is less effec-140 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.20: Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 68.75 70.55 67.93\n300 0.7 0.7 |15-20| 0.1 0.0 69.83 72.52 68.68\n600 0.5 0.5 |15-20| 0.1 0.5 72.07 73.64 71.48Angle200 0.5 0.5 |10-20| 0.0 0.0 66.42 68.57 65.26\n300 0.7 0.7 |15-20| 0.1 0.0 63.92 66.33 62.67\n600 0.5 0.5 |15-20| 0.1 0.5 65.32 68.15 63.77Phase200 0.5 0.5 |10-20| 0.0 0.0 68.33 69.75 67.68\n300 0.7 0.7 |15-20| 0.1 0.0 68.58 70.00 67.97\n600 0.5 0.5 |15-20| 0.1 0.5 69.37 71.17 68.66PSD200 0.5 0.5 |10-20| 0.0 0.0 69.25 70.38 68.84\n300 0.7 0.7 |15-20| 0.1 0.0 69.67 71.06 69.12\n600 0.5 0.5 |15-20| 0.1 0.5 72.10 72.82 71.82\ntive for pain assessment tasks in EMG signals. Conversely, the other visual representations\ndemonstrated consistent improvements in each training configuration. The spectrogram-\nPSD achieved the highest accuracy at 72.10% and an F1 score of 71.82%. The waveform\nrepresentation obtained the highest recall at 73.64%. These results are presented in Table\n7.20 for the EMG modality.\nGSR: For the GSR modality, distinct performance variations among the four representa-\ntions are evident. The waveform -based representations significantly outshine the others,\nstarting with an initial accuracy of 87.75% in the 200-epoch configuration, which is over\n14% higher than other metrics. With extended training sessions, a modest improvement is\nnoted across all representations, indicating that the GSR modality might have reached its\nmaximum potential performance. Among the spectrograms, the spectrogram-phase proves\nto be the most informative, culminating in final accuracy, recall, and F1 scores of 76.41%,\n77.23%, and 76.47%, respectively. The waveform representation emerges as the most effec-\ntive, achieving the highest metrics with an accuracy of 88.99%, recall of 89.55%, and an\nF1 score of 88.88%. The distinct performance of these representations can be related to the\ninherent characteristics of the GSR signal. As depicted in Fig. 7.7, GSR typically presented\nas a smooth curve with gradual slopes, indicative of slow and steady changes in skin conduc-\ntivity due to variations in sweat gland activity triggered by stress or arousal. In comparison,\nEMG signals are marked by sharp spikes and erratic fluctuations, reflecting rapid electrical\nactivities from skeletal muscle contractions. On the other hand, ECG signals display distinct7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 141\nTable 7.21: Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 87.75 88.68 87.56\n300 0.7 0.7 |15-20| 0.1 0.0 88.50 89.16 88.34\n600 0.5 0.5 |15-20| 0.1 0.5 88.99 89.55 88.88Angle200 0.5 0.5 |10-20| 0.0 0.0 73.67 75.00 73.26\n300 0.7 0.7 |15-20| 0.1 0.0 73.08 74.60 72.66\n600 0.5 0.5 |15-20| 0.1 0.5 73.24 75.02 72.83Phase200 0.5 0.5 |10-20| 0.0 0.0 75.17 76.13 74.79\n300 0.7 0.7 |15-20| 0.1 0.0 75.92 76.60 75.57\n600 0.5 0.5 |15-20| 0.1 0.5 76.41 77.23 76.47PSD200 0.5 0.5 |10-20| 0.0 0.0 72.83 73.91 72.34\n300 0.7 0.7 |15-20| 0.1 0.0 73.08 73.96 72.68\n600 0.5 0.5 |15-20| 0.1 0.5 73.96 74.81 73.50\ncyclical patterns, including the P and T waves and the QRS complex. These observations\nimply that the simpler patterns in GSR are not as well suited for spectral and frequency do-\nmain analyses, which are more effectively captured by spectrograms. However, waveform\nrepresentations excel in capturing critical physiological data from GSR signals, outperform-\ning all other modalities and visual representations due to their ability to effectively represent\nthe essential dynamics of GSR activity. The results for the GSR modality are summarized in\nTable 7.21.\nFusion: Various fusion techniques were tested to evaluate whether combining different rep-\nresentations or modalities could enhance performance. In this research, using inputs from\nthe same sensor type, such as RGB with depth-estimation videos or ECG waveforms with\nECG spectrogram-PSD , was considered an unimodal fusion approach. On the other hand,\ncombining inputs from different sensor types, like GSR with EMG, was treated as a multi-\nmodal fusion. Three primary methods of fusion were explored: feature fusion and decision\nfusion. Feature fusion includes strategies such as addition, where embeddings from various\ninputs are summed before progressing to the following processing stage, and concatenation,\nwhich aligns them along the y-axis . Decision fusion, meanwhile, involves processing each\nembedding through the Embedding-Mixer , which then aggregates the predictions from each\ninput to generate a final decision. All related experiments were conducted under the previ-\nously detailed 600-epoch training configuration, with results compiled in Table 7.22.\nIn video modality fusion, we assessed combinations of RGB with thermal, RGB with142 CHAPTER 7. GENERAL-PURPOSE MODELS\ndepth, and thermal with depth, plus a three-input amalgamation of RGB, thermal, and depth.\nThe RGB and thermal blend underperformed compared to RGB alone, with the best perfor-\nmance ( 75.66% accuracy) achieved through decision fusion. The RGB and depth combina-\ntion similarly yielded optimal results through decision fusion, achieving 75.53% accuracy\nbut falling short of RGB-only performance. Notably, merging thermal and depth videos im-\nproved upon using depth alone, particularly via decision fusion, which attained a 73.02%\naccuracy rate. The combination of RGB, thermal, and depth inputs was the sole group that\noutperformed the standalone RGB setup, with decision fusion delivering the highest metrics:\n76.55% accuracy, 77.91% recall, and 76.11% F1 score, indicating marginal improvements\nacross all measures. Decision fusion consistently outperformed the addition method in all\nvideo-based experiments.\nFor biosignals, experiments focused on ECG and EMG using the waveform and the rep-\nresentations of spectrogram-PSD . No fusion experiments were conducted for GSR due to\nthe waveform\u2019s dominance in performance. For ECG, all fusion methods were less effective\nthan the spectrogram-PSD alone, except for the addition method, which slightly improved\nrecall by 0.21%. EMG results were enhanced by all fusion techniques, with concatenation\nproving to be the most beneficial, leading to increases in accuracy, recall, and F1 score by\n0.74%,0.36%, and 0.64%, respectively.\nPhysiological and behavioral modalities were integrated into our multimodal setup, com-\nbining GSR signals with RGB, synthetic thermal, and estimated depth videos. The GSR\u2019s\nwaveform representation and video features, described in 7.21, were merged into a unified\nvector of dimension 22,080. This vector was then processed through the Video-Encoder into\na smaller space of 40. The resulting combined vector of 160`40\u201c200dimensions was\nformed by concatenating the GSR and video embeddings, represented as:\nMh\u201cGd}Enc\u201c\npVRGB\nD`VThermal\nD`VDepth\nDq\u2030\n, hPRN2, (7.22)\nwhere Gdenotes the GSR embedding and Mthe fused vector with N2equal to 200. This\napproach, visualized in Fig. 7.7 (bottom right), achieved the highest performance in the\nstudy, with accuracy, recall, and F1 scores of 89.08%,89.88%, and 88.87%, respectively.\nThis method slightly surpassed the performance of GSR used independently, especially in\naccuracy and recall.\nAI4Pain\nIn the AI4Pain dataset, experiments were conducted utilizing both unimodal and multimodal\napproaches. The original RGB videos were employed for the behavioral modality, while\nwaveforms from the fNIRS\u2019s HBO2 channels were used for the physiological modality. It\nis important to note that out of the 24available HBO2 channels, 2were excluded due to7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 143\nTable 7.22: Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and F1 score.\nModality Input FusionMetric\nAcc Rec F1\nVideoRGB, ThermalAdd 75.09\n-1.2076.97\n-0.5973.98\n-1.58\nDF 75.66\n-0.6377.23\n-0.3375.08\n-0.48\nRGB, DepthAdd 74.93\n-1.3676.41\n-1.1573.38\n-2.18\nDF 75.53\n-0.7677.18\n-0.3875.00\n-0.56\nThermal, DepthAdd 71.44\n-0.2373.15\n+0.3170.73\n-0.50\nDF 73.02\n+1.3574.46\n+1.6272.59\n+1.33\nRGB, Thermal, DepthAdd 76.26\n-0.0377.70\n+0.1475.78\n+0.22\nDF 76.55\n+0.2677.91\n+0.3576.11\n+0.55\nECG Wave, PSDAdd 75.43\n-0.0677.36\n+0.2174.75\n-0.15\nConcat 74.74\n-0.7576.77\n-0.3874.00\n-0.90\nEMG Wave, PSDAdd 72.79\n+0.6974.15\n+0.5172.28\n+0.46\nConcat 72.84\n+0.7474.00\n+0.3672.46\n+0.64\nVideo, GSRRGB, Thermal, Depth,\nWaveAdd &\nConcat89.08\n+0.0989.88\n+0.3388.87\n-0.01\n\u02da: All experiments follow the augmentation and regularization settings for the 600 epoch con-\nfiguration outlined in the unimodal experiments. + and - indicate an increase or decrease in\nperformance, respectively, compared to the best unimodal input approach. DF: Decision Fusion\nAdd: Addition Concat: Concatenation\nmalfunctions. Table 7.23 presents the corresponding results.\nVideo: Similar to 7.3.2, an embedding of dimension d\u201c160is extracted for every frame\nin the AI4Pain dataset. However, in this instance, the extracted embeddings are aggregated\ninto a unified vector:\nVd\u201crd1`d2`\u00a8\u00a8\u00a8` dms, dPRN3, (7.23)\nwhere mrepresents the number of frames in a video, and N3is the dimensionality of the\nunified embedding, set at 160. After processing the embedding through the Embedding-\nMixer and employing the same 600-epoch training configuration as used in prior experi-\nments, this setup achieved an accuracy of 49.77%, with recall and F1 scores of 50.11% and\n49.77%, respectively. Increasing the DropOut rate to 0.3improved the accuracy and F1\nscores to 51.39% and51.31%. Further elevating the DropOut rate to 0.8enhanced the recall\nto52.74%.144 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.23: Results on the validation set of AI4Pain dataset, multilevel classification task, re-\nported on accuracy, recall and F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Video600 0.5 0.5 |15-20| 0.1 0.5 49.77 50.11 49.77\n600 0.5 0.5 |15-20| 0.1 0.3 51.39 51.50 51.31\n600 0.5 0.5 |15-20| 0.1 0.8 48.38 52.74 46.69fNIRS600 0.5 0.5 |15-20| 0.1 0.5 43.06 42.80 42.07\n600 0.5 0.5 |15-20| 0.1 0.3 44.44 45.55 43.74\n600 0.4 0.4 |15-20| 0.1 0.1 43.06 44.18 42.44Fusion600 0.5 0.5 |15-20| 0.1 0.5 50.00 51.01 48.54\n600 0.1 0.1 |15-20| 0.1 0.8 50.23 50.25 50.24\n600 0.4 0.4 |15-20| 0.1 0.6 51.85 51.87 51.35\nFusion: the Addition method of the modalities applied\nfNIRS: For the fNIRS modality, embeddings were aggregated across the 22HBO2 chan-\nnels to produce a feature representation of Od\u201c160. The 600-epoch training setup initially\nyielded 43.06% accuracy, 42.80% recall, and 42.07% F1 score. By increasing the DropOut\nrate to 0.3, peak performance metrics of 44.44% accuracy, 45.55% recall, and 43.74% F1\nscore were achieved.\nFusion: For the fusion of video and fNIRS data, the following aggregation approach was\nutilized:\nFd\u201cVd`Od, dPRN3, (7.24)\nwhere Fdrepresents the combined feature representation. Starting with the same 600-epoch\ntraining configuration, the initial results were 50.00% accuracy, 51.01% recall, and 48.54%\nF1 score. Increasing the DropOut rate to 0.8slightly improved the accuracy and F1 score\nby0.23% and1.7%, respectively, though recall decreased by 0.75%. The optimal DropOut\nsetting of 0.6achieved peak performances of 51.85% accuracy, 51.57% recall, and 51.35%\nF1 score.\n7.3.3 Comparison with existing methods\nTo evaluate PainFormer , we compared it against studies from the literature that utilized the\nBioVid dataset ( Part A ), included all available subjects ( 87), conducted the same task, ad-\nhered to the leave-one-subject-out (LOSO) validation protocol, and reported accuracy met-\nrics. For the AI4Pain dataset, our comparisons were made with studies that strictly followed\nthe evaluation guidelines outlined in the corresponding challenge.7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 145\nInBioVid , the proposed approach using RGB, thermal, and depth video inputs is among\nthe top performers in video-based studies, achieving an accuracy of 76.55%. This surpasses\nall methods utilizing hand-crafted features, as referenced in [284,295,339] and outperforms\nmost deep learning-based methods cited in [37, 180, 217, 219]. Exceptions include results\nfrom [38] at 77.10% and [340] at 78.90%, with the study in [269] achieving 77.50% using a\n3D CNN approach, which, when combined with pseudo heart rate data extracted from videos,\nreached the highest reported result of 88.10%. These results are documented in Table 7.24.\nRegarding biosignals, in ECG-based studies, PainFormer achieved the highest accuracy\nin the literature at 75.49% using the spectrogram-PSD representation, significantly outper-\nforming subsequent studies [36,38] by over 6%and8%, respectively. In EMG-based studies\nutilizing waveform andspectrogram-PSD representations, we achieved a 72.84% accuracy,\nsignificantly exceeding the nearest study [339] at 63.10%. For GSR-based studies, utiliz-\ning solely waveform representation led to the highest performance with an 88.99% accuracy.\nStudies using raw biosignals instead of extracting domain-specific features generally exhib-\nited better results, with the second [341] and third [342] ranked studies achieving 85.56%\nand84.80% accuracy, respectively. Table 7.25 presents these biosignal results.\nIn multimodal scenarios, our approach combining video inputs and GSR achieved the\nhighest reported accuracy of 89.08% (refer to Table 7.26). Notably, with one exception [38],\nall studies incorporated the GSR signal. GSR is consistently recognized as the most effective\nmodality for pain assessment, with the second-highest-performing study [343] using a GSR\nand ECG combination achieving 87.06%. A study including videos, ECG, EMG, and GSR\n[344] reached 86.00% accuracy.\nFor the AI4Pain dataset, PainFormer achieved a 53.67% accuracy using the RGB video\nmodality, outperforming [345] at 49.00% but falling behind [346] at 55.00% utilizing a\ntransformer-based masked autoencoder. Using only fNIRS, an accuracy of 52.60% was\nachieved. In a multimodal approach combining videos and waveform representations, an\naccuracy of 55.69% was attained, surpassing [40] by over 9%and establishing the highest\nperformance on this dataset to date.\n7.3.4 Interpretation\nEnhancing the interpretability of models is crucial for their acceptance and effective integra-\ntion into clinical settings. In this study, PainFormer generates attention maps, as illustrated\nin Fig. 7.8. The weights from the \u201cStage 4\u201d self-attention heads are applied by interpolating\nthem onto the input images, enabling visualization of the model\u2019s focal areas.\nIn Fig. 7.8(a), (1strow), we display examples from the RGB, thermal, and depth modali-\nties, and in Fig. 7.8(a), (2ndrow), the corresponding attention maps are presented. Observa-\ntions indicate that the model primarily focuses on the glabella region (the area between the146 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.24: Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task and\nLOSO validation.\nStudyMethod\nAcc%\nFeatures ML\n[217] raw SLSTM 61.70\n[219] raw 2D CNN, biLSTM 69.25\n[295] optical flow RF 70.20\n[180] raw 2D CNN 71.00\n[39] raw\u0015Vision-MLP 71.03\n[135]:raw 2D CNN 71.30\n[267] facial landmarks, 3D distances RF 71.60\n[296] facial 3D distances Deep RF 72.10\n[296] facial action descriptors Deep RF 72.40\n[297] facial landmarks, 3D distances RF 72.70\n[284] fiducial points GNN 73.20\n[37] raw Transformer 73.28\n[211]:raw 2D CNN, GRU 73.90\n[339] facial landmarks, head pose RF 76.60\n[38] raw Transformer 77.10\n[269] raw 3D CNN, 77.50\n[340] raw, rPPG\u27263D CNN 78.90\n[269] raw, heart rate\u26053D CNN 88.10\nOur raw\u273fTransformer 76.55\n:: reimplemented for pain intensity estimation on BioVid by [269] \u0015: RGB, syn-\nthetic thermal videos \u2726: remote photo plethysmography (estimated from videos)\n\u2605: pseudo heart rate gain (estimated from videos) \u273f: RGB-thermal-depth (DF)\nRF: Random Forest GNN: Graph Neural Networks MLP: Multi-Layer Percep-\ntron\neyebrows) in the RGB frame, a key area for facial expressions. Additional focus is observed\non the mental protuberance area (the chin), which is also associated with expressions of pain.\nFor the thermal frame, the model concentrates on areas around the eyes and the sides of\nthe mouth, where brighter colors in the thermal imagery suggest higher temperatures, indi-\ncating that temperature variations rather than facial expressions drive the model\u2019s attention\nin the thermal modality. In the depth frame, the model targets areas showing variations in\ndepth, particularly across the horizontal eye region, with slight attention to the frame\u2019s lower\nleft and right edges, highlighting depth differences in body parts beyond the face, which\nillustrates a nuanced understanding of the model\u2019s representation of depth.\nThe ECG attention maps in Fig. 7.8(b), (top left), primarily emphasizes a distinct R peak\nin the trace\u2019s center. Significant attention is also directed towards the T waves, especially\nthose following the central R peak, highlighting the model\u2019s sensitivity to these elements in\nthe signal. In the EMG attention maps of Fig. 7.8(b), (top right), PainFormer mainly focuses\non the initial and middle sections of the signals. Despite a muscle contraction burst appearing7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 147\nTable 7.25: Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation.\nStudy ModalityMethod\nAcc%\nFeatures ML\n[235] ECG raw 1D CNN 57.04\n[347] ECG domain-specific\u00b8LR 57.40\n[270] ECG domain-specific\u00b8LR 57.69\n[35] ECG domain-specific\u00b8SVM 58.39\n[342] ECG raw 1D CNN, biLSTM 61.20\n[267] ECG domain-specific\u00b8RF 62.00\n[297] ECG domain-specific\u00b8SVM 62.40\n[297] ECG domain-specific\u00b8SVM 62.40\n[348] ECG raw 2D CNN, biLSTM 63.20\n[339] ECG domain-specific\u00b8RF 64.00\n[269] ECG heart rate\u26053D CNN 65.00\n[38] ECG heart rate Transformer 67.04\n[36] ECG domain-specific\u00b8FCN 69.40\nOur ECG raw\u2726Transformer 75.49\n[347] EMG domain-specific\u00b8LR 58.59\n[235] EMG raw 2D CNN 58.65\n[339] EMG domain-specific\u00b8RF 63.10\nOur EMG raw\u2740Transformer 72.84\n[339] GSR domain-specific\u00b8RF 71.90\n[270] GSR domain-specific\u00b8LR 74.21\n[297] GSR domain-specific\u00b8RF 74.40\n[349] GSR domain-specific\u00b8RF 80.40\n[350] GSR domain-specific\u00b8RF 81.90\n[347] GSR domain-specific\u00b8LR 82.36\n[351] GSR domain-specific\u00b8SVM 83.30\n[348] GSR raw 1D CNN, biLSTM 83.60\n[232] GSR domain-specific\u00b8MLP 84.22\n[235] GSR raw 1D CNN 84.57\n[342] GSR raw 1D CNN, biLSTM 84.80\n[341] GSR raw 1D CNN,\nTransformer85.56\nOur GSR raw\u273fTransformer 88.99\n\u00b8: numerous features \u2605: pseudo heart rate gain (estimated from videos) \u2726: PSD\n\u2740: waveform-PSD (Concat) \u273f: waveform SVM: Support Vector Machines LR:\nLogistic Regression\nlater in the sequence, the model exhibits less attention to this portion. This observation may\nbe related to the PainFormer \u2019s pre-training on the Silent-EMG dataset [329], which might\ninfluence its responsiveness to specific sections of the EMG signals.\nFor the GSR signal in Fig. 7.8(b), (bottom left), mild attention is noted at the onset of the\nresponse, marking the start of the conductance increase, with the most intense attention near\nthe peak amplitude, where conductance reaches its maximum level. In the fNIRS signal\nshown in Fig. 7.8(b) (bottom right), the attention map predominantly highlights regions148 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.26: Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation.\nStudy ModalityMethod\nAcc%\nFeatures ML\n[270] ECG, GSR domain-specific\u00b8SVM 72.20\n[267] ECG, EMG, GSR domain-specific\u00b8RF 74.10\n[267] Video1,ECG2, EMG2,\nGSR2facial landmarks1, 3D distances1,\ndomain-specific2\u00b8RF 77.80\n[297] Video1, ECG2, GSR2facial landmarks1, 3D distances1,\ndomain-specific2\u00b8RF 78.90\n[339] Video1, ECG2, EMG2,\nGSR2facial landmarks1, head pose1,\ndomain-specific2RF 80.60\n[38] Video1, ECG2raw1, heart rate2Transformer 82.74\n[350] Video1, ECG2, EMG2,\nGSR2geometric1, appearance1,\ndomain-specific2RF 83.10\n[347] ECG, EMG, GSR domain-specific LR 83.20\n[239] ECG, EMG, GSR domain-specific biLSTM 83.30\n[243] ECG, EMG, GSR raw DDCAE 83.99\n[352] ECG, EMG, GSR raw DDCAE, NN 84.25\n[235] ECG, EMG, GSR raw 2D CNN 84.40\n[353] GSR, ECG domain-specific\u00b8NN 84.58\n[348] Video, GSR raw 2D CNN,\nbiLSTM84.80\n[342] ECG, GSR raw 1D CNN,\nbiLSTM84.80\n[354] ECG, EMG, GSR domain-specific RF 85.70\n[355] ECG, EMG, GSR domain-specific RF 85.80\n[344] Video1, ECG2, EMG2,\nGSR2facial descriptors1, domain-specific2RF 86.00\n[343] GSR, ECG domain-specific\u00b8NN 87.06\nOur Video\u2722, GSR\u273draw Transformer 89.08\n\u2722: RGB-thermal-depth \u273d: waveform \u00b8: numerous features DDCAE: deep denoising convolutional autoencoders\nNN: neural network\nTable 7.27: Comparison of studies on the testing set of AI4Pain dataset.\nStudyModality\nML Acc%\nVideo fNIRS Fusion\n[40] \u2013 \u2013 \u2713 Transformer 46.67\n[345] \u2713 \u2013 \u2013 2D CNN 49.00\n[346] \u2713 \u2013 \u2013 Transformer 55.00\nOur\u2713 \u2013 \u2013\nTransformer53.67\n\u2013 \u2713 \u2013 52.60\n\u2013 \u2013 \u2713 55.697.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 149\naligning with peaks and rapid changes in HbO2 levels. Significant attention is concentrated\nin the map\u2019s left, middle, and right sections, where distinct peaks and dips in the signal are\nobserved, indicating that PainFormer consistently focuses on substantial fluctuations in the\nHbO2 signal, likely associated with pain conditions. Areas with lower or moderate attention\ncorrespond to segments of the time series with stable or minor variations in HbO2, reflecting\nlower levels of brain activation typically associated with mild or no pain responses.\n7.3.5 Discussion\nThis research introduces PainFormer , a foundation model utilizing a vision-transformer ar-\nchitecture tailored for pain assessment across diverse modalities. The model was pre-trained\non14datasets, totaling 10.9million samples, using a multi-task learning framework to en-\nhance its capability in processing behavioral and physiological inputs. PainFormer is com-\nplemented by the Embedding-Mixer for analyzing embeddings and the Video-Encoder for re-\nducing the dimensionality of video embeddings. We tested our model using the BioVid and\nAI4Pain datasets, experimenting with modalities including RGB, synthetic thermal, depth\nvideos, and biosignal representations such as ECG, EMG, GSR, and fNIRS. The evaluation\ndemonstrated that RGB videos provided the highest accuracy among behavioral modalities,\nrecording 76.29% accuracy in the BioVid dataset. Thermal and depth modalities also per-\nformed well, with accuracies of 71.55% and71.67%, respectively. Interestingly, integrat-\ning thermal and depth modalities improved accuracy by 1.35%, suggesting their potential\nto match the efficacy of RGB while addressing privacy concerns associated with direct fa-\ncial recordings. ECG was particularly effective regarding physiological signals, with the\nspectrogram-PSD achieving the highest accuracy at 75.49%. Although combining differ-\nent ECG representations did not enhance performance, EMG signals showed exceptional\naccuracy above 72% when combining waveform andspectrogram-PSD . GSR, known for\nits efficacy in pain assessment, achieved an impressive accuracy of 88.9%using waveform\nrepresentations alone. Our multimodal approach that integrated GSR with RGB, thermal,\nand depth video embeddings led to a notable accuracy of 89.08% in the BioVid dataset, un-\nderscoring the strength of combining multiple modalities. Further, the creation of attention\nmaps revealed PainFormer \u2019s consistent focus on key areas indicative of pain across all tested\nmodalities. This ability highlights the model\u2019s utility in clinical settings, where understand-\ning pain through various indicators is crucial. However, the influence of pre-training on\nspecific areas requires further investigation to ensure the model\u2019s generalizability and accu-\nracy in real-world applications. Our findings place PainFormer at the forefront of current\nmethodologies, achieving state-of-the-art results across the board. While our model excels\nwith video-based and biosignal inputs in unimodal and multimodal settings, continuous ad-\nvancements and further explorations are needed, especially with the newer AI4Pain dataset.150 CHAPTER 7. GENERAL-PURPOSE MODELS\n7.4 Summary\nThis chapter has introduced general-purpose models and pipelines for automatic pain as-\nsessment. Such approaches have attained popularity recently, driven by the development of\nadvanced architectures and the availability of substantial data and computing resources nec-\nessary for training these models. While this combination has led to notable achievements\nin the broader fields of deep learning and AI, its effect on pain research has been virtu-\nally nonexistent. This distinction motivated our exploration of these methodologies in pain\nrecognition tasks. We presented a modality-agnostic method that homogenizes the pipeline\nirrespective of the input type. Our experiments with RGB videos and fNIRs showed promis-\ning results in both unimodal and multimodal settings. However, there is potential for further\nimprovement. We believe increasing the pre-training data for models within a modality-\nagnostic framework could significantly improve performance. It is generally accepted that\nfoundation models deliver superior outcomes. Our study introduced the first foundation\nmodel specifically developed for and applied to pain assessment. The results demonstrated\nthat this approach is highly effective, not only for well-established applications such as lan-\nguage understanding but also for automatic pain assessment. While further investigation\nis necessary, it is essential to acknowledge a fundamental challenge: the generally limited\navailability of pain-related data, which could restrict the effectiveness of these models.7.4. SUMMARY 151\n\u2026\nVideo\n\u2026\nfNIRSaddwaveform \ngeneration embedding \nextraction# frames \naddwaveform \ngeneration embedding \nextraction# channels \n PainViT-1\nPainViT-2\nPain \nAssessmenteFFN\nFully-connectedRELUFully-connectedc\nOutput\nInput\nHead h\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 2\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 1\nK\nQ\nV\nDWConv\nSelf-attention\nSegment\u2026\nConcatenation & Projectiond\nCascaded Attention PainViTLinear ProjectionToken Mixer\nCascaded Attention\nToken Mixer\n1xBlock-1+Token Mixer\nCascaded Attention\nToken Mixer\n3xBlock-2+Token Mixer\nCascaded Attention\nToken Mixer\n4xBlock-3+a\nInput ImageFFN\nDWConvBatchNorm++\nToken-Mixerb\nFFN\nFully-connectedRELUFully-connectedc\nOutput\nInput\nHead h\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 2\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 1\nK\nQ\nV\nDWConv\nSelf-attention\nSegment\u2026\nConcatenation & Projectiond\nCascaded Attention PainViTLinear ProjectionToken Mixer\nCascaded Attention\nToken Mixer\n1xBlock-1+Token Mixer\nCascaded Attention\nToken Mixer\n3xBlock-2+Token Mixer\nCascaded Attention\nToken Mixer\n4xBlock-3+a\nInput ImageFFN\nDWConvBatchNorm++\nToken-Mixerb\n\u2026\nVideo\n\u2026\nfNIRSaddwaveform \ngeneration embedding \nextraction# frames \naddwaveform \ngeneration embedding \nextraction# channels \nPainViT-1\nPainViT-2\nPain \nAssessment\nFigure 7.1: PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer having\nvarying depths, showcasing how token resolution decreases at each stage; (b)Com-\nposition of the Token-Mixer module, featuring elements like depthwise convolu-\ntion (DWConv) and batch normalization; (c)Architecture of the Feed-Forward\nNetwork (FFN) within the Token-Mixer ;(d)The Cascaded Attention mechanism\nimplemented across multiple heads, illustrating how outputs from preceding heads\nare incorporated to refine the self-attention process, culminating in the final out-\nput projection; (e)Configuration of the proposed multimodal pipeline, employing\nvideos and fNIRS. The embeddings from PainViT\u20131 are represented as waveform\ndiagrams, which are merged into a single diagram that illustrates both modalities\nbefore entering PainViT\u20132 for final pain evaluation.152 CHAPTER 7. GENERAL-PURPOSE MODELS\nabc\nFigure 7.2: Waveform illustrations for various data types: (a)original fNIRS signal, (b)video\nembedding derived from PainViT\u20131 , and (c)fNIRS embedding obtained from\nPainViT\u20131 .\nabc\nFigure 7.3: Attention maps from the PainViT\u20132 .7.4. SUMMARY 153\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nEmbedding-Mixerd\n++++++\nx2\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nVideo-Encodere\n++\nx1MLP-3\nFully-connectedGeGLUFully-connectedhMLP-1\nFully-connectedGELUFully-connected\nDWConvf\nLayer Norm\nLayer NormMLP-2\nMHSA x(2-4-10-16)\nSelf-Attention Layer+\n+c\nLayer Norm\nLayer NormMLP-1\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+ba\nPainFormerInput ImagePosition Embedding\nLinear ProjectionStage-1x2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2x2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3x12 Self-Attention LayerDownsamplerStage-4x3 Self-Attention LayerDownsampler\nMLP-2\nFully-connectedGELUFully-connectedg\nFigure 7.4: Overview of primary models and their components outlined in this research: (a)\nPainFormer is structured hierarchically into four stages, incorporating Spectral\nandSelf-Attention Layers to extract embeddings from the inputs; (b)The Spec-\ntral Layer , a key element of PainFormer , uses FFT to analyze frequency-specific\ndata along with a learnable filter Kto highlight critical frequencies; (c)TheSelf-\nAttention Layer , crucial for PainFormer , enables parallel processing of features and\ntheir interconnections; (d)TheEmbedding-Mixer , employing both cross and self-\nattention mechanisms, functions as the component for the final classification of\nembeddings in pain assessment; (e)TheVideo-Encoder , designed for compact and\nefficient encoding, compresses video data into a reduced dimensional form; (f)The\nMLP-1 is part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)The MLP-3 configuration is integrated into the Embedding-Mixer and\nVideo-Encoder .\nabc\nabcd\na\nb\nc\nd\nFigure 7.5: Examples of different vision modalities in frame samples: (a)RGB frame, (b)\nsynthetic thermal frame, and (c)depth estimation frame.154 CHAPTER 7. GENERAL-PURPOSE MODELS\na\nb\nc\nd\nFigure 7.6: Examples of different visual representations for biosignals: (a)waveform ,(b)\nspectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD .\nFusion N\nEmbedding-Mixer\nadd\nadd\nadd\nVideo-Encoder\nconcatFusion 2DF\n Embedding-Mixer\n add\nadd\nadd\nFusion 1\nadd\nEmbedding-Mixer\naddFusion Pipelines\nadd\nEmbedding-Mixer Modality 1\nconcat\nModality 2\n Embedding-MixerUnimodal Pipelines\nPainFormer\n\u2026Depth\n\u2026Thermal\n\u2026\nRGB\nECG\nEMG\nGSR\n\u2026\nfNIRS\nEmbeddingsadd:      addition\nconcat: concatenation\nDF:      decision fusion\nFusion N\nEmbedding-Mixer\nadd\nadd\nadd\nVideo-Encoder\nconcat\nadd\nFusion 1\nadd\n Embedding-Mixer\naddMultimodal Pipelines\nadd\nEmbedding-MixerModality 1\nconcat\nModality 2\n Embedding-Mixer\nadd\nEmbedding-Mixer Modality NUnimodal Pipelines\nPainFormer\n\u2026Depth\n\u2026Thermal\n\u2026\nRGB\nECG\nEMG\nGSR\n\u2026\nfNIRS\nEmbeddings\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nEmbedding-Mixerd\n++++++\nx2\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nVideo-Encodere\n++\nx1MLP-3\nFully-connectedGeGLUFully-connectedhMLP-1\nFully-connectedGELUFully-connected\nDWConvf\nLayer Norm\nLayer NormMLP-2\nMHSA x(2-4-10-16)\nSelf-Attention Layer+\n+c\nLayer Norm\nLayer NormMLP-1\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+ba\nPainFormerInput ImagePosition Embedding\nLinear ProjectionStage-1x2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2x2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3x12 Self-Attention LayerDownsamplerStage-4x3 Self-Attention LayerDownsampler\nMLP-2\nFully-connectedGELUFully-connectedg\nLayer Norm\nLayer Norm\nCross-AttentionMLP-1\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-1\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-1\nx8\nEmbedding-Mixerb\n++++++\nx2\nLayer Norm\nLayer Norm\nCross-AttentionMLP-1\nx1\nVideo-Encoderc\n++\nx1MLP-3\nFully-connectedGELUFully-connectedhMLP-1\nFully-connectedGeGLUFully-connectedf\nMLP-2\nFully-connectedGELUFully-connected\nDWConvg\nLayer Norm\nLayer NormMLP-3\nMHSA\nSelf-Attention Layer+\n+e\nLayer Norm\nLayer NormMLP-2\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+d a\nPainFormerInput ImagePosition Embedding\nLinear ProjectionStage-1\nx2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2\nx2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3 x12 Self-Attention LayerDownsamplerStage-4 x3 Self-Attention LayerDownsampler\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nVideo-Encoderf\n++\nx1\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nEmbedding-Mixerf\n++++++\nx2MLP-2\nFully-connectedGELUFully-connectede\nMLP-3\nFully-connectedGeGLUFully-connectedgMLP-1\nFully-connectedGELUFully-connected\nDWConvc\nLayer Norm\nLayer NormMLP-2\nMHSA\nSelf-Attention Layer+\n+d\nLayer Norm\nLayer NormMLP-1\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+ba\nPainFormerInput ImagePotition Embedding\nLinear ProjectionStage-1\nx2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2\nx2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3 x12 Self-Attention LayerDownsamplerStage-4 x3 Self-Attention LayerDownsamplerFFN\nFully-connectedRELUFully-connectedc\nOutput\nInput\nHead h\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 2\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 1\nK\nQ\nV\nDWConv\nSelf-attention\nSegment\u2026\nConcatenation & Projectiond\nCascaded Attention PainViTLinear ProjectionToken Mixer\nCascaded Attention\nToken Mixer\n1xBlock-1+Token Mixer\nCascaded Attention\nToken Mixer\n3xBlock-2+Token Mixer\nCascaded Attention\nToken Mixer\n4xBlock-3+a\nInput ImageFFN\nDWConvBatchNorm++\nToken-Mixerb\n\u2026\nVideo\n\u2026\nfNIRSaddwaveform \ngeneration embedding \nextraction# frames \naddwaveform \ngeneration embedding \nextraction# channels \n PainViT-1\nPainViT-2\nPain \nAssessmente\nFigure 7.7: An overview of the presented framework. PainFormer , the foundational model,\nexcels in deriving high-quality embeddings from a diverse array of behavioral and\nphysiological modalities. The evaluation of RGB, thermal, and depth videos, along-\nside various representations of ECG, EMG, GSR, and fNIRS such as waveforms\nand spectrograms, underscores the rich information captured within these embed-\ndings. Leveraging the embeddings from PainFormer facilitates the creation of var-\nious and diverse unimodal and multimodal pipelines designed for the pain assess-\nment task. Each pipeline can be customized to suit the specific modalities involved,\ndataset characteristics, and the demands of the intended application or clinical set-\nting. Our assessments included the development and implementation of several\npipelines in both unimodal and multimodal contexts, achieving leading-edge re-\nsults across various modalities and data representations.7.4. SUMMARY 155\na\nb\nFigure 7.8: Attention maps from the PainFormer :(a)(1strow) frames from RGB, thermal, and\ndepth video modalities; (a)(2ndrow) corresponding attention maps; (b)(1strow)\nattention maps for ECG and EMG; (b)(2ndrow) attention maps for EDA and fNIRS\nmodalities.156Chapter 8\nConclusions, Perspectives and Future Work\nContents\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n8.1 Summary of Thesis Achievements\nThe primary objective of this thesis was to explore and improve methods for automatic pain\nassessment. We developed innovative methods that either improved assessment accuracy\nor introduced new approaches, potentially paving the way for advanced methodologies in\nthe future. Additionally, this thesis aimed to integrate ideas and insights from psychology,\nbiology, and nursing, translating them into engineering concepts.\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment\nChapter 3 presented a comprehensive review of deep learning-based approaches in the field.\nThis systematic review, conducted at the beginning of this Ph.D. project, laid the groundwork\nfor our understanding of the domain. We believe that this foundational work will continue\nto benefit other researchers interested in pain assessment from a machine learning and AI\nperspective. In addition to documenting existing approaches, this work identified emerging\ntrends and suggested potential improvements, offering valuable insights for future research.\n157158 CHAPTER 8. CONCLUSIONS, PERSPECTIVES AND FUTURE WORK\n8.1.2 Insights from Gender and Age Analysis\nIn Chapter 4, we explored the impact of demographic factors on an automatic pain assess-\nment pipeline, focusing specifically on gender and age. For decades, it has been recognized\nthat these factors significantly affect pain expression, sensation, and perception in diverse\nand intriguing ways. Our study utilized ECG signals to explore variations in pain sensation\namong people. Our findings confirmed significant differences between males and females,\nwith the latter group exhibiting higher sensitivity\u2014a phenomenon well-documented in pain\nliterature from psychological and biological perspectives. Moreover, substantial variations\nwere observed across different age groups. A critical discovery was that pain sensation\ntends to decrease with age. This observation is particularly concerning as it implies that\nolder individuals might sustain injuries without adequate perception of pain due to neurolog-\nical reasons, potentially leading to further harm. To our knowledge, such explorations and\nfindings have not been previously addressed in automatic pain assessment. We expect our re-\nsearch will inspire more studies into these phenomena from computational and engineering\nperspectives.\n8.1.3 Pain Assessment with Compact, High-Performance Models\nIn Chapter 5, we introduced methods aimed at developing effective and efficient approaches\nsuitable for real-world applications, focusing on computational cost and efficiency. A pri-\nmary concern addressed is the tendency in both automatic pain research and the broader\nfield of deep learning to rely on large models that require high-end GPUs for basic infer-\nence. We investigated whether achieving comparable performance with more efficient and\nfaster models is feasible. Additionally, we conducted one of the first studies to use heart\nrate as the sole feature for pain assessment. This choice was motivated by the widespread\navailability of heart rate data from consumer wearables, prompting us to examine its viabil-\nity for pain assessment. Our findings indicate that well-designed and optimized models can\ndeliver performance on par with, or even superior, systems that use more complex features\nor raw ECG signals. This is significant, as effective real-world applications must balance\npeak performance with manageable computational demands, particularly in clinical or home\nmonitoring settings.\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy\nIn Chapter 6, we explored the creation of synthetic data and its potential utility within an\nautomatic pain assessment framework. Motivated by literature suggesting that thermal im-\nagery can reflect skin temperature increases during arousal events like pain, we generated\nsynthetic thermal videos. Our efforts led to the development of this new synthetic modality,8.1. SUMMARY OF THESIS ACHIEVEMENTS 159\nand we assessed its effectiveness compared to authentic RGB videos. The results demon-\nstrated that the synthetic videos we produced were not only of high quality but also reached\nthe performance of RGB videos. This significant outcome may pave the way for new au-\ntomatic systems that rely on synthetic data. Such systems could ( i) utilize modalities that\nare rare, challenging, or costly to record and ( ii) enhance privacy by concealing or partially\nconcealing the identities of individuals involved. Furthermore, it opens the possibility for\nsystems that integrate both authentic and synthetic modalities, leveraging the strengths of\neach.\n8.1.5 Universal Modeling for Automatic Pain Assessment\nIn Chapter 7, we explored and presented general-purpose models and pipelines for auto-\nmatic pain assessment tasks. Our initial focus was on conceiving a single pipeline applica-\nble across all input modalities, simplifying the development process by eliminating the need\nfor specialized models and modules for each modality. Our strategy employed vision-based\nmodels, which are highly effective since all inputs can be transformed into image formats\u2014\nfor instance, a video frame remains an image. Signals can be visualized as images, such\nas spectrograms or waveforms. This approach proved straightforward and effective in our\nexperiments with videos and FNIRs, applying waveforms in unimodal and multimodal sce-\nnarios. A major advancement in our research was the development of the PainFormer , a\nfoundation model specifically crafted for automatic pain assessment. This model is notable\nfor being the first in its field and represents a significant step forward in pain assessment tech-\nnology. The most important finding from this study is the exceptional performance of the\nfoundation model across a range of modalities, including RGB, thermal, depth videos, ECG,\nEMG, GSR, and fNIRs. It consistently delivered state-of-the-art results, demonstrating its\nversatility and effectiveness. We believe that this research will encourage further investiga-\ntions into similar models, enhancing their performance in benchmarks and possibly leading\nto broader real-world applications by proving their effectiveness and utility.\n8.1.6 Explainable Deep Learning\nAlthough explainability was not the primary focus of the thesis, our research consistently\naimed to provide insights and explanations on how the model performs and makes decisions.\nUsing attention maps, we aimed to deliver these explanations, which were often clear and\nvaluable. This approach is crucial, especially if such models are to be implemented in clinical\nsettings in the future.160 CHAPTER 8. CONCLUSIONS, PERSPECTIVES AND FUTURE WORK\n8.2 Perspectives for Automatic Pain Assessment Methods\nAutomatic pain assessment is a fascinating subject from engineering and research perspec-\ntives and a critical matter in healthcare and clinical environments. Our observations and\ncritiques focus on the datasets, the cornerstone of any computational science research. In\nSection 3.7.1, we discussed the challenges in automatic pain assessment. The observations\nand statements remain largely unchanged a few years after this writing.\nThe comments provided here cannot be addressed with existing datasets; they are in-\ntended for future ones. The most superficial data to incorporate involves demographic fac-\ntors such as age and gender. As explored in our research, both are crucial and including\nthem should be considered a minimum standard for future datasets. Another factor criti-\ncal to the generalization of future models is the inclusion of individuals from various racial\nbackgrounds. We have observed notable improvements in this area; for instance, the AI4Pain\ndataset, a recent addition, features participants from diverse backgrounds. One important el-\nement not yet considered in any dataset is social interactions, which, as discussed, influence\npain perception. Future datasets should incorporate this factor, designing experiments to\nassess responses to stimuli when individuals are alone, accompanied by a female and male\nperson. Additionally, attractiveness also affects pain perception. Incorporating these aspects\ninto experiments is feasible and should be relatively straightforward. Recording sound and\nspeech is essential in terms of modalities. This data will benefit signal processing and lan-\nguage analysis and be valuable for assessing pain.\nLastly, we observe that even though significant funds are invested in equipment for\nrecording biosignals, video capture quality in some datasets is only medium to low. Fu-\nture research must utilize cameras capable of recording high-resolution videos, such as 4K.\nIt is important to note that while mobile phones may meet these specifications on paper,\nthe quality of their sensors and lenses is often inadequate. More professional video record-\ning equipment is necessary. Additionally, a high frame rate is essential for capturing facial\nmicro-expressions. Furthermore, as we have discussed, incorporating visual modalities be-\nyond RGB, such as thermal and depth imaging, can be highly valuable. The cost of these\ntechnologies is comparable to that of biosignal recording equipment and should not be pro-\nhibitively expensive.\n8.3 Future Work\nBased on the work and findings presented in this thesis, we propose recommendations for\nfuture research. As emphasized earlier, any modality must incorporate and extract features\nthat capture the temporal dimension. This is vital because pain is a dynamic condition,\nexpressed and evolving over time, not static. This principle applies equally to many other8.3. FUTURE WORK 161\naffective-related tasks.\nWe have a primary recommendation regarding the computational methods used in auto-\nmatic pain assessment. We have observed that researchers in this field often recycle the same\nmethods repeatedly. This repetition does not appear to stem from efforts to enhance or refine\nexisting approaches. We encourage future researchers to be more adventurous and explore\nnew concepts and techniques. The literature on deep learning, which is readily accessible\nand constantly evolving, provides a solid foundation. Researchers should strive to adopt and\nadapt these ideas to their specific problems and tasks and seek to innovate and improve upon\nthem.\nA more specific technical recommendation regarding methods concerns the use of foun-\ndation models. In our research, we explored this concept and developed PainFormer . It is\nimportant to note that the specific model is not large compared to others in the literature,\nespecially the well-known language models; it can be considered reasonably compact. This\ndemonstrates that small, efficient models can be developed for tasks like pain assessment and\nbeing effective. We believe greater efforts should be directed toward creating foundational,\ngeneral-purpose models incorporating multimodality. This approach represents a promising\nand significant path for future research. In addition, we believe that methods related to gener-\nation processes hold valuable potential. In particular, until new, high-quality pain datasets are\ndeveloped, techniques like super-resolution (upscaling) to produce higher-resolution video\nframes and methods for generating auxiliary frames to increase FPS are promising areas for\nexploration.162Bibliography\n[1] HAFD Merskey. Pain terms: a list with definitions and notes on usage. recommended\nby the iasp subcommittee on taxonomy. Pain, 6:249\u2013252, 1979.\n[2] Amanda C de C Williams and Kenneth D Craig. Updating the definition of pain. Pain,\n157(11):2420\u20132423, nov 2016.\n[3] Rebecca Pillai Riddell and Kenneth D Craig. Developmental Dimensions in Under-\nstanding Interpersonal Features of Pain , pages 43\u201355. Springer International Publish-\ning, Cham, 2018.\n[4] Amanda Williams and Judith Kappesser. Why Do We Care? Evolutionary Mecha-\nnisms in the Social Dimension of Pain , pages 3\u201322. 2018.\n[5] Steven J. Mithen. The Prehistory of the Mind: A Search for the Origins of Art, Reli-\ngion and Science . Orion Publishing Group, 1996.\n[6] Rebecca Redfern. A regional examination of surgery and fracture treatment in iron\nage and roman britain. International Journal of Osteoarchaeology , 20(4):443\u2013471,\n2010.\n[7] Steven P Cohen, Lene Vase, and William M Hooten. Chronic pain: an update on\nburden, best practices, and new advances. The Lancet , 397(10289):2082\u20132097, 2021.\n[8] Aza Abdulla, Nicola Adams, Margaret Bone, Alison M Elliott, Jean Gaffin, Derek\nJones, Roger Knaggs, Denis Martin, Liz Sampson, Pat Schofield, and British Geriatric\nSociety. Guidance on the management of pain in older people. Age and ageing , 42\nSuppl 1:1\u201357, March 2013.\n[9] Global, regional, and national incidence, prevalence, and years lived with disabil-\nity for 354 Diseases and Injuries for 195 countries and territories, 1990-2017: A\nsystematic analysis for the Global Burden of Disease Study 2017. The Lancet ,\n392(10159):1789\u20131858, nov 2018.\n[10] US Burden of Disease Collaborators. The State of US Health, 1990-2010: Burden of\nDiseases, Injuries, and Risk Factors. JAMA , 310(6):591\u2013606, 08 2013.\n163164 BIBLIOGRAPHY\n[11] Darrell J. Gaskin and Patrick Richard. The Economic Costs of Pain in the United\nStates. The Journal of Pain , 13(8):715\u2013724, aug 2012.\n[12] Harald Breivik, Elon Eisenberg, and Tony O\u2019Brien. The individual and societal bur-\nden of chronic pain in europe: the case for strategic prioritisation and action to im-\nprove knowledge and availability of appropriate care. BMC public health , 13:1\u201314,\n2013.\n[13] Deloitte Access Economics. The cost of pain in australia, 2019.\n[14] Pradeep Dinakar and Alexandra Marion Stillman. Pathogenesis of Pain. Seminars in\nPediatric Neurology , 23(3):201\u2013208, aug 2016.\n[15] Puja Seth, Rose A. Rudd, Rita K. Noonan, and Tamara M. Haegerich. Quantifying the\nepidemic of prescription opioid overdose deaths. American Journal of Public Health ,\n108(4):500\u2013502, 2018.\n[16] Ramsin Benyamin, Andrea M Trescot, Sukdeb Datta, Ricardo M Buenaventura, Ra-\njive Adlaka, Nalini Sehgal, Scott E Glaser, and Ricardo Vallejo. Opioid complications\nand side effects. Pain Physician , 2s;11(3;2s):S105\u2013S120, 03 2008.\n[17] Stefanos Gkikas and Manolis Tsiknakis. Automatic assessment of pain based on\ndeep learning methods: A systematic review. Computer Methods and Programs in\nBiomedicine , 231:107365, 2023.\n[18] Lucille A Joel. The fifth vital sign: pain. AJN The American Journal of Nursing ,\n99(2):9, 1999.\n[19] Aleksandra Badura, Aleksandra Maslowska, Andrzej My \u00b4sliwiec, and Ewa Pietka.\nMultimodal signal analysis for pain recognition in physiotherapy using wavelet scat-\ntering transform. Sensors , 21(4), 2021.\n[20] Kyle Vader, Geoff P. Bostick, Lisa C. Carlesso, Judith Hunter, Giulia Mesaroli, Kadija\nPerreault, Yannick Tousignant-Laflamme, Susan Tupper, David M. Walton, Timo-\nthy H. Wideman, and Jordan Miller. The revised iasp definition of pain and accompa-\nnying notes: Considerations for the physiotherapy profession. Physiotherapy Canada ,\n73(2):103\u2013106, 2021.\n[21] Lauren N Straatman, Michael J Lukacs, Joshua Y Lee, Maryam Ghodrati, Emily A\nLalone, and David M Walton. Are people good prognosticators of their own pain?\nan exploration of the relationship between sex-specific pain beliefs and clinical pain\nevaluation. Musculoskeletal Science and Practice , 62:102667, December 2022.BIBLIOGRAPHY 165\n[22] Raul Fernandez Rojas, Nicholas Brown, Gordon Waddington, and Roland Goecke.\nA systematic review of neurophysiological sensing for the assessment of acute pain.\nNPJ Digital Medicine , 6(1):76, 2023.\n[23] Raul Fernandez Rojas, Mingyu Liao, Julio Romero, Xu Huang, and Keng-Liang Ou.\nCortical network response to acupuncture and the effect of the hegu point: An fnirs\nstudy. Sensors , 19(2), 2019.\n[24] Seyed Amir Hossein Aqajari, Rui Cao, Emad Kasaeyan Naeini, Michael-David\nCalderon, Kai Zheng, Nikil Dutt, Pasi Liljeberg, Sanna Salanter \u00a8a, Ariana M Nelson,\nand Amir M Rahmani. Pain assessment tool with electrodermal activity for postop-\nerative patients: method validation study. JMIR mHealth and uHealth , 9(5):e25258,\n2021.\n[25] H H Yong, S J Gibson, D J Horne, and R D Helme. Development of a pain atti-\ntudes questionnaire to assess stoicism and cautiousness for possible age differences.\nThe journals of gerontology. Series B, Psychological sciences and social sciences ,\n56(5):P279\u201384, sep 2001.\n[26] E J Bartley and R B Fillingim. Sex differences in pain: a brief review of clinical and\nexperimental findings. British journal of anaesthesia , 111(1):52\u201358, jul 2013.\n[27] Jean-Michel Rou \u00b4e, Iris Morag, Wassim M Haddad, Behnood Gholami, and Kanwal-\njeet JS Anand. Using sensor-fusion and machine-learning algorithms to assess acute\npain in non-verbal infants: a study protocol. BMJ open , 11(1):e039292, 2021.\n[28] Boaz Gedaliahu Samolsky Dekel, Alberto Gori, Alessio Vasarri, Maria Cristina\nSorella, Gianfranco Di Nino, and Rita Maria Melotti. Medical evidence influence\non inpatients and nurses pain ratings agreement. Pain Research and Management ,\n2016.\n[29] Kelly M. Hoffman, Sophie Trawalter, Jordan R. Axt, and M. Norman Oliver. Racial\nbias in pain assessment and treatment recommendations, and false beliefs about bio-\nlogical differences between blacks and whites. Proceedings of the National Academy\nof Sciences , 113(16):4296\u20134301, apr 2016.\n[30] Francis J Keefe, Tamara J Somers, David A Williams, and Suzanne J Smith. Assess-\nment of pain behaviors. In Handbook of pain assessment, 3rd ed. , pages 134\u2013150.\nThe Guilford Press, New York, NY , US, 2011.\n[31] Nicole Miglio and Jessica Stanier. Beyond pain scales: A critical phenomenology of\nthe expression of pain. Frontiers in Pain Research , 3, 2022.166 BIBLIOGRAPHY\n[32] Andrew Leroux, Rachael Rzasa-Lynn, Ciprian Crainiceanu, and Tushar Sharma.\nWearable Devices: Current Status and Opportunities in Pain Assessment and Man-\nagement. Digital Biomarkers , 5(1):89\u2013102, 04 2021.\n[33] Philipp Werner, Daniel Lopez-Martinez, Steffen Walter, Ayoub Al-Hamadi, Sascha\nGruss, and Rosalind Picard. Automatic recognition methods supporting pain assess-\nment: A survey. IEEE Transactions on Affective Computing , 2019.\n[34] Gioacchino D. De Sario, Clifton R. Haider, Karla C. Maita, Ricardo A. Torres-\nGuzman, Omar S. Emam, Francisco R. Avila, John P. Garcia, Sahar Borna, Christo-\npher J. McLeod, Charles J. Bruce, Rickey E. Carter, and Antonio J. Forte. Using ai to\ndetect pain through facial expressions: A review. Bioengineering , 10(5), 2023.\n[35] Stefanos Gkikas., Chariklia Chatzaki., Elisavet Pavlidou., Foteini Verigou., Kyriakos\nKalkanis., and Manolis Tsiknakis. Automatic pain intensity estimation based on\nelectrocardiogram and demographic factors. In Proceedings of the 8th International\nConference on Information and Communication Technologies for Ageing Well and\ne-Health - ICT4AWE, , pages 155\u2013162. INSTICC, SciTePress, 2022.\n[36] Stefanos Gkikas, Chariklia Chatzaki, and Manolis Tsiknakis. Multi-task neural net-\nworks for pain intensity estimation using electrocardiogram and demographic factors.\nInInformation and Communication Technologies for Ageing Well and e-Health , pages\n324\u2013337. Springer Nature Switzerland, 2023.\n[37] Stefanos Gkikas and Manolis Tsiknakis. A full transformer-based framework for au-\ntomatic pain estimation using videos. In 2023 45th Annual International Conference\nof the IEEE Engineering in Medicine & Biology Society (EMBC) , pages 1\u20136, 2023.\n[38] Stefanos Gkikas, Nikolaos S. Tachos, Stelios Andreadis, Vasileios C. Pezoulas, Dim-\nitrios Zaridis, George Gkois, Anastasia Matonaki, Thanos G. Stavropoulos, and Dim-\nitrios I. Fotiadis. Multimodal automatic assessment of acute pain through facial videos\nand heart rate signals utilizing transformer-based architectures. Frontiers in Pain Re-\nsearch , 5, 2024.\n[39] Stefanos Gkikas and Manolis Tsiknakis. Synthetic thermal and rgb videos for auto-\nmatic pain assessment utilizing a vision-mlp architecture. In 2024 12th International\nConference on Affective Computing and Intelligent Interaction Workshops and Demos\n(ACIIW) , pages 4\u201312, 2024.\n[40] Stefanos Gkikas and Manolis Tsiknakis. Twins-painvit: Towards a modality-agnostic\nvision transformer framework for multimodal automatic pain assessment using facialBIBLIOGRAPHY 167\nvideos and fnirs. In 2024 12th International Conference on Affective Computing and\nIntelligent Interaction Workshops and Demos (ACIIW) , pages 13\u201321, 2024.\n[41] Stefanos Gkikas, Raul Fernandez Rojas, and Manolis Tsiknakis. Painformer: a\nvision foundation model for automatic pain assessment, 2025. arXiv preprint\narXiv:2505.01571.\n[42] Srinivasa N Raja, Daniel B Carr, Milton Cohen, Nanna B Finnerup, Herta Flor,\nStephen Gibson, Francis J Keefe, Jeffrey S Mogil, Matthias Ringkamp, Kathleen A\nSluka, Xue-Jun Song, Bonnie Stevens, Mark D Sullivan, Perri R Tutelman, Takahiro\nUshida, and Kyle Vader. The revised international association for the study of pain\ndefinition of pain: concepts, challenges, and compromises. Pain, 161(9):1976\u20131982,\nSeptember 2020.\n[43] Khalid S and Tubbs RS. Neuroanatomy and Neuropsychology of Pain. Cureus , 9(10),\n2017.\n[44] Eric L. Garland. Pain Processing in the Human Nervous System: A Selective Review\nof Nociceptive and Biobehavioral Pathways. Primary Care: Clinics in Office Practice ,\n39(3):561\u2013571, sep 2012.\n[45] Tatiana F. Almeida, Suely Roizenblatt, and Sergio Tufik. Afferent pain pathways:\na neuroanatomical review. Brain Research , 1000(1):40\u201356, 2004. Brain Research\nV olume 1000.\n[46] A. Vania Apkarian, M. Catherine Bushnell, Rolf-Detlef Treede, and Jon-Kar Zubieta.\nHuman brain mechanisms of pain perception and regulation in health and disease.\nEuropean Journal of Pain , 9(4):463\u2013463, 2005.\n[47] Mary Beth Babos, Brittany Grady, Warren Wisnoff, and Christy McGhee. Pathophys-\niology of pain. Disease-a-Month , 59(10):330\u2013358, 2013. Pathophysiology of pain.\n[48] Clifford J. Woolf. What is this thing called pain? The Journal of Clinical Investigation ,\n120(11):3742\u20133744, 11 2010.\n[49] Allan I Basbaum, Diana M Bautista, Gr \u00b4egory Scherrer, and David Julius. Cellular and\nmolecular mechanisms of pain. Cell, 139(2):267\u2013284, 2009.\n[50] Rolf-Detlef Treede, Winfried Rief, Antonia Barke, Qasim Aziz, Michael I Bennett,\nRafael Benoliel, Milton Cohen, Stefan Evers, Nanna B Finnerup, Michael B First,\net al. A classification of chronic pain for icd-11. Pain, 156(6):1003\u20131007, 2015.168 BIBLIOGRAPHY\n[51] Joy Onyekachukwu Egede. Automatic pain assessment from face video (continuous\npain intensity estimation in adults and newborns) . PhD thesis, University of Notting-\nham, 2019.\n[52] Raymond Sinatra. Causes and consequences of inadequate management of acute pain.\nPain Medicine , 11(12):1859\u20131871, 12 2010.\n[53] Lies De Ruddere and Raymond Tait. Facing Others in Pain: Why Context Matters ,\npages 241\u2013269. Springer International Publishing, Cham, 2018.\n[54] MVM De Oliveira, JAL De Jesus, and RM Tristao. Psychophysical parameters of\na multidimensional pain scale in newborns. Physiological measurement , 33(1):39,\n2011.\n[55] K Feldh. The checklist of nonverbal pain indicators. Pain Management Nursing, I ,\n1:13\u201321, 2000.\n[56] JC Evans, DG V ogelpohl, CM Bourguignon, and CS Morcott. Pain behaviors in lbw\ninfants accompany some\u201d nonpainful\u201d caregiving procedures. Neonatal network: NN ,\n16(3):33\u201340, 1997.\n[57] Huda Huijer Abu-Saad, Gerrie JJW Bours, Bonnie Stevens, and Jan PH Hamers. As-\nsessment of pain in the neonate. In Seminars in perinatology , volume 22, pages 402\u2013\n416. Elsevier, 1998.\n[58] Richard Stephens, John Atkins, and Andrew Kingston. Swearing as a response to\npain. Neuroreport , 20(12):1056\u20131060, 2009.\n[59] Richard Stephens and Claudia Umland. Swearing as a response to pain\u2013effect of daily\nswearing frequency. The Journal of Pain , 12(12):1274\u20131281, 2011.\n[60] Jacob Greisen, Claus B Juhl, Thorbj\u00f8rn Gr\u00f8fte, Hendrik Vilstrup, Troels S Jensen,\nand Ole Schmitz. Acute pain induces insulin resistance in humans. The Journal of the\nAmerican Society of Anesthesiologists , 95(3):578\u2013584, 2001.\n[61] Bonnie J Stevens and C Celeste Johnston. Physiological responses of premature in-\nfants to a painful stimulus. Nursing research , 43(4):226\u2013231, 1994.\n[62] Hunter G Hoffman, Todd L Richards, Barbara Coda, Aric R Bills, David Blough,\nAnne L Richards, and Sam R Sharar. Modulation of thermal pain-related brain activity\nwith virtual reality: evidence from fmri. Neuroreport , 15(8):1245\u20131248, 2004.\n[63] PJ Mathew and Joseph L Mathew. Assessment and management of pain in infants.\nPostgraduate medical journal , 79(934):438\u2013443, 2003.BIBLIOGRAPHY 169\n[64] R Melzack and P D Wall. Pain mechanisms: a new theory. Science (New York, N.Y.) ,\n150(3699):971\u2013979, nov 1965.\n[65] Thomas Hadjistavropoulos, Kenneth D Craig, Steve Duck, Annmarie Cano, Liesbet\nGoubert, Philip L Jackson, Jeffrey S Mogil, Pierre Rainville, Michael J L Sullivan,\nAmanda C de C Williams, Tine Vervoort, and Theresa Dever Fitzgerald. A biopsy-\nchosocial formulation of pain communication. Psychological bulletin , 137(6):910\u2013\n939, nov 2011.\n[66] Katelynn E Boerner, Kathryn A Birnie, Line Caes, Meghan Schinkel, and Christine T\nChambers. Sex differences in experimental pain among healthy children: a systematic\nreview and meta-analysis. Pain, 155(5):983\u2013993, may 2014.\n[67] Edmund Keogh. Men, masculinity, and pain. Pain, 156(12):2408\u20132412, dec 2015.\n[68] Fredric M Levine and Laura Lee De Simone. The effects of experimenter gender on\npain report in male and female subjects. Pain, 44(1):69\u201372, jan 1991.\n[69] Laura E McClelland and James A McCubbin. Social influence and pain response in\nwomen and men. Journal of behavioral medicine , 31(5):413\u2013420, oct 2008.\n[70] Jacob M Vigil and Joe Alcock. Tough guys or sensitive guys? Disentangling the role\nof examiner sex on patient pain reports. Pain Research & Management : The Journal\nof the Canadian Pain Society , 19(1):e9, 2014.\n[71] K A Raftery, R Smith-Coggins, and A H Chen. Gender-associated differences in\nemergency department pain management. Annals of emergency medicine , 26(4):414\u2013\n421, oct 1995.\n[72] E M Hooper, L M Comstock, J M Goodwin, and J S Goodwin. Patient characteristics\nthat influence physician behavior. Medical care , 20(6):630\u2013638, jun 1982.\n[73] Edmund Keogh. Sex and Gender as Social-Contextual Factors in Pain , pages 433\u2013\n453. Springer International Publishing, 2018.\n[74] Thomas Hadjistavropoulos and Natasha L Gallant. Pain in Older Adults: Caregiver\nChallenges , pages 415\u2013429. Springer International Publishing, 2018.\n[75] Christine J McPherson, Thomas Hadjistavropoulos, Alana Devereaux, and\nMichelle M Lobchuk. A qualitative investigation of the roles and perspectives of\nolder patients with advanced cancer and their family caregivers in managing pain in\nthe home. BMC palliative care , 13:39, 2014.170 BIBLIOGRAPHY\n[76] T Hadjistavropoulos, D L LaChapelle, F K MacLeod, B Snider, and K D Craig. Mea-\nsuring movement-exacerbated pain in cognitively impaired frail elders. The Clinical\njournal of pain , 16(1):54\u201363, mar 2000.\n[77] Clive Ballard, Maria Luisa Hanney, Megan Theodoulou, Simon Douglas, Rupert Mc-\nShane, Katja Kossakowski, Randeep Gill, Edmund Juszczak, Ly-Mee Yu, and Robin\nJacoby. The dementia antipsychotic withdrawal trial (DART-AD): long-term follow-\nup of a randomised placebo-controlled trial. The Lancet Neurology , 8(2):151\u2013157,\nfeb 2009.\n[78] Ryuta Ochi and Akira Midorikawa. Decline in Emotional Face Recognition Among\nElderly People May Reflect Mild Cognitive Impairment. Frontiers in Psychology , 12,\n2021.\n[79] R Pillai Riddell and Nicole Racine. Assessing pain in infancy: The caregiver context.\nPain Research & Management : The Journal of the Canadian Pain Society , 14(1):27,\n2009.\n[80] Tine Vervoort, Line Caes, Zina Trost, Michael Sullivan, Karoline Vangronsveld, and\nLiesbet Goubert. Social modulation of facial pain display in high-catastrophizing chil-\ndren: an observational study in schoolchildren and their parents. Pain, 152(7):1591\u2013\n1599, jul 2011.\n[81] Francis J. Keefe, Robert H. Wilkins, Wesley A. Cook, James E. Crisson, and\nLawrence H. Muhlbaier. Depression, Pain, and Pain Behavior. Journal of Consulting\nand Clinical Psychology , 54(5):665\u2013669, oct 1986.\n[82] John W. Burns, Phillip Quartana, Wesley Gilliam, Erika Gray, Justin Matsuura, Carla\nNappi, Brandy Wolfe, and Kenneth Lofland. Effects of Anger Suppression on Pain\nSeverity and Pain Behaviors Among Chronic Pain Patients: Evaluation of an Ironic\nProcess Model. Health Psychology , 27(5):645\u2013652, sep 2008.\n[83] Lies De Ruddere, Liesbet Goubert, Micha \u00a8el Andr \u00b4e Louis Stevens, Myriam Deveugele,\nKenneth Denton Craig, and Geert Crombez. Health Care Professionals\u2019 Reactions\nto Patient Pain: Impact of Knowledge About Medical Evidence and Psychosocial\nInfluences. The Journal of Pain , 15(3):262\u2013270, mar 2014.\n[84] Lisa J Staton, Mukta Panda, Ian Chen, Inginia Genao, James Kurz, Mark Pasanen,\nAlex J Mechaber, Madhusudan Menon, Jane O\u2019Rorke, JoAnn Wood, Eric Rosenberg,\nCharles Faeslis, Tim Carey, Diane Calleson, and Sam Cykert. When race matters:\ndisagreement in pain perception between patients and their physicians in primary care.\nJournal of the National Medical Association , 99(5):532\u2013538, may 2007.BIBLIOGRAPHY 171\n[85] Samantha M. Meints, Madison Stout, Samuel Abplanalp, and Adam T. Hirsh. Pain-\nRelated Rumination, But Not Magnification or Helplessness, Mediates Race and Sex\nDifferences in Experimental Pain. The Journal of Pain , 18(3):332\u2013339, mar 2017.\n[86] Brian Blake Drwecki. Race and Pain: A Dual Injustice , pages 455\u2013480. Springer\nInternational Publishing, 2018.\n[87] C S Cleeland, R Gonin, A K Hatfield, J H Edmonson, R H Blum, J A Stewart, and\nK J Pandya. Pain and its treatment in outpatients with metastatic cancer. The New\nEngland journal of medicine , 330(9):592\u2013596, mar 1994.\n[88] Lies De Ruddere and Raymond Tait. Facing Others in Pain: Why Context Matters ,\npages 241\u2013269. Springer International Publishing, Cham, 2018.\n[89] Raymond C. Tait, John T. Chibnall, Laura Miller, and Chas A. Werner. Judging pain\nand disability: effects of pain severity and physician specialty. Journal of Behavioral\nMedicine 2010 34:3 , 34(3):218\u2013224, nov 2010.\n[90] Relieving pain in America: A blueprint for transforming prevention, care, education,\nand research . National Academies Press, oct 2011.\n[91] Line Caes, Liesbet Goubert, and Laura Simons. An ecological and lifespan approach\nof social influences on childhood pain experiences. Social and Interpersonal Dynam-\nics in Pain , pages 395\u2013413, may 2018.\n[92] Geert Crombez, Patricia Bijttebier, Chris Eccleston, Tamara Mascagni, Gustaaf\nMertens, Liesbet Goubert, and Katrien Verstraeten. The child version of the pain\ncatastrophizing scale (PCS-C): a preliminary validation. Pain, 104(3):639\u2013646, aug\n2003.\n[93] Paula A Forgeron, Patrick McGrath, Bonnie Stevens, Joan Evans, Bruce Dick,\nAllen G Finley, and Torie Carlson. Social information processing in adolescents with\nchronic pain: my friends don\u2019t really understand me. Pain, 152(12):2773\u20132780, dec\n2011.\n[94] Liesbet Goubert and Laura E. Simons. Cognitive styles and processes in paediatric\npain. In Oxford Textbook of Paediatric Pain , pages 95\u2013101. Oxford University Press,\nnov 2013.\n[95] Maria Fitzgerald and Suellen M Walker. Infant pain management: a developmental\nneurobiological approach. Nature clinical practice. Neurology , 5(1):35\u201350, jan 2009.172 BIBLIOGRAPHY\n[96] David Kang, Negin Hesam-Shariati, James H. McAuley, Monzurul Alam, Zina Trost,\nCaroline D. Rae, and Sylvia M. Gustin. Disruption to normal excitatory and inhibitory\nfunction within the medial prefrontal cortex in people with chronic pain. European\nJournal of Pain , jul 2021.\n[97] Domenica A Delgado, Bradley S Lambert, Nickolas Boutris, Patrick C McCulloch,\nAndrew B Robbins, Michael R Moreno, and Joshua D Harris. Validation of Digi-\ntal Visual Analog Scale Pain Scoring With a Traditional Paper-based Visual Analog\nScale in Adults. Journal of the American Academy of Orthopaedic Surgeons. Global\nresearch & reviews , 2(3):e088, mar 2018.\n[98] Mathias Haefeli and Achim Elfering. Pain assessment. European spine journal :\nofficial publication of the European Spine Society, the European Spinal Deformity\nSociety, and the European Section of the Cervical Spine Research Society , 15 Suppl\n1(Suppl 1):S17\u201324, jan 2006.\n[99] Kenneth M. Prkachin and Patricia E. Solomon. The structure, reliability and validity\nof pain expression: Evidence from patients with shoulder pain. Pain, 139(2):267\u2013274,\noct 2008.\n[100] J Lawrence, D Alcock, P McGrath, J Kay, S B MacMurray, and C Dulberg. The\ndevelopment of a tool to assess neonatal pain. Neonatal network : NN , 12(6):59\u201366,\nsep 1993.\n[101] David E Weissman and David J Haddox. Opioid pseudoaddiction\u2013an iatrogenic syn-\ndrome. Pain, 36(3):363\u2013366, mar 1989.\n[102] Kenneth M. Prkachin. Assessing pain by facial expression: Facial expression as nexus.\nPain Research and Management , 14(1):53\u201358, 2009.\n[103] Ghada Zamzmi, Rangachar Kasturi, Dmitry Goldgof, Ruicong Zhi, Terri Ashmeade,\nand Yu Sun. A Review of Automated Pain Assessment in Infants: Features, Classi-\nfication Tasks, and Databases. IEEE Reviews in Biomedical Engineering , 11:77\u201396,\nnov 2018.\n[104] Zhanli Chen, Rashid Ansari, and Diana Wilkie. Automated Pain Detection from Fa-\ncial Expressions using FACS: A Review. arXiv , 2018.\n[105] Teena Hassan, Dominik Seus, Johannes Wollenberg, Katharina Weitz, Miriam Kunz,\nStefan Lautenbacher, Jens-Uwe Garbas, and Ute Schmid. Automatic Detection of\nPain from Facial Expressions: A Survey. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , pages 1\u20131, apr 2019.BIBLIOGRAPHY 173\n[106] Philipp Werner, Daniel Lopez-Martinez, Steffen Walter, Ayoub Al-Hamadi, Sascha\nGruss, and Rosalind Picard. Automatic Recognition Methods Supporting Pain As-\nsessment: A Survey. IEEE Transactions on Affective Computing , 2019.\n[107] Rasha M. Al-Eidan, Hend Al-Khalifa, and AbdulMalik Al-Salman. Deep-Learning-\nBased Models for Pain Recognition: A Systematic Review. Applied Sciences ,\n10(17):5984, aug 2020.\n[108] Patrick Lucey, Jeffrey F. Cohn, Kenneth M. Prkachin, Patricia E. Solomon, and Iain\nMatthews. Painful data: The UNBC-McMaster shoulder pain expression archive\ndatabase. In 2011 IEEE International Conference on Automatic Face and Gesture\nRecognition and Workshops, FG 2011 , pages 57\u201364, 2011.\n[109] Steffen Walter, Sascha Gruss, Hagen Ehleiter, Junwen Tan, Harald C. Traue, Stephen\nCrawcour, Philipp Werner, Ayoub Al-Hamadi, Adriano O. Andrade, and Gustavo Mor-\neira Da Silva. The biovid heat pain database: Data for the advancement and systematic\nvalidation of an automated pain recognition. In 2013 IEEE International Conference\non Cybernetics , pages 128\u2013131, 2013.\n[110] Mohammad A. Haque, Ruben B. Bautista, Fatemeh Noroozi, Kaustubh Kulkarni,\nChristian B. Laursen, Ramin Irani, Marco Bellantonio, Sergio Escalera, Golamreza\nAnbarjafari, Kamal Nasrollahi, Ole K. Andersen, Erika G. Spaich, and Thomas B.\nMoeslund. Deep multimodal pain recognition: A database and comparison of spatio-\ntemporal visual modalities. In 13th IEEE International Conference on Automatic\nFace and Gesture Recognition, FG 2018 , pages 250\u2013257. Institute of Electrical and\nElectronics Engineers Inc., jun 2018.\n[111] Sheryl Brahnam, Chao-Fa Chuang, Frank Y Shih, and Melinda R Slack. SVM Clas-\nsification of Neonatal Facial Images of Pain. In Isabelle Bloch, Alfredo Petrosino,\nand Andrea G B Tettamanzi, editors, Fuzzy Logic and Applications , pages 121\u2013128,\nBerlin, Heidelberg, 2006. Springer Berlin Heidelberg.\n[112] Sheryl Brahnam, Loris Nanni, Shannon McMurtrey, Alessandra Lumini, Rick Brat-\ntin, Melinda Slack, and Tonya Barrier. Neonatal pain detection in videos using the\niCOPEvid dataset and an ensemble of descriptors extracted from Gaussian of Local\nDescriptors. Applied Computing and Informatics , 2019.\n[113] Ghada Zamzmi, Pai Chih-Yun, Dmitry Goldgof, R. Kasturi, Terri Ashmeade, and\nYu Sun. A Comprehensive and Context-Sensitive Neonatal Pain Assessment Using\nComputer Vision. IEEE Transactions on Affective Computing , 2019.174 BIBLIOGRAPHY\n[114] Joy Egede, Michel Valstar, Mercedes Torres Torres, and Don Sharkey. Automatic\nNeonatal Pain Estimation: An Acute Pain in Neonates Database. 2019 8th Inter-\nnational Conference on Affective Computing and Intelligent Interaction, ACII 2019 ,\npages 475\u2013481, 2019.\n[115] Min S H Aung, Sebastian Kaltwang, Bernardino Romera-Paredes, Brais Martinez,\nAneesha Singh, Matteo Cella, Michel Valstar, Hongying Meng, Andrew Kemp,\nMoshen Shafizadeh, Aaron C Elkins, Natalie Kanakam, Amschel de Rothschild, Nick\nTyler, Paul J Watson, Amanda C de C Williams, Maja Pantic, and Nadia Bianchi-\nBerthouze. The Automatic Detection of Chronic Pain-Related Expression: Require-\nments, Challenges and the Multimodal EmoPain Dataset. IEEE transactions on affec-\ntive computing , 7(4):435\u2013451, 2016.\n[116] Maria Velana, Sascha Gruss, Georg Layher, Patrick Thiam, Yan Zhang, Daniel\nSchork, Viktor Kessler, Sascha Meudt, Heiko Neumann, Jonghwa Kim, Friedhelm\nSchwenker, Elisabeth Andr \u00b4e, Harald C. Traue, and Steffen Walter. The senseemo-\ntion database: A multimodal database for the development and systematic validation\nof an automatic pain and emotion-recognition system. In Friedhelm Schwenker and\nStefan Scherer, editors, Multimodal Pattern Recognition of Social Signals in Human-\nComputer-Interaction , pages 127\u2013139, Cham, 2017. Springer International Publish-\ning.\n[117] Sascha Gruss, Mattis Geiger, Philipp Werner, Oliver Wilhelm, Harald C Traue, Ayoub\nAl-Hamadi, and Steffen Walter. Multi-Modal Signals for Analyzing Pain Responses\nto Thermal and Electrical Stimuli. Journal of visualized experiments : JoVE , (146),\napr 2019.\n[118] Raul Fernandez Rojas, Niraj Hirachan, Calvin Joseph, Ben Seymour, and Roland\nGoecke. The ai4pain grand challenge 2024: Advancing pain assessment with mul-\ntimodal fnirs and facial video analysis. In 2024 12th International Conference on\nAffective Computing and Intelligent Interaction . IEEE, 2024.\n[119] Henrik Pedersen. Learning appearance features for pain detection using the UNBC-\nMcMaster shoulder pain expression archive database. Lecture Notes in Computer\nScience (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes\nin Bioinformatics) , 9163:128\u2013136, 2015.\n[120] Joy O. Egede, Siyang Song, Temitayo A. Olugbade, Chongyang Wang, Amanda C.\nDe C. Williams, Hongying Meng, Min Aung, Nicholas D. Lane, Michel Valstar, and\nNadia Bianchi-Berthouze. Emopain challenge 2020: Multimodal pain evaluationBIBLIOGRAPHY 175\nfrom facial and bodily expressions. In 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition (FG 2020) , pages 849\u2013856, 2020.\n[121] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-\nscale image recognition. In 3rd International Conference on Learning Representa-\ntions, ICLR 2015 - Conference Track Proceedings . International Conference on Learn-\ning Representations, ICLR, sep 2015.\n[122] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE Computer Society Conference\non Computer Vision and Pattern Recognition , volume 2016-Decem, pages 770\u2013778.\nIEEE Computer Society, dec 2016.\n[123] Ruijing Yang, Xiaopeng Hong, Jinye Peng, Xiaoyi Feng, and Guoying Zhao. Incor-\nporating high-level and low-level cues for pain intensity estimation. In Proceedings -\nInternational Conference on Pattern Recognition , volume 2018-Augus, pages 3495\u2013\n3500. Institute of Electrical and Electronics Engineers Inc., 2018.\n[124] Ashish Semwal and Narendra D Londhe. Computer aided pain detection and inten-\nsity estimation using compact CNN based fusion network. Applied Soft Computing ,\n112:107780, 2021.\n[125] Saandeep Aathreya Sidhapur Lakshminarayan, Saurabh Hinduja, and Shaun Cana-\nvan. Three-level Training of Multi-Head Architecture for Pain Detection. In Gomez-\nFernandez F Struc V ., editor, Proceedings - 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition, FG 2020 , pages 839\u2013843. Institute of\nElectrical and Electronics Engineers Inc., 2020.\n[126] Van Thong Huynh, Hyung Jeong Yang, Guee Sang Lee, and Soo Hyung Kim. Mul-\ntimodality Pain and related Behaviors Recognition based on Attention Learning. In\nGomez-Fernandez F Struc V ., editor, Proceedings - 2020 15th IEEE International\nConference on Automatic Face and Gesture Recognition, FG 2020 , pages 814\u2013818.\nInstitute of Electrical and Electronics Engineers Inc., 2020.\n[127] Ashish Semwal and Narendra D. Londhe. Automated Pain Severity Detection Using\nConvolutional Neural Network. In Rajpurohit V S Nadkatti M N Niranjan S.K. Desai\nV ., editor, Proceedings of the International Conference on Computational Techniques,\nElectronics and Mechanical Systems, CTEMS 2018 , pages 66\u201370. Institute of Electri-\ncal and Electronics Engineers Inc., 2018.\n[128] Mohammad Tavakolian and Abdenour Hadid. Deep binary representation of facial\nexpressions: A novel framework for automatic pain intensity recognition. In Proceed-176 BIBLIOGRAPHY\nings - International Conference on Image Processing, ICIP , pages 1952\u20131956. IEEE\nComputer Society, 2018.\n[129] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning Face Representation from\nScratch, 2014.\n[130] Ashish Semwal and Narendra D. Londhe. ECCNET: An ensemble of compact convo-\nlution neural network for pain severity assessment from face images. In Proceedings\nof the Confluence 2021: 11th International Conference on Cloud Computing, Data\nScience and Engineering , pages 761\u2013766, 2021.\n[131] Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon,\nChou P. Hung, and Brent J. Lance. EEGNet: A compact convolutional neural network\nfor EEG-based brain-computer interfaces. Journal of Neural Engineering , 15(5), nov\n2018.\n[132] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi.\nInception-v4, inception-ResNet and the impact of residual connections on learning.\nIn31st AAAI Conference on Artificial Intelligence, AAAI 2017 , pages 4278\u20134284.\nAAAI press, feb 2017.\n[133] Reza Kharghanian, Ali Peiravi, and Farshad Moradi. Pain detection from facial im-\nages using unsupervised feature learning approach. In Proceedings of the Annual\nInternational Conference of the IEEE Engineering in Medicine and Biology Society,\nEMBS , volume 2016-Octob, pages 419\u2013422. Institute of Electrical and Electronics\nEngineers Inc., 2016.\n[134] Reza Kharghanian, Ali Peiravi, Farshad Moradi, and Alexandros Iosifidis. Pain detec-\ntion using batch normalized discriminant restricted Boltzmann machine layers. Jour-\nnal of Visual Communication and Image Representation , 76, 2021.\n[135] Dong Huang, Zhaoqiang Xia, Lei Li, Kunwei Wang, and Xiaoyi Feng. Pain-\nawareness multistream convolutional neural network for pain estimation. Journal\nof Electronic Imaging , 28(04):1, 2019.\n[136] Xuwu Xin, Xiaoyan Lin, Shengfu Yang, and Xin Zheng. Pain intensity estimation\nbased on a spatial transformation and attention CNN. PLoS ONE , 15(8 August\n2020):1\u201315, aug 2020.\n[137] S Cui, D Huang, Y Ni, and X Feng. Multi-scale regional attention networks for pain\nestimation. In 2021 13th International Conference on Bioinformatics and Biomedical\nTechnology , pages 1\u20138. Association for Computing Machinery, 2021.BIBLIOGRAPHY 177\n[138] Conghui Li, Zhaocheng Zhu, and Yuming Zhao. Saliency Supervision: An Intuitive\nand Effective Approach for Pain Intensity Regression. Lecture Notes in Computer\nScience (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes\nin Bioinformatics) , 11307 LNCS:455\u2013464, 2018.\n[139] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified em-\nbedding for face recognition and clustering. In 2015 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 815\u2013823. IEEE Computer Society, mar\n2015.\n[140] Xianlin Peng, Dong Huang, and Haixi Zhang. Pain intensity recognition via multi-\nscale deep network. IET Image Processing , 14(8):1645\u20131652, 2020.\n[141] X Xin, X Li, S Yang, X Lin, and X Zheng. Pain expression assessment based on a\nlocality and identity aware network. IET Image Processing , 15(12):2948\u20132958, 2021.\n[142] Ashish Semwal and Narendra D. Londhe. S-PANET: A Shallow Convolutional Neural\nNetwork for Pain Severity Assessment in Uncontrolled Environment. In 2021 IEEE\n11th Annual Computing and Communication Workshop and Conference, CCWC 2021 ,\npages 800\u2013806. Institute of Electrical and Electronics Engineers Inc., jan 2021.\n[143] Ashish Semwal and Narendra D. Londhe. MVFNet: A multi-view fusion network for\npain intensity assessment in unconstrained environment. Biomedical Signal Process-\ning and Control , 67, may 2021.\n[144] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going\ndeeper with convolutions. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , volume 07-12-June, pages 1\u20139. IEEE Computer Society, oct\n2015.\n[145] Jiann Shu Lee and Chuan Wei Wang. Facial pain intensity estimation for ICU patient\nwith partial occlusion coming from treatment. In 3rd International Conference on Bi-\nological Information and Biomedical Engineering, BIBE 2019 , pages 106\u2013109. VDE\nVerlag GmbH, 2019.\n[146] Reneiro Andal Virrey, Wahyu Caesarendra, Muhammad Iskandar Bin Pg Hj Petra,\nEmeroylariffion Abas, Asmah Husaini, and Chandratilak De Silva Liyanage. Mile-\nstone of Pain Intensity Evaluation from Facial Action Units. In ICECOS 2019 - 3rd\nInternational Conference on Electrical Engineering and Computer Science, Proceed-\ning, pages 54\u201357. Institute of Electrical and Electronics Engineers Inc., 2019.178 BIBLIOGRAPHY\n[147] Hermawan Nugroho, Dani Harmanto, and Hamada Rasheed Hassan Al-Absi. On the\nDevelopment of Smart Home Care: Application of Deep Learning for Pain Detection.\nIn2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) ,\npages 612\u2013616, 2019.\n[148] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Unified Em-\nbedding for Face Recognition and Clustering. Proceedings of the IEEE Computer So-\nciety Conference on Computer Vision and Pattern Recognition , 07-12-June:815\u2013823,\nmar 2015.\n[149] Laduona Dai, Joost Broekens, and Khiet P. Truong. Real-time pain detection in facial\nexpressions for health robotics. In 2019 8th International Conference on Affective\nComputing and Intelligent Interaction Workshops and Demos, ACIIW 2019 , pages\n277\u2013283. Institute of Electrical and Electronics Engineers Inc., 2019.\n[150] Guglielmo Menchetti, Zhanli Chen, DIana J. Wilkie, Rashid Ansari, Yasemin\nYardimci, and A. Enis Cetin. Pain detection from facial videos using two-stage deep\nlearning. In GlobalSIP 2019 - 7th IEEE Global Conference on Signal and Information\nProcessing, Proceedings . Institute of Electrical and Electronics Engineers Inc., 2019.\n[151] Chittaranjan Andrade. Internal, external, and ecological validity in research design,\nconduct, and evaluation. Indian Journal of Psychological Medicine , 40(5):498\u2013499,\n2018. PMID: 30275631.\n[152] Dianbo Liu, Peng Fengjiao, Ognjen (Oggi) Rudovic, and Rosalind Picard. Deep-\nfacelift: Interpretable personalized models for automatic estimation of self-reported\npain. In Neil Lawrence and Mark Reid, editors, Proceedings of IJCAI 2017 Work-\nshop on Artificial Intelligence in Affective Computing , volume 66 of Proceedings of\nMachine Learning Research , pages 1\u201316. PMLR, 20 Aug 2017.\n[153] Xiaojing Xu, Jeannie S Huang, and Virginia R De Sa. Pain Evaluation in Video using\nExtended Multitask Learning from Multidimensional Measurements. In Proceedings\nof the Machine Learning for Health NeurIPS Workshop , volume 116, pages 141\u2013154.\nPMLR, apr 2020.\n[154] Paola Casti, Arianna Mencattini, Maria Colomba Comes, Giuseppina Callari, Davide\nDi Giuseppe, Silvia Natoli, Mauro Dauri, Elena Daprati, and Eugenio Martinelli. Cal-\nibration of Vision-Based Measurement of Pain Intensity with Multiple Expert Ob-\nservers. IEEE Transactions on Instrumentation and Measurement , 68(7):2442\u20132450,\n2019.BIBLIOGRAPHY 179\n[155] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification\nwith deep convolutional neural networks. In Advances in Neural Information Process-\ning Systems 25 (NIPS 2012) , 2012.\n[156] Luigi Celona and Luca Manoni. Neonatal Facial Pain Assessment Combining Hand-\nCrafted and Deep Features. Lecture Notes in Computer Science , 10590 LNCS:197\u2013\n204, 2017.\n[157] Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep Face Recognition.\nInProceedings of the British Machine Vision Conference (BMVC) , pages 41.1\u201341.12.\nBritish Machine Vision Association and Society for Pattern Recognition, dec 2015.\n[158] Gil Levi and Tal Hassner. Emotion recognition in the wild via convolutional neural\nnetworks and mapped binary patterns. In ICMI 2015 - Proceedings of the 2015 ACM\nInternational Conference on Multimodal Interaction , pages 503\u2013510, New York, NY ,\nUSA, 2015. ACM.\n[159] Guanming Lu, Qiang Hao, Kaiting Kong, Jingjie Yan, Haibo Li, and Xiaonan Li.\nDeep convolutional neural networks with transfer learning for neonatal pain expres-\nsion recognition. In Xiao G Ning X Li K Li M Xiao Z. Wang L., editor, 2018 14th\nInternational Conference on Natural Computation, Fuzzy Systems and Knowledge\nDiscovery (ICNC-FSKD) , pages 251\u2013256. Institute of Electrical and Electronics En-\ngineers Inc., 2018.\n[160] Ghada Zamzmi, Rahul Paul, Md. Sirajus Salekin, Dmitry Goldgof, Rangachar Kas-\nturi, Thao Ho, and Yu Sun. Convolutional Neural Networks for Neonatal Pain Assess-\nment. IEEE Transactions on Biometrics, Behavior, and Identity Science , 1(3):192\u2013\n200, 2019.\n[161] Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, and Yu Sun. Neonatal Pain\nExpression Recognition Using Transfer Learning. arXiv , jul 2018.\n[162] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of\nthe devil in the details: Delving deep into convolutional nets, 2014.\n[163] Luigi Celona, Sheryl Brahnam, and Simone Bianco. Getting the most of few data\nfor neonatal pain assessment. In ACM International Conference Proceeding Series ,\npages 298\u2013301. Association for Computing Machinery, 2019.\n[164] Martin Arjovsky, Soumith Chintala, and L \u00b4eon Bottou. Wasserstein GAN. arXiv , jan\n2017.180 BIBLIOGRAPHY\n[165] Ognjen Rudovic, Nicolas Tobis, Sebastian Kaltwang, Bj \u00a8orn Schuller, Daniel Rueckert,\nJeffrey F. Cohn, and Rosalind W. Picard. Personalized Federated Deep Learning for\nPain Estimation From Face Images. arXiv , jan 2021.\n[166] Kornprom Pikulkaew, Ekkarat Boonchieng, Waraporn Boonchieng, and Varin Chou-\nvatut. Pain detection using deep learning with evaluation system. In Advances in In-\ntelligent Systems and Computing , volume 1184, pages 426\u2013435. Springer Singapore,\n2021.\n[167] S El Morabit, A Rivenq, M.-E.-N. Zighem, A Hadid, A Ouahabi, and A Taleb-Ahmed.\nAutomatic pain estimation from facial expressions: A comparative analysis using off-\nthe-shelf cnn architectures. Electronics , 10(16), 2021.\n[168] C Li, A Pourtaherian, L Van Onzenoort, W.E.T.A. Ten, and P H N De With. Infant\nFacial Expression Analysis: Towards a Real-Time Video Monitoring System Using\nR-CNN and HMM. IEEE Journal of Biomedical and Health Informatics , 25(5):1429\u2013\n1440, 2021.\n[169] Feng Wang, Xiang Xiang, Chang Liu, Trac D. Tran, Austin Reiter, Gregory D. Hager,\nHarry Quon, Jian Cheng, and Alan L. Yuille. Regularizing face verification nets for\npain intensity regression. In Proceedings - International Conference on Image Pro-\ncessing, ICIP , volume 2017-Septe, pages 1087\u20131091. IEEE Computer Society, 2018.\n[170] Marilena Ctlina Dragomir, Corneliu Florea, and Valentin Pupezescu. Automatic Sub-\nject Independent Pain Intensity Estimation using a Deep Learning Approach. In 2020\n8th E-Health and Bioengineering Conference, EHB 2020 , pages 1\u20134, 2020.\n[171] Ashish Semwal and Narendra D. Londhe. Automated facial expression based pain\nassessment using deep convolutional neural network. In Proceedings of the 3rd Inter-\nnational Conference on Intelligent Sustainable Systems, ICISS 2020 , pages 366\u2013370.\nInstitute of Electrical and Electronics Engineers Inc., 2020.\n[172] N Rathee, S Pahal, and P Sheoran. Pain detection from facial expressions using do-\nmain adaptation technique. Pattern Analysis and Applications , 2021.\n[173] Ghada Zamzmi, Rahul Paul, Dmitry Goldgof, Rangachar Kasturi, and Yu Sun. Pain\nAssessment from Facial Expression: Neonatal Convolutional Neural Network (N-\nCNN). In Proceedings of the International Joint Conference on Neural Networks ,\nvolume 2019-July. Institute of Electrical and Electronics Engineers Inc., 2019.\n[174] L P Carlini, L A Ferreira, G S Coutrin, V V Varoto, T M Heiderich, R X Balda,\nM M Barros, R Guinsburg, and C E Thomaz. A Convolutional Neural Network-\nbased Mobile Application to Bedside Neonatal Pain Assessment. In Conference onBIBLIOGRAPHY 181\nGraphics, Patterns and Images (SIBGRAPI) , pages 394\u2013401, Los Alamitos, CA, USA,\n2021. IEEE Computer Society.\n[175] Subhash Nerella, Julie Cupka, Matthew Ruppert, Patrick Tighe, Azra Bihorac, and\nParisa Rashidi. Pain action unit detection in critically ill patients. In 2021 IEEE\n45th Annual Computers, Software, and Applications Conference (COMPSAC) , pages\n645\u2013651, 2021.\n[176] Joy Egede, Michel Valstar, and Brais Martinez. Fusing deep learned and hand-crafted\nfeatures of appearance, shape, and dynamics for automatic pain estimation. In 12th\nIEEE International Conference on Automatic Face & Gesture Recognition , pages 689\u2013\n696, 2017.\n[177] Joy O. Egede and Michel Valstar. Cumulative attributes for pain intensity estimation.\nInICMI 2017 - Proceedings of the 19th ACM International Conference on Multi-\nmodal Interaction , volume 2017-Janua, pages 146\u2013153. Association for Computing\nMachinery, Inc, nov 2017.\n[178] Shashank Jaiswal, Joy Egede, and Michel Valstar. Deep learned cumulative attribute\nregression. In Proceedings - 13th IEEE International Conference on Automatic Face\nand Gesture Recognition, FG 2018 , pages 715\u2013722. Institute of Electrical and Elec-\ntronics Engineers Inc., 2018.\n[179] Mohammad Tavakolian, Carlos Guillermo Bermudez Cruces, and Abdenour Hadid.\nLearning to detect genuine versus posed pain from facial expressions using residual\ngenerative adversarial networks. In Proceedings - 14th IEEE International Confer-\nence on Automatic Face and Gesture Recognition, FG 2019 . Institute of Electrical\nand Electronics Engineers Inc., may 2019.\n[180] Mohammad Tavakolian, Miguel Bordallo Lopez, and Li Liu. Self-supervised pain in-\ntensity estimation from facial videos via statistical spatiotemporal distillation. Pattern\nRecognition Letters , 140:26\u201333, 2020.\n[181] Ehsan Othman, Philipp Werner, Frerk Saxen, Ayoub Al-Hamadi, Sascha Gruss, and\nSteffen Walter. Automatic vs. Human recognition of pain intensity from facial expres-\nsion on the x-ite pain database. Sensors , 21(9), may 2021.\n[182] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-\nChieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. Proceedings\nof the IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\ntion, pages 4510\u20134520, jan 2018.182 BIBLIOGRAPHY\n[183] Ehsan Othman, Philipp Werner, Frerk Saxen, Ayoub Al-Hamadi, and Steffen Walter.\nCross-database evaluation of pain recognition from facial video. In International\nSymposium on Image and Signal Processing and Analysis, ISPA , volume 2019-Septe,\npages 181\u2013186. IEEE Computer Society, sep 2019.\n[184] Mohammad Tavakolian and Abdenour Hadid. Deep Spatiotemporal Representation of\nthe Face for Automatic Pain Intensity Estimation. In 2018 24th International Confer-\nence on Pattern Recognition (ICPR) , volume 2018-Augus, pages 350\u2013354. Institute\nof Electrical and Electronics Engineers Inc., 2018.\n[185] Jinwei Wang and Huazhi Sun. Pain intensity estimation using deep spatiotem-\nporal and handcrafted features. IEICE Transactions on Information and Systems ,\nE101D(6):1572\u20131580, 2018.\n[186] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.\nLearning Spatiotemporal Features with 3D Convolutional Networks. In 2015 IEEE\nInternational Conference on Computer Vision (ICCV) , pages 4489\u20134497, 2015.\n[187] Yibo Huang, Linbo Qing, Shengyu Xu, Lu Wang, and Yonghong Peng. HybNet: a\nhybrid network structure for pain intensity estimation. Visual Computer , 2021.\n[188] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethink-\ning spatiotemporal feature learning: Speed-accuracy trade-offs in video classification.\nInComputer Vision \u2013 ECCV 2018 , volume 11219 LNCS, pages 318\u2013335, dec 2018.\n[189] Mohammad Tavakolian and Abdenour Hadid. A Spatiotemporal Convolutional Neu-\nral Network for Automatic Pain Intensity Estimation from Facial Dynamics. Interna-\ntional Journal of Computer Vision , 127(10):1413\u20131425, oct 2019.\n[190] Gnana Praveen R, Eric Granger, and Patrick Cardinal. Deep Domain Adaptation\nfor Ordinal Regression of Pain Intensity Estimation Using Weakly-Labeled Videos.\nCoRR , aug 2020.\n[191] R. Gnana Praveen, Eric Granger, and Patrick Cardinal. Deep Weakly Supervised Do-\nmain Adaptation for Pain Localization in Videos. In Gomez-Fernandez F Struc V .,\neditor, 15th IEEE International Conference on Automatic Face and Gesture Recogni-\ntion, pages 473\u2013480. Institute of Electrical and Electronics Engineers Inc., 2020.\n[192] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model\nand the kinetics dataset. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , July 2017.BIBLIOGRAPHY 183\n[193] Ghazal Bargshady, Xujuan Zhou, Ravinesh C. Deo, Jeffrey Soar, Frank Whittaker,\nand Hua Wang. The modeling of human facial pain intensity based on Temporal\nConvolutional Networks trained with video frames in HSV color space. Applied Soft\nComputing Journal , 97, 2020.\n[194] Siavash Rezaei, Abhishek Moturu, Shun Zhao, Kenneth M. Prkachin, Thomas Had-\njistavropoulos, and Babak Taati. Unobtrusive pain monitoring in older adults with\ndementia using pairwise and contrastive training. IEEE Journal of Biomedical and\nHealth Informatics , 25(5):1450\u20131462, 2021.\n[195] Hinton GE. Training products of experts by minimizing contrastive divergence. Neu-\nral computation , 14(8):1771\u20131800, aug 2002.\n[196] Vedhas Pandit, Maximilian Schmitt, Nicholas Cummins, and Bj \u00a8orn Schuller. I see it\nin your eyes: Training the shallowest-possible CNN to recognise emotions and pain\nfrom muted web-assisted in-the-wild video-chats in real-time. Information Processing\nand Management , 57(6):102347, nov 2020.\n[197] Jing Zhou, Xiaopeng Hong, Fei Su, and Guoying Zhao. Recurrent Convolutional\nNeural Network Regression for Continuous Pain Intensity Estimation in Video. IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition Work-\nshops , pages 1535\u20131543, may 2016.\n[198] Pau Rodriguez, Guillem Cucurull, Jordi Gonalez, Josep M. Gonfaus, Kamal Nasrol-\nlahi, Thomas B. Moeslund, and F. Xavier Roca. Deep Pain: Exploiting Long Short-\nTerm Memory Networks for Facial Expression Classification. IEEE Transactions on\nCybernetics , feb 2017.\n[199] Marco Bellantonio, Mohammad A. Haque, Pau Rodriguez, Kamal Nasrollahi, Taisi\nTelve, Sergio Escarela, Jordi Gonzalez, Thomas B. Moeslund, Pejman Rasti, and Gho-\nlamreza Anbarjafari. Spatio-temporal pain recognition in CNN-based super-resolved\nfacial images. Lecture Notes in Computer Science (including subseries Lecture Notes\nin Artificial Intelligence and Lecture Notes in Bioinformatics) , 10165 LNCS:151\u2013162,\n2017.\n[200] Ghazal Bargshady, Jeffrey Soar, Xujuan Zhou, Ravinesh C. Deo, Frank Whittaker,\nand Hua Wang. A joint deep neural network model for pain recognition from face.\n2019 IEEE 4th International Conference on Computer and Communication Systems,\nICCCS 2019 , pages 52\u201356, 2019.184 BIBLIOGRAPHY\n[201] Ghazal Bargshady, Xujuan Zhou, Ravinesh C. Deo, Jeffrey Soar, Frank Whittaker, and\nHua Wang. Enhanced deep learning algorithm development to detect pain intensity\nfrom facial expression images. Expert Systems with Applications , 149, 2020.\n[202] Antoni Mauricio, F \u00b4abio Cappabianco, Adriano Veloso, and Guillermo C \u00b4amara. A\nSequential Approach for Pain Recognition Based on Facial Representations. Lecture\nNotes in Computer Science (including subseries Lecture Notes in Artificial Intelli-\ngence and Lecture Notes in Bioinformatics) , 11754 LNCS:295\u2013304, 2019.\n[203] Selvarajah Thuseethan, Sutharshan Rajasegarar, and John Yearwood. Deep hybrid\nspatiotemporal networks for continuous pain intensity estimation. Lecture Notes in\nComputer Science (including subseries Lecture Notes in Artificial Intelligence and\nLecture Notes in Bioinformatics) , 11955 LNCS:449\u2013461, 2019.\n[204] Ghazal Bargshady, Xujuan Zhou, Ravinesh C. Deo, Jeffrey Soar, Frank Whittaker,\nand Hua Wang. Ensemble neural network approach detecting pain intensity from\nfacial expressions. Artificial Intelligence in Medicine , 109, 2020.\n[205] Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho,\nand Yu Sun. First Investigation into the Use of Deep Learning for Continuous As-\nsessment of Neonatal Postoperative Pain. In Gomez-Fernandez F Struc V ., editor,\nProceedings - 2020 15th IEEE International Conference on Automatic Face and Ges-\nture Recognition, FG 2020 , pages 415\u2013419. Institute of Electrical and Electronics\nEngineers Inc., 2020.\n[206] Nikolai Kalischek, Patrick Thiam, Peter Bellmann, and Friedhelm Schwenker. Deep\nDomain Adaptation for Facial Expression Analysis. In 2019 8th International Con-\nference on Affective Computing and Intelligent Interaction Workshops and Demos,\nACIIW 2019 , pages 317\u2013323. Institute of Electrical and Electronics Engineers Inc.,\n2019.\n[207] Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual\ndomain adaptation. In 6th International Conference on Learning Representations,\nICLR 2018 - Conference Track Proceedings , jun 2018.\n[208] Daniel Lopez Martinez, Ognjen Rudovic, and Rosalind Picard. Personalized Auto-\nmatic Estimation of Self-Reported Pain Intensity from Facial Expressions. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition Work-\nshops , volume 2017-July, pages 2318\u20132327. IEEE Computer Society, jun 2017.\n[209] Diyala Erekat, Zakia Hammal, Maimoon Siddiqui, and Hamdi Dibeklioglu. Enforcing\nmultilabel consistency for automatic spatio-temporal assessment of shoulder pain in-BIBLIOGRAPHY 185\ntensity. In ICMI 2020 Companion - Companion Publication of the 2020 International\nConference on Multimodal Interaction , pages 156\u2013164. Association for Computing\nMachinery, Inc, 2020.\n[210] Manh Tu Vu, Marie Beurton-Aimar, Pierre-yves Dezaunay, and Marine Cotty Es-\nlous. Automated Pain Estimation based on Facial Action Units from Multi-Databases.\nInJoint International Conference on Informatics, Electronics Vision (ICIEV) and In-\nternational Conference on Imaging, Vision Pattern Recognition (icIVPR) , pages 1\u20138,\n2021.\n[211] Dong Huang, Zhaoqiang Xia, Joshua Mwesigye, and Xiaoyi Feng. Pain-attentive\nnetwork: a deep spatio-temporal attention model for pain estimation. Multimedia\nTools and Applications , 79(37-38):28329\u201328354, 2020.\n[212] Jun Yu, Toru Kurihara, and Shu Zhan. Frame by Frame Pain Estimation Using Locally\nSpatial Attention Learning. Lecture Notes in Computer Science (including subseries\nLecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) , 11868\nLNCS:229\u2013238, 2019.\n[213] Haochen Xu and Manhua Liu. A Deep Attention Transformer Network for Pain Es-\ntimation with Facial Expression Video. In Jianjiang Feng, Junping Zhang, Manhua\nLiu, and Yuchun Fang, editors, Biometric Recognition , pages 112\u2013119, Cham, 2021.\nSpringer International Publishing.\n[214] Adria Mallol-Ragolta, Shuo Liu, Nicholas Cummins, and Bjorn Schuller. A Cur-\nriculum Learning Approach for Pain Intensity Recognition from Facial Expressions.\nIn Gomez-Fernandez F Struc V ., editor, 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition (FG 2020) , pages 829\u2013833. Institute of\nElectrical and Electronics Engineers Inc., 2020.\n[215] Yikang Guo, Li Wang, Yan Xiao, and Yingzi Lin. A Personalized Spatial-Temporal\nCold Pain Intensity Estimation Model Based on Facial Expression. IEEE journal of\ntranslational engineering in health and medicine , 9, 2021.\n[216] Sowmya Rasipuram, Bukka Nikhil Sai, Dinesh Babu Jayagopi, and Anutosh Maitra.\nUsing Deep 3D Features and an LSTM Based Sequence Model for Automatic Pain\nDetection in the Wild. In Gomez-Fernandez F Struc V ., editor, 2020 15th IEEE Inter-\nnational Conference on Automatic Face and Gesture Recognition (FG 2020) , pages\n781\u2013785. Institute of Electrical and Electronics Engineers Inc., 2020.\n[217] Ruicong Zhi and Ming Wan. Dynamic facial expression feature learning based on\nsparse RNN. In Proceedings of 2019 IEEE 8th Joint International Information Tech-186 BIBLIOGRAPHY\nnology and Artificial Intelligence Conference, ITAIC 2019 , pages 1373\u20131377. Institute\nof Electrical and Electronics Engineers Inc., may 2019.\n[218] Thomas Blumensath and Mike Davies. Iterative Thresholding for Sparse Approxima-\ntions. Journal of Fourier Analysis and Applications , 14:629\u2013654, 2008.\n[219] Patrick Thiam, Hans A. Kestler, and Friedhelm Schwenker. Two-stream attention\nnetwork for pain recognition from video sequences. Sensors (Switzerland) , 20(3):839,\nfeb 2020.\n[220] Antoni Mauricio, Jonathan Pe \u02dcna, Erwin Dianderas, Leonidas Mauricio, Jose D \u00b4\u0131az, and\nAntonio Mor \u00b4an. Chronic Pain Estimation Through Deep Facial Descriptors Analysis.\nCommunications in Computer and Information Science , 1070 CCIS:173\u2013185, 2020.\n[221] Jie Ting, Yi-Cheng Yang, Li-Chen Fu, Chu-Lin Tsai, and Chien-Hua Huang. Distance\nOrdering: A Deep Supervised Metric Learning for Pain Intensity Estimation. In 2021\n20th IEEE International Conference on Machine Learning and Applications (ICMLA) ,\npages 1083\u20131088, 2021.\n[222] Mingxin Yu, Yichen Sun, Bofei Zhu, Lianqing Zhu, Yingzi Lin, Xiaoying Tang,\nYikang Guo, Guangkai Sun, and Mingli Dong. Diverse frequency band-based convo-\nlutional neural networks for tonic cold pain assessment using EEG. Neurocomputing ,\n378:270\u2013282, 2020.\n[223] Jiahao Wang, Mengying Wei, Li Zhang, Gan Huang, Zhen Liang, Linling Li, and\nZhiguo Zhang. An Autoencoder-based Approach to Predict Subjective Pain Percep-\ntion from High-density Evoked EEG Potentials. In International Conference of the\nIEEE Engineering in Medicine Biology Society , volume 2020-July, pages 1507\u20131511.\nInstitute of Electrical and Electronics Engineers Inc., 2020.\n[224] Raul Fernandez Rojas, Julio Romero, Jehu Lopez-Aparicio, and Keng-Liang Ou.\nPain Assessment based on fNIRS using Bi-LSTM RNNs. In 10th International\nIEEE/EMBS Conference on Neural Engineering (NER) , pages 399\u2013402. IEEE, may\n2021.\n[225] Hyunjun Lim, Byeongnam Kim, Gyu Jeong Noh, and Sun K. Yoo. A deep neural\nnetwork-based pain classifier using a photoplethysmography signal. Sensors (Switzer-\nland) , 19(2), jan 2019.\n[226] Boyi Hu, Chong Kim, Xiaopeng Ning, and Xu Xu. Using a deep learning network to\nrecognise low back pain in static standing. Ergonomics , 61(10):1374\u20131381, oct 2018.BIBLIOGRAPHY 187\n[227] Danila Mamontov, Iana Polonskaia, Alina Skorokhod, Eugene Semenkin, Viktor\nKessler, and Friedhelm Schwenker. Evolutionary algorithms for the design of neu-\nral network classifiers for the classification of pain intensity. Multimodal Pattern\nRecognition of Social Signals in Human-Computer-Interaction , 11377 LNAI:84\u2013100,\n2019.\n[228] Chuan Yu Chang and Jia Jing Li. Application of deep learning for recognizing in-\nfant cries. In 2016 IEEE International Conference on Consumer Electronics-Taiwan,\nICCE-TW 2016 . Institute of Electrical and Electronics Engineers Inc., jul 2016.\n[229] Md Sirajus Salekin, Ghada Zamzmi, Rahul Paul, Dmitry Goldgof, Rangachar Kas-\nturi, Thao Ho, and Yu Sun. Harnessing the Power of Deep Learning Methods in\nHealthcare: Neonatal Pain Assessment from Crying Sound. In 2019 IEEE Healthcare\nInnovations and Point of Care Technologies, HI-POCT 2019 , pages 127\u2013130. Institute\nof Electrical and Electronics Engineers Inc., 2019.\n[230] Patrick Thiam and Friedhelm Schwenker. Combining deep and hand-crafted features\nfor audio-based pain intensity classification. Lecture Notes in Computer Science (in-\ncluding subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioin-\nformatics) , 11377 LNAI:49\u201358, 2019.\n[231] Fu Sheng Tsai, Yi Ming Weng, Chip Jin Ng, and Chi Chun Lee. Embedding stacked\nbottleneck vocal features in a LSTM architecture for automatic pain level classifica-\ntion during emergency triage. In 2017 7th International Conference on Affective Com-\nputing and Intelligent Interaction, ACII 2017 , volume 2018-Janua, pages 313\u2013318.\nInstitute of Electrical and Electronics Engineers Inc., 2018.\n[232] P Gouverneur, F Li, W M Adamczyk, T M Szikszay, K Luedtke, and M Grzegorzek.\nComparison of feature extraction methods for physiological signals for heat-based\npain recognition. Sensors , 21(14), 2021.\n[233] Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho,\nand Yu Sun. Multi-channel neural network for assessing neonatal pain from videos.\nInIEEE International Conference on Systems, Man and Cybernetics , volume 2019-\nOctob, pages 1551\u20131556. Institute of Electrical and Electronics Engineers Inc., oct\n2019.\n[234] Emad Kasaeyan Naeini, Sina Shahhosseini, Ajan Subramanian, Tingjue Yin, Amir M.\nRahmani, and Nikil Dutt. An Edge-Assisted and Smart System for Real-Time Pain\nMonitoring. In Proceedings - 4th IEEE/ACM Conference on Connected Health: Appli-188 BIBLIOGRAPHY\ncations, Systems and Engineering Technologies, CHASE 2019 , pages 47\u201352. Institute\nof Electrical and Electronics Engineers Inc., 2019.\n[235] Patrick Thiam, Peter Bellmann, Hans A. Kestler, and Friedhelm Schwenker. Explor-\ning deep physiological models for nociceptive pain recognition. Sensors , 19(20):4503,\noct 2019.\n[236] Ahmad Al-Qerem. An efficient machine-learning model based on data augmentation\nfor pain intensity recognition. Egyptian Informatics Journal , 21(4):241\u2013257, 2020.\n[237] R Zhi, C Zhou, J Yu, T Li, and G Zamzmi. Multimodal-based stream integrated\nneural networks for pain assessment. IEICE Transactions on Information and Systems ,\nE104D(12):2184\u20132194, 2021.\n[238] Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho,\nand Yu Sun. Multimodal spatio-temporal deep learning approach for neonatal postop-\nerative pain assessment. Computers in Biology and Medicine , 129, 2021.\n[239] Run Wang, Ke Xu, Hui Feng, and Wei Chen. Hybrid RNN-ANN Based Deep Phys-\niological Network for Pain Recognition. In Proceedings of the Annual International\nConference of the IEEE Engineering in Medicine and Biology Society, EMBS , volume\n2020-July, pages 5584\u20135587. Institute of Electrical and Electronics Engineers Inc., jul\n2020.\n[240] Yun Zhao, Franklin Ly, Qinghang Hong, Zhuowei Cheng, Tyler Santander, Henry T.\nYang, Paul K. Hansma, and Linda Petzold. How Much Does It Hurt: A Deep Learning\nFramework for Chronic Pain Score Assessment. In Cuzzocrea A Zaniolo C Wu X Di\nFatta G. Sheng V ., editor, IEEE International Conference on Data Mining Workshops,\nICDMW , volume 2020-Novem, pages 651\u2013660. IEEE Computer Society, 2020.\n[241] Xinhui Yuan and Marwa Mahmoud. ALANet:Autoencoder-LSTM for pain and pro-\ntective behaviour detection. In Proceedings - 2020 15th IEEE International Confer-\nence on Automatic Face and Gesture Recognition, FG 2020 , pages 824\u2013828, 2020.\n[242] Yi Li, Shreya Ghosh, Jyoti Joshi, and Sharon Oviatt. LSTM-DNN based Approach\nfor Pain Intensity and Protective Behaviour Prediction. In Gomez-Fernandez F Struc\nV ., editor, Proceedings - 2020 15th IEEE International Conference on Automatic Face\nand Gesture Recognition, FG 2020 , pages 819\u2013823. Institute of Electrical and Elec-\ntronics Engineers Inc., 2020.\n[243] Patrick Thiam, Hans A. Kestler, and Friedhelm Schwenker. Multimodal deep denois-\ning convolutional autoencoders for pain intensity classification based on physiologicalBIBLIOGRAPHY 189\nsignals. In ICPRAM 2020 - Proceedings of the 9th International Conference on Pat-\ntern Recognition Applications and Methods , pages 289\u2013296. SCITEPRESS - Science\nand Technology Publications, 2020.\n[244] Patrick Thiam, Heinke Hihn, Daniel A Braun, Hans A Kestler, and Friedhelm\nSchwenker. Multi-Modal Pain Intensity Assessment Based on Physiological Signals:\nA Deep Learning Perspective. Frontiers in Physiology , 12, 2021.\n[245] Saranya Devi Subramaniam and Brindha Dass. Automated Nociceptive Pain Assess-\nment Using Physiological Signals and a Hybrid Deep Learning Network. IEEE Sen-\nsors Journal , 21(3):3335\u20133343, 2021.\n[246] Yi Li, Shreya Ghosh, and Jyoti Joshi. PLAAN: Pain Level Assessment with Anomaly-\ndetection based Network. Journal on Multimodal User Interfaces , 2021.\n[247] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transform-\ners.AI Open , 3:111\u2013132, 2022.\n[248] Philipp Werner, Ayoub Al-Hamadi, Kerstin Limbrecht-Ecklundt, Steffen Walter,\nSascha Gruss, and Harald C. Traue. Automatic Pain Assessment with Facial Activity\nDescriptors. IEEE Transactions on Affective Computing , 8(3):286\u2013299, jul 2017.\n[249] Kunz M and Lautenbacher S. The faces of pain: a cluster analysis of individual\ndifferences in facial activity patterns of pain. European journal of pain (London,\nEngland) , 18(6):813\u2013823, 2014.\n[250] Manon Ranger, Celeste Johnston, and Karthika Anand. Current controversies regard-\ning pain assessment in neonates. Seminars in perinatology , 31:283\u20138, 11 2007.\n[251] Mark R. Jones, Ken P. Ehrhardt, Juan G. Ripoll, Bharat Sharma, Ira W. Padnos,\nRachel J. Kaye, and Alan D. Kaye. Pain in the Elderly, feb 2016.\n[252] Laura Pence Forsythe, Beverly Thorn, Melissa Day, and Grace Shelby. Race and Sex\nDifferences in Primary Appraisals, Catastrophizing, and Experimental Pain Outcomes.\nThe Journal of Pain , 12(5):563\u2013572, may 2011.\n[253] Philipp Tschandl, Noel Codella, Beng \u00a8u Nisa Akay, Giuseppe Argenziano, Ralph P\nBraun, Horacio Cabo, David Gutman, Allan Halpern, Brian Helba, Rainer Hofmann-\nWellenhof, Aimilios Lallas, Jan Lapins, Caterina Longo, Josep Malvehy, Michael A\nMarchetti, Ashfaq Marghoob, Scott Menzies, Amanda Oakley, John Paoli, Susana\nPuig, Christoph Rinner, Cliff Rosendahl, Alon Scope, Christoph Sinz, H Peter Soyer,\nLuc Thomas, Iris Zalaudek, and Harald Kittler. Comparison of the accuracy of human190 BIBLIOGRAPHY\nreaders versus machine-learning algorithms for pigmented skin lesion classification:\nan open, web-based, international, diagnostic study. The Lancet Oncology , 20(7):938\u2013\n947, 2019.\n[254] Guang Yang, Qinghao Ye, and Jun Xia. Unbox the black-box for the medical explain-\nable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases\nand beyond. Information Fusion , 77:29\u201352, 2022.\n[255] Karim Lekadir, Richard Osuala, Catherine Gallin, Noussair Lazrak, Kaisar Kushibar,\nGianna Tsakou, Susanna Ausso, Leonor Cerda Alberich, Kostas Marias, Manolis\nTsiknakis, Sara Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, Henry C\nWoodruff, Philippe Lambin, and Luis Marti-Bonmati. Future-ai: Guiding princi-\nples and consensus recommendations for trustworthy artificial intelligence in medical\nimaging, 2021.\n[256] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable ai:\nA review of machine learning interpretability methods. Entropy , 23(1):1\u201345, 2021.\n[257] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu.\nDefinitions, methods, and applications in interpretable machine learning. Proceedings\nof the National Academy of Sciences , 116(44):22071\u201322080, 2019.\n[258] Rachael E. Jack. Culture and facial expressions of emotion. Visual Cognition , 21(9-\n10):1248\u20131286, 2013.\n[259] Erik Cambria, Newton Howard, Jane Hsu, and Amir Hussain. Sentic blending: Scal-\nable multimodal fusion for the continuous interpretation of semantics and sentics. In\n2013 IEEE Symposium on Computational Intelligence for Human-like Intelligence\n(CIHLI) , pages 108\u2013117, 2013.\n[260] Saurabh Hinduja, Shaun Canavan, and Gurmeet Kaur. Multimodal Fusion of Physio-\nlogical Signals and Facial Action Units for Pain Recognition. In Gomez-Fernandez F\nStruc V ., editor, IEEE International Conference on Automatic Face and Gesture\nRecognition , pages 577\u2013581. Institute of Electrical and Electronics Engineers Inc.,\n2020.\n[261] Tobias B. Ricken, Peter Bellmann, Sascha Gruss, Hans A. Kestler, Steffen Walter,\nand Friedhelm Schwenker. Pain recognition differences between female and male\nsubjects: An analysis based on the physiological signals of the x-ite pain database. In\nCompanion Publication of the 25th International Conference on Multimodal Interac-\ntion, ICMI \u201923 Companion, pages 121\u2013130, New York, NY , USA, 2023. Association\nfor Computing Machinery.BIBLIOGRAPHY 191\n[262] A real-time qrs detection algorithm. IEEE Transactions on Biomedical Engineering ,\nBME-32:230\u2013236, 1985.\n[263] M A Z Fariha, R Ikeura, S Hayakawa, and S Tsutsumi. Analysis of Pan-Tompkins\nAlgorithm Performance with Noisy ECG Signals. Journal of Physics: Conference\nSeries , 1532(1):12022, 2020.\n[264] Feifei Liu, Shoushui Wei, Yibin Li, Xinge Jiang, Zhimin Zhang, Ling Zhang, and\nChengyu Liu. The accuracy on the common Pan-Tompkins based QRS detection\nmethods through low-quality electrocardiogram database. Journal of Medical Imag-\ning and Health Informatics , 7(5):1039\u20131043, 2017.\n[265] Kai Zhao, Yongfu Li, Guoxing Wang, Yu Pu, and Yong Lian. A robust QRS detec-\ntion and accurate R-peak identification algorithm for wearable ECG sensors. Science\nChina Information Sciences , 64(8):182401, 2021.\n[266] Daniel Lopez-Martinez and Rosalind Picard. Continuous Pain Intensity Estimation\nfrom Autonomic Signals with Recurrent Neural Networks. Conference proceedings\n: ... Annual International Conference of the IEEE Engineering in Medicine and Biol-\nogy Society. IEEE Engineering in Medicine and Biology Society. Annual Conference ,\n2018:5624\u20135627, jul 2018.\n[267] Philipp Werner, Ayoub Al-Hamadi, Robert Niese, Steffen Walter, Sascha Gruss, and\nHarald C. Traue. Automatic pain recognition from video and biomedical signals. In\nInternational Conference on Pattern Recognition , pages 4582\u20134587. Institute of Elec-\ntrical and Electronics Engineers Inc., 2014.\n[268] Roberto Cipolla, Yarin Gal, and Alex Kendall. Multi-task learning using uncertainty\nto weigh losses for scene geometry and semantics. In 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 7482\u20137491, 2018.\n[269] Dong Huang, Xiaoyi Feng, Haixi Zhang, Zitong Yu, Jinye Peng, Guoying Zhao, and\nZhaoqiang Xia. Spatio-temporal pain estimation network with measuring pseudo\nheart rate gain. IEEE Transactions on Multimedia , 24:3300\u20133313, 2022.\n[270] Daniel Lopez-Martinez and Rosalind Picard. Continuous Pain Intensity Estimation\nfrom Autonomic Signals with Recurrent Neural Networks. In International Confer-\nence of the IEEE Engineering in Medicine and Biology Society. , volume 2018, pages\n5624\u20135627, 2018.\n[271] Philipp Werner, Daniel Lopez-Martinez, Steffen Walter, Ayoub Al-Hamadi, Sascha\nGruss, and Rosalind Picard. Automatic Recognition Methods Supporting Pain As-\nsessment: A Survey. IEEE Transactions on Affective Computing , 2019.192 BIBLIOGRAPHY\n[272] Anderson Faria Claret, Karina Rabello Casali, Tatiana Sousa Cunha, and Matheus Car-\ndoso Moraes. Automatic classification of emotions based on cardiac signals: A\nsystematic literature review. Annals of Biomedical Engineering , 51(11):2393\u20132414,\n2023.\n[273] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceed-\nings of the 31st International Conference on Neural Information Processing Systems ,\nNIPS\u201917, pages 5998\u20136008, Red Hook, NY , USA, 2017. Curran Associates Inc.\n[274] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale, 2021.\n[275] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing XU, and Yunhe Wang. Trans-\nformer in transformer. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems ,\nvolume 34, pages 15908\u201315919. Curran Associates, Inc., 2021.\n[276] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye\nTeh. Set transformer: A framework for attention-based permutation-invariant neural\nnetworks. In International conference on machine learning , pages 3744\u20133753. PMLR,\n2019.\n[277] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and\nJoao Carreira. Perceiver: General perception with iterative attention. In International\nconference on machine learning , pages 4651\u20134664. PMLR, 2021.\n[278] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint Face Detection\nand Alignment using Multi-task Cascaded Convolutional Networks. IEEE Signal\nProcessing Letters , 23(10):1499\u20131503, apr 2016.\n[279] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d\nface alignment problem? (and a dataset of 230,000 3d facial landmarks). In Proceed-\nings of the IEEE International Conference on Computer Vision (ICCV) , Oct 2017.\n[280] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman. VG-\nGFace2: A dataset for recognising faces across pose and age. In Proceedings -\n13th IEEE International Conference on Automatic Face and Gesture Recognition, FG\n2018 , pages 67\u201374. Institute of Electrical and Electronics Engineers Inc., oct 2018.BIBLIOGRAPHY 193\n[281] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji\nLakshminarayanan. Augmix: A simple data processing method to improve robustness\nand uncertainty. arXiv preprint arXiv:1912.02781 , 2019.\n[282] Samuel G. MG\u2019Oller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-\nart data augmentation. In 2021 IEEE/CVF International Conference on Computer\nVision (ICCV) , pages 754\u2013762, 2021.\n[283] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention\nvisualization. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 782\u2013791, 2021.\n[284] Sabrina Patania, Giuseppe Boccignone, Sathya Bur \u02c7si\u00b4c, Alessandro D\u2019Amelio, and\nRaffaella Lanzarotti. Deep graph neural network for video-based facial pain expres-\nsion assessment. SAC \u201922, pages 585\u2013591, New York, NY , USA, 2022. Association\nfor Computing Machinery.\n[285] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,\nU. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran\nAssociates, Inc., 2017.\n[286] Tsz-Him Cheung and Dit-Yan Yeung. {MODALS }: Modality-agnostic automated\ndata augmentation in the latent space. In International Conference on Learning Rep-\nresentations , 2021.\n[287] Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. Affectnet: A\ndatabase for facial expression, valence, and arousal computing in the wild. IEEE\nTransactions on Affective Computing , 10(1):18\u201331, 2019.\n[288] Shichuan Du, Yong Tao, and Aleix M. Martinez. Compound facial expressions of\nemotion. Proceedings of the National Academy of Sciences , 111(15):E1454\u2013E1462,\n2014.\n[289] Shan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep locality-\npreserving learning for expression recognition in the wild. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , July 2017.\n[290] Roberto Cipolla, Yarin Gal, and Alex Kendall. Multi-task learning using uncertainty\nto weigh losses for scene geometry and semantics. In 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 7482\u20137491, 2018.194 BIBLIOGRAPHY\n[291] Mohammad Kachuee, Shayan Fazeli, and Majid Sarrafzadeh. Ecg heartbeat classifi-\ncation: A deep transferable representation. In 2018 IEEE International Conference\non Healthcare Informatics (ICHI) , pages 443\u2013444, 2018.\n[292] G.B. Moody and R.G. Mark. The impact of the mit-bih arrhythmia database. IEEE\nEngineering in Medicine and Biology Magazine , 20(3):45\u201350, 2001.\n[293] R. Bousseljot, D. Kreiseler, and A. Schnabel. Nutzung der ekg-signaldatenbank car-\ndiodat der ptb G\u2019Ober das internet. Biomedical Engineering / Biomedizinische Tech-\nnik, 40(s1):317\u2013318, 1995.\n[294] AL Goldberger, LA Amaral, L Glass, JM Hausdorff, PC Ivanov, RG Mark, JE Mietus,\nGB Moody, CK Peng, and HE Stanley. Physiobank, physiotoolkit, and physionet:\ncomponents of a new research resource for complex physiologic signals. Circulation ,\n101(23):E215\u201320, June 2000.\n[295] Philipp Werner, Ayoub Al-Hamadi, and Steffen Walter. Analysis of facial expressive-\nness during experimentally induced heat pain. In 2017 Seventh International Con-\nference on Affective Computing and Intelligent Interaction Workshops and Demos\n(ACIIW) , pages 176\u2013180, 2017.\n[296] Philipp Werner, Ayoub Al-Hamadi, Kerstin Limbrecht-Ecklundt, Steffen Walter,\nSascha Gruss, and Harald C Traue. Automatic pain assessment with facial activity\ndescriptors. IEEE Transactions on Affective Computing , 8(3):286\u2013299, 2016.\n[297] Markus K \u00a8achele, Philipp Werner, Ayoub Al-Hamadi, G \u00a8unther Palm, Steffen Walter,\nand Friedhelm Schwenker. Bio-visual fusion for person-independent recognition of\npain intensity. In Multiple Classifier Systems , pages 220\u2013230. Springer International\nPublishing, 2015.\n[298] Vasileios C. Pezoulas, Dimitrios I. Zaridis, Eugenia Mylona, Christos Androutsos,\nKosmas Apostolidis, Nikolaos S. Tachos, and Dimitrios I. Fotiadis. Synthetic data\ngeneration methods in healthcare: A review on open-source tools and methods. Com-\nputational and Structural Biotechnology Journal , 23:2892\u20132910, 2024.\n[299] Mustafa MM Al Qudah, Ahmad SA Mohamed, and Syaheerah L Lutfi. Affective\nstate recognition using thermal-based imaging: A survey. Computer Systems Science\n& Engineering , 37(1), 2021.\n[300] Stephanos Ioannou, Vittorio Gallese, and Arcangelo Merla. Thermal infrared imaging\nin psychophysiology: Potentialities and limits. Psychophysiology , 51(10):951\u2013963,\n2014.BIBLIOGRAPHY 195\n[301] Sophie Jarlier, Didier Grandjean, Sylvain Delplanque, Karim N\u2019Diaye, Isabelle\nCayeux, Maria Ines Velazco, David Sander, Patrik Vuilleumier, and Klaus R. Scherer.\nThermal analysis of facial muscles contractions. IEEE Transactions on Affective Com-\nputing , 2(1):2\u20139, 2011.\n[302] A Merla, L Di Donato, PM Rossini, and GL Romani. Emotion detection through func-\ntional infrared imaging: preliminary results. Biomedizinische Technick , 48(2):284\u2013\n286, 2004.\n[303] Youssef Mohamed, Arzu G \u00a8uneysu, S \u00b4everin Lemaignan, and Iolanda Leite. Multi-\nmodal affect detection using thermal and optical imaging in a gamified robotic exer-\ncise. International Journal of Social Robotics , Oct 2023.\n[304] Varlik K Erel and Heval Selman Ozkan. Thermal camera as a pain monitor. Journal\nof Pain Research , 10:2827\u20132832, 2017.\n[305] Mohammad A. Haque, Ruben B. Bautista, Fatemeh Noroozi, Kaustubh Kulkarni,\nChristian B. Laursen, Ramin Irani, Marco Bellantonio, Sergio Escalera, Golamreza\nAnbarjafari, Kamal Nasrollahi, Ole K. Andersen, Erika G. Spaich, and Thomas B.\nMoeslund. Deep multimodal pain recognition: A database and comparison of spatio-\ntemporal visual modalities. In 2018 13th IEEE International Conference on Automatic\nFace & Gesture Recognition (FG 2018) , pages 250\u2013257, 2018.\n[306] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv\npreprint arXiv:1411.1784 , 2014.\n[307] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C\nCourville. Improved training of wasserstein gans. In I. Guyon, U. V on Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.\n[308] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan\nCatanzaro. High-resolution image synthesis and semantic manipulation with condi-\ntional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , June 2018.\n[309] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image trans-\nlation with conditional adversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 1125\u20131134, 2017.\n[310] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,\nThomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkor-\neit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for196 BIBLIOGRAPHY\nvision. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Processing Systems , volume 34,\npages 24261\u201324272. Curran Associates, Inc., 2021.\n[311] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang.\nAn image patch is a wave: Phase-aware vision mlp. In 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , pages 10925\u201310934, 2022.\n[312] Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat Khassanov,\nMichael Lewis, and Huseyin Atakan Varol. Speakingfaces: A large-scale multimodal\ndataset of voice commands with visual and thermal video streams. Sensors , 21(10),\n2021.\n[313] Gwangbin Bae, Martin de La Gorce, Tadas Baltru \u02c7saitis, Charlie Hewitt, Dong Chen,\nJulien Valentin, Roberto Cipolla, and Jingjing Shen. Digiface-1m: 1 million digital\nface images for face recognition. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV) , pages 3526\u20133535, January 2023.\n[314] Xu Zheng, Yuanhuiyi Lyu, and Lin Wang. Learning modality-agnostic representation\nfor semantic segmentation from any modalities, 2024.\n[315] Dingkang Yang, Mingcheng Li, Linhao Qu, Kun Yang, Peng Zhai, Song Wang, and\nLihua Zhang. Asynchronous multimodal video sequence fusion via learning modality-\nexclusive and -agnostic representations, 2024.\n[316] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Syd-\nney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brun-\nskill, et al. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258 , 2021.\n[317] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr\nDollar, and Ross Girshick. Segment anything. In 2023 IEEE/CVF International Con-\nference on Computer Vision (ICCV) , pages 3992\u20134003, 2023.\n[318] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything\nin medical images. Nature Communications , 15(1):654, 2024.\n[319] Junde Wu, Wei Ji, Yuanpei Liu, Huazhu Fu, Min Xu, Yanwu Xu, and Yueming Jin.\nMedical sam adapter: Adapting segment anything model for medical image segmen-\ntation. arXiv preprint arXiv:2304.12620 , 2023.BIBLIOGRAPHY 197\n[320] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander\nNovikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias\nSpringenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175 , 2022.\n[321] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer,\nHisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan.\nFoundational models defining a new era in vision: A survey and outlook. arXiv\npreprint arXiv:2307.13721 , 2023.\n[322] Pau Rodriguez, Guillem Cucurull, Jordi Gonzalez, Josep M. Gonfaus, Kamal Nasrol-\nlahi, Thomas B. Moeslund, and F. Xavier Roca. Deep pain: Exploiting long short-term\nmemory networks for facial expression classification. IEEE Transactions on Cyber-\nnetics , 52(5):3314\u20133324, 2022.\n[323] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recog-\nnition at scale. arXiv preprint arXiv:2010.11929 , 2020.\n[324] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan,\nand Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. In 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages\n5260\u20135269, 2022.\n[325] Sitong Wu, Tianyi Wu, Haoru Tan, and Guodong Guo. Pale transformer: A general\nvision transformer backbone with pale-shaped attention. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 36, pages 2731\u20132739, 2022.\n[326] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. Swin transformer: Hierarchical vision transformer using shifted\nwindows. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV) ,\npages 9992\u201310002, 2021.\n[327] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan.\nEfficientvit: Memory efficient vision transformer with cascaded group attention. In\n2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,\npages 14420\u201314430, 2023.\n[328] Judith M. Ford, Vanessa A. Palzes, Brian J. Roach, and Daniel H. Mathalon. Did I\nDo That? Abnormal Predictive Processes in Schizophrenia When Button Pressing to\nDeliver a Tone. Schizophrenia Bulletin , 40(4):804\u2013812, 07 2013.198 BIBLIOGRAPHY\n[329] David Gaddy and Dan Klein. Digital voicing of silent speech. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) ,\npages 5521\u20135530, Online, 2020. Association for Computational Linguistics.\n[330] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V . Le. Randaugment: Prac-\ntical automated data augmentation with a reduced search space. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops (CVPRW) , pages\n3008\u20133017, 2020.\n[331] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wo-\njna. Rethinking the inception architecture for computer vision. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 2818\u20132826,\n2016.\n[332] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R\nSalakhutdinov. Improving neural networks by preventing co-adaptation of feature\ndetectors. arXiv preprint arXiv:1207.0580 , 2012.\n[333] Raul Fernandez Rojas, Niraj Hirachan, Nicholas Brown, Gordon Waddington, Luke\nMurtagh, Ben Seymour, and Roland Goecke. Multimodal physiological sensing for\nthe assessment of acute pain. Frontiers in Pain Research , 4, 2023.\n[334] Raul Fernandez Rojas, Xu Huang, Jesus Hernandez-Juarez, and Keng-Liang Ou.\nPhysiological fluctuations show frequency-specific networks in fnirs signals during\nresting state. In 2017 39th Annual International Conference of the IEEE Engineering\nin Medicine and Biology Society (EMBC) , pages 2550\u20132553, 2017.\n[335] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and\nBryan Catanzaro. Efficient token mixing for transformers via adaptive fourier neural\noperators. In International Conference on Learning Representations , 2021.\n[336] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang\nZhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 10371\u201310381, 2024.\n[337] Fadi Boutros, Marco Huber, Patrick Siebke, Tim Rieber, and Naser Damer. Sface:\nPrivacy-friendly and accurate face recognition using synthetic data. In 2022 IEEE\nInternational Joint Conference on Biometrics (IJCB) , pages 1\u201311, 2022.\n[338] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep\nneural networks without residuals. arXiv preprint arXiv:1605.07648 , 2016.BIBLIOGRAPHY 199\n[339] Philipp Werner, Ayoub Al-Hamadi, Robert Niese, Steffen Walter, Sascha Gruss, and\nHarald C. Traue. Automatic pain recognition from video and biomedical signals. In\n2014 22nd International Conference on Pattern Recognition , pages 4582\u20134587, 2014.\n[340] Ruijing Yang, Ziyu Guan, Zitong Yu, Xiaoyi Feng, Jinye Peng, and Guoying Zhao.\nNon-contact pain recognition from video sequences with remote physiological mea-\nsurements prediction. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth Interna-\ntional Joint Conference on Artificial Intelligence, IJCAI-21 , pages 1231\u20131237. Inter-\nnational Joint Conferences on Artificial Intelligence Organization, 8 2021.\n[341] Zhenyuan Lu, Burcu Ozek, and Sagar Kamarthi. Transformer encoder with multi-\nscale deep learning for pain classification using physiological signals. Frontiers in\nPhysiology , 14, 2023.\n[342] Kim Ngan Phan, Ngumimi Karen Iyortsuun, Sudarshan Pant, Hyung-Jeong Yang, and\nSoo-Hyung Kim. Pain recognition with physiological signals using multi-level con-\ntext information. IEEE Access , 11:20114\u201320127, 2023.\n[343] Mingzhe Jiang, Yufei Li, Jiangshan He, Yuqiang Yang, Hui Xie, and Xueli Chen.\nPhysiological time-series fusion with hybrid attention for adaptive recognition of pain.\nIEEE Journal of Biomedical and Health Informatics , pages 1\u20139, 2024.\n[344] Ruicong Zhi and Junwei Yu. Multi-modal fusion based automatic pain assessment.\nInProceedings of 2019 IEEE 8th Joint International Information Technology and Ar-\ntificial Intelligence Conference, ITAIC 2019 , pages 1378\u20131382. Institute of Electrical\nand Electronics Engineers Inc., may 2019.\n[345] Pooja Prajod, Dominik Schiller, Daksitha Withanage Don, and Elisabeth Andre. Faces\nof experimental pain: Transferability of deep learned heat pain features to electrical\npain, 2024.\n[346] Minh-Duc Nguyen, Hyung-Jeong Yang, Soo-Hyung Kim, Ji-Eun Shin, and Seung-\nWon Kim. Transformer with leveraged masked autoencoder for video-based pain\nassessment, 2024.\n[347] Manisha Shantaram Patil and Hitendra Dhansing Patil. Logistic regression based\nmodel for pain intensity level detection from biomedical signal. International Re-\nsearch Journal of Multidisciplinary Scope , 2024.\n[348] Manisha S. Patil and Hitendra D. Patil. Ensemble neural networks for multimodal\nacute pain intensity evaluation using video and physiological signals. Journal of Com-\nputational Analysis and Applications (JoCAAA) , 33(05):779\u2013791, Sep. 2024.200 BIBLIOGRAPHY\n[349] Xinwei Ji, Tianming Zhao, Wei Li, and Albert Zomaya. Automatic pain assess-\nment with ultra-short electrodermal activity signal. In Proceedings of the 38th\nACM/SIGAPP Symposium on Applied Computing , SAC \u201923, pages 618\u2013625, New\nYork, NY , USA, 2023. Association for Computing Machinery.\n[350] Markus K \u00a8achele, Patrick Thiam, Mohammadreza Amirian, Philipp Werner, Steffen\nWalter, Friedhelm Schwenker, and G \u00a8unther Palm. Multimodal data fusion for person-\nindependent, continuous estimation of pain intensity. In Lazaros Iliadis and Chrisina\nJayne, editors, Engineering Applications of Neural Networks , pages 275\u2013285, Cham,\n2015. Springer International Publishing.\n[351] Fatemeh Pouromran, Srinivasan Radhakrishnan, and Sagar Kamarthi. Exploration\nof physiological sensors, features, and machine learning models for pain intensity\nestimation. PLOS ONE , 16(7):1\u201317, 07 2021.\n[352] Patrick Thiam, Heinke Hihn, Daniel A. Braun, Hans A. Kestler, and Friedhelm\nSchwenker. Multi-modal pain intensity assessment based on physiological signals:\nA deep learning perspective. Frontiers in Physiology , 12, 2021.\n[353] Mingzhe Jiang, Riitta Rosio, Sanna Salantera, Amir M. Rahmani, Pasi Liljeberg,\nDaniel S. da Silva, Victor Hugo C. de Albuquerque, and Wanqing Wu. Personal-\nized and adaptive neural networks for pain detection from multi-modal physiological\nfeatures. Expert Systems with Applications , 235:121082, 2024.\n[354] Markus K \u00a8achele, Patrick Thiam, Mohammadreza Amirian, Friedhelm Schwenker,\nand GG\u2019Onther Palm. Methods for person-centered continuous pain intensity assess-\nment from bio-physiological channels. IEEE Journal of Selected Topics in Signal\nProcessing , 10(5):854\u2013864, 2016.\n[355] Peter Bellmann and Friedhelm Schwenker. Automated pain assessment: Is it use-\nful to combine person-specific data samples? In 2020 IEEE Symposium Series on\nComputational Intelligence (SSCI) , pages 1588\u20131593, 2020.Appendix\nSupplementary Metrics\nTable 1: Results utilizing the video modality reported on precision, recall, and F1 score\n(refer to Section 5.3).\nEpochs MetricPretraining stage Pipeline Augmentations Task\n1st2ndFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n500Precision \u2713 - \u2713 -\u2713 - - 72.53 31.24\nRecall \u2713 - \u2713 -\u2713 - - 74.31 29.61\nF1 \u2713 - \u2713 -\u2713 - - 71.95 27.16\n500Precision - \u2713 \u2713 -\u2713 - - 74.21 33.36\nRecall - \u2713 \u2713 -\u2713 - - 76.74 33.41\nF1 - \u2713 \u2713 -\u2713 - - 72.24 28.77\n500Precision - \u2713 - \u2713 \u2713 - - 68.11 31.50\nRecall - \u2713 - \u2713 \u2713 - - 72.15 27.99\nF1 - \u2713 - \u2713 \u2713 - - 65.92 25.14\n500Precision - \u2713 \u2713 \u2713 \u2713 - - 65.14 27.78\nRecall - \u2713 \u2713 \u2713 \u2713 - - 70.36 18.42\nF1 - \u2713 \u2713 \u2713 \u2713 - - 61.93 18.86\n500Precision - \u2713 \u2713 \u2713c\u2713 - - 74.88 33.96\nRecall - \u2713 \u2713 \u2713c\u2713 - - 77.41 34.31\nF1 - \u2713 \u2713 \u2713c\u2713 - - 73.90 29.20\n500Precision - \u2713 \u2713 \u2713c\u2713 \u2713 - 73.09 32.17\nRecall - \u2713 \u2713 \u2713c\u2713 \u2713 - 75.72 28.41\nF1 - \u2713 \u2713 \u2713c\u2713 \u2713 - 71.92 26.02\n500Precision - \u2713 \u2713 \u2713c\u2713 - \u2713 74.87 33.88\nRecall - \u2713 \u2713 \u2713c\u2713 - \u2713 77.80 29.30\nF1 - \u2713 \u2713 \u2713c\u2713 - \u2713 73.59 27.74\n500Precision - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 73.12 32.79\nRecall - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 76.18 28.51\nF1 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 71.91 26.57\n800Precision - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 77.15 35.39\nRecall - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 79.35 35.11\nF1 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 76.33 31.70\n201202 Appendix\nTable 2: Results utilizing the heart rate modality reported on precision, recall, and F1 score\n(refer to Section 5.3).\nEpochs MetricHR\nEncoderAugmentations Task\nBasic Mask AugmNet NP vs P 4 MC\n500Precision \u2713 \u2713 - - 61.73 27.66\nRecall \u2713 \u2713 - - 65.04 20.91\nF1 \u2713 \u2713 - - 57.74 19.73\n500Precision - \u2713 - - 61.97 27.71\nRecall - \u2713 - - 66.01 22.13\nF1 - \u2713 - - 57.79 20.61\n500Precision \u2713 \u2713 \u2713 - 61.97 27.80\nRecall \u2713 \u2713 \u2713 - 65.27 20.98\nF1 \u2713 \u2713 \u2713 - 57.38 20.97\n500Precision \u2713 \u2713 - \u2713 62.09 28.00\nRecall \u2713 \u2713 - \u2713 65.73 21.27\nF1 \u2713 \u2713 - \u2713 58.04 21.61\n500Precision \u2713 \u2713 \u2713 \u2713 61.63 27.86\nRecall \u2713 \u2713 \u2713 \u2713 65.08 21.24\nF1 \u2713 \u2713 \u2713 \u2713 56.78 21.17\n800Precision \u2713 \u2713 \u2713 \u2713 65.44 29.73\nRecall \u2713 \u2713 \u2713 \u2713 69.85 27.40\nF1 \u2713 \u2713 \u2713 \u2713 62.07 23.71\n800Precision \u2713 - - \u2713 67.07 31.11\nRecall \u2713 - - \u2713 71.24 29.33\nF1 \u2713 - - \u2713 63.97 25.83\nTable 3: Results utilizing the video & the heart rate modality reported on precision, recall\nand F1 score (refer to Section 5.3).\nEpochs MetricHR\nEncoderPipeline Augmentations Task\nFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n800Precision \u2713 \u2713 \u2713c- - \u2713 82.69 39.13\nRecall \u2713 \u2713 \u2713c- - \u2713 84.71 37.67\nF1 \u2713 \u2713 \u2713c- - \u2713 81.44 36.31203\nTable 4: Results utilizing the RGB video modality, reported on recall and F1 score (refer to\nSection 6.2).\nEpochsAugmentations\nMetricTask\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 30-50 0.9Recall 71.29 29.61\nF1 68.53 27.22\n200 \u2713 30-50 0.9Recall 71.93 24.43\nF1 69.61 23.78\n300 \u2713 30-50 0.9Recall 71.34 30.64\nF1 69.65 26.12\nTable 5: Results utilizing the synthetic thermal video modality, reported on recall and F1\nscore (refer to Section 6.2).\nEpochsAugmentations\nMetricTask\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 30-50 0.9Recall 72.04 28.80\nF1 69.16 26.45\n200 \u2713 30-50 0.9Recall 72.18 30.89\nF1 69.44 26.45\n300 \u2713 30-50 0.9Recall 72.52 24.96\nF1 70.01 23.43204 Appendix\nTable 6: Results utilizing the fusion of RGB & synthetic thermal video modality, reported\non recall and F1 score (refer to Section 6.2).\nEpochsFusion\nweightsAugmentations\nMetricTask\nBasic Masking P(Aug) NP vs P 4 MC\n100 \u2013 \u2713 30-50 0.9Recall 67.05 21.68\nF1 62.96 18.29\n100 W2 \u2713 30-50 0.9Recall 68.72 21.69\nF1 62.98 19.35\n100 W3 \u2713 30-50 0.9Recall 66.12 23.12\nF1 59.72 19.67\n300 W2 \u2713 30-50 0.9Recall 71.40 26.39\nF1 68.82 26.18\n500 W2 \u2713 10-20 0.7Recall 73.20 29.69\nF1 70.30 27.84\nTable 7: Results of the proposed approaches, reported on macro-averaged precision, recall\nand F1 score (refer to Section 7.2).\nModality ApproachMetrics\nPrecision Recall F1\nVideo Addition 44.91 44.97 44.60\nfNIRS HbO & Addition 44.68 45.08 43.60\nFusion Single Diagram 46.76 47.29 46.70205\nSupplementary Figures\nFigure 1: Attention maps generated by the Spatial-Module .Yellow and red colors signify\nintense focus on specific areas. (1strow) Sequence of original frames. (2ndrow)\nDerived from the Spatial-Module after initial stage pretraining. (3rdrow) Derived\nfrom the Spatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module following training on BioVid (refer to Section 5.3).206 Appendix\nFigure 2: Attention maps generated by the Spatial-Module .Yellow and red colors signify\nintense focus on specific areas. (1strow) Sequence of original frames. (2ndrow)\nDerived from the Spatial-Module after initial stage pretraining. (3rdrow) Derived\nfrom the Spatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module following training on BioVid (refer to Section 5.3).207\nabc\nFigure 3: Additional attention maps from the PainViT\u20132 (refer to Section 7.2).208Acronyms\nAI A rtificial Intelligence\nECG E lectrocardio graphy\nEDA E lectro dermal Activity\nEEG E lectro encephalo graphy\nEMG E lectro myography\nFACS F acial Action Coding System\nFLOPS Fl oating-point Operations perSecond\nfMRI F unctional Magnetic Resonance Imaging\nfNIRS F unctional Near-Infrared Spectroscopy\nGSR G alvanic SkinResponse\nML M achine Learning\nNIPS N eonatal/ Infant PainScale\nNIRS N ear-Infrared Spectroscopy\nPPG P hotoplethysmo graphy\nPSPI P rkachin and Solomon PainIntensity Scale\nRGB R edGreen Blue\nSpO2 S aturation of Peripheral Oxygen\nV AS V isual Analog Scale\nVRS V erbal Rating Scale\n209", "Impact of Inadequate Pain Management": ". . . . . . . . . . . . . . . . . . . 19\n2.7", "Pain Measurement Scales and Metrics": ". . . . . . . . . . . . . . . . . . . . 20\n3", "Automatic Pain Assessment\u2013A Literature Review": "21\n3.1", "Modalities and Hardware for Automatic Pain Assessment": ". . . . . . . . . . 23\n3.3", "Pain Databases": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1", "The UNBC-McMaster Shoulder Pain Expression Archive Database": "24\n3.3.2", "The Biopotential and Video (BioVid) Heat Pain Database": ". . . . . . 25\n3.3.3", "The EmoPain Database": ". . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain\nDatabase . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . 49\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research\nDirections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4 Demographic Variables: Their Role and Impact 53\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5 Optimization: Balancing Efficiency and Performance 75\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n6 Synthetic Data: The Role of Thermal Imaging 103\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7 General-Purpose Models 117\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n8 Conclusions, Perspectives and Future Work 157\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nBibliography 163\nAppendix 201\nAcronyms 209List of Figures\n2.1 The spinothalamic tract (STT) [43]. Pain, temperature, and some touch affer-\nents end in the posterior horn, where second-order fibers cross the midline\nto form the spinothalamic tract, ascending to the thalamus and projecting to\nvarious cortical areas. Along the way, collaterals connect to the reticular for-\nmation. Due to the rostral inclination of fibers in Lissauer\u2019s tract, cordotomy\nmust be performed several segments above the pain level for effective relief. 12\n2.2 Pain classification [48]: (A)Nociceptive pain , which results from detecting\npotentially harmful stimuli and serves a protective function. (B)Inflamma-\ntory pain is linked to tissue damage and immune cell infiltration, increas-\ning pain sensitivity during healing. (C)Pathological pain is a disease state\ncaused by either nervous system damage (neuropathic) or abnormal nervous\nsystem function (dysfunctional). . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 The number of studies utilizing these specific datasets. Note that various\nstudies used multiple datasets to conduct their experiments. . . . . . . . . . 25\n4.1 The PQRST waveform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure. . 55\n4.3 The signal preprocessing using the Pan-Tompkins algorithm. . . . . . . . . 57\n4.4 Results for the Gender Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Results for the Age Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.6 Results for the Gender-Age Scheme . . . . . . . . . . . . . . . . . . . . . . 64\n4.7 The proposed MTL network: The sizes of the extracted vectors for the net-\nwork are as follows: for the Pain classifier, n\u02c61, where nis the number of\npain estimation tasks ( e.g.,2for binary classification, 5for multi-class clas-\nsification); for the Age classifier, 36\u02c61, where 36represents the possible\nage values of the subjects; for the Gender classifier, 2\u02c61, corresponding to\nthe two possible gender categories ( i.e., males and females). . . . . . . . . 66\n4.8 Results for the proposed Schemes. . . . . . . . . . . . . . . . . . . . . . . 69\n4.9 Comparison of performances utilizing various neural networks approaches. 72\nxix5.1 The application of face alignment illustrates landmarks in 2D (left) and 3D\n(right) space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 An overview of our proposed transformer-based framework for automatic\npain assessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 The impact of the number of input frames on accuracy (left) and on runtime\nin milliseconds (right). Runtime calculated during inference on a NVIDIA\nRTX-3090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Relevance Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5 Outline of the proposed framework. . . . . . . . . . . . . . . . . . . . . . 86\n5.6 Comparison of mean accuracy and inference period for unimodal and multi-\nmodal strategies across NP versus P 4and MC tasks. The diagram adopts a\ndual-y-axis configuration\u2014accuracy measurements on the left and time met-\nrics on the right\u2014to outline the balance between performance efficacy and\ncomputational load, categorizing the methodologies along the x-axis. . . . . 98\n5.7 Regions highlighted in yellow and red denote areas of significant attention.\n(a) (1strow) Sequence of original frames. (2ndrow) Derived from the\nSpatial-Module after initial stage pretraining. (3rdrow) Derived from the\nSpatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module trained on the BioVid dataset. (b) (1strow) Derived from\ntheTemporal-Module incorporating video embeddings. (2ndrow) Derived\nfrom the Temporal-Module with heart rate embeddings. (3rdrow) Derived\nfrom the Temporal-Module using a combined embedding of video and heart\nrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.1 Illustration of the procedure for creating thermal images, featuring the archi-\ntecture of the Generator G(Encoder, mid-stage ResNet, Decoder), and the\nDiscriminator D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2 Representation of the proposed framework, illustrating its components and\ntheir main functions: (a)The Vision-MLP module, tasked with extracting\nfeature embeddings from video frames. (b)TheToken-Mixer , an important\nsub-module of Vision-MLP , generates the wave representation for the tokens.\n(c)The Channel-Mixer , a crucial sub-module within Vision-MLP .(d)The\nMLP, a core component of the Channel-Mixer .(e)The fusion procedure\nthat combines RGB and synthetic thermal embeddings, succeeded by the\nTransformer module, which conducts the final pain assessment. . . . . . . . 107\n6.3 Gradual blurring of RGB and synthetic thermal facial images: a series dis-\nplaying varying levels of Gaussian blur applied, with kernel sizes gradually\nincreased from k\u201c0(no blur) to k\u201c191(extensively blurred). . . . . . . 1136.4 Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain)\nclasses in RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191\n(heavily blurred). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1 PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer hav-\ning varying depths, showcasing how token resolution decreases at each stage;\n(b)Composition of the Token-Mixer module, featuring elements like depth-\nwise convolution (DWConv) and batch normalization; (c)Architecture of the\nFeed-Forward Network (FFN) within the Token-Mixer ;(d)The Cascaded\nAttention mechanism implemented across multiple heads, illustrating how\noutputs from preceding heads are incorporated to refine the self-attention\nprocess, culminating in the final output projection; (e)Configuration of the\nproposed multimodal pipeline, employing videos and fNIRS. The embed-\ndings from PainViT\u20131 are represented as waveform diagrams, which are\nmerged into a single diagram that illustrates both modalities before entering\nPainViT\u20132 for final pain evaluation. . . . . . . . . . . . . . . . . . . . . . . 151\n7.2 Waveform illustrations for various data types: (a)original fNIRS signal,\n(b)video embedding derived from PainViT\u20131 , and (c)fNIRS embedding\nobtained from PainViT\u20131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n7.3 Attention maps from the PainViT\u20132 . . . . . . . . . . . . . . . . . . . . . . 152\n7.4 Overview of primary models and their components outlined in this research:\n(a)PainFormer is structured hierarchically into four stages, incorporating\nSpectral andSelf-Attention Layers to extract embeddings from the inputs;\n(b)The Spectral Layer , a key element of PainFormer , uses FFT to ana-\nlyze frequency-specific data along with a learnable filter Kto highlight\ncritical frequencies; (c)The Self-Attention Layer , crucial for PainFormer ,\nenables parallel processing of features and their interconnections; (d)The\nEmbedding-Mixer , employing both cross and self-attention mechanisms, func-\ntions as the component for the final classification of embeddings in pain as-\nsessment; (e)TheVideo-Encoder , designed for compact and efficient encod-\ning, compresses video data into a reduced dimensional form; (f)TheMLP-1\nis part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)TheMLP-3 configuration is integrated into the Embedding-Mixer\nandVideo-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n7.5 Examples of different vision modalities in frame samples: (a)RGB frame,\n(b)synthetic thermal frame, and (c)depth estimation frame. . . . . . . . . 153\n7.6 Examples of different visual representations for biosignals: (a)waveform ,\n(b)spectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD . . 1547.7 An overview of the presented framework. PainFormer , the foundational\nmodel, excels in deriving high-quality embeddings from a diverse array of\nbehavioral and physiological modalities. The evaluation of RGB, thermal,\nand depth videos, alongside various representations of ECG, EMG, GSR,\nand fNIRS such as waveforms and spectrograms, underscores the rich infor-\nmation captured within these embeddings. Leveraging the embeddings from\nPainFormer facilitates the creation of various and diverse unimodal and mul-\ntimodal pipelines designed for the pain assessment task. Each pipeline can\nbe customized to suit the specific modalities involved, dataset characteristics,\nand the demands of the intended application or clinical setting. Our assess-\nments included the development and implementation of several pipelines\nin both unimodal and multimodal contexts, achieving leading-edge results\nacross various modalities and data representations. . . . . . . . . . . . . . 154\n7.8 Attention maps from the PainFormer :(a)(1strow) frames from RGB, ther-\nmal, and depth video modalities; (a)(2ndrow) corresponding attention maps;\n(b)(1strow) attention maps for ECG and EMG; (b)(2ndrow) attention maps\nfor EDA and fNIRS modalities. . . . . . . . . . . . . . . . . . . . . . . . . 155\n1 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n2 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n3 Additional attention maps from the PainViT\u20132 (refer to Section 7.2). . . . . 207List of Tables\n3.1 Most commonly utilized pain databases. . . . . . . . . . . . . . . . . . . . 24\n3.2 Vision-based studies with static analysis. . . . . . . . . . . . . . . . . . . . 32\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 33\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 34\n3.3 Vision-based studies with temporal utilization. . . . . . . . . . . . . . . . . 39\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 40\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 41\n3.4 Touch sensor-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Audio-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6 Multimodal-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.7 Interpretation approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1 Results for the Basic Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Results for the Gender Scheme (1). . . . . . . . . . . . . . . . . . . . . . . 60\n4.3 Results for the Age Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Results for the Gender-Age Scheme (Males) (1). . . . . . . . . . . . . . . . 62\n4.5 Results for the Gender-Age Scheme (Females) (1). . . . . . . . . . . . . . . 63\n4.6 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (1). 63\n4.7 Hyper-parameters used in our approach. . . . . . . . . . . . . . . . . . . . 65\n4.8 Results for the Basic Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . 68\n4.9 Results for the Gender Scheme (2). . . . . . . . . . . . . . . . . . . . . . . 68\n4.10 Results for the Age Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.11 Results for the Gender-Age Scheme (2). . . . . . . . . . . . . . . . . . . . 68\n4.12 Comparison of results adopting the feature augmentation approach. . . . . . 70\n4.13 Comparison of results adopting the MT-NN approach. . . . . . . . . . . . . 71\n4.14 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2). 72\n5.1 Training details for the automatic pain assessment. . . . . . . . . . . . . . 79\n5.2 Results on the pain estimation tasks. . . . . . . . . . . . . . . . . . . . . . 81\n5.3 Results for the pain estimation tasks using various numbers of input frames. 82\n5.4 Comparison of studies utilizing BioVid , RGB videos, and LOSO validation. 84\n5.5 Datasets utilized for the pre-training process of the framework. . . . . . . . 91\nxxiii5.6 Training details for the automatic pain assessment. . . . . . . . . . . . . . 92\n5.7 Results utilizing the video modality. . . . . . . . . . . . . . . . . . . . . . 93\n5.8 Results utilizing the heart rate modality. . . . . . . . . . . . . . . . . . . . 95\n5.9 Results utilizing the video &the heart rate modality. . . . . . . . . . . . . . 95\n5.10 Comparison of studies utilizing BioVid &LOSO validation, reported on ac-\ncuracy %. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.11 Module parameters and computational cost in FLOPS for the proposed frame-\nwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.1 Datasets utilized for the pretraining process of the framework. . . . . . . . 110\n6.2 Training specifications, and number of parameters and FLOPS for each mod-\nule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.3 Results utilizing the RGB video. . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Results utilizing the synthetic thermal video. . . . . . . . . . . . . . . . . . 112\n6.5 Results utilizing the RGB &the synthetic thermal video. . . . . . . . . . . . 113\n6.6 Results utilizing the fusion of RGB &synthetic thermal video. . . . . . . . 115\n6.7 Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.116\n6.8 Comparison with the MIntPAIN dataset. . . . . . . . . . . . . . . . . . . . 116\n7.1 Number of parameters and FLOPS for the components of the proposed Twins-\nPainViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.2 Datasets utilized for the pretraining process of the framework. . . . . . . . 123\n7.3 Training details for the automatic pain assessment. . . . . . . . . . . . . . 124\n7.4 Results utilizing the video modality & Addition method. . . . . . . . . . . . 125\n7.5 Results utilizing the video modality & Concatenation method. . . . . . . . . 125\n7.6 Results utilizing the HbR & Addition method. . . . . . . . . . . . . . . . . 126\n7.7 Results utilizing the HbR & Concatenation method. . . . . . . . . . . . . . 126\n7.8 Results utilizing the HbO & Addition method. . . . . . . . . . . . . . . . . 126\n7.9 Results utilizing the HbO & Concatenation method. . . . . . . . . . . . . . 127\n7.10 Results utilizing the HbR, HbO & Addition method. . . . . . . . . . . . . . 127\n7.11 Results utilizing the videos, HbO & Addition method. . . . . . . . . . . . . 127\n7.12 Results utilizing the videos, HbO & Single Diagram method. . . . . . . . . 128\n7.13 Comparison with the validation baseline provided by the AI4PAIN challenge\norganizers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.14 Number of parameters and FLOPS for the modules of the proposed framework.130\n7.15 Details of the PainFormer\u2019s architecture. . . . . . . . . . . . . . . . . . . . 132\n7.16 Datasets utilized for the multitask learning-based pretraining process of the\nframework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347.17 Training details of the proposed framework. . . . . . . . . . . . . . . . . . 135\n7.18 Results utilizing the video modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.19 Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.20 Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.21 Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.22 Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and\nF1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.23 Results on the validation set of AI4Pain dataset, multilevel classification task,\nreported on accuracy, recall and F1 score. . . . . . . . . . . . . . . . . . . 144\n7.24 Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.25 Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4\ntask and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.26 Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs.\nP4task and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.27 Comparison of studies on the testing set of AI4Pain dataset. . . . . . . . . . 148\n1 Results utilizing the video modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n2 Results utilizing the heart rate modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n3 Results utilizing the video &the heart rate modality reported on precision,\nrecall and F1 score (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . 202\n4 Results utilizing the RGB video modality, reported on recall and F1 score\n(refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n5 Results utilizing the synthetic thermal video modality, reported on recall and\nF1 score (refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . 203\n6 Results utilizing the fusion of RGB &synthetic thermal video modality, re-\nported on recall and F1 score (refer to Section 6.2). . . . . . . . . . . . . . 204\n7 Results of the proposed approaches, reported on macro-averaged precision,\nrecall and F1 score (refer to Section 7.2). . . . . . . . . . . . . . . . . . . . 204Chapter 1\nIntroduction\nContents\n1.1 Context and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Scope and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions \u2013 Peer-review Publications . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.1 Context and Motivation\nPain is a complex and deeply personal experience that is subjective by nature. Traditionally,\nit has been described in terms of its sensory dimension [1]. However, extensive research\nhas highlighted the importance of affective, cognitive and social aspects in shaping this ex-\nperience [2]. Studies have explored physiological, psychological, and socio-environmental\nfactors that contribute to the experience of pain. It is understood as a result of biological evo-\nlution and as influenced by psychological and social factors. As Ridell et al. [3] noted, \u201cPain\nis a synthesis\u2013a sum that is greater than its parts. \u201d The brain\u2019s ability to alter the perception\nof sensory inputs through the interplay of emotion, cognition, and social processes is signifi-\ncant. Although natural systems establish the initial biological framework for pain perception,\nthis structure is highly adaptable, particularly in humans. Throughout a person\u2019s life, both\nbiological developments and personal experiences significantly reshape this framework.\nA key question driving pain research across biological, psychological, and computational\nfields is why this topic of pain is meaningful and important. This question also forms the\nbasis for initiating this thesis, highlighting the broader relevance of studying pain. Williams\nand Kappesser [4] provide a compelling explanation, stating, \u201cWe care because we are wired\nto care: to attend to other people\u2019s expression of pain and to understand its meaning; to feel\ndistress in relation to their distress; and to be motivated to reduce their distress, and ours,\nif we are able to do so. \u201d This highlights the intrinsic human response to empathize and\n12 CHAPTER 1. INTRODUCTION\nalleviate pain, underlining the fundamental importance of this research area. Indeed, from a\nDarwinian perspective, pain serves a crucial role. The manifestation of pain in humans and\nthe reactions it elicits are examined through an evolutionary lens. Pain facilitates recovery by\npromoting responses to harmful stimuli and behaviors that demonstrate the adverse nature\nof painful experiences, common among animals. Specifically, the facial expression of pain,\nwhich communicates discomfort directly to those nearby, is universally recognized across\ndifferent ages, ethnicities, roles, and relationships. Evidence from healed major fractures\n[5, 6] suggests that injured members of hominid groups were not left to fend for themselves\nbut were supported through their recovery, indicating the fundamental importance of pain\nexpression in our evolutionary history.\nPain is a widespread health concern globally, affecting up to 30% of the adult popula-\ntion [7] and between 83% and93% of elderly adults in residential care [8]. The Global Bur-\nden of Disease (GBD) study identifies pain as the primary cause of years lived with disability\n(YLD) [9], with major contributors including chronic back pain, musculoskeletal disorders,\nand neck pain [10]. Pain impacts individuals and poses significant clinical, economic, and\nsocial challenges. In the United States, the economic and healthcare costs related to pain\ndue to reduced work productivity range from $560 to$635 billion annually, surpassing the\ncosts associated with heart disease, cancer, and diabetes combined [11]. In Europe, chronic\npain\u2019s direct healthcare costs and indirect socioeconomic impacts account for 3%to10% of\nthe GDP [12]. In Australia, the average annual cost for individuals among the 15.4%living\nwith chronic pain ranges from AU $22,588to AU $42,979, including non-financial costs [13].\nBeyond direct effects on health, pain contributes to a range of adverse outcomes, such as opi-\noid dependency, drug overuse, addiction, declining social relationships, and psychological\ndisorders [14]. In the last two decades, prescription opioid use has surged in the United\nStates, where overdose deaths have increased more than fourfold from 1999 to2016 [15].\nAdditionally, side effects from these opioids, like lethargy, depression, anxiety, and nausea,\nseverely impact workforce productivity and overall life quality [16].\nAccurate pain assessment is crucial for early diagnosis, disease progression monitoring,\nand treatment effectiveness evaluation, particularly in managing chronic pain [17]. This criti-\ncal role has resulted in pain being recognized as \u201cthe fifth vital sign\u201d in nursing literature [18].\nPain assessment is also fundamental in physiotherapy, where therapists apply external stim-\nuli and need to gauge the patient\u2019s pain levels accurately [19]. Objective evaluation of pain is\nessential to provide appropriate care, especially for vulnerable populations who may not be\nable to communicate their pain effectively, such as infants, young children, individuals with\nmental health issues, and the elderly. Various methods are used for pain assessment, with\nself-reporting\u2013where individuals describe their pain experiences\u2013considered the gold stan-\ndard [20]. Pain evaluation methods in clinical environments include quantifiable measures\nlike the Numeric Pain Rating Scale (NPRS), Visual Analogue Scale (V AS), and quantitative1.1. CONTEXT AND MOTIVATION 3\nsensory testing techniques such as the pressure pain detection threshold (PPDT) [21]. Behav-\nioral indicators are also crucial and include facial expressions ( e.g., grimacing, open mouth,\nlifted eyebrows), vocalizations (like crying, moaning, or screaming), and movements of the\nbody and head [22]. Physiological measures such as electrocardiography (ECG), electromyo-\ngraphy (EMG), galvanic skin responses (GSR), and respiration rates further contribute to\nunderstanding pain\u2019s physiological aspects [17]. Additionally, brain monitoring techniques\nlike near-infrared spectroscopy (fNIRS) have effectively detected changes in hemodynamic\nactivity associated with pain stimuli [23].\nCaregivers and family members often determine the presence or absence of pain in pa-\ntients by observing their behavioral or physiological responses [17]. However, accurately\nassessing pain poses a significant challenge for clinicians [24], especially with nonverbal\npatients such as the elderly, who may have reduced expressive abilities or may be reluctant\nto communicate pain [25]. Extensive research indicates that pain manifestations vary signif-\nicantly across different genders and ages, adding to the complexity of its assessment [26].\nFurther complicating the assessment process are the heightened workload and fatigue ex-\nperienced by nursing staff due to the demands of patient monitoring [27]. Technological\nsolutions are necessary for continuous patient monitoring. Nevertheless, concerns remain\nabout the objectivity and accuracy of these observations, as inadequately trained or biased\nobservers may struggle to assess pain [28] accurately. Even among trained observers, in-\nterpretations of behaviors can vary [22], and social and interpersonal dynamics can signif-\nicantly affect the pain assessment process, influencing both the evaluators\u2019 judgments and\nthe patients\u2019 expressions of pain [29]. Additionally, the presence of an observer can lead pa-\ntients to modify their behavior [30], and expressing pain through scales and measurements\ncan be challenging [31]. While self-reporting is used because pain is inherently subjective,\nrelying solely on a one-dimensional pain score fails to capture this complex phenomenon,\noften leading to inadequate pain management [32].\nGiven the challenges described above, scientific computing (SC) researchers have fo-\ncused on developing models and algorithms to enhance automatic pain recognition systems\nover the last two decades. Their goal is to accurately determine the presence and intensity\nof pain by analyzing physiological and behavioral indicators. Adopting deep learning and\nartificial intelligence (AI) techniques has expanded these automatic methods, designed to\ninterpret the complex and varied nature of pain [17]. Numerous studies have underscored\nthe effectiveness of automated systems that utilize behavioral or physiological modalities\nfor pain assessment [33]. Sario et al. [34] have shown the capability of these systems to\naccurately recognize pain through facial expressions, proving their utility in clinical envi-\nronments. Multimodal sensing has shown particular promise, offering enhanced accuracy\nin pain detection systems [22]. Furthermore, including temporal aspects in these modalities\nhas proven to significantly improve the accuracy of pain assessments [17].4 CHAPTER 1. INTRODUCTION\n1.2 Scope and Challenges\nAlthough considerable research has been conducted on automatic pain assessment, studies\nhave yet to explore factors like demographics and social aspects from a computational angle.\nFurthermore, despite the existence of deep learning-based methods, the approaches we ob-\nserve are often outdated and repeatedly recycled. For these reasons, we aimed to address two\nissues by (i)attempting to evaluate the social or demographic context, which significantly\nimpacts and influences pain sensation and perception, and (ii)introducing innovative deep\nlearning methods inspired by the latest developments in AI and generative AI literature. We\nbelieve these approaches can forge new paths in pain research, enhance the accuracy of rec-\nognizing this complex phenomenon, and, ultimately, be adopted in real-world scenarios to\nassist those in need. Additionally, (iii)recognizing the skepticism towards new technologies\namong clinicians and the general public, especially regarding the limited understanding of\nhow deep learning models function, we have devoted a portion of our research to interpret-\ning these models to offer some level of explanation and help the adoption process of them in\nclinical settings.\nNevertheless, this thesis initially faced challenges related to our objectives and goals as\nthe research progressed. The availability of pain datasets (to be discussed in the next chapter)\nis limited. Only a few datasets are available, and crucially, they are limited in size. This re-\nstriction poses a significant challenge for developing deep learning models, which typically\nrequire a large volume of data. In automatic pain assessment, researchers who develop deep\nlearning methods typically confront a decision: either train their models from scratch, which\ncan introduce performance limitations, or employ pre-trained models. These pre-trained\nmodels are generally trained on broadly available image datasets that include a variety of\nsubjects like animals and objects, or they rely on older architectures that were trained explic-\nitly on facial datasets. In this thesis, we addressed these issues by independently pre-training\nour deep-learning models using diverse datasets related explicitly to human facial images\nand biosignals. This strategy allowed us to design specific architectures to meet our unique\nneeds for each scenario, free from the constraints of relying on models developed and trained\nby others. Furthermore, we explored and evaluated several pre-training techniques to assess\ntheir effectiveness in pain assessment applications.\nRegarding, our objective to explore methods that utilize various modalities individually\nand in combination in a multimodal manner further constrains our dataset options. More-\nover, as previously outlined, our interest in the sociodemographic aspects of pain necessitates\ndatasets that include this information type, intensifying our challenges. For these reasons,\nthis thesis focuses specifically on examining the impact of age and gender on pain. In addi-\ntion, led us to utilize two pain datasets that most closely match the characteristics necessary\nfor our research, particularly in terms of demographic elements and multimodality.1.3. CONTRIBUTIONS \u2013 PEER-REVIEW PUBLICATIONS 5\n1.3 Contributions \u2013 Peer-review Publications\nThis section outlines the publications and projects produced during the Ph.D. research on\nautomatic pain assessment, where I was the first author.\n1.Automatic assessment of pain based on deep learning methods: A systematic re-\nview [17]\nThis systematic literature review (SLR) was conducted at the start of this Ph.D. re-\nsearch. This paper aims to explore the surge in recent years of deep learning algorithms\nadopted by researchers to encode the multidimensional nature of pain into meaning-\nful features. Specifically, this systematic review examines the models, methods, and\ndata types used to establish the foundation for deep learning-based automatic pain\nassessment systems. It identified relevant original studies from digital libraries such\nasScopus ,IEEE Xplore , and ACM Digital Library , following defined inclusion and\nexclusion criteria for studies published until December 2021 . The findings highlight\nthe critical role of multimodal approaches in automatic pain estimation, particularly\nin clinical environments, and emphasize the substantial gains observed with the inclu-\nsion of temporal exploitation of modalities. The review also recommends selecting\nhigh-performing deep learning architectures and methods, encouraging the adoption\nof robust evaluation protocols and interpretability techniques to deliver reliable and\nunderstandable outcomes. Additionally, it underscores the current limitations of exist-\ning pain databases in adequately supporting the development, validation, and practical\napplication of deep learning models as decision-support tools in real-world settings.\nFurthermore, we believe this paper is valuable not only for this Ph.D. project but also\nfor other practitioners and researchers in the field.\n2.Automatic Pain Intensity Estimation based on Electrocardiogram and Demographic\nFactors [35]\nThis study investigated the relationship between gender, age, and pain sensation and\ntheir effects on the automatic pain assessment process. By analyzing physiological\nsignals, particularly electrocardiography (ECG), we estimated pain intensity and ex-\namined the influence of these demographic factors. Utilizing the Pan-Tompkins algo-\nrithm for feature extraction and applying well-established classification methods, we\nexplored the correlation between gender, age, and pain manifestation.\n3.Multi-task Neural Networks for Pain Intensity Estimation Using Electrocardiogram\nand Demographic Factors [36]\nInspired by the previous study, this research further explored the influence of gender\nand age on pain perception. In this work, we analyze electrocardiography signals\nto uncover variations in pain perception across different demographic groups. We6 CHAPTER 1. INTRODUCTION\nleveraged these insights by developing a novel multi-task neural network for automatic\npain estimation, incorporating age and gender data for each individual. The study\ndemonstrated the advantages of this approach compared to other existing methods.\n4.A Full Transformer-based Framework for Automatic Pain Estimation using Videos\n[37]\nThis study introduced an innovative full transformer-based framework featuring a Trans-\nformer in Transformer (TNT) model combined with cross-attention and self-attention\nblocks. We achieved state-of-the-art performance using video data from the BioVid\ndatabase, demonstrating the model\u2019s effectiveness, efficiency, and strong generaliza-\ntion across primary pain estimation tasks.\n5.Multimodal automatic assessment of acute pain through facial videos and heart rate\nsignals utilizing transformer-based architectures [38]\nThis study presented a multimodal automatic acute pain assessment framework, inte-\ngrating video and heart rate signals. The framework consists of four key modules:\ntheSpatial Module , which extracts embeddings from videos; the Heart Rate Encoder ,\nwhich maps heart rate signals into a higher-dimensional space; the AugmNet , which\ngenerates learning-based augmentations in the latent space; and the Temporal Mod-\nule, which leverages the video and heart rate embeddings for the final assessment.\nThe Spatial Module undergoes a two-stage pre-training process: first, it learns uni-\nversal facial features through face recognition, followed by emotion recognition in a\nmultitask learning approach, enabling high-quality embeddings for pain assessment.\nExperiments with facial videos and heart rate data extracted from electrocardiograms\nin the BioVid database, alongside direct comparisons to 29studies, demonstrate state-\nof-the-art performance in unimodal and multimodal settings while maintaining high\nefficiency. In the multimodal setting, the framework achieved 82.74% accuracy for bi-\nnary pain classification and 39.77% for multi-level pain classification, using only 9.62\nmillion parameters across the entire framework.\n6.Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-\nMLP Architecture [39]\nThis paper introduced synthetic thermal videos generated by Generative Adversarial\nNetworks , which are integrated into the pain recognition process to assess their effec-\ntiveness. The framework employs a Vision-MLP andTransformer -based module, lever-\naging RBG and synthetic thermal videos in unimodal and multimodal settings. Exper-\niments conducted using facial videos from the BioVid database highlighted synthetic\nthermal videos\u2019 effectiveness and showcased their potential benefits in pain recognition\ntasks.1.4. THESIS OUTLINE 7\n7.Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for\nMultimodal Automatic Pain Assessment using Facial Videos and fNIRS [40]\nThis study was submitted to the First Multimodal Sensing Grand Challenge for Next-\nGen Pain Assessment (AI4PAIN) . The proposed multimodal framework leverages fa-\ncial videos and fNIRS, offering a modality-agnostic approach that eliminates the need\nfor domain-specific models. Utilizing a dual ViT configuration and waveform repre-\nsentations for both fNIRS and the extracted embeddings from the two modalities, the\nmethod demonstrates its effectiveness, achieving an accuracy of 46.76% in the multi-\nlevel pain assessment task.\n8.PainFormer: a Vision Foundation Model for Automatic Pain Assessment [41]1\nThis study introduces PainFormer , a vision foundation model built on multi-task learn-\ning principles and trained across 14distinct tasks and datasets comprising 10.9million\nsamples. As an embedding extractor for various input modalities, PainFormer provides\nfeature representations to the Embedding-Mixer , a transformer-based module respon-\nsible for conducting the final pain assessment. Extensive experimentation using both\nbehavioral modalities\u2013including RGB, synthetic thermal, and estimated depth videos\u2013\nand physiological modalities like ECG, EMG, GSR, and fNIRS revealed PainFormer \u2019s\nability to extract high-quality embeddings from diverse inputs. Tested on the BioVid\nandAI4Pain datasets and compared to more than 60existing methods, the framework\ndemonstrated state-of-the-art performance in unimodal and multimodal settings, posi-\ntioning itself as a step toward developing general-purpose models for automated pain\nevaluation.\n1.4 Thesis Outline\nThe dissertation is organized into the following chapters:\nChapter 2 introduces the foundational concepts of pain from biological, psychological, and\nclinical perspectives.\nChapter 3 reviews existing literature on automatic pain assessment using deep learning\nmethods and details the pain datasets used.\nChapter 4 outlines and proposes methods for evaluating demographic variables, their uti-\nlization, and their integration into an automatic pain assessment framework.\nChapter 5 discusses methods that utilize video and wearable device data, exploring the\ntrade-offs between efficiency and accuracy. It also proposes efficient, fast, effective models\nsuitable for real-world applications.\nChapter 6 explores synthetic data in pain assessment and introduces synthetic thermal im-\n1Under Review8 CHAPTER 1. INTRODUCTION\nagery techniques to enhance performance in automatic pain recognition.\nChapter 7 discusses general-purpose models, introduces a modality-agnostic framework,\nand presents the first foundation model used in automatic pain assessment.\nChapter 8 concludes the thesis with a final discussion, offering perspectives and ideas for\nfuture research in automatic pain assessment.Chapter 2\nClinical Pain Assessment\nContents\n2.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . . . . 20\n2.1 Chapter Overview\nThis chapter provides an anatomical and physiological overview of pain, focusing on the\nmechanisms responsible for generating, transmitting, processing, and interpreting pain sig-\nnals. It examines the various types of pain and explores the actions and expressions typically\nassociated with pain. Additionally, it reviews current pain assessment methods used in clin-\nical settings for adults, children, and newborns. The chapter also discusses developing and\nvalidating existing clinical pain assessment tools. This foundational knowledge is essen-\ntial for understanding the development and validation of computer-assisted pain assessment\n910 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nmethods discussed in later chapters. Finally, it highlights the challenges faced in clinical\npain assessment and underscores the need for automated pain assessment techniques.\n2.2 Biology of Pain\nPain, according to the International Association for the Study of Pain (IASP) [42], is \u201can\nunpleasant sensory and emotional experience associated with actual or potential tissue dam-\nage, or described in terms of such damage\u201d. Biologically, pain is an undesirable sensation\noriginating from the peripheral nervous system. Its fundamental function is to engage sen-\nsory neurons, notifying the organism of potential harm and playing a vital role in recognizing\nand responding to threats [43].\nThe transmission of a noxious stimulus from the periphery to the central nervous sys-\ntem involves a complex pathway through the spinal cord, resulting in the physical sensation\nof pain and a corresponding emotional response and memory. This process culminates in\nthe perception of pain. The initial stage of pain processing occurs when a stimulus at noci-\nceptive sensory fibers in the periphery is converted into an action potential. A nerve signal\nis generated if the stimulus is strong enough to surpass the action potential threshold [44].\nThis signal travels along the primary afferent fiber toward the central nervous system. As the\nstimulus intensity grows, more nerve fibers and areas of the nervous system are engaged [44].\nDue to their branching nature, primary afferent fibers typically relay information from sev-\neral pain receptors. These fibers and their receptors comprise a sensory unit, which gathers\ndata from a specific receptive field [44]. When receptive fields are larger and overlap with\nnearby fields, it becomes more challenging for the sensory system to locate the source of pain\naccurately. The primary afferent neuron is a pseudounipolar neuron that splits into a periph-\neral and central axon. The cell bodies of these neurons are located in the peripheral nervous\nsystem, within the posterior or cranial root ganglia. The peripheral axon extends to the skin,\nmuscles, tendons, or joints, branching into terminal fibers that connect with somatosensory\nreceptors. In contrast, the central axon leads to the central nervous system [45].\nPeripheral somatosensory fibers are categorized into three main groups. The first group\nincludes A\u00b4\u03b1,A\u00b4\u03b2,A\u00b4\u03b3fibers, large, myelinated fibers that rapidly conduct sig-\nnals [46]. These fibers involve touch and proprioception but are not associated with pain\nperception. The second group consists of A\u00b4\u03b4fibers, which are smaller and slower con-\nducting. Certain A\u00b4\u03b4fibers play a key role in pain sensation, with some responding only\nto intense mechanical stimuli and others reacting to noxious and non-noxious heat. The\nthird group comprises Cfibers, which are small, unmyelinated, and conduct signals very\nslowly. Most Cfibers are polymodal for pain perception, responding to various noxious\nmechanical, thermal, and chemical stimuli. These fibers are mainly linked to burning pain\nsensations [43]. The sensation of pain, known as nociception, is primarily facilitated by2.3. CLASSIFICATION AND CHARACTERISTICS OF PAIN 11\nvarious intracellular and extracellular molecular messengers. When activated by a specific\nstimulus, nociceptors relay information through glutamate, an excitatory neurotransmitter.\nAdditionally, inflammatory mediators are released at the injury site, further stimulating no-\nciceptor activation by releasing chemicals such as neurotransmitters ( e.g., serotonin), lipids\n(e.g., prostaglandins), peptides ( e.g., bradykinin), and neurotrophins ( e.g., nerve growth fac-\ntor) [46]. There are ascending tracts responsible for transmitting sensory information from\nthe periphery to the central nervous system. Fibers that convey two-point discrimination, tac-\ntile information, pressure, vibration, and proprioception ascend via the dorsal column of the\nspinal cord, forming the gracile and cuneate fasciculi. Fibers transmitting pain, temperature,\nand crude touch from somatic and visceral structures travel through the lateral spinothalamic\ntract. The anterior spinothalamic tract also transmits pain, temperature, and touch informa-\ntion to the brainstem and diencephalon (Figure 2.1) [47].\n2.3 Classification and Characteristics of Pain\nAccording to neurobiologist Clifford Woolf [48], pain can be classified into three categories\nbased on its function and characteristics: nociceptive ,inflammatory , and pathological pain.\nThese classes and their respective functions are illustrated in Figure 2.2.\nNociceptive pain (refer to Figure. 2.2(A)), arising from tissue damage, is a high-threshold\npain that activates only in response to intense stimuli [49], serving as a vital warning signal\nto the body. The neurobiological system responsible for nociceptive pain evolved from the\nability of even the most primitive nervous systems to detect impending or actual tissue dam-\nage caused by external stimuli. Its protective role requires immediate attention and action,\nachieved through the withdrawal reflex it initiates, the unpleasant sensation it produces, and\nthe emotional distress it triggers. Nociceptive pain demands avoidance in the present mo-\nment, and when activated, it overrides most other neural processes [48].\nInflammatory pain (refer to Figure. 2.2(B)) is also protective and adaptive, increasing\nsensory sensitivity following tissue damage to aid healing by discouraging movement and\ncontact with the injured area. This heightened sensitivity, or tenderness, helps prevent further\nharm and supports recovery, as seen after surgical wounds or inflamed joints where normally\nnon-painful stimuli now cause pain. It is triggered by immune system activation in response\nto tissue injury or infection. Despite its adaptive role, this pain often needs to be alleviated in\npatients with persistent inflammation, such as in rheumatoid arthritis or severe injuries [48].\nPathological pain (Figure. 2.2(C)) is maladaptive, arising from abnormal nervous sys-\ntem functioning and not serving a protective role. Unlike nociceptive and inflammatory pain,\npathological pain is a disease state of the nervous system itself. It may occur following\nnerve damage (neuropathic pain) or in conditions without apparent damage or inflammation\n(dysfunction l pain). Examples of dysfunctional pain include fibromyalgia, irritable bowel12 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFIGURE\n 2: Spinothalamic tract.\nPain, temperature, and some touch and pressure afferents end in the posterior horn. Second- or\nhigher-order fibers cross the midline, form the spinothalamic tract, and ascend to the ventral\nposterolateral (VPL) nucleus of the thalamus (and also to other thalamic nuclei not shown).\nThalamic cells then project to the somatosensory cortex of the postcentral gyrus, to the insula,\nand to other cortical areas (also not shown). Along their course through the brainstem,\nspinothalamic fibers give off many collaterals to the reticular formation (RF). The inset to the left\nshows the lamination of fibers in the posterior columns and the spinothalamic tract in a leg-\nlower trunk-upper trunk-arm sequence. The inset to the right shows the longitudinal formation\nof the spinothalamic tract. Primary afferents ascend several segments in Lissauer\u02bcs tract before\nall their branches terminate; fibers crossing to join the spinothalamic tract do so with a rostral\ninclination. As a result, a cordotomy incision at any given level would spare most of the\ninformation entering the contralateral side of the spinal cord at that level, and to be effective,\nthe incision must be made several segments rostral to the highest dermatomal level of pain.\n2017 Khalid et al. Cureus 9(10): e1754. DOI 10.7759/cureus.1754\n5\n of \n14\nFigure 2.1: The spinothalamic tract (STT) [43]. Pain, temperature, and some touch afferents\nend in the posterior horn, where second-order fibers cross the midline to form the\nspinothalamic tract, ascending to the thalamus and projecting to various cortical\nareas. Along the way, collaterals connect to the reticular formation. Due to the ros-\ntral inclination of fibers in Lissauer\u2019s tract, cordotomy must be performed several\nsegments above the pain level for effective relief.\nsyndrome, tension headaches, and temporomandibular joint disease, where significant pain\nexists without an apparent noxious stimulus or peripheral pathology. Pathological pain, a\nlow-threshold pain primarily driven by amplified sensory signals in the central nervous sys-\ntem, is the clinical pain syndrome with the greatest unmet need. To analogize, while nocicep-\ntive pain acts as a fire alarm for intense heat, and inflammatory pain reacts to warm tempera-\ntures, pathological pain is a false alarm triggered by a system malfunction. Thus, treatment\nmust specifically target the underlying mechanisms causing each type of pain [48].\nPain from a time-duration perspective can be categorized by duration into acute and2.4. PAIN INDICATORS 13\nchronic , with chronic pain persisting or recurring for more than three months [50]. Acute\npain is typically related to identifiable physiological damage from injury, surgery, illness,\ntrauma, or medical procedures and generally subsides once the underlying cause is resolved.\nHowever, if untreated, it may develop into chronic pain. Acute pain is further classified\nintoprocedural pain, caused by medical interventions such as muscular injections [51], and\npostoperative pain, which occurs after surgery and is a significant concern for both patients\nand healthcare providers. Effective management is crucial to aid recovery and prevent the\ntransition to chronic pain [52]. Chronic pain manifests in various forms, including chronic-\nrecurrent pain, like migraine headaches, and chronic-continuous pain, such as persistent low\nback pain [53].\n2.4 Pain Indicators\nPain can manifest in numerous ways and is often shaped by individual characteristics and\nenvironmental influences. Various human expressions, actions, and bodily responses have\nbeen linked to pain, serving both communicative and coping purposes. These pain indicators\nare generally categorized into three primary groups: (i)behavioral, (ii)physiological, and\n(iii)biochemical. While these indicators are universally present, certain expressions are more\nprominent in specific groups. For instance, crying is a common pain response across all age\ngroups but is more frequently observed in younger infants. This may be due to contextual\nfactors\u2014such as culture, social status, age, and ego\u2014influencing how pain is expressed\nover time. Adults, for example, may suppress crying in favor of other vocalizations, such as\ngroans and moans, as crying could be perceived as inappropriate in certain contexts. These\nmediating factors are often considered when interpreting pain indicators. The following\nsections will delve into each of these three categories [51].\n2.4.1 Behavioral Indicators\nBehavioral indicators such as facial expressions ( e.g., grimacing, open mouth, raised eye-\nbrows), vocalizations ( e.g., crying, moaning, screaming), and various bodily movements\n(e.g., changes in posture, signs of tension) are vital markers used in assessing pain [22].\nFacial expressions and limb movements in response to acute pain are typically rapid and\ninvoluntary. Facial reactions include brow bulging, eye squeezing, nasolabial furrow forma-\ntion [54], grimacing, clenched teeth, jaw-dropping, and tightened lips [55]. Body movements\nassociated with pain include bracing (gripping an object or the affected area during move-\nment), rubbing (massaging the painful area), restlessness (constant shifting of position) [55],\nand knee flexion [56]. Non-verbal vocalizations such as groaning, moaning, sighing, crying,\nand gasping [57] also indicate pain. Verbal expressions like \u201couch\u201d ,\u201cstop\u201d ,\u201cthat hurts\u201d ,14 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFigure 2.2: Pain classification [48]: (A)Nociceptive pain , which results from detecting po-\ntentially harmful stimuli and serves a protective function. (B)Inflammatory pain is\nlinked to tissue damage and immune cell infiltration, increasing pain sensitivity dur-\ning healing. (C)Pathological pain is a disease state caused by either nervous sys-\ntem damage (neuropathic) or abnormal nervous system function (dysfunctional).\n\u201cthat is enough\u201d , and even cursing [55] also serve as pain indicators. Interestingly, swearing\nhas been found to significantly alleviate pain, although its effect diminishes with frequent\nuse over a short period [58, 59].2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 15\n2.4.2 Physiological Indicators\nVital signs can reflect the state of the central nervous system, and since pain is mediated\nthrough this system, trends in vital signs can provide insights into pain levels. Clinical stud-\nies [60, 61] have examined physiological changes in response to pain and established em-\npirical solid evidence linking pain to vital sign alterations. However, as vital signs can also\nchange due to other non-pain-related pathological conditions, it is recommended that they be\nassessed alongside behavioral pain indicators for accuracy. Physiological pain responses are\nconsidered more reliable than behavioral signals, as they cannot be consciously controlled\nor altered. Physiological measurements such as electrocardiography (ECG), electromyogra-\nphy (EMG), galvanic skin responses (GSR), and respiration rate provide critical insights into\nthe body\u2019s reaction to pain [17]. In addition, brain monitoring techniques like near-infrared\nspectroscopy (fNIRS) have demonstrated the ability to detect pain-related hemodynamic\nchanges [23]. At the same time, functional magnetic resonance imaging (fMRI) has been\nexplored for assessing pain in both normal and pathological conditions [62].\n2.4.3 Biochemical Indicators\nCompared to other pain indicators, biochemical changes are the most precise and sensitive\nreactions to pain. However, their routine use in pain assessment is restricted due to the\ninvasive nature of measurement techniques [63]. These biochemical responses are most\nevident during surgical procedures with limited anesthesia, leading to increased levels of\nendorphins, norepinephrine, cortisol, growth hormones, renin, glucagon, aldosterone, and\ncatecholamines, along with a decrease in insulin levels [60].\n2.5 Sociodemographic and Psychological Variables\nIn1965 , Melzack and Wall [64] introduced the \u201cGate Control Theory\u201d , which interprets pain\nfrom two perspectives. The first involves the mechanisms of nociceptive signal transmission\nand modulation, while the second emphasizes pain as a psychophysiological phenomenon\narising from the interaction between physiological and psychological factors [53]. Observa-\ntions, empirical research, and theoretical models increasingly suggest that a comprehensive\nunderstanding of pain requires a biopsychological approach. It is also becoming apparent\nthat, although pain is often regarded as private and subjective, it is also fundamentally a so-\ncial experience [53]. Pain is not solely explained by biomedical components ( e.g., muscle\ndamage) but also involves psychological ( e.g., cognitive, affective) and social factors ( e.g.,\nfriends, family, health professionals), leading to what is known as a biopsychosocial sensa-\ntion [65]. Numerous factors contribute to how painful experiences are expressed and per-\nceived, varying wildly due to social and personal biases. These factors prompted Williams16 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nand Craig [2] to define pain as \u201ca distressing experience associated with actual or potential\ntissue damage with sensory, emotional, cognitive, and social components. \u201d\n2.5.1 Sex and Gender\nSeveral studies have explored the relationship between gender and pain expressiveness, as\nwell as variations in pain reporting. Research indicates that women generally exhibit a lower\npain threshold compared to men. A meta-analysis by Boerner et al. [66] on gender differ-\nences in children and adolescents found that girls over the age of 12reported higher pain\nintensity in response to cold-induced pain than boys. Furthermore, multiple studies suggest\nthat women tend to describe a greater degree of pain compared to men. In addition to bi-\nological differences, psychological aspects linked to gender also play a role. For instance,\nindividuals with a masculine identity may be less inclined to express or report their pain or\nseek assistance [67].\nMoreover, the manifestation of pain is not only influenced by the individual\u2019s gender but\nalso by dyadic interactions between people of different sexes. Levine and Desimone [68]\nconducted one of the initial studies on this phenomenon, showing that male participants in\na cold pressure experiment reported lower pain intensity when a female experimenter was\npresent. Similarly, McClelland and McCubbin [69] found that female participants expressed\nand reported higher pain levels when accompanied by a female friend. This dynamic also\nextends to patient-healthcare provider interactions. In studying health records, Vigil and\nAlcock [70] discovered that when the pain intensity was reported as high, the patients ( i.e.,\nmen and women) were examined by a female doctor or nurse. Additionally, studies exam-\nining gender differences among physicians in pain treatment options revealed that female\npatients were more likely to receive prescriptions for more potent drugs, such as analgesics,\nand female physicians were more likely to prescribe medications. Extensive research has\nalso shown that both lay observers and healthcare professionals tend to estimate higher pain\nlevels for female patients compared to male patients [71]. Hooper et al. [72] further noted\nthat clinicians communicate more effectively with female patients, often displaying greater\nempathy. Gender roles, beliefs, and expectations play a significant role in understanding\nhow social factors influence the differences in pain perception and experience between men\nand women [73].\n2.5.2 Age\nAge plays a crucial role in pain assessment and management. At the same time, there are\nsignificant challenges, limitations, and biases related to the patient\u2019s age group. Two of the\nmost vulnerable groups, albeit for different reasons, are the elderly and infants.\nPain recognition and interpretation among the elderly, particularly by caregivers, often2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 17\npresent unique challenges. Older adults frequently exhibit stoicism and reluctance to ex-\npress their pain, while healthcare providers struggle to accurately assess the patient\u2019s pain,\nleading to inappropriate pain management decisions [74]. McPherson et al. [75] noted that\ndespite caregivers\u2019 accommodating and empathetic relationships with elderly patients, con-\nflicts still arise. Older patients may resist acknowledging their weaknesses and accepting\nhelp, which can cause them to conceal their pain. The situation becomes even more complex\nwhen dealing with dementia, a disorder encompassing a range of conditions ( e.g., Parkin-\nson\u2019s, Alzheimer\u2019s, Vascular dementia), characterized by abnormal brain changes that im-\npair cognitive and linguistic abilities. A person with dementia may find it challenging to\ncommunicate their pain verbally. However, non-verbal pain expressions remain intact even\nin moderate dementia, although such reactions can be exaggerated [76]. However, aggres-\nsive behavior and disturbances in dementia patients, often caused by pain, are frequently\nmisinterpreted as psychiatric symptoms, leading to improper medication that can have life-\nthreatening consequences [77]. Caregivers of dementia patients face additional challenges,\nnot only related to pain management but also in addressing dementia\u2019s impact on language\nand memory. Particularly in the later stages of dementia, patients encounter severe pain\ncommunication difficulties due to cognitive decline, necessitating that caregivers recognize\nbehavioral and contextual indicators of pain [74]. Age is also known to cause changes in\nskin characteristics, such as texture, rigidity, and elasticity, which impact the performance of\nemotional face recognition tasks [78].\nInfants represent another vulnerable age group where pain assessment requires special-\nized attention, particularly when they experience painful events. The first challenge is obvi-\nously their limited reporting ability to express their pain through language. Although crying\nmight appear to signal pain, this is an oversimplified and unreliable method, as crying can in-\ndicate a variety of situations, such as discomfort, hunger, or pain. Accurately discerning the\ntype of cry is only one part of the challenge; assessing pain in infants is far more complex and\ninfluenced by numerous factors, including the interpersonal relationships within their envi-\nronment. Riddell and Racine [79] found that through various distressing experiences, infants\ncan learn that specific signaling behaviors can prompt their caregiver\u2019s proximity. This at-\ntachment dynamic suggests that, to some extent, infants may consciously utilize pain-related\nbehaviors to elicit responses from their caregivers. Similarly, the context affects older chil-\ndren as well; for example, self-reports of pain tend to be significantly lower when a parent is\npresent compared to when the child is alone [80].\n2.5.3 Psychological Factors\nMultiple studies have revealed that several psychological factors are consistently linked with\npain-related behavior, including depression, pain-related fear, and catastrophizing. Research18 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nfocusing on the impact of depression and anxiety on pain-related behavior has been con-\nducted mainly on patient populations. These studies have shown that depressed individ-\nuals exhibit more pronounced protective and communicative behaviors compared to non-\ndepressed patients [81]. Similarly, numerous studies suggest that patients with higher levels\nof anxiety demonstrate more pain-related behaviors than those with lower anxiety levels [82].\nDespite the frequent coexistence of pain with psychological conditions, research indicates\nthat these patients often experience underestimation of their pain. For instance, De Ruddere\net al. [83] found that patients dealing with psychological stressors such as anxiety, depres-\nsion, and daily life challenges are often perceived by physiotherapists as experiencing less\nsevere pain, illustrating the influence of psychosocial factors on the patient\u2019s pain experience.\n2.5.4 Race and Culture\nPain expression is generally understood across ethnicities and cultures, though differences\nexist in how it is conveyed [4]. However, cultural variations and the nuances of facial ex-\npressions related to emotion are complex and necessitate deeper study. Additionally, racial\nand cultural biases significantly influence pain assessment, judgment, and interpretation.\nExtensive research highlights the impact of a patient\u2019s race as a sociodemographic factor\non observer responses. The most examined topic relates to the different responses toward\nCaucasian versus non-Caucasian individuals, particularly African Americans, who are more\nlikely to have their pain underestimated and undertreated by healthcare providers [84].\nEthnocultural factors are crucial in shaping how individuals perceive and express pain.\nFor example, Western cultures often emphasize conservative expressions and self-control,\nleading to restrained responses in personal pain experiences and in perceiving others\u2019 pain\n[3]. Differences also arise in coping mechanisms; African Americans, for instance, are more\nprone to catastrophizing pain events compared to European Americans [85]. Furthermore,\nevidence shows racial biases in pain treatment across various racial groups, with certain\ngroups being more sensitive to pain but receiving lower-quality treatment [86]. For exam-\nple, Cleeland et al. [87] found that minority cancer patients, mainly Black and Hispanic\nindividuals, were more likely to experience inadequate analgesia compared to non-minority\npatients.\n2.5.5 Observer\u2019s Impact on Pain\nThe variability in pain management stems from the interplay of various elements, including\nsociocultural, biomedical, and psychosocial factors, especially in cases of chronic pain [88].\nWhen it comes to the observer responsible for assessing a patient\u2019s pain, several characteris-\ntics directly influence the objectivity of their evaluation. The first and perhaps most critical\nfactor is the observer\u2019s experience level. One would expect that more experience leads to2.6. IMPACT OF INADEQUATE PAIN MANAGEMENT 19\nbetter and more accurate assessments, but studies show that even experienced healthcare\nproviders consistently underestimate pain, much like laypersons [28]. The greater the expe-\nrience, the more pronounced the underestimation tends to be. This may be due to desensitiza-\ntion caused by repeated exposure to pain events, as seen in the differences between internists\nand surgeons in their evaluation of postoperative pain, with surgeons often encountering se-\nvere pain more regularly [89]. Another significant factor is the observer\u2019s knowledge and\nbeliefs about pain. For example, [83] found that laypersons and healthcare professionals\nwithout physical signs of pain might view the patient\u2019s complaints less seriously. Proper\ntraining is also essential for adequate pain assessment, which is why the Department of\nHealth and Human Services (DHHS) initiated a strategic program to improve healthcare\nproviders\u2019 education and knowledge regarding pain management, following evidence of in-\nadequate training in the field [90].\n2.6 Impact of Inadequate Pain Management\nThe experience of pain, particularly persistent pain, can have detrimental effects on the indi-\nvidual and their surrounding environment. Thoughts about severe pain often lead to grief and\nfear, causing individuals to perceive pain as a threat and feel incapable of managing it. This\ncan prompt avoidance behaviors aimed at escaping perceived harm [91]. Studies have shown\nthat children with a catastrophizing mindset about pain struggle with daily activities, while\nadolescents with chronic pain tend to have fewer friends and may miss out on social and\nentertainment opportunities, putting them at greater risk of victimization [92]. These adoles-\ncents often feel isolated and lonely compared to their healthy peers, and they may experience\nanxiety in social interactions [93]. Parental reactions to their children\u2019s pain can further com-\nplicate the situation, as parents with catastrophic tendencies tend to engage in overprotective\nbehaviors that hinder the child\u2019s functioning and psychosocial development [94]. Addition-\nally, the family\u2019s overall dynamic is affected, with the patient\u2019s sadness, sleep disorders, and\nchanges in leisure activities impacting the household [74].\nOn a biological level, pain, particularly when experienced early and severely, can alter\nthe brain and nervous system. These early pain experiences can disrupt neurobiological\ndevelopment and affect how pain is processed later in life [95]. A growing body of research\nlinks chronic pain to changes in the medial prefrontal cortex, a region crucial to emotional\nprocessing. Chronic pain is associated with structural and biochemical alterations in this\nbrain area, suggesting that these changes play a role in the pathophysiology of chronic pain\n[96].20 CHAPTER 2. CLINICAL PAIN ASSESSMENT\n2.7 Pain Measurement Scales and Metrics\nIn clinical settings, self-reporting remains the gold standard for assessing pain, allowing in-\ndividuals to describe their pain\u2019s intensity and location. Various self-report scales have been\ndeveloped for different age groups, such as the visual analog scale (V AS) [97] and the verbal\nrating scale (VRS) [98]. Additionally, observation-based scales, where a third party eval-\nuates the pain\u2019s severity, include tools like the Prkachin and Solomon pain intensity scale\n(PSPI) [99] and the neonatal/infant pain scale (NIPS) [100]. However, some studies suggest\nthat patients may exaggerate their pain severity to prompt more aggressive treatment inter-\nventions [101], raising concerns about the accuracy of self-reported symptoms. Therefore,\nobjective pain measurement remains clinically crucial.Chapter 3\nAutomatic Pain Assessment\u2013A Literature\nReview\nContents\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database . . . . 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4", "The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database": "26\n3.3.5", "The AI4Pain Database": ". . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4", "Unimodal studies": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1", "Vision-based: Static Analysis": ". . . . . . . . . . . . . . . . . . . . 28\n3.4.2", "Vision-based: Temporal Utilization (Non-ML Approach)": ". . . . . . 35\n3.4.3", "Vision-based: Implicit Temporal Utilization": ". . . . . . . . . . . . . 36\n3.4.4", "Vision-based: Explicit Temporal Utilization": ". . . . . . . . . . . . . 37\n3.4.5", "Touch sensor-based": ". . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6", "Audio-based": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5", "Multimodal studies": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1", "Static Analysis": ". . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based:", "Temporal Utilization": "(Non-ML Approach) . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6", "Summary of Automatic Pain Assessment Methods": ". . . . . . . . . . . . . . 47\n3.6.1", "Input": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2", "Processing": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3", "Evaluation": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4", "Pain Databases for Evaluation": ". . . . . . . . . . . . . . . . . . . . 49\n3.6.5", "Interpretation of Results": ". . . . . . . . . . . . . . . . . . . . . . . 50\n3.7", "Challenges and Future Directions": ". . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research\nDirections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4 Demographic Variables: Their Role and Impact 53\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5 Optimization: Balancing Efficiency and Performance 75\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n6 Synthetic Data: The Role of Thermal Imaging 103\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7 General-Purpose Models 117\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n8 Conclusions, Perspectives and Future Work 157\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nBibliography 163\nAppendix 201\nAcronyms 209List of Figures\n2.1 The spinothalamic tract (STT) [43]. Pain, temperature, and some touch affer-\nents end in the posterior horn, where second-order fibers cross the midline\nto form the spinothalamic tract, ascending to the thalamus and projecting to\nvarious cortical areas. Along the way, collaterals connect to the reticular for-\nmation. Due to the rostral inclination of fibers in Lissauer\u2019s tract, cordotomy\nmust be performed several segments above the pain level for effective relief. 12\n2.2 Pain classification [48]: (A)Nociceptive pain , which results from detecting\npotentially harmful stimuli and serves a protective function. (B)Inflamma-\ntory pain is linked to tissue damage and immune cell infiltration, increas-\ning pain sensitivity during healing. (C)Pathological pain is a disease state\ncaused by either nervous system damage (neuropathic) or abnormal nervous\nsystem function (dysfunctional). . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 The number of studies utilizing these specific datasets. Note that various\nstudies used multiple datasets to conduct their experiments. . . . . . . . . . 25\n4.1 The PQRST waveform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure. . 55\n4.3 The signal preprocessing using the Pan-Tompkins algorithm. . . . . . . . . 57\n4.4 Results for the Gender Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Results for the Age Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.6 Results for the Gender-Age Scheme . . . . . . . . . . . . . . . . . . . . . . 64\n4.7 The proposed MTL network: The sizes of the extracted vectors for the net-\nwork are as follows: for the Pain classifier, n\u02c61, where nis the number of\npain estimation tasks ( e.g.,2for binary classification, 5for multi-class clas-\nsification); for the Age classifier, 36\u02c61, where 36represents the possible\nage values of the subjects; for the Gender classifier, 2\u02c61, corresponding to\nthe two possible gender categories ( i.e., males and females). . . . . . . . . 66\n4.8 Results for the proposed Schemes. . . . . . . . . . . . . . . . . . . . . . . 69\n4.9 Comparison of performances utilizing various neural networks approaches. 72\nxix5.1 The application of face alignment illustrates landmarks in 2D (left) and 3D\n(right) space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 An overview of our proposed transformer-based framework for automatic\npain assessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 The impact of the number of input frames on accuracy (left) and on runtime\nin milliseconds (right). Runtime calculated during inference on a NVIDIA\nRTX-3090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Relevance Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5 Outline of the proposed framework. . . . . . . . . . . . . . . . . . . . . . 86\n5.6 Comparison of mean accuracy and inference period for unimodal and multi-\nmodal strategies across NP versus P 4and MC tasks. The diagram adopts a\ndual-y-axis configuration\u2014accuracy measurements on the left and time met-\nrics on the right\u2014to outline the balance between performance efficacy and\ncomputational load, categorizing the methodologies along the x-axis. . . . . 98\n5.7 Regions highlighted in yellow and red denote areas of significant attention.\n(a) (1strow) Sequence of original frames. (2ndrow) Derived from the\nSpatial-Module after initial stage pretraining. (3rdrow) Derived from the\nSpatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module trained on the BioVid dataset. (b) (1strow) Derived from\ntheTemporal-Module incorporating video embeddings. (2ndrow) Derived\nfrom the Temporal-Module with heart rate embeddings. (3rdrow) Derived\nfrom the Temporal-Module using a combined embedding of video and heart\nrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.1 Illustration of the procedure for creating thermal images, featuring the archi-\ntecture of the Generator G(Encoder, mid-stage ResNet, Decoder), and the\nDiscriminator D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2 Representation of the proposed framework, illustrating its components and\ntheir main functions: (a)The Vision-MLP module, tasked with extracting\nfeature embeddings from video frames. (b)TheToken-Mixer , an important\nsub-module of Vision-MLP , generates the wave representation for the tokens.\n(c)The Channel-Mixer , a crucial sub-module within Vision-MLP .(d)The\nMLP, a core component of the Channel-Mixer .(e)The fusion procedure\nthat combines RGB and synthetic thermal embeddings, succeeded by the\nTransformer module, which conducts the final pain assessment. . . . . . . . 107\n6.3 Gradual blurring of RGB and synthetic thermal facial images: a series dis-\nplaying varying levels of Gaussian blur applied, with kernel sizes gradually\nincreased from k\u201c0(no blur) to k\u201c191(extensively blurred). . . . . . . 1136.4 Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain)\nclasses in RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191\n(heavily blurred). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1 PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer hav-\ning varying depths, showcasing how token resolution decreases at each stage;\n(b)Composition of the Token-Mixer module, featuring elements like depth-\nwise convolution (DWConv) and batch normalization; (c)Architecture of the\nFeed-Forward Network (FFN) within the Token-Mixer ;(d)The Cascaded\nAttention mechanism implemented across multiple heads, illustrating how\noutputs from preceding heads are incorporated to refine the self-attention\nprocess, culminating in the final output projection; (e)Configuration of the\nproposed multimodal pipeline, employing videos and fNIRS. The embed-\ndings from PainViT\u20131 are represented as waveform diagrams, which are\nmerged into a single diagram that illustrates both modalities before entering\nPainViT\u20132 for final pain evaluation. . . . . . . . . . . . . . . . . . . . . . . 151\n7.2 Waveform illustrations for various data types: (a)original fNIRS signal,\n(b)video embedding derived from PainViT\u20131 , and (c)fNIRS embedding\nobtained from PainViT\u20131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n7.3 Attention maps from the PainViT\u20132 . . . . . . . . . . . . . . . . . . . . . . 152\n7.4 Overview of primary models and their components outlined in this research:\n(a)PainFormer is structured hierarchically into four stages, incorporating\nSpectral andSelf-Attention Layers to extract embeddings from the inputs;\n(b)The Spectral Layer , a key element of PainFormer , uses FFT to ana-\nlyze frequency-specific data along with a learnable filter Kto highlight\ncritical frequencies; (c)The Self-Attention Layer , crucial for PainFormer ,\nenables parallel processing of features and their interconnections; (d)The\nEmbedding-Mixer , employing both cross and self-attention mechanisms, func-\ntions as the component for the final classification of embeddings in pain as-\nsessment; (e)TheVideo-Encoder , designed for compact and efficient encod-\ning, compresses video data into a reduced dimensional form; (f)TheMLP-1\nis part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)TheMLP-3 configuration is integrated into the Embedding-Mixer\nandVideo-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n7.5 Examples of different vision modalities in frame samples: (a)RGB frame,\n(b)synthetic thermal frame, and (c)depth estimation frame. . . . . . . . . 153\n7.6 Examples of different visual representations for biosignals: (a)waveform ,\n(b)spectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD . . 1547.7 An overview of the presented framework. PainFormer , the foundational\nmodel, excels in deriving high-quality embeddings from a diverse array of\nbehavioral and physiological modalities. The evaluation of RGB, thermal,\nand depth videos, alongside various representations of ECG, EMG, GSR,\nand fNIRS such as waveforms and spectrograms, underscores the rich infor-\nmation captured within these embeddings. Leveraging the embeddings from\nPainFormer facilitates the creation of various and diverse unimodal and mul-\ntimodal pipelines designed for the pain assessment task. Each pipeline can\nbe customized to suit the specific modalities involved, dataset characteristics,\nand the demands of the intended application or clinical setting. Our assess-\nments included the development and implementation of several pipelines\nin both unimodal and multimodal contexts, achieving leading-edge results\nacross various modalities and data representations. . . . . . . . . . . . . . 154\n7.8 Attention maps from the PainFormer :(a)(1strow) frames from RGB, ther-\nmal, and depth video modalities; (a)(2ndrow) corresponding attention maps;\n(b)(1strow) attention maps for ECG and EMG; (b)(2ndrow) attention maps\nfor EDA and fNIRS modalities. . . . . . . . . . . . . . . . . . . . . . . . . 155\n1 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n2 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n3 Additional attention maps from the PainViT\u20132 (refer to Section 7.2). . . . . 207List of Tables\n3.1 Most commonly utilized pain databases. . . . . . . . . . . . . . . . . . . . 24\n3.2 Vision-based studies with static analysis. . . . . . . . . . . . . . . . . . . . 32\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 33\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 34\n3.3 Vision-based studies with temporal utilization. . . . . . . . . . . . . . . . . 39\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 40\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 41\n3.4 Touch sensor-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Audio-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6 Multimodal-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.7 Interpretation approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1 Results for the Basic Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Results for the Gender Scheme (1). . . . . . . . . . . . . . . . . . . . . . . 60\n4.3 Results for the Age Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Results for the Gender-Age Scheme (Males) (1). . . . . . . . . . . . . . . . 62\n4.5 Results for the Gender-Age Scheme (Females) (1). . . . . . . . . . . . . . . 63\n4.6 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (1). 63\n4.7 Hyper-parameters used in our approach. . . . . . . . . . . . . . . . . . . . 65\n4.8 Results for the Basic Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . 68\n4.9 Results for the Gender Scheme (2). . . . . . . . . . . . . . . . . . . . . . . 68\n4.10 Results for the Age Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.11 Results for the Gender-Age Scheme (2). . . . . . . . . . . . . . . . . . . . 68\n4.12 Comparison of results adopting the feature augmentation approach. . . . . . 70\n4.13 Comparison of results adopting the MT-NN approach. . . . . . . . . . . . . 71\n4.14 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2). 72\n5.1 Training details for the automatic pain assessment. . . . . . . . . . . . . . 79\n5.2 Results on the pain estimation tasks. . . . . . . . . . . . . . . . . . . . . . 81\n5.3 Results for the pain estimation tasks using various numbers of input frames. 82\n5.4 Comparison of studies utilizing BioVid , RGB videos, and LOSO validation. 84\n5.5 Datasets utilized for the pre-training process of the framework. . . . . . . . 91\nxxiii5.6 Training details for the automatic pain assessment. . . . . . . . . . . . . . 92\n5.7 Results utilizing the video modality. . . . . . . . . . . . . . . . . . . . . . 93\n5.8 Results utilizing the heart rate modality. . . . . . . . . . . . . . . . . . . . 95\n5.9 Results utilizing the video &the heart rate modality. . . . . . . . . . . . . . 95\n5.10 Comparison of studies utilizing BioVid &LOSO validation, reported on ac-\ncuracy %. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.11 Module parameters and computational cost in FLOPS for the proposed frame-\nwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.1 Datasets utilized for the pretraining process of the framework. . . . . . . . 110\n6.2 Training specifications, and number of parameters and FLOPS for each mod-\nule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.3 Results utilizing the RGB video. . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Results utilizing the synthetic thermal video. . . . . . . . . . . . . . . . . . 112\n6.5 Results utilizing the RGB &the synthetic thermal video. . . . . . . . . . . . 113\n6.6 Results utilizing the fusion of RGB &synthetic thermal video. . . . . . . . 115\n6.7 Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.116\n6.8 Comparison with the MIntPAIN dataset. . . . . . . . . . . . . . . . . . . . 116\n7.1 Number of parameters and FLOPS for the components of the proposed Twins-\nPainViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.2 Datasets utilized for the pretraining process of the framework. . . . . . . . 123\n7.3 Training details for the automatic pain assessment. . . . . . . . . . . . . . 124\n7.4 Results utilizing the video modality & Addition method. . . . . . . . . . . . 125\n7.5 Results utilizing the video modality & Concatenation method. . . . . . . . . 125\n7.6 Results utilizing the HbR & Addition method. . . . . . . . . . . . . . . . . 126\n7.7 Results utilizing the HbR & Concatenation method. . . . . . . . . . . . . . 126\n7.8 Results utilizing the HbO & Addition method. . . . . . . . . . . . . . . . . 126\n7.9 Results utilizing the HbO & Concatenation method. . . . . . . . . . . . . . 127\n7.10 Results utilizing the HbR, HbO & Addition method. . . . . . . . . . . . . . 127\n7.11 Results utilizing the videos, HbO & Addition method. . . . . . . . . . . . . 127\n7.12 Results utilizing the videos, HbO & Single Diagram method. . . . . . . . . 128\n7.13 Comparison with the validation baseline provided by the AI4PAIN challenge\norganizers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.14 Number of parameters and FLOPS for the modules of the proposed framework.130\n7.15 Details of the PainFormer\u2019s architecture. . . . . . . . . . . . . . . . . . . . 132\n7.16 Datasets utilized for the multitask learning-based pretraining process of the\nframework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347.17 Training details of the proposed framework. . . . . . . . . . . . . . . . . . 135\n7.18 Results utilizing the video modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.19 Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.20 Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.21 Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.22 Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and\nF1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.23 Results on the validation set of AI4Pain dataset, multilevel classification task,\nreported on accuracy, recall and F1 score. . . . . . . . . . . . . . . . . . . 144\n7.24 Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.25 Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4\ntask and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.26 Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs.\nP4task and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.27 Comparison of studies on the testing set of AI4Pain dataset. . . . . . . . . . 148\n1 Results utilizing the video modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n2 Results utilizing the heart rate modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n3 Results utilizing the video &the heart rate modality reported on precision,\nrecall and F1 score (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . 202\n4 Results utilizing the RGB video modality, reported on recall and F1 score\n(refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n5 Results utilizing the synthetic thermal video modality, reported on recall and\nF1 score (refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . 203\n6 Results utilizing the fusion of RGB &synthetic thermal video modality, re-\nported on recall and F1 score (refer to Section 6.2). . . . . . . . . . . . . . 204\n7 Results of the proposed approaches, reported on macro-averaged precision,\nrecall and F1 score (refer to Section 7.2). . . . . . . . . . . . . . . . . . . . 204Chapter 1\nIntroduction\nContents\n1.1 Context and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Scope and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions \u2013 Peer-review Publications . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.1 Context and Motivation\nPain is a complex and deeply personal experience that is subjective by nature. Traditionally,\nit has been described in terms of its sensory dimension [1]. However, extensive research\nhas highlighted the importance of affective, cognitive and social aspects in shaping this ex-\nperience [2]. Studies have explored physiological, psychological, and socio-environmental\nfactors that contribute to the experience of pain. It is understood as a result of biological evo-\nlution and as influenced by psychological and social factors. As Ridell et al. [3] noted, \u201cPain\nis a synthesis\u2013a sum that is greater than its parts. \u201d The brain\u2019s ability to alter the perception\nof sensory inputs through the interplay of emotion, cognition, and social processes is signifi-\ncant. Although natural systems establish the initial biological framework for pain perception,\nthis structure is highly adaptable, particularly in humans. Throughout a person\u2019s life, both\nbiological developments and personal experiences significantly reshape this framework.\nA key question driving pain research across biological, psychological, and computational\nfields is why this topic of pain is meaningful and important. This question also forms the\nbasis for initiating this thesis, highlighting the broader relevance of studying pain. Williams\nand Kappesser [4] provide a compelling explanation, stating, \u201cWe care because we are wired\nto care: to attend to other people\u2019s expression of pain and to understand its meaning; to feel\ndistress in relation to their distress; and to be motivated to reduce their distress, and ours,\nif we are able to do so. \u201d This highlights the intrinsic human response to empathize and\n12 CHAPTER 1. INTRODUCTION\nalleviate pain, underlining the fundamental importance of this research area. Indeed, from a\nDarwinian perspective, pain serves a crucial role. The manifestation of pain in humans and\nthe reactions it elicits are examined through an evolutionary lens. Pain facilitates recovery by\npromoting responses to harmful stimuli and behaviors that demonstrate the adverse nature\nof painful experiences, common among animals. Specifically, the facial expression of pain,\nwhich communicates discomfort directly to those nearby, is universally recognized across\ndifferent ages, ethnicities, roles, and relationships. Evidence from healed major fractures\n[5, 6] suggests that injured members of hominid groups were not left to fend for themselves\nbut were supported through their recovery, indicating the fundamental importance of pain\nexpression in our evolutionary history.\nPain is a widespread health concern globally, affecting up to 30% of the adult popula-\ntion [7] and between 83% and93% of elderly adults in residential care [8]. The Global Bur-\nden of Disease (GBD) study identifies pain as the primary cause of years lived with disability\n(YLD) [9], with major contributors including chronic back pain, musculoskeletal disorders,\nand neck pain [10]. Pain impacts individuals and poses significant clinical, economic, and\nsocial challenges. In the United States, the economic and healthcare costs related to pain\ndue to reduced work productivity range from $560 to$635 billion annually, surpassing the\ncosts associated with heart disease, cancer, and diabetes combined [11]. In Europe, chronic\npain\u2019s direct healthcare costs and indirect socioeconomic impacts account for 3%to10% of\nthe GDP [12]. In Australia, the average annual cost for individuals among the 15.4%living\nwith chronic pain ranges from AU $22,588to AU $42,979, including non-financial costs [13].\nBeyond direct effects on health, pain contributes to a range of adverse outcomes, such as opi-\noid dependency, drug overuse, addiction, declining social relationships, and psychological\ndisorders [14]. In the last two decades, prescription opioid use has surged in the United\nStates, where overdose deaths have increased more than fourfold from 1999 to2016 [15].\nAdditionally, side effects from these opioids, like lethargy, depression, anxiety, and nausea,\nseverely impact workforce productivity and overall life quality [16].\nAccurate pain assessment is crucial for early diagnosis, disease progression monitoring,\nand treatment effectiveness evaluation, particularly in managing chronic pain [17]. This criti-\ncal role has resulted in pain being recognized as \u201cthe fifth vital sign\u201d in nursing literature [18].\nPain assessment is also fundamental in physiotherapy, where therapists apply external stim-\nuli and need to gauge the patient\u2019s pain levels accurately [19]. Objective evaluation of pain is\nessential to provide appropriate care, especially for vulnerable populations who may not be\nable to communicate their pain effectively, such as infants, young children, individuals with\nmental health issues, and the elderly. Various methods are used for pain assessment, with\nself-reporting\u2013where individuals describe their pain experiences\u2013considered the gold stan-\ndard [20]. Pain evaluation methods in clinical environments include quantifiable measures\nlike the Numeric Pain Rating Scale (NPRS), Visual Analogue Scale (V AS), and quantitative1.1. CONTEXT AND MOTIVATION 3\nsensory testing techniques such as the pressure pain detection threshold (PPDT) [21]. Behav-\nioral indicators are also crucial and include facial expressions ( e.g., grimacing, open mouth,\nlifted eyebrows), vocalizations (like crying, moaning, or screaming), and movements of the\nbody and head [22]. Physiological measures such as electrocardiography (ECG), electromyo-\ngraphy (EMG), galvanic skin responses (GSR), and respiration rates further contribute to\nunderstanding pain\u2019s physiological aspects [17]. Additionally, brain monitoring techniques\nlike near-infrared spectroscopy (fNIRS) have effectively detected changes in hemodynamic\nactivity associated with pain stimuli [23].\nCaregivers and family members often determine the presence or absence of pain in pa-\ntients by observing their behavioral or physiological responses [17]. However, accurately\nassessing pain poses a significant challenge for clinicians [24], especially with nonverbal\npatients such as the elderly, who may have reduced expressive abilities or may be reluctant\nto communicate pain [25]. Extensive research indicates that pain manifestations vary signif-\nicantly across different genders and ages, adding to the complexity of its assessment [26].\nFurther complicating the assessment process are the heightened workload and fatigue ex-\nperienced by nursing staff due to the demands of patient monitoring [27]. Technological\nsolutions are necessary for continuous patient monitoring. Nevertheless, concerns remain\nabout the objectivity and accuracy of these observations, as inadequately trained or biased\nobservers may struggle to assess pain [28] accurately. Even among trained observers, in-\nterpretations of behaviors can vary [22], and social and interpersonal dynamics can signif-\nicantly affect the pain assessment process, influencing both the evaluators\u2019 judgments and\nthe patients\u2019 expressions of pain [29]. Additionally, the presence of an observer can lead pa-\ntients to modify their behavior [30], and expressing pain through scales and measurements\ncan be challenging [31]. While self-reporting is used because pain is inherently subjective,\nrelying solely on a one-dimensional pain score fails to capture this complex phenomenon,\noften leading to inadequate pain management [32].\nGiven the challenges described above, scientific computing (SC) researchers have fo-\ncused on developing models and algorithms to enhance automatic pain recognition systems\nover the last two decades. Their goal is to accurately determine the presence and intensity\nof pain by analyzing physiological and behavioral indicators. Adopting deep learning and\nartificial intelligence (AI) techniques has expanded these automatic methods, designed to\ninterpret the complex and varied nature of pain [17]. Numerous studies have underscored\nthe effectiveness of automated systems that utilize behavioral or physiological modalities\nfor pain assessment [33]. Sario et al. [34] have shown the capability of these systems to\naccurately recognize pain through facial expressions, proving their utility in clinical envi-\nronments. Multimodal sensing has shown particular promise, offering enhanced accuracy\nin pain detection systems [22]. Furthermore, including temporal aspects in these modalities\nhas proven to significantly improve the accuracy of pain assessments [17].4 CHAPTER 1. INTRODUCTION\n1.2 Scope and Challenges\nAlthough considerable research has been conducted on automatic pain assessment, studies\nhave yet to explore factors like demographics and social aspects from a computational angle.\nFurthermore, despite the existence of deep learning-based methods, the approaches we ob-\nserve are often outdated and repeatedly recycled. For these reasons, we aimed to address two\nissues by (i)attempting to evaluate the social or demographic context, which significantly\nimpacts and influences pain sensation and perception, and (ii)introducing innovative deep\nlearning methods inspired by the latest developments in AI and generative AI literature. We\nbelieve these approaches can forge new paths in pain research, enhance the accuracy of rec-\nognizing this complex phenomenon, and, ultimately, be adopted in real-world scenarios to\nassist those in need. Additionally, (iii)recognizing the skepticism towards new technologies\namong clinicians and the general public, especially regarding the limited understanding of\nhow deep learning models function, we have devoted a portion of our research to interpret-\ning these models to offer some level of explanation and help the adoption process of them in\nclinical settings.\nNevertheless, this thesis initially faced challenges related to our objectives and goals as\nthe research progressed. The availability of pain datasets (to be discussed in the next chapter)\nis limited. Only a few datasets are available, and crucially, they are limited in size. This re-\nstriction poses a significant challenge for developing deep learning models, which typically\nrequire a large volume of data. In automatic pain assessment, researchers who develop deep\nlearning methods typically confront a decision: either train their models from scratch, which\ncan introduce performance limitations, or employ pre-trained models. These pre-trained\nmodels are generally trained on broadly available image datasets that include a variety of\nsubjects like animals and objects, or they rely on older architectures that were trained explic-\nitly on facial datasets. In this thesis, we addressed these issues by independently pre-training\nour deep-learning models using diverse datasets related explicitly to human facial images\nand biosignals. This strategy allowed us to design specific architectures to meet our unique\nneeds for each scenario, free from the constraints of relying on models developed and trained\nby others. Furthermore, we explored and evaluated several pre-training techniques to assess\ntheir effectiveness in pain assessment applications.\nRegarding, our objective to explore methods that utilize various modalities individually\nand in combination in a multimodal manner further constrains our dataset options. More-\nover, as previously outlined, our interest in the sociodemographic aspects of pain necessitates\ndatasets that include this information type, intensifying our challenges. For these reasons,\nthis thesis focuses specifically on examining the impact of age and gender on pain. In addi-\ntion, led us to utilize two pain datasets that most closely match the characteristics necessary\nfor our research, particularly in terms of demographic elements and multimodality.1.3. CONTRIBUTIONS \u2013 PEER-REVIEW PUBLICATIONS 5\n1.3 Contributions \u2013 Peer-review Publications\nThis section outlines the publications and projects produced during the Ph.D. research on\nautomatic pain assessment, where I was the first author.\n1.Automatic assessment of pain based on deep learning methods: A systematic re-\nview [17]\nThis systematic literature review (SLR) was conducted at the start of this Ph.D. re-\nsearch. This paper aims to explore the surge in recent years of deep learning algorithms\nadopted by researchers to encode the multidimensional nature of pain into meaning-\nful features. Specifically, this systematic review examines the models, methods, and\ndata types used to establish the foundation for deep learning-based automatic pain\nassessment systems. It identified relevant original studies from digital libraries such\nasScopus ,IEEE Xplore , and ACM Digital Library , following defined inclusion and\nexclusion criteria for studies published until December 2021 . The findings highlight\nthe critical role of multimodal approaches in automatic pain estimation, particularly\nin clinical environments, and emphasize the substantial gains observed with the inclu-\nsion of temporal exploitation of modalities. The review also recommends selecting\nhigh-performing deep learning architectures and methods, encouraging the adoption\nof robust evaluation protocols and interpretability techniques to deliver reliable and\nunderstandable outcomes. Additionally, it underscores the current limitations of exist-\ning pain databases in adequately supporting the development, validation, and practical\napplication of deep learning models as decision-support tools in real-world settings.\nFurthermore, we believe this paper is valuable not only for this Ph.D. project but also\nfor other practitioners and researchers in the field.\n2.Automatic Pain Intensity Estimation based on Electrocardiogram and Demographic\nFactors [35]\nThis study investigated the relationship between gender, age, and pain sensation and\ntheir effects on the automatic pain assessment process. By analyzing physiological\nsignals, particularly electrocardiography (ECG), we estimated pain intensity and ex-\namined the influence of these demographic factors. Utilizing the Pan-Tompkins algo-\nrithm for feature extraction and applying well-established classification methods, we\nexplored the correlation between gender, age, and pain manifestation.\n3.Multi-task Neural Networks for Pain Intensity Estimation Using Electrocardiogram\nand Demographic Factors [36]\nInspired by the previous study, this research further explored the influence of gender\nand age on pain perception. In this work, we analyze electrocardiography signals\nto uncover variations in pain perception across different demographic groups. We6 CHAPTER 1. INTRODUCTION\nleveraged these insights by developing a novel multi-task neural network for automatic\npain estimation, incorporating age and gender data for each individual. The study\ndemonstrated the advantages of this approach compared to other existing methods.\n4.A Full Transformer-based Framework for Automatic Pain Estimation using Videos\n[37]\nThis study introduced an innovative full transformer-based framework featuring a Trans-\nformer in Transformer (TNT) model combined with cross-attention and self-attention\nblocks. We achieved state-of-the-art performance using video data from the BioVid\ndatabase, demonstrating the model\u2019s effectiveness, efficiency, and strong generaliza-\ntion across primary pain estimation tasks.\n5.Multimodal automatic assessment of acute pain through facial videos and heart rate\nsignals utilizing transformer-based architectures [38]\nThis study presented a multimodal automatic acute pain assessment framework, inte-\ngrating video and heart rate signals. The framework consists of four key modules:\ntheSpatial Module , which extracts embeddings from videos; the Heart Rate Encoder ,\nwhich maps heart rate signals into a higher-dimensional space; the AugmNet , which\ngenerates learning-based augmentations in the latent space; and the Temporal Mod-\nule, which leverages the video and heart rate embeddings for the final assessment.\nThe Spatial Module undergoes a two-stage pre-training process: first, it learns uni-\nversal facial features through face recognition, followed by emotion recognition in a\nmultitask learning approach, enabling high-quality embeddings for pain assessment.\nExperiments with facial videos and heart rate data extracted from electrocardiograms\nin the BioVid database, alongside direct comparisons to 29studies, demonstrate state-\nof-the-art performance in unimodal and multimodal settings while maintaining high\nefficiency. In the multimodal setting, the framework achieved 82.74% accuracy for bi-\nnary pain classification and 39.77% for multi-level pain classification, using only 9.62\nmillion parameters across the entire framework.\n6.Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-\nMLP Architecture [39]\nThis paper introduced synthetic thermal videos generated by Generative Adversarial\nNetworks , which are integrated into the pain recognition process to assess their effec-\ntiveness. The framework employs a Vision-MLP andTransformer -based module, lever-\naging RBG and synthetic thermal videos in unimodal and multimodal settings. Exper-\niments conducted using facial videos from the BioVid database highlighted synthetic\nthermal videos\u2019 effectiveness and showcased their potential benefits in pain recognition\ntasks.1.4. THESIS OUTLINE 7\n7.Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for\nMultimodal Automatic Pain Assessment using Facial Videos and fNIRS [40]\nThis study was submitted to the First Multimodal Sensing Grand Challenge for Next-\nGen Pain Assessment (AI4PAIN) . The proposed multimodal framework leverages fa-\ncial videos and fNIRS, offering a modality-agnostic approach that eliminates the need\nfor domain-specific models. Utilizing a dual ViT configuration and waveform repre-\nsentations for both fNIRS and the extracted embeddings from the two modalities, the\nmethod demonstrates its effectiveness, achieving an accuracy of 46.76% in the multi-\nlevel pain assessment task.\n8.PainFormer: a Vision Foundation Model for Automatic Pain Assessment [41]1\nThis study introduces PainFormer , a vision foundation model built on multi-task learn-\ning principles and trained across 14distinct tasks and datasets comprising 10.9million\nsamples. As an embedding extractor for various input modalities, PainFormer provides\nfeature representations to the Embedding-Mixer , a transformer-based module respon-\nsible for conducting the final pain assessment. Extensive experimentation using both\nbehavioral modalities\u2013including RGB, synthetic thermal, and estimated depth videos\u2013\nand physiological modalities like ECG, EMG, GSR, and fNIRS revealed PainFormer \u2019s\nability to extract high-quality embeddings from diverse inputs. Tested on the BioVid\nandAI4Pain datasets and compared to more than 60existing methods, the framework\ndemonstrated state-of-the-art performance in unimodal and multimodal settings, posi-\ntioning itself as a step toward developing general-purpose models for automated pain\nevaluation.\n1.4 Thesis Outline\nThe dissertation is organized into the following chapters:\nChapter 2 introduces the foundational concepts of pain from biological, psychological, and\nclinical perspectives.\nChapter 3 reviews existing literature on automatic pain assessment using deep learning\nmethods and details the pain datasets used.\nChapter 4 outlines and proposes methods for evaluating demographic variables, their uti-\nlization, and their integration into an automatic pain assessment framework.\nChapter 5 discusses methods that utilize video and wearable device data, exploring the\ntrade-offs between efficiency and accuracy. It also proposes efficient, fast, effective models\nsuitable for real-world applications.\nChapter 6 explores synthetic data in pain assessment and introduces synthetic thermal im-\n1Under Review8 CHAPTER 1. INTRODUCTION\nagery techniques to enhance performance in automatic pain recognition.\nChapter 7 discusses general-purpose models, introduces a modality-agnostic framework,\nand presents the first foundation model used in automatic pain assessment.\nChapter 8 concludes the thesis with a final discussion, offering perspectives and ideas for\nfuture research in automatic pain assessment.Chapter 2\nClinical Pain Assessment\nContents\n2.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . . . . 20\n2.1 Chapter Overview\nThis chapter provides an anatomical and physiological overview of pain, focusing on the\nmechanisms responsible for generating, transmitting, processing, and interpreting pain sig-\nnals. It examines the various types of pain and explores the actions and expressions typically\nassociated with pain. Additionally, it reviews current pain assessment methods used in clin-\nical settings for adults, children, and newborns. The chapter also discusses developing and\nvalidating existing clinical pain assessment tools. This foundational knowledge is essen-\ntial for understanding the development and validation of computer-assisted pain assessment\n910 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nmethods discussed in later chapters. Finally, it highlights the challenges faced in clinical\npain assessment and underscores the need for automated pain assessment techniques.\n2.2 Biology of Pain\nPain, according to the International Association for the Study of Pain (IASP) [42], is \u201can\nunpleasant sensory and emotional experience associated with actual or potential tissue dam-\nage, or described in terms of such damage\u201d. Biologically, pain is an undesirable sensation\noriginating from the peripheral nervous system. Its fundamental function is to engage sen-\nsory neurons, notifying the organism of potential harm and playing a vital role in recognizing\nand responding to threats [43].\nThe transmission of a noxious stimulus from the periphery to the central nervous sys-\ntem involves a complex pathway through the spinal cord, resulting in the physical sensation\nof pain and a corresponding emotional response and memory. This process culminates in\nthe perception of pain. The initial stage of pain processing occurs when a stimulus at noci-\nceptive sensory fibers in the periphery is converted into an action potential. A nerve signal\nis generated if the stimulus is strong enough to surpass the action potential threshold [44].\nThis signal travels along the primary afferent fiber toward the central nervous system. As the\nstimulus intensity grows, more nerve fibers and areas of the nervous system are engaged [44].\nDue to their branching nature, primary afferent fibers typically relay information from sev-\neral pain receptors. These fibers and their receptors comprise a sensory unit, which gathers\ndata from a specific receptive field [44]. When receptive fields are larger and overlap with\nnearby fields, it becomes more challenging for the sensory system to locate the source of pain\naccurately. The primary afferent neuron is a pseudounipolar neuron that splits into a periph-\neral and central axon. The cell bodies of these neurons are located in the peripheral nervous\nsystem, within the posterior or cranial root ganglia. The peripheral axon extends to the skin,\nmuscles, tendons, or joints, branching into terminal fibers that connect with somatosensory\nreceptors. In contrast, the central axon leads to the central nervous system [45].\nPeripheral somatosensory fibers are categorized into three main groups. The first group\nincludes A\u00b4\u03b1,A\u00b4\u03b2,A\u00b4\u03b3fibers, large, myelinated fibers that rapidly conduct sig-\nnals [46]. These fibers involve touch and proprioception but are not associated with pain\nperception. The second group consists of A\u00b4\u03b4fibers, which are smaller and slower con-\nducting. Certain A\u00b4\u03b4fibers play a key role in pain sensation, with some responding only\nto intense mechanical stimuli and others reacting to noxious and non-noxious heat. The\nthird group comprises Cfibers, which are small, unmyelinated, and conduct signals very\nslowly. Most Cfibers are polymodal for pain perception, responding to various noxious\nmechanical, thermal, and chemical stimuli. These fibers are mainly linked to burning pain\nsensations [43]. The sensation of pain, known as nociception, is primarily facilitated by2.3. CLASSIFICATION AND CHARACTERISTICS OF PAIN 11\nvarious intracellular and extracellular molecular messengers. When activated by a specific\nstimulus, nociceptors relay information through glutamate, an excitatory neurotransmitter.\nAdditionally, inflammatory mediators are released at the injury site, further stimulating no-\nciceptor activation by releasing chemicals such as neurotransmitters ( e.g., serotonin), lipids\n(e.g., prostaglandins), peptides ( e.g., bradykinin), and neurotrophins ( e.g., nerve growth fac-\ntor) [46]. There are ascending tracts responsible for transmitting sensory information from\nthe periphery to the central nervous system. Fibers that convey two-point discrimination, tac-\ntile information, pressure, vibration, and proprioception ascend via the dorsal column of the\nspinal cord, forming the gracile and cuneate fasciculi. Fibers transmitting pain, temperature,\nand crude touch from somatic and visceral structures travel through the lateral spinothalamic\ntract. The anterior spinothalamic tract also transmits pain, temperature, and touch informa-\ntion to the brainstem and diencephalon (Figure 2.1) [47].\n2.3 Classification and Characteristics of Pain\nAccording to neurobiologist Clifford Woolf [48], pain can be classified into three categories\nbased on its function and characteristics: nociceptive ,inflammatory , and pathological pain.\nThese classes and their respective functions are illustrated in Figure 2.2.\nNociceptive pain (refer to Figure. 2.2(A)), arising from tissue damage, is a high-threshold\npain that activates only in response to intense stimuli [49], serving as a vital warning signal\nto the body. The neurobiological system responsible for nociceptive pain evolved from the\nability of even the most primitive nervous systems to detect impending or actual tissue dam-\nage caused by external stimuli. Its protective role requires immediate attention and action,\nachieved through the withdrawal reflex it initiates, the unpleasant sensation it produces, and\nthe emotional distress it triggers. Nociceptive pain demands avoidance in the present mo-\nment, and when activated, it overrides most other neural processes [48].\nInflammatory pain (refer to Figure. 2.2(B)) is also protective and adaptive, increasing\nsensory sensitivity following tissue damage to aid healing by discouraging movement and\ncontact with the injured area. This heightened sensitivity, or tenderness, helps prevent further\nharm and supports recovery, as seen after surgical wounds or inflamed joints where normally\nnon-painful stimuli now cause pain. It is triggered by immune system activation in response\nto tissue injury or infection. Despite its adaptive role, this pain often needs to be alleviated in\npatients with persistent inflammation, such as in rheumatoid arthritis or severe injuries [48].\nPathological pain (Figure. 2.2(C)) is maladaptive, arising from abnormal nervous sys-\ntem functioning and not serving a protective role. Unlike nociceptive and inflammatory pain,\npathological pain is a disease state of the nervous system itself. It may occur following\nnerve damage (neuropathic pain) or in conditions without apparent damage or inflammation\n(dysfunction l pain). Examples of dysfunctional pain include fibromyalgia, irritable bowel12 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFIGURE\n 2: Spinothalamic tract.\nPain, temperature, and some touch and pressure afferents end in the posterior horn. Second- or\nhigher-order fibers cross the midline, form the spinothalamic tract, and ascend to the ventral\nposterolateral (VPL) nucleus of the thalamus (and also to other thalamic nuclei not shown).\nThalamic cells then project to the somatosensory cortex of the postcentral gyrus, to the insula,\nand to other cortical areas (also not shown). Along their course through the brainstem,\nspinothalamic fibers give off many collaterals to the reticular formation (RF). The inset to the left\nshows the lamination of fibers in the posterior columns and the spinothalamic tract in a leg-\nlower trunk-upper trunk-arm sequence. The inset to the right shows the longitudinal formation\nof the spinothalamic tract. Primary afferents ascend several segments in Lissauer\u02bcs tract before\nall their branches terminate; fibers crossing to join the spinothalamic tract do so with a rostral\ninclination. As a result, a cordotomy incision at any given level would spare most of the\ninformation entering the contralateral side of the spinal cord at that level, and to be effective,\nthe incision must be made several segments rostral to the highest dermatomal level of pain.\n2017 Khalid et al. Cureus 9(10): e1754. DOI 10.7759/cureus.1754\n5\n of \n14\nFigure 2.1: The spinothalamic tract (STT) [43]. Pain, temperature, and some touch afferents\nend in the posterior horn, where second-order fibers cross the midline to form the\nspinothalamic tract, ascending to the thalamus and projecting to various cortical\nareas. Along the way, collaterals connect to the reticular formation. Due to the ros-\ntral inclination of fibers in Lissauer\u2019s tract, cordotomy must be performed several\nsegments above the pain level for effective relief.\nsyndrome, tension headaches, and temporomandibular joint disease, where significant pain\nexists without an apparent noxious stimulus or peripheral pathology. Pathological pain, a\nlow-threshold pain primarily driven by amplified sensory signals in the central nervous sys-\ntem, is the clinical pain syndrome with the greatest unmet need. To analogize, while nocicep-\ntive pain acts as a fire alarm for intense heat, and inflammatory pain reacts to warm tempera-\ntures, pathological pain is a false alarm triggered by a system malfunction. Thus, treatment\nmust specifically target the underlying mechanisms causing each type of pain [48].\nPain from a time-duration perspective can be categorized by duration into acute and2.4. PAIN INDICATORS 13\nchronic , with chronic pain persisting or recurring for more than three months [50]. Acute\npain is typically related to identifiable physiological damage from injury, surgery, illness,\ntrauma, or medical procedures and generally subsides once the underlying cause is resolved.\nHowever, if untreated, it may develop into chronic pain. Acute pain is further classified\nintoprocedural pain, caused by medical interventions such as muscular injections [51], and\npostoperative pain, which occurs after surgery and is a significant concern for both patients\nand healthcare providers. Effective management is crucial to aid recovery and prevent the\ntransition to chronic pain [52]. Chronic pain manifests in various forms, including chronic-\nrecurrent pain, like migraine headaches, and chronic-continuous pain, such as persistent low\nback pain [53].\n2.4 Pain Indicators\nPain can manifest in numerous ways and is often shaped by individual characteristics and\nenvironmental influences. Various human expressions, actions, and bodily responses have\nbeen linked to pain, serving both communicative and coping purposes. These pain indicators\nare generally categorized into three primary groups: (i)behavioral, (ii)physiological, and\n(iii)biochemical. While these indicators are universally present, certain expressions are more\nprominent in specific groups. For instance, crying is a common pain response across all age\ngroups but is more frequently observed in younger infants. This may be due to contextual\nfactors\u2014such as culture, social status, age, and ego\u2014influencing how pain is expressed\nover time. Adults, for example, may suppress crying in favor of other vocalizations, such as\ngroans and moans, as crying could be perceived as inappropriate in certain contexts. These\nmediating factors are often considered when interpreting pain indicators. The following\nsections will delve into each of these three categories [51].\n2.4.1 Behavioral Indicators\nBehavioral indicators such as facial expressions ( e.g., grimacing, open mouth, raised eye-\nbrows), vocalizations ( e.g., crying, moaning, screaming), and various bodily movements\n(e.g., changes in posture, signs of tension) are vital markers used in assessing pain [22].\nFacial expressions and limb movements in response to acute pain are typically rapid and\ninvoluntary. Facial reactions include brow bulging, eye squeezing, nasolabial furrow forma-\ntion [54], grimacing, clenched teeth, jaw-dropping, and tightened lips [55]. Body movements\nassociated with pain include bracing (gripping an object or the affected area during move-\nment), rubbing (massaging the painful area), restlessness (constant shifting of position) [55],\nand knee flexion [56]. Non-verbal vocalizations such as groaning, moaning, sighing, crying,\nand gasping [57] also indicate pain. Verbal expressions like \u201couch\u201d ,\u201cstop\u201d ,\u201cthat hurts\u201d ,14 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFigure 2.2: Pain classification [48]: (A)Nociceptive pain , which results from detecting po-\ntentially harmful stimuli and serves a protective function. (B)Inflammatory pain is\nlinked to tissue damage and immune cell infiltration, increasing pain sensitivity dur-\ning healing. (C)Pathological pain is a disease state caused by either nervous sys-\ntem damage (neuropathic) or abnormal nervous system function (dysfunctional).\n\u201cthat is enough\u201d , and even cursing [55] also serve as pain indicators. Interestingly, swearing\nhas been found to significantly alleviate pain, although its effect diminishes with frequent\nuse over a short period [58, 59].2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 15\n2.4.2 Physiological Indicators\nVital signs can reflect the state of the central nervous system, and since pain is mediated\nthrough this system, trends in vital signs can provide insights into pain levels. Clinical stud-\nies [60, 61] have examined physiological changes in response to pain and established em-\npirical solid evidence linking pain to vital sign alterations. However, as vital signs can also\nchange due to other non-pain-related pathological conditions, it is recommended that they be\nassessed alongside behavioral pain indicators for accuracy. Physiological pain responses are\nconsidered more reliable than behavioral signals, as they cannot be consciously controlled\nor altered. Physiological measurements such as electrocardiography (ECG), electromyogra-\nphy (EMG), galvanic skin responses (GSR), and respiration rate provide critical insights into\nthe body\u2019s reaction to pain [17]. In addition, brain monitoring techniques like near-infrared\nspectroscopy (fNIRS) have demonstrated the ability to detect pain-related hemodynamic\nchanges [23]. At the same time, functional magnetic resonance imaging (fMRI) has been\nexplored for assessing pain in both normal and pathological conditions [62].\n2.4.3 Biochemical Indicators\nCompared to other pain indicators, biochemical changes are the most precise and sensitive\nreactions to pain. However, their routine use in pain assessment is restricted due to the\ninvasive nature of measurement techniques [63]. These biochemical responses are most\nevident during surgical procedures with limited anesthesia, leading to increased levels of\nendorphins, norepinephrine, cortisol, growth hormones, renin, glucagon, aldosterone, and\ncatecholamines, along with a decrease in insulin levels [60].\n2.5 Sociodemographic and Psychological Variables\nIn1965 , Melzack and Wall [64] introduced the \u201cGate Control Theory\u201d , which interprets pain\nfrom two perspectives. The first involves the mechanisms of nociceptive signal transmission\nand modulation, while the second emphasizes pain as a psychophysiological phenomenon\narising from the interaction between physiological and psychological factors [53]. Observa-\ntions, empirical research, and theoretical models increasingly suggest that a comprehensive\nunderstanding of pain requires a biopsychological approach. It is also becoming apparent\nthat, although pain is often regarded as private and subjective, it is also fundamentally a so-\ncial experience [53]. Pain is not solely explained by biomedical components ( e.g., muscle\ndamage) but also involves psychological ( e.g., cognitive, affective) and social factors ( e.g.,\nfriends, family, health professionals), leading to what is known as a biopsychosocial sensa-\ntion [65]. Numerous factors contribute to how painful experiences are expressed and per-\nceived, varying wildly due to social and personal biases. These factors prompted Williams16 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nand Craig [2] to define pain as \u201ca distressing experience associated with actual or potential\ntissue damage with sensory, emotional, cognitive, and social components. \u201d\n2.5.1 Sex and Gender\nSeveral studies have explored the relationship between gender and pain expressiveness, as\nwell as variations in pain reporting. Research indicates that women generally exhibit a lower\npain threshold compared to men. A meta-analysis by Boerner et al. [66] on gender differ-\nences in children and adolescents found that girls over the age of 12reported higher pain\nintensity in response to cold-induced pain than boys. Furthermore, multiple studies suggest\nthat women tend to describe a greater degree of pain compared to men. In addition to bi-\nological differences, psychological aspects linked to gender also play a role. For instance,\nindividuals with a masculine identity may be less inclined to express or report their pain or\nseek assistance [67].\nMoreover, the manifestation of pain is not only influenced by the individual\u2019s gender but\nalso by dyadic interactions between people of different sexes. Levine and Desimone [68]\nconducted one of the initial studies on this phenomenon, showing that male participants in\na cold pressure experiment reported lower pain intensity when a female experimenter was\npresent. Similarly, McClelland and McCubbin [69] found that female participants expressed\nand reported higher pain levels when accompanied by a female friend. This dynamic also\nextends to patient-healthcare provider interactions. In studying health records, Vigil and\nAlcock [70] discovered that when the pain intensity was reported as high, the patients ( i.e.,\nmen and women) were examined by a female doctor or nurse. Additionally, studies exam-\nining gender differences among physicians in pain treatment options revealed that female\npatients were more likely to receive prescriptions for more potent drugs, such as analgesics,\nand female physicians were more likely to prescribe medications. Extensive research has\nalso shown that both lay observers and healthcare professionals tend to estimate higher pain\nlevels for female patients compared to male patients [71]. Hooper et al. [72] further noted\nthat clinicians communicate more effectively with female patients, often displaying greater\nempathy. Gender roles, beliefs, and expectations play a significant role in understanding\nhow social factors influence the differences in pain perception and experience between men\nand women [73].\n2.5.2 Age\nAge plays a crucial role in pain assessment and management. At the same time, there are\nsignificant challenges, limitations, and biases related to the patient\u2019s age group. Two of the\nmost vulnerable groups, albeit for different reasons, are the elderly and infants.\nPain recognition and interpretation among the elderly, particularly by caregivers, often2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 17\npresent unique challenges. Older adults frequently exhibit stoicism and reluctance to ex-\npress their pain, while healthcare providers struggle to accurately assess the patient\u2019s pain,\nleading to inappropriate pain management decisions [74]. McPherson et al. [75] noted that\ndespite caregivers\u2019 accommodating and empathetic relationships with elderly patients, con-\nflicts still arise. Older patients may resist acknowledging their weaknesses and accepting\nhelp, which can cause them to conceal their pain. The situation becomes even more complex\nwhen dealing with dementia, a disorder encompassing a range of conditions ( e.g., Parkin-\nson\u2019s, Alzheimer\u2019s, Vascular dementia), characterized by abnormal brain changes that im-\npair cognitive and linguistic abilities. A person with dementia may find it challenging to\ncommunicate their pain verbally. However, non-verbal pain expressions remain intact even\nin moderate dementia, although such reactions can be exaggerated [76]. However, aggres-\nsive behavior and disturbances in dementia patients, often caused by pain, are frequently\nmisinterpreted as psychiatric symptoms, leading to improper medication that can have life-\nthreatening consequences [77]. Caregivers of dementia patients face additional challenges,\nnot only related to pain management but also in addressing dementia\u2019s impact on language\nand memory. Particularly in the later stages of dementia, patients encounter severe pain\ncommunication difficulties due to cognitive decline, necessitating that caregivers recognize\nbehavioral and contextual indicators of pain [74]. Age is also known to cause changes in\nskin characteristics, such as texture, rigidity, and elasticity, which impact the performance of\nemotional face recognition tasks [78].\nInfants represent another vulnerable age group where pain assessment requires special-\nized attention, particularly when they experience painful events. The first challenge is obvi-\nously their limited reporting ability to express their pain through language. Although crying\nmight appear to signal pain, this is an oversimplified and unreliable method, as crying can in-\ndicate a variety of situations, such as discomfort, hunger, or pain. Accurately discerning the\ntype of cry is only one part of the challenge; assessing pain in infants is far more complex and\ninfluenced by numerous factors, including the interpersonal relationships within their envi-\nronment. Riddell and Racine [79] found that through various distressing experiences, infants\ncan learn that specific signaling behaviors can prompt their caregiver\u2019s proximity. This at-\ntachment dynamic suggests that, to some extent, infants may consciously utilize pain-related\nbehaviors to elicit responses from their caregivers. Similarly, the context affects older chil-\ndren as well; for example, self-reports of pain tend to be significantly lower when a parent is\npresent compared to when the child is alone [80].\n2.5.3 Psychological Factors\nMultiple studies have revealed that several psychological factors are consistently linked with\npain-related behavior, including depression, pain-related fear, and catastrophizing. Research18 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nfocusing on the impact of depression and anxiety on pain-related behavior has been con-\nducted mainly on patient populations. These studies have shown that depressed individ-\nuals exhibit more pronounced protective and communicative behaviors compared to non-\ndepressed patients [81]. Similarly, numerous studies suggest that patients with higher levels\nof anxiety demonstrate more pain-related behaviors than those with lower anxiety levels [82].\nDespite the frequent coexistence of pain with psychological conditions, research indicates\nthat these patients often experience underestimation of their pain. For instance, De Ruddere\net al. [83] found that patients dealing with psychological stressors such as anxiety, depres-\nsion, and daily life challenges are often perceived by physiotherapists as experiencing less\nsevere pain, illustrating the influence of psychosocial factors on the patient\u2019s pain experience.\n2.5.4 Race and Culture\nPain expression is generally understood across ethnicities and cultures, though differences\nexist in how it is conveyed [4]. However, cultural variations and the nuances of facial ex-\npressions related to emotion are complex and necessitate deeper study. Additionally, racial\nand cultural biases significantly influence pain assessment, judgment, and interpretation.\nExtensive research highlights the impact of a patient\u2019s race as a sociodemographic factor\non observer responses. The most examined topic relates to the different responses toward\nCaucasian versus non-Caucasian individuals, particularly African Americans, who are more\nlikely to have their pain underestimated and undertreated by healthcare providers [84].\nEthnocultural factors are crucial in shaping how individuals perceive and express pain.\nFor example, Western cultures often emphasize conservative expressions and self-control,\nleading to restrained responses in personal pain experiences and in perceiving others\u2019 pain\n[3]. Differences also arise in coping mechanisms; African Americans, for instance, are more\nprone to catastrophizing pain events compared to European Americans [85]. Furthermore,\nevidence shows racial biases in pain treatment across various racial groups, with certain\ngroups being more sensitive to pain but receiving lower-quality treatment [86]. For exam-\nple, Cleeland et al. [87] found that minority cancer patients, mainly Black and Hispanic\nindividuals, were more likely to experience inadequate analgesia compared to non-minority\npatients.\n2.5.5 Observer\u2019s Impact on Pain\nThe variability in pain management stems from the interplay of various elements, including\nsociocultural, biomedical, and psychosocial factors, especially in cases of chronic pain [88].\nWhen it comes to the observer responsible for assessing a patient\u2019s pain, several characteris-\ntics directly influence the objectivity of their evaluation. The first and perhaps most critical\nfactor is the observer\u2019s experience level. One would expect that more experience leads to2.6. IMPACT OF INADEQUATE PAIN MANAGEMENT 19\nbetter and more accurate assessments, but studies show that even experienced healthcare\nproviders consistently underestimate pain, much like laypersons [28]. The greater the expe-\nrience, the more pronounced the underestimation tends to be. This may be due to desensitiza-\ntion caused by repeated exposure to pain events, as seen in the differences between internists\nand surgeons in their evaluation of postoperative pain, with surgeons often encountering se-\nvere pain more regularly [89]. Another significant factor is the observer\u2019s knowledge and\nbeliefs about pain. For example, [83] found that laypersons and healthcare professionals\nwithout physical signs of pain might view the patient\u2019s complaints less seriously. Proper\ntraining is also essential for adequate pain assessment, which is why the Department of\nHealth and Human Services (DHHS) initiated a strategic program to improve healthcare\nproviders\u2019 education and knowledge regarding pain management, following evidence of in-\nadequate training in the field [90].\n2.6 Impact of Inadequate Pain Management\nThe experience of pain, particularly persistent pain, can have detrimental effects on the indi-\nvidual and their surrounding environment. Thoughts about severe pain often lead to grief and\nfear, causing individuals to perceive pain as a threat and feel incapable of managing it. This\ncan prompt avoidance behaviors aimed at escaping perceived harm [91]. Studies have shown\nthat children with a catastrophizing mindset about pain struggle with daily activities, while\nadolescents with chronic pain tend to have fewer friends and may miss out on social and\nentertainment opportunities, putting them at greater risk of victimization [92]. These adoles-\ncents often feel isolated and lonely compared to their healthy peers, and they may experience\nanxiety in social interactions [93]. Parental reactions to their children\u2019s pain can further com-\nplicate the situation, as parents with catastrophic tendencies tend to engage in overprotective\nbehaviors that hinder the child\u2019s functioning and psychosocial development [94]. Addition-\nally, the family\u2019s overall dynamic is affected, with the patient\u2019s sadness, sleep disorders, and\nchanges in leisure activities impacting the household [74].\nOn a biological level, pain, particularly when experienced early and severely, can alter\nthe brain and nervous system. These early pain experiences can disrupt neurobiological\ndevelopment and affect how pain is processed later in life [95]. A growing body of research\nlinks chronic pain to changes in the medial prefrontal cortex, a region crucial to emotional\nprocessing. Chronic pain is associated with structural and biochemical alterations in this\nbrain area, suggesting that these changes play a role in the pathophysiology of chronic pain\n[96].20 CHAPTER 2. CLINICAL PAIN ASSESSMENT\n2.7 Pain Measurement Scales and Metrics\nIn clinical settings, self-reporting remains the gold standard for assessing pain, allowing in-\ndividuals to describe their pain\u2019s intensity and location. Various self-report scales have been\ndeveloped for different age groups, such as the visual analog scale (V AS) [97] and the verbal\nrating scale (VRS) [98]. Additionally, observation-based scales, where a third party eval-\nuates the pain\u2019s severity, include tools like the Prkachin and Solomon pain intensity scale\n(PSPI) [99] and the neonatal/infant pain scale (NIPS) [100]. However, some studies suggest\nthat patients may exaggerate their pain severity to prompt more aggressive treatment inter-\nventions [101], raising concerns about the accuracy of self-reported symptoms. Therefore,\nobjective pain measurement remains clinically crucial.Chapter 3\nAutomatic Pain Assessment\u2013A Literature\nReview\nContents\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database . . . . 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 49\n2122 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research Di-\nrections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.1 Chapter Overview\nThis chapter corresponds to the publication [17], a systematic literature review (SLR) con-\nducted at the start of this Ph.D. research. This review facilitated an understanding of au-\ntomatic pain assessment methods, particularly those based on deep learning, and the tech-\nniques and strategies employed. It enabled the identification and proposal of new approaches\nthat could enhance the effectiveness of pain recognition.\nAdditionally, it allowed for identifying gaps in the literature from other reviews con-\nducted on this specific research topic. Every existing systematic review on pain assessment\nwas identified and assessed, revealing several insights. The first review on automatic pain\nassessment, published by Prkachin in 2009 [102], did not cover papers on deep learning,\nas the practical implementations of deep architectures only began around 2012. Zamzmi et\nal.[103] focused their review exclusively on infants, omitting deep learning methods. In\n2018, Chen et al. [104] reviewed automated pain detection methods using the Facial Ac-\ntion Coding System (FACS), noting only three publications that employed deep learning\ntechniques. In 2019, Hassan et al. [105] included only seven papers that used deep learn-\ning methods in their review. Similarly, Werner et al. [106], also in 2019, discussed pain\nassessment without restrictions on modalities or age groups, finding fewer than ten papers\nthat reported on deep learning methods. In 2020, Al-Eidan et al. [107] published the first\nsystematic literature review titled \u201cDeep-Learning-Based Models for Pain Recognition: A\nSystematic Review\u201d, which included fifteen papers but was critiqued for having significant\nlimitations and incorrect information. It was noted that some papers analyzed might not be\nrelevant, and there was confusion between \u201cneural networks\u201d and \u201cdeep learning\u201d. For in-\nstance, while study [105] mentioned using neural network approaches, they did not provide\nevidence of using deep learning methods. Moreover, in the study [104], the authors devel-\noped a neural network with only two layers combined with handcrafted features, which does\nnot qualify as a deep learning method. Additionally, studies [103, 107] focused on detect-\ning protective movement behaviors in chronic pain patients, which deviates from the central\ntopic of automatic pain assessment. Several reviews and SLRs on automatic pain assessment\nhave been published, but none exclusively or adequately focus on deep learning methods.\nThis SLR aims to bridge this gap by thoroughly reviewing deep learning techniques used for\nautomatic pain assessment.3.2. MODALITIES AND HARDWARE FOR AUTOMATIC PAIN ASSESSMENT 23\n3.2 Modalities and Hardware for Automatic Pain Assessment\nCreating an automatic pain assessment system hinges on capturing the necessary input data\nthrough various information channels, referred to as modalities. These modalities are cat-\negorized into behavioral and physiological types. A system utilizing only one modality is\ntermed unimodal, whereas a multimodal system incorporates multiple modalities.\nKey behavioral modalities encompass facial expressions, body movements, gestures, and\nauditory signals. Researchers use a range of optical and light sensors to record images or\nvideo sequences of facial and body movements. Commonly, researchers employ color RGB\ncameras, but depth and thermal sensors are also used to enhance visual data. Motion capture\nsensors are also employed to track movements, and microphones are frequently employed\nto capture sound. On the physiological front, modalities often involve biosignals that detect\nelectrical activities from various tissues and organs. Techniques such as electrocardiogra-\nphy (ECG), electromyography (EMG), electrodermal activity (EDA), photoplethysmogra-\nphy (PPG), blood oxygen saturation (SpO2), near-infrared spectroscopy (NIRS), respiration\nrate, and skin temperature are commonly used to gauge pain. Multiple sensors can mea-\nsure several modalities simultaneously \u2014 for instance, strain sensors and cameras can track\nrespiration rates.\nBesides the sensors that gather input data, the computational hardware is crucial. Deep\nlearning-based systems operate in two phases: training and inference. The training phase is\nparticularly resource-intensive, necessitating a graphics processing unit (GPU). The trained\nmodel makes predictions on new data during inference, typically processed on a central\nprocessing unit (CPU). The choice of hardware depends on various factors, especially in\nreal-time scenarios where low latency is crucial, compared to offline settings where data\nprocessing can be deferred. Additionally, characteristics of the model, such as floating point\noperations per second (FLOPS) and total computational operations, are significant consider-\nations.\n3.3 Pain Databases\nAccess to data is crucial for evaluating methods and algorithms in automatic pain assess-\nment. However, only a few databases have explicitly been developed for automatic pain\nrecognition based on human behavioral and physiological changes. Unlike the extensive\ndata found in most facial expression databases, publicly accessible pain datasets often offer\nlimited samples and suffer from significant class imbalance. This primarily stems from the\nethical concerns associated with collecting pain data. Table 3.1 lists the principal databases\nreviewed in the studies. Figure 3.1 shows how frequently each database was used. Most\nresearch utilized publicly available datasets, with some studies exploring multiple datasets.24 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nFew studies used private datasets, mainly those aimed at detecting pain in neonates. The\nUNBC-McMaster Shoulder Pain Archive Database [108] is the most utilized, followed by\nTheBioVid Heat Pain Database [109]. The former contains 200facial videos of 25individ-\nuals with shoulder pain. At the same time, the latter includes facial videos and biopotentials\nof90healthy participants subjected to experimentally induced heat pain at four intensity\nlevels. The following subsections provide a brief description of some of these datasets.\nTable 3.1: Most commonly utilized pain databases.\nDatabase Modality Population Annotation\nGranularityAnnotation Labels\nUNBC-McMaster\nShoulder PainA[108]RGB video of face 25 adults with shoulder painFrame level\nSequence levelFACS\nV AS, OPI\nBioVidA[109] RGB video of face, EDA, ECG,\nEMG87 healthy adults Sequence level stimulus\n(calibrated per person)\nMIntPAINA[110] RGB-Depth-Thermal video of\nface20 healthy adults Sequence level stimulus\n(calibrated per person),\nV AS\niCOPEA[111] RGB photographs of face 26 healthy neonates Frame level pain, cry, rest, air puff,\nfriction\niCOPEvidA[112] Grayscale video of face 49 neonates Sequence level pain, no pain\nNPAD-IA[113] RGB video of face & body, HR,\nSpO2, BP, NIRS36 healthy neonates & 9 neonates\nwith tissue injured by surgerySequence level NIPS, N-PASS\nAPN-dbA[114] RGB video of face 112 healthy neonates Sequence level NFLAPS, NIPS, NFCS\nEmoPainN[115] video, audio, EMG, MoCap 22 adults with chronic pack pain &\n28 healthy adultsSequence level self-report, naive OPI\nSenseEmotionN\n[116]video of face, audio, EDA, ECG,\nEMG, RSP45 healthy adults Sequence level stimulus\n(calibrated per person)\nX-ITEN[117] RGB-Thermal video of face,\nRGB-Depth video of body, au-\ndio, EDA, ECG, EMG134 healthy adults Sequence level stimulus\n(calibrated per person)\nA: Publicly available by request, complete or part of the dataset N: Not yet available Modality: HR: heart rate SpO2: oxygen saturation rate BP:\nblood pressure NIRS: near-infrared spectroscopy MoCap: motion capture RSP: respiration rate EDA: electrodermal activity ECG: electrocardiogram EMG:\nelectromyogram Annotation Labels: FACS: Facial Action Coding System V AS: visual analogue scale OPI: observer pain intensity NIPS: neonatal infant\nscale N-PASS: neonatal pain, agitation and sedation scale NFLAPS: neonatal face and limb acute pain scale NFCS: neonatal facial coding system\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database\nTheUNBC McMaster Shoulder Pain Database [108] comprises 200video sequences show-\ning the facial expressions of 25subjects undergoing motion tests, including arm abduction\nand external and internal rotations. The data collection utilized both active and passive ap-\nproaches: in the active mode, subjects moved their affected arms to their bearable limit,\nwhile in the passive mode, a physiotherapist moved the subjects\u2019 arms. Each video sequence\ncontains about 60to700frames, totaling 48,398, with 82.71% of frames scoring a pain\nrating of zero, indicating a significant imbalance in the data. All frames are FACS-coded\nfor pain-related action units (AUs)\u2014AU4, AU6, AU7, AU9, AU10, AU12, AU20, AU25,\nAU26, AU27, and AU43\u2014with each AU coded for intensity from A to E, 0, or5, except\nfor AU43 (closed eyes), which is coded as either present or absent. Pain scores are assigned\nusing the PSPI metric based on the intensity of the AUs present. Additionally, the database3.3. PAIN DATABASES 25\n0815233038455360\nUNBC-McMasterBioVidEmoPainSenseEmotionX-ITEMIntPAINiCOPEiCOPEvidNPAD-IAPN-dbother\nTable 1Category AUNBC-McMaster59BioVid21EmoPain7SenseEmotion5X-ITE2MIntPAIN4iCOPE3iCOPEvid1NPAD-I5APN-db1other21\n1\nFigure 3.1: The number of studies utilizing these specific datasets. Note that various studies\nused multiple datasets to conduct their experiments.\nincludes 66facial landmarks per frame, determined by an active appearance model. Pain as-\nsessments also include self-reports using two Likert scales with 15options each and a visual\nanalog scale (V AS) from 1(no pain) to 10(extreme pain). One scale measures the sen-\nsory intensity from \u201cextremely weak\u201d to \u201cextremely intense\u201d , while the other assesses the\naffective-motivation aspect of pain from \u201cbearable\u201d to \u201cextremely excruciating\u201d. Indepen-\ndent observer pain intensity (OPI) ratings use a 6-point scale from 0(no pain) to 5(intense\npain). The UNBC database is currently the most extensively utilized dataset for automatic\npain recognition among publicly available resources.\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database\nTheBioVid dataset [109] is a prominent resource in pain research, comprising facial videos,\nelectrocardiograms, electromyograms, and galvanic skin response data from eighty-seven\npn=87qhealthy participants ( 44males and 43females, aged 20to65). The pain was in-\nduced using a thermode on the participants\u2019 right arm, with pain and tolerance thresholds\nestablished before data collection. These thresholds defined the range of pain from No Pain\n(NP) to Very Severe Pain (P 4), encompassing five levels of pain intensity. The temperatures\nfor the pain inductions ranged from P 1to P 4and did not exceed 50.5\u02ddC. Each participant\nunderwent 20inductions at each of four pain levels, with each induction lasting 4sfollowed\nby a recovery period of 8to12s. In addition, 20baseline measurements were taken at 32\u02ddC\n(NP), totaling 100stimulations per participant, randomly administered. Data processing\nsegmented these into 5.5sdurations starting 1safter the target temperature was reached, re-26 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsulting in 8,700samples across the five pain intensity classes, equally distributed among all\nmodalities for each participant. Video recordings were made at a frame rate of 25FPS, and\nbiosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\n3.3.3 The EmoPain Database\nTheEmoPain [115] dataset encompasses various pain indicators, including body movements,\naudio, biosignals, and postural and facial expressions. It features video and audio recordings\nof22patients ( 7male, 15female) exhibiting natural pain expressions while engaging in\nphysiotherapy-like exercises. These exercises, performed at regular and challenging levels,\ninclude a sitting-standing sequence, balancing on one leg for five minutes, and reaching for-\nward while standing. The video signals are captured in high resolution ( 1024\u02c61024 pixels)\nusing eight cameras positioned at various angles, enhanced by specialized lighting condi-\ntions. Audio is recorded with two microphones: an AKG C-1000S MKIII placed near the\ncameras and an AKG HC 577 L worn by the patients, both operating at a 48kHz sampling\nrate with bit Pulse Code Modulation. Body movements and postures are tracked using a mo-\ntion capture suit with 18sensors distributed across the body. Biosignals are monitored with\nfour sEMG sensors attached to the trapezius and lumbar para-spinal muscles. Additionally,\nthe dataset provides continuous frame-wise pain ratings for facial expressions by eight naive\nannotators and binary frame-wise annotations for protective behaviors by four experts, along\nwith coordinates from 26body nodes. Six annotated protective behaviors include stiffness,\nbracing, hesitation, limping, rubbing, and abrupt actions. Audio and EMG signals from the\neight activities per subject also contribute to multimodal pain recognition. Like the UNBC\ndatabase, EmoPain faces significant challenges due to data sparsity and imbalance\u2014only\n11.4%of frames show facial expressions of pain, and 8.6%show protective behaviors. This\nscarcity complicates pain recognition research, necessitating the development of methods\nthat efficiently utilize limited data to achieve optimal performance.\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database\nTheX-ITE [117] dataset is one of the largest pain datasets but is not publicly available. It\ninvolved 134healthy adults ( 67men and 67women) aged between 18and50. The aver-\nage age was 31.4years (SD = 9.7), with men averaging 33.4years (SD = 9.3) and women\n32.9years (SD = 10.2). Participants had no chronic pain, depression, psychiatric disorders,\nneurological conditions, headache syndromes, or cardiovascular disease, nor had they taken\npain medication or painkillers before the experiment. Pain stimuli were stimulated using the\nMedoc PATHWAY Model ATS for heat pain on the forearm and the Digitimer DS7A for elec-\ntrical pain on the index and middle fingers. Both modalities featured phasic stimuli (short,\n5seconds) and tonic stimuli (long, 60seconds), each in three intensities. After calibration,3.4. UNIMODAL STUDIES 27\nparticipants underwent a 90-minute stimulation phase where phasic stimuli were repeated\n30times in a randomized sequence with 8-12-second pauses. The tonic stimuli were applied\nonce per intensity, totaling six per participant, each followed by a five-minute pause. The\nhighest intensity tonic stimuli for heat and electrical pain were induced at the experiment\u2019s\nending, with the other stimuli randomly interspersed during the phasic period. Simultane-\nous to the pain stimulation, various sensors collected multimodal pain response data: frontal\nand side view RGB videos for facial expression and head pose analysis, audio for paralin-\nguistic response analysis, electrocardiogram (ECG) to monitor heart rate variability, surface\nelectromyography (EMG) to assess muscle activity in the trapezius, corrugator supercilii,\nand zygomaticus major, electrodermal activity (EDA) to measure sweating, video for body\nmovement analysis, and thermal video for facial temperature changes.\n3.3.5 The AI4Pain Database\nThe AI4Pain Grand Challenge 2024 [118] dataset is a recent contribution to the pain re-\nsearch field, tailored for sophisticated pain recognition tasks using fNIRS and facial video\ndata. This dataset involves sixty-five volunteers pn=65q, including 23females, with ages\nranging from 17to52years (mean age of 29.06years and a standard deviation of 8.28years).\nAlthough it captures physiological signals such as photoplethysmography (PPG), electroder-\nmal activity (EDA), and respiration (RESP), these signals are not publicly available yet. The\ndataset is segmented into three parts: training ( 41volunteers), validation ( 12volunteers),\nand testing ( 12volunteers). The experimental setup includes fNIRS data recorded with an\nArtinis device, measuring changes in oxygenated and deoxygenated haemoglobin concentra-\ntions across 24channels targeting the prefrontal cortex. The optodes configuration includes\n10sources and 8detectors spaced 30mm apart, using near-infrared light at 760nm and 840\nnm, sampled at 50Hz. Additionally, facial movements are captured by a Logitech Stream-\nCam at30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low Pain ,\nandHigh Pain . It features 65instances of No Pain (each lasting 60s),780instances of Low\nPain (each lasting 10s), and 780instances of High Pain (each lasting 10s). The No Pain\ninstances, recorded during baseline, serve as control data. The Low Pain instances reflect\nmild pain responses, and the High Pain instances capture significant pain, both derived from\na pain tolerance test and reflected in the corresponding neurological and behavioral data\nrecorded.\n3.4 Unimodal studies\nThis section presents the studies that utilized only one information channel to estimate the\nsubject\u2019s pain condition.28 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.1 Vision-based: Static Analysis\nThe first publicly available pain database that significantly contributed to the development\nof automatic pain assessment methods was the UNBC-McMaster Shoulder Pain Database .\nNumerous studies have employed this dataset. Pedersen [119] implemented the first deep\nlearning approach in 2015 to address the pain assessment problem, utilizing a 4-layer contrac-\ntive autoencoder. He combined the encoded representations with a support vector machine\n(SVM), achieving high performance in frame-level pain detection. A significant advance-\nment in vision-based pain recognition methods was the EmoPain challenge in 2020, which\nbecame the first international competition to compare machine learning methods for chronic\npain assessment. Egede et al. [120] presented the EMOPAIN 2020 Challenge , utilizing a\ndataset composed of features extracted via both handcrafted methods and deep-learned mod-\nels. They utilized facial landmarks, histogram of oriented gradients (HOG), and deep vectors\nfrom VGG-16 [121] and ResNet-50 [122], both pre-trained on the Aff-Wild dataset1. The au-\nthors report that combining hand-engineered features with deep learning cues led to the best\nperformance. Similarly, Yang et al. [123] extracted both low- and high-level features from lo-\ncal descriptors and the pre-trained VGG-16 CNN, combining them through weighted coeffi-\ncients. Semwal and Londhe [124] demonstrated that fusing deep-learned features with facial\nlandmarks is beneficial for multi-class pain estimation. Lakshminarayan et al. [125] com-\nbined deep-learned features with handcrafted ones\u2014namely features from VGG-16 [121]\nandResNet-50 [122], HOG, action unit occurrence and intensity, facial landmarks, and head\npose\u2014through a fully connected network. Their study found that combining VGG-16 with\nhandcrafted features lowered regression error, whereas [126] achieved maximal performance\nusing only VGG-16 features with a fully connected network.\nConversely, Semwal and Londhe [127] noted the limitations of traditional handcrafted\nfeature engineering and the computational expense of deep neural networks. As a solution,\nthey proposed a relatively shallow 4-layer CNN, which reduces computational costs due to\nfewer parameters while achieving performance comparable to deeper models. A different\napproach came from [128], where the authors focused on representing facial expressions\nas compact binary codes for pain intensity classification. Feature extraction was conducted\nusing a pre-trained model [129], with a fully connected network used to generate the binary\ncodes.\nSeveral studies utilized CNN ensemble designs with varying architectures to exploit fea-\nture diversity. Semwal and Londhe [130] combined predictions from three compact CNNs\u2014\nVGG-16 ,M-MobileNet [131], and GoogleNet [132]\u2014using the average ensemble rule, re-\nsulting in improved classification performance. Kharghanian et al. [133] developed a con-\nvolutional deep belief network (CDBN) using unsupervised feature learning. An SVM used\n1https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge3.4. UNIMODAL STUDIES 29\nthe extracted features to differentiate between two states for binary pain classification ( i.e.,\npain vs. no pain). Later, [134] added two layers to the CDBN, though the results were not\ndirectly comparable due to differing evaluation methods.\nSeveral papers suggest that because pain is predominantly expressed in specific facial\nregions, focusing on these areas rather than the whole face could improve model accuracy by\nreducing noise. Huang et al. [135] initially identified the left eye, right eye, nose, and mouth\nas key regions and utilized a multi-stream CNN for feature extraction, assigning learned\nweights to enhance attention on these regions. Xin et al. [136] employed a 9-layer CNN\nwith an attention mechanism to assign different weights to face regions, resulting in more\naccurate attention face maps and boosting prediction accuracy by up to 19%. Cui and Huang\n[137] introduced a multi-scale regional attention network (MSRAN), which uses multiple\ncropping regions from video frames. The framework includes self-attention and relation-\nattention modules to highlight pain-relevant regions and explore interrelationships. Li et\nal.[138] extended this concept by integrating contrastive and multi-task training through an\nautoencoder, building on the work of [139].\nOne challenge in pain intensity estimation is that individual facial features, such as face\nshape, can introduce significant variability in how pain is expressed. This makes it difficult\nto distinguish between adjacent intensity levels. To address this, Peng et al. [140] examined\nfacial shape information and developed a deep multi-task network to account for the rela-\ntionship between pain recognition and shape, which improved pain estimation performance.\nSimilarly, Xin et al. [141] proposed a novel multi-task framework that combines a CNN\nfeature learning module with an autoencoder attention component, also estimating subject\nidentity, as individual differences in pain manifestation are key. Their experiments achieved\nstate-of-the-art results on publicly available datasets.\nMost studies report results obtained from controlled laboratory settings, which typically\nfeature proper lighting, minimal head pose variability, and no occlusions. However, such\nconditions do not represent typical hospital environments. Semwal and Londhe [142] ad-\ndressed this by focusing on pain assessment in uncontrolled settings, developing a shallow\nCNN with three convolutional layers that performed comparably to deeper pre-trained mod-\nels. In a subsequent study [143], they introduced a more complex framework comprising\nthree modules that leveraged high-level spatial descriptors with both local and global geomet-\nric cues, achieving results comparable to models like GoogleNet [144] and VGG [121]. Lee\nand Wang [145] explored pain assessment in intensive care unit (ICU) settings, where par-\ntially occluded faces frequently complicate facial analysis. They developed a 4-layer CNN\ncombined with an extreme learning machine (ELM) for final estimation. Virrey and Cae-\nsarendra [146] used CNNs to classify sections of frames where pain was triggered, peaked,\nand subsided. Nugroho et al. [147] tackled pain detection in smart home-care settings, par-\nticularly for elderly patients, using relatively low-power mobile devices. They modified the30 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nOpenFace2library, based on pre-trained FaceNets [148], and showed that transfer learning\ncould enable real-time binary classification ( pain vs.no pain ), even on low-powered hard-\nware.\nResearchers like Dai et al. [149] and Menchetti et al. [150] have noted that most models,\nwhether deep or shallow, are trained on dataset-specific features rather than actual pain-\nrelated features. Moreover, most studies employ validation methods using the same dataset,\nwhile cross-dataset performance is rarely addressed, limiting real-world applicability. To\ntackle these issues, Dai et al. [149] combined pain and emotion detection datasets to develop\na real-time pain assessment system with better generalization capabilities. They emphasized\nthe importance of cross-corpus evaluation, real-time testing, and the need for well-balanced,\necologically valid pain datasets [151].\nSeveral studies have explored combining pain scales to improve prediction objectivity\nand reliability. Liu et al. [152] developed a two-stage personalized model trained using active\nappearance model (AAM) facial landmarks and multi-task learning, with visual analog scale\n(V AS) and observed pain index (OPI) as ground truth. Xu et al. [153] similarly reduced\nmean square error (MSE) by incorporating various pain scales with the VGG-Face model.\nHowever, Casti et al. [154] pointed out the limitations of original ground truth data due to\nsubjectivity and annotation inconsistencies. To address this, they re-annotated their dataset\nwith judgments from multiple experts, using multidimensional scaling to map frames to\nillumination-invariant 3D space, which they then fed into a pre-trained AlexNet [155].\nCelona and Manoni [156] investigated neonatal facial expressions to detect pain, achiev-\ning the highest accuracy when utilizing two pre-trained models: VGG-Face [157] and mapped\nLBP+CNN (MBPCNN) [158]. Similarly, Lu and Hao [159] found that pre-trained models\nwere crucial for small datasets like neonates, as training from scratch led to overfitting. They\nachieved optimal classification performance by fine-tuning the entire VGG-16 model [122].\nHowever, Zamzmi et al. [160] argue that most face recognition methods are tailored for\nadults and thus less applicable to infants. They developed a lightweight 2D CNN trained\nend-to-end and achieved high pain detection accuracy, but external validation on a different\nneonatal dataset revealed challenges with generalizability. In 2019, Brahnam et al. [112]\nintroduced the iCOPEvid neonatal video dataset, a significant contribution since the only\npublicly available neonatal pain dataset [111] previously contained only static images. Their\nexperiments showed that local descriptors based on the bag-of-features (BoF) approach out-\nperformed deep learning models like VGG-Face andResNet . Combining handcrafted and\ndeep-learned features offered only a marginal improvement in performance. In contrast, Za-\nmzmi et al. [161] found that the most effective approach for binary classification (pain vs.\nno pain) was the fusion of high-level features from VGG [162] and optical flow strains, with\n2http://cmusatyalab.github.io/openface3.4. UNIMODAL STUDIES 31\nnaive Bayes serving as the classifier. Celona and Brahnam [163] applied a Wasserstein gen-\nerative adversarial network with gradient penalty (WGAN-GP) [164], demonstrating that\ntraining set augmentation with synthetic samples improved classification performance. Ta-\nble 3.2 summarizes the vision-based studies focusing exclusively on the spatial dimension.32 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [112]F (RGB) texture\ndescriptors- FF 2D CNN`SVM SL C P O 49 k-fold iCOPEvid 79.80 AUC\n'15 [119]F (RGB) - - - AE SVM SeSL,\nSLC P PS 25 LOSO UNBC 86.10 ACC,\n96.50 AUC\n'20 [120]F (RGB) - - FF 2D CNN`NN SL R IC O 36 hold-out EmoPain 0.91 MAE;\n'18 [123]F (RGB) HOG,\nstatistics- FF 2D CNN`SVR SL R IC PS 25 LOSO UNBC 1.44 MSE;\n'21 [130]F (RGB) - - DF 2D CNN`- SL C ID PS 25 k-fold UNBC 93.87 ACC;\n'16 [133]F (RGB) - - - CDBN SVM UL C P PS 25 LOSO:UNBC 87.20 ACC;\n'21 [134]F (RGB) - - - CDBN SVM SL C P PS 25 LOSO UNBC 93.16 AUC\n'19 [135]F (RGB) - - FF 2D CNN - SL C ID1, IC PS 25 LOSO UNBC 88.191ACC\n'20 [136]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 51.10 ACC;\n'20 [140]F (RGB) - - FF 2D CNN`- SL R ID S 25 ? UNBC 79.94 ACC;\n'21 [142]F (RGB) - - - 2D CNN - SL C ID O 8 k-fold other 97.48 ACC;\n'19 [145]F (RGB) - - - 2D CNN ELM SL R IC PS 25 k-fold UNBC\u201a1.22 MSE;\n'19 [146]F (RGB) - - - 2D CNN - SL C TR, CL, DI PS 25 k-fold UNBC 60.00 ACC\n'19 [150]F (RGB) - - - 2D CNN`- SL C AUs-D PS 25, 43 k-fold UNBC & CK+1,\nWilkie97.701ACC;\n'17 [152]F (RGB) statistics - - NN GPM WSL R IC O, S 25 k-fold UNBC 2.18 MAE\n'20 [153]F (RGB) statistics - FF 2D CNN`NN SL R IC S 25 k-fold UNBC 1.95 MAE;\n'19 [154]F (RGB) LBP, MDS - - 2D CNN`- SL C ID O 25 hold-out UNBC 80.00 ACC\n'18 [159]F (RGB) - - - 2D CNN`- SL C ID O ? hold-out other 78.30 ACC\n`: Pre-trained model -:Not exist &: in Dataset indicates the utilization of cross-database training/validation ?: Not found :: The authors provide additional experiments with other validation methods \u201a: The authors\nutilized occluded facial images ;: The authors provide additional metrics Modality: F: face region Non deep features: LBP: local binary pattern MDS: multidimensional scaling Fusion: M: fusion of modalities E:\nfusion of deep learned features or hand-crafted features Deep models: AE: autoencoder RCNN: recurrent convolutional neural network CDBN: convolutional deep belief network CNN: convolutional neural network\nNN: neural network WGAN-GP: Wasserstein generative adversarial model with gradient penalty Non deep model: SVM: support vector machine GPM: Gaussian process regression model kNN: k-nearest neighbors\nNB: naive Bayes ELM: extreme learning machine Learning Method: SL: supervised learning SeSL: semi-supervised learning UL: unsupervised learning WSL: weakly supervised learning Classific./Regres.: C:\nclassification R: regression Objective: P: presence of pain ID: intensity in discrete scale IC: intensity in continuous scale TR: trigger CL: climax DI: diminishing AUs-D: Action Units detection GT: ground truth\nPS: Prkachin and Solomon S: self-report O: observer rating ST: stimulus Validation Method: LOSO: leave one subject out Metrics: AUC: Area Under the ROC Curve ACC: accuracy PPV: precision MSE: mean\nsquared error MAE: mean absolute error3.4. UNIMODAL STUDIES 33Table 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [124]F (RGB) facial landmarks - FF 2D CNN NN SL C, R ID, IC1P 25 LOSO:UNBC 0.171MSE;\n'20 [125]F (RGB) HOG, head pose,\nAUs intensity/\noccurrence, facial\nlandmarksFF - 2D CNN`NN SL R IC O 36 hold-out EmoPain 5.48 RMSE;\n'20 [126]F (RGB) - - - 2D CNN`NN SL R IC O 36 hold-out EmoPain 1.49 RMSE;\n'18 [127]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 92.00 ACC;\n'18 [128]F (RGB) statistics, distance\nmetrics- FF 2D CNN`- SL C, R ID, IC PS 25 LOSO UNBC 0.81 PCC,\n0.69 MSE\n'21 [137]F (RGB) - - FF 2D CNN`- SL C, R ID, IC P 25 LOSO UNBC 91.13 ACC,\n0.78 PCC,\n0.46 MSE\n'18 [138]F (RGB) - - - AE`- SL R IC PS 25 k-fold UNBC 0.33 MAE;\n'21 [141]F (RGB) - - FF [AE, 2D CNN]Y- SL C, R ID1, IC2,\nP3P, ST 25, 87 LOSO UNBC1,\nBioVid (A)289.1711ACC,\n0.8121PCC,\n85.6532ACC,\n40.4012ACC\n'21 [143]F (RGB) entropy texture\ndescriptors- - 2D CNN`- SL C ID O 8 k-fold other 0.92 PPV;\n'18 [147]F (RGB) - - - 2D CNN`- SL C P PS 14 k-fold UNBC 93.00 ACC\n'19 [149]F (RGB) - - - 2D CNN - SL C P PS 25, 20 k-fold UNBC &\nBioVid (A)\u02db56.75 ACC\n'17 [156]F (RGB) HOG, LBP - FF 2D CNN`SVM SL C P O 26 LOSO iCOPE 73.78 ACC\n'19 [160]F (RGB) - - - 2D CNN - SL C P O 31 LOSO NPAD1,\niCOPE296.981ACC;,\n89.802ACC\n'21 [165]F (RGB) - - - 2D CNN`- FL C P PS 25 LOSO UNBC 76.00 ACC;\n'21 [166]F (RGB) - - - 2D CNN`- SL C P O 25 hold-out UNBC 75.49 ACC\n'21 [167]F (RGB) - - - 2D CNN`SVR SL R IC P 25 LOSO UNBC 0.34 MSE\n'21 [168]F (RGB) - - - 2D R-CNN - SL C P O ? hold-out other 87.80 PPV\nY: The authors combined the deep models into a unified framework \u02db: The authors experimented with additional datasets combinations Non deep features: AUs: actions units HOG: histogram of oriented gradients Non\ndeep model: SVR: support vector regression Learning Method: FL: federated learning Metrics: RMSE: root mean squared error34 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [161]F (RGB) optical flow - FF 2D CNN`SVM,\nkNN, NBSL C P O 31 k-fold other 92.71 ACC,\n94.80 AUR\n'19 [163]F (RGB) - - - WGAN-GP - SL C P O 26 LOSO iCOPE 93.38 ACC\n'17 [169]F (RGB) - - - 2D CNN`- SL R IC PS 25 LOSO UNBC 0.99 MAE;\n'20 [170]F (RGB) - - - 2D CNN - SL C ID ST 87 hold-out BioVid (A) 36.60 ACC\n'20 [171]F (RGB) - - - 2D CNN - SL C P PS 25 hold-out UNBC 97.00 PPV;\n'21 [172]F (RGB) - - - 2D CNN - SL C ID P 28 LOSO:UNBC 90.30 ACC\n'19 [173]F (RGB) - - - 2D CNN - SL C P O 31 hold-out NPAD1,\niCOPE291.001ACC;,\n84.502ACC;\n'21 [174]F (RGB) - - - 2D CNN`- SL C P O 26, 30 hold-out iCOPE &\nUNIFESP89.90 ACC;\n'21 [175]F (RGB) - - - 2D CNN - SL C AUs-D P 10 hold-out Pain-ICU 77.00 ACC;3.4. UNIMODAL STUDIES 35\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach)\nPain assessment is particularly challenging due to its complex and dynamic nature. Rely-\ning on static, individual frames to assess pain fails to capture the phenomenon\u2019s temporal\nprogression and often leads to inaccurate estimations. Additionally, many studies highlight\nthe difficulties of applying deep learning techniques to small datasets, with one proposed\nsolution being the combination of deep learning and traditional feature extraction methods.\nEgede et al. [176] addressed this by extracting deep features from a pre-trained CNN, explic-\nitly targeting the eyes and mouth regions. Using a relevance vector regressor (RVR), they\ndemonstrated that combining deep and hand-crafted features led to optimal performance. De-\nspite the valuable insights the UNBC-McMaster database provides, its imbalanced sample\ndistribution\u2014particularly the limited number of frames showing pain\u2014poses a significant\nchallenge for deep learning models. In response, Egede and Valstar [177] devised a method\nbased on the observation that neighboring pain level classes share many common features.\nThis approach allowed them to avoid extracting all possible features for classes with fewer\nsamples, as certain features had already been utilized from other related classes. The study\nalso showed that combining deep and hand-crafted features improved performance. How-\never, in a later study [178], the authors applied a similar approach, using only deep-learned\nfeatures to address data imbalance, but could not replicate the same high-performance levels.\nTavakolian et al. [179] took a different approach, focusing on the detection of genuine\nversus acted pain through facial expressions, a technique with important applications in both\nmedical and forensic contexts. They developed a residual GAN (R-GAN) to capture subtle\nfacial changes and the dynamic nature of expressions, using a weighted spatio-temporal pool-\ning (WSP) method. In a subsequent study [180], the authors suggested that self-supervised\nlearning could reduce the time and effort needed for data labeling, as it does not require\ncomplete dataset annotation. They introduced a new similarity function for learning general-\nized representations with a Siamese network. They also employed statistical spatio-temporal\ndistillation (SSD) based on the Gaussian scale mixture (GSM) to improve computational effi-\nciency. This technique encodes spatiotemporal variations in facial videos into a single RGB\nimage, simplifying the model while maintaining effectiveness.\nOther studies also aim to capture the dynamic aspects of pain. For instance, [181] com-\nbined a random forest classifier with the pre-trained MobileNetV2 model [182], encoding\nvideos by selecting and merging three frames from different time points into a single image.\nOthman et al. [183] emphasized the importance of using diverse datasets\u2014including vary-\ning age, gender, pose, occlusion, and lighting conditions\u2014to improve model generalization.\nThey used multiple data combinations and a reduced version of MobileNetV2 , showing that\ncross-dataset training is essential for achieving better generalizability.36 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.3 Vision-based: Implicit Temporal Utilization\nSeveral studies have explored the application of 3D CNNs for pain assessment. Tavakolian\nand Hadid [184] developed a 3D CNN to capture dynamic facial representations from videos.\nThey noted that researchers often use fixed temporal kernel depths when employing 3D\nconvolution techniques, which limits the ability to capture short, mid, and long temporal\nranges simultaneously. To address this, they designed a model with parallel 3D convolutional\nlayers featuring variable temporal depths, allowing the capture of temporal dependencies\nfrom 32consecutive frames. Similarly, Wang and Sun [185] applied 3D convolutions based\non the architecture proposed in [186], consisting of 8convolutional layers with 3\u02c63\u02c63\nfilters. While they reported high performance, the authors acknowledged that extracting deep\nfeatures from small datasets posed a challenge for model generalization. In a related study,\nHuang et al. [187] developed a framework that integrated 3D, 2D, and 1D CNNs to extract\nspatio-temporal, spatial, and geometric features. For the 3D CNN component, they modified\nthe architecture from [188] by using discrete kernels of 1\u02c63\u02c63and3\u02c61\u02c61rather than the\ntraditional 3\u02c63\u02c63kernel. Other researchers have also proposed 3D deep CNNs with varying\ntemporal depths to capture short, mid, and long-range facial expression variations [189].\nRecognizing the difficulty and time consumption involved in training a deep 3D CNN from\nscratch, they introduced a cross-architecture knowledge transfer learning technique, utilizing\na pre-trained 2D CNN to assist in the training of the 3D CNN. In studies by Praveen et\nal.[190] and [191], the authors employed weakly-supervised domain adaptation, where the\nsource domain focused on human affective expressions and the target domain was explicitly\nrelated to pain expressions. Their framework featured an inflated 3D-CNN (I3D) [192],\nincorporating 3convolutional layers and 3inception modules [132] to capture both spatial\nand temporal information from video data.\nBargshady et al. [193] opted to use the HSV color space instead of RGB, arguing that it\nbetter reflects human visual perception for tasks such as skin pixel detection and multi-face\ndetection. They employed the pre-trained VGG-Face [157] for feature extraction, followed\nby a temporal convolutional network (TCN) using dilated causal convolutional operations to\nleverage temporal dependencies. Rezaei et al. [194] tackled the challenge of pain detection\nin people with dementia, a difficult task due to insufficient pain-related images or videos\nof elderly subjects in existing datasets. They developed a 10-layer 2D CNN that processed\npairs of pain and no-pain images, analyzing frame-to-frame changes and employing con-\ntrastive training methods [195]. The model demonstrated high performance in both healthy\nindividuals and people with dementia. In another study, Pandit and Schmitt [196] explored\nthe potential of using shallow 1D CNN architectures for real-time pain recognition. They ex-\ntracted facial action units from each frame using the OpenFace 2.03toolkit, with promising\n3https://github.com/TadasBaltrusaitis/OpenFace3.4. UNIMODAL STUDIES 37\nresults for pain detection in real-time settings.\n3.4.4 Vision-based: Explicit Temporal Utilization\nSeveral efforts have focused on addressing the limitations of static frames by developing\ndedicated temporal modules. Zhou et al. [197] tackled this issue using a regression frame-\nwork based on a 4-layer recurrent convolutional neural network (RCNN), each with a se-\nquence length of 3time steps. Rodriguez et al. [198] leveraged dynamic information by\ndesigning an LSTM model fed with feature vectors extracted from VGG-16 [122]. Simi-\nlarly, Bellantonio et al. [199] emphasized that facial expressions evolve, making it essential\nto analyze the spatio-temporal dimension of pain. They improved estimation performance\nusing a fine-tuned 16-layer CNN model [157], an LSTM processing 16frames as a time\nwindow, and super-resolution techniques. In another study, Bargshady et al. [200] com-\nbined the VGG-Face CNN [157] with a 3-layer LSTM to extract spatio-temporal features\nfrom grayscale images, applying zero-phase component analysis (ZCA). In [201], principal\ncomponent analysis (PCA) was used to reduce dimensionality. Mauricio et al. [202] also\nemployed VGG-Face but replaced LSTM with a 2-layer gated recurrent unit (GRU) to cap-\nture temporal dependencies. Thuseethan et al. [203] used a conventional 2D CNN and two\nRCNNs to extract temporal features from previous and subsequent frames, enhancing the\ntime dimension of expression analysis.\nA similar approach was followed by Bargshady et al. [204], who employed ensemble\nlearning with three distinct CNN-biLSTM modules, merging their outputs for the final pre-\ndiction. Salekin et al. [205] used a bilinear CNN (B-CNN) based on the VGG architecture\n[121], pre-trained on VGGFace24andImageNet5datasets, along with an LSTM to capture\ntemporal dependencies in image sequences. Kalischek et al. [206] explored deep domain\nadaptation for facial expression and pain detection, utilizing the self-ensembling approach\n[207] with a long-term recurrent convolutional network (LRCN). While they achieved state-\nof-the-art results for facial expression recognition, performance was lower for pain detection,\nlikely due to the subtle nature of pain-related expressions.\nDespite the availability of additional information in pain datasets, multi-task approaches\nremain limited. Martinez et al. [208] proposed a personalized multi-task learning method\nbased on individual physiological and behavioral pain responses. They extracted AAM fa-\ncial landmarks, processed them through a biLSTM to produce PSPI scores, and predicted the\nfinal V AS score. Erekat et al. [209] combined AlexNet [155] with 2 GRU layers to capture\ntemporal dependencies, using both self and observer-reported pain intensity as ground truth.\nVuet al. [210] developed a multi-task framework to estimate pain levels while reconstruct-\n4https://www.robots.ox.ac.uk/ \u02dcvgg/data/vgg_face\n5https://www.image-net.org38 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\ning heatmaps of action unit locations, improving model generalization with a CNN-LSTM\ncombination to capture micro facial movements.\nHuang et al. [211] noted that specific frames within a video sequence exhibit more pro-\nnounced pain expressions, requiring special handling. They developed a novel framework\nusing attention saliency maps with a VGG-16 model, GRUs and learned weights for each\nframe\u2019s contribution to pain intensity estimation. The study demonstrated that dynamic and\nsalient features can significantly improve performance. Similarly, Yu et al. [212] used VGG-\n11 (configuration A) and an LSTM to create an attention mechanism, predicting pain in-\ntensity from 16consecutive frames. Xu and Liu [213] adopted a ResNet-50 model with an\nattention mechanism to extract spatial features, followed by a transformer encoder to capture\ntemporal sequences, achieving promising results.\nIn other studies, Ragolta et al. [214] used extracted action units to train a 2-layer LSTM\npredicting pain on an 11-point scale, employing curriculum learning. Guo et al. [215] devel-\noped a convolutional LSTM (C-LSTM) to extract both spatial and temporal features from\nvideos, showing that temporal models outperform non-temporal models for pain estimation\naccuracy. Rasipuram et al. [216] utilized in-the-wild video data for pain detection, gener-\nating a 3D morphable model without relying on facial landmarks and combining it with an\nLSTM. Zhi and Wan [217] introduced sparse coding with LSTM (SLTM), using the iterative\nhard thresholding algorithm (ISTA) [218] to capture dynamic facial expressions. Although\nSLTM did not achieve high performance, it offers speed and efficiency for specific applica-\ntions. Finally, Thiam et al. [219] developed a method combining motion history and optical\nflow images with a 10-layer CNN and 2-layer biLSTM, showing that weighted score aggre-\ngation improves performance. Table 3.3 summarizes studies incorporating the modalities\u2019\ntemporal dimensions.3.4. UNIMODAL STUDIES 39Table 3.3: Vision-based studies with temporal utilization.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'17 [176] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVR SL R IC PS 25 LOSO UNBC 0.99 RMSE,\n0.67 PCC\n'17 [177] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVM SL R IC PS 25 LOSO UNBC 1.04 RMSE,\n0.64 PCC\n\u201918 [178] F (RGB) - - - NL 2D CNN - SL R IC PS 25 LOSO UNBC 1.20 RMSE,\n0.47 PCC\n'18 [184] F (RGB) - - - I 3D CNN - SL R IC PS 25 LOSO UNBC 0.53 MSE,\n0.84 PCC;\n'18 [185] F (RGB) HOG,\ngeometric\ndifference- DF I 3D CNN SVR SL R IC PS 25 LOSO UNBC 0.94 RMSE,\n0.67 PCC\n'20 [191] F (RGB) - - - I 3D CNN`- WSL R IC PS 24, ? LOSO UNBC\n& RECOLA0.64 MAE,\n0.82 PCC;\n'16 [197] F (RGB) - - FF E RCNN - SL R IC PS 25 LOSO UNBC 1.54 MSE,\n0.65 PCC\n'17 [198] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C, R P, IC1PS 25 LOSO UNBC 0.741MSE,\n0.781PCC;\n'17 [199] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 61.90 ACC\n'19 [200] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 75.20 ACC\n'20 [201] F (RGB) PCA - DF E [2D CNN`,\n1D CNN, biLSTM]Y- SL C ID PS 25 LOSO:UNBC 85.00 ACC;\n'19 [202] F (RGB) - - - E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 85.40 ACC,\n0.62 MSE;\n'19 [203] F (RGB) - - FF E [2D CNN, RCNN]Y- SL R IC PS 25 LOSO UNBC 1.29 MSE,\n0.73 PCC\n'17 [208] F (RGB) - - FF E biLSTM HCRF,\nFCSL C IC O,\nS25 hold-out UNBC 2.46 MAE;\nNon deep features: PCA: principal component analysis Temporal Exploitation: NL: non-machine learning method I: implicit method E: explicit method Deep models: RCNN: recurrent convolutional neural network\nLSTM: long short memory networks biLSTM: bidirectional neural network GRU: gated recurrent unit Non deep models: SVM: support vector machine RVM: relevance vector machine GPM: Gaussian process regression\nmodel HCRF: hidden conditional random fields FC: fully connected SVR: support vector regression Objective: I2: intensity in binary pairs Metrics: PCC: Pearson correlation coefficient40 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [114]F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN RVR SL R IC O 13 LOSO APN-DB 1.71 MAE;\n'19 [179]F (RGB) - - - NL R-GAN - UL C genuine\nvs posedPS,\nST25,\n34,\n87,\n87? UNBC\n& STOIC\n& BioVid (A)\n& BioVid (D)90.97 ACC\n'20 [180]F (RGB) - - FF NL 2D CNN`- SSL C IC P,\nST25\n87LOSO UNBC1,\nBioVid (A)2\u20180.781PCC;,\n71.022AUC;\n'21 [181]F (RGB) AUs\nintensity- H NL 2D CNN`RF SL C ID ST 127 k-fold X-ITE 25.00 ACC\n'19 [183]F (RGB) - - - NL 2D CNN - SL C P ST 87\n134k-fold BioVid (A)\n& X-ITE\u201867.90 ACC\n'20 [209]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC O,\nS25 k-fold UNBC 2.34 MAE\n'20 [211]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC PS 19 LOSO UNBC 0.21 MSE,\n0.89 PCC\n'19 [212]F (RGB) - - FF E [2D CNN, LSTM]Y- SL R IC PS 24 LOSO UNBC 1.22 MSE;,\n0.40 PCC;\n'20 [214]F (RGB) AUs\nintensity- - E LSTM - SL R IC O 36 hold-out EmoPain 2.12 RMSE,\n1.60 MAE;\n'20 [216]F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C P O ? k-fold UNBC 78.20 ACC;\n'20 [219]F (RGB) - - DF E [2D CNN, biLSTM,\nNN]Y- SL C P ST 87\n40LOSO BioVid (A)1,\nSenseEmotion269.251ACC,\n64.352ACC\n'20 [220]F (RGB) - - FF E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 0.84 ACC,\n0.69 PCC;\n\u2018: The authors provide experiments with cross-dataset settings Fusion: H: hybrid Non deep models: RF: random forest classifier3.4. UNIMODAL STUDIES 41Table 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [187]F (RGB) facial\nlandmarks- DF I [3D CNN`,\n2D CNN`,\n1D CNN, FC]Y- SL R IC PS 25 LOSO UNBC 0.76 MSE,\n0.82 PCC;\n'19 [189]F (RGB) - - - I [2D CNN`,\n3D CNN]Y- UL,\nSLC, R IC1, P2P,\nST25, 87 LOSO UNBC1,\nBioVid (A)20.9211PCC;,\n86.0222AUC\n'20 [190]F (RGB) - - - I 3D CNN`- WSL R IC PS 24,?,\n87, 18LOSO UNBC1\n& RECOLA\n& BioVid (A)2\u20180.741PCC,\n0.342PCC\n'20 [193]F (RGB) PCA - FF I [2D CNN`,\nTCN]Y- SL C ID P,\nST25, 20 LOSO:UNBC1,\nMIntPAIN292.441ACC;,\n89.002ACC;\n'20 [194]F (RGB) - - - I 2D CNN - SL C, R IC, P1P 95, 25 k-fold UofR & UNBC182.0011PCC;\n'20 [196]F (RGB) AUs\noccurrence- FF I 1D CNN - SL R IC P 24, 87 hold-\noutUNBC1,\nBioVid (A)0.801CCC\n'20 [204]F (RGB) PCA - DF E [2D CNN`, 1D\nCNN, biLSTM]Y- SL C ID PS,\nST25, 20 k-fold UNBC1,\nMIntPAIN286.001ACC;\n92.262ACC;\n'20 [205]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- SL R P, IC1O 45 LOSO NPAD 3.991MSE,\n1.552MAE\n'19 [206]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- UL C P ST 40 LOSO SenseEmotion 60.61 ACC\n'21 [210]F (RGB) - - - E [2D CNN`,\nLSTM]Y- SL R IC P 25, 27 LOSO UNBC1,\nDISFA\u20180.60`MSE,\n0.82`PCC;\n'21 [213]F (RGB) - - - E [2D CNN`,\nTransformer]Y- SL R IC P 25 LOSO UNBC 0.40 MSE,\n0.76 PCC;\n'21 [215]F (RGB) - - - E 2D C-LSTM - SL C ID S 29 hold-\noutother 69.58 F1\n'19 [217]F (RGB) - - FF E SLSTM - SL C P1, ID2ST 85 LOSO BioVid (A) 61.701ACC\n29.702ACC\n'21 [221]F (RGB) - - - I 3D CNN`- SL R IC S 25 k-fold UNBC 0.66 ICC;\nFusion: H: hybrid Deep models: TCN: temporal convolutional neural network C-LSTM: convolutional-LSTM SLTM: sparse long short memory network Learning Method: SSL: self-supervised learning Metrics: F1:\nF1 score CCC: concordance correlation coefficient42 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.5 Touch sensor-based\nTouch (contact) sensors provide a viable alternative for pain assessment, often outperforming\nvision-based methods. Table 3.4 highlights studies that utilized contact sensor data to evalu-\nate pain. Yu et al. [222] analyzed three categories of pain-no pain, moderate pain, and severe\npain\u2014using EEG signals. They extracted several bands from the biosignals, including al-\npha, beta, and gamma, and applied a convolutional module. The study found that combining\nthese bands yielded better results than evaluating them independently. Similarly, [223] used\nEEG potentials with an autoencoder to compress the raw data and applied a logistic regressor\nfor classification.\nOther researchers, such as Rojas et al. [224], utilized functional near-infrared spec-\ntroscopy (fNIRS) for pain detection. They developed three models\u2014multilayer perceptron\n(MLP), LSTM, and biLSTM\u2014with biLSTM demonstrating superior accuracy. Addition-\nally, [225] focused on PPG signals, extracting hand-crafted features from the time and fre-\nquency domains, which were then combined with a deep belief network (DBN) to achieve\nover65% accuracy in a 4-class pain assessment task. Hu et al. [226] used kinematic data\nto compare healthy individuals with those suffering from low back pain (LBP). Their ap-\nproach, which employed two stacked LSTM layers, reached over 97% accuracy in binary\nclassification using raw motion data. Lastly, Mamontov et al. [227] were the first to apply\nevolutionary algorithms in the design of an optimized recurrent neural network (RNN) for\npain estimation, achieving 91.94% accuracy using EDA signals.\n3.4.6 Audio-based\nA few studies have explored using audio information for pain detection and intensity esti-\nmation, as outlined in Table 3.5. These methods are especially relevant for neonates, where\nfrequent facial and body occlusions make analyzing cries a more effective approach for pain\ndetection. Chang and Li [228] concentrated on infant cries to differentiate between hunger,\npain, and sleepiness. They transformed the audio signals into 2D spectrograms using a fast\nFourier transform (FFT) and trained a 2D CNN for feature extraction. Similarly, [229] uti-\nlized spectrograms generated from recorded sounds, employing a model identical to that\nused in [160]. Thiam and Schwenker [230] focused on detecting adult pain by analyzing\nbreathing sounds. They leveraged deep-learned features from spectrograms with Mel-scaled\nshort-time Fourier transform, combined with various handcrafted cues. A CNN followed by\na biLSTM was used to capture spatial and temporal dependencies, integrating both low- and\nhigh-level features. In a different approach, Tsai et al. [231] examined pain events during\nemergency triage. They developed an LSTM autoencoder framework to extract temporal\nfeatures from verbal behavior, reporting encouraging results.3.4. UNIMODAL STUDIES 43Table 3.4: Touch sensor-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'20 [222]EEG - - FF I 1D TCN - S C ID S 32 k-fold other 97.30 ACC;\n'20 [223]EEG - - - I AE (TCN) LR UL, S C P S 29 LOSO other 74.60 ACC\n'21 [224]fNIRS - - - E biLSTM - SL C ID S 18 k-fold other 90.60 ACC;\n'19 [225]PPG - - - NL DBN SBM U, SL C P1, ID2S 100 k-fold other 86.791ACC,\n65.572ACC\n'18 [226]kinematatics - - FF E LSTM - SL C P LBP 44 LOSO other 97.20 ACC;\n'19 [227]EDA - - FF E [RNN, LSTM,\nGRU, NN]YSelfCGA,\nselfCGP,\nPSOPBSL C P ST 40 LOSO Sense-\nEmotion81.94 ACC\n'21 [232]EDA - - - I NN - SL C P1, I2 ST 87,\n55LOSO BioVid (A)1,\nPainMonit284.2211ACC;,\n86.5012ACC;\nModality: PPG: photoplethysmogram fNIRS: functional near-infrared spectroscopy EEG: electroencephalography EDA: electrodermal activity Deep models: DBN: Deep belief network RNN: recurrent neural network\nNon deep models: SBM: selective bagging model LR: Logistic Regression SelfCGA: Self-Configuring Genetic Algorithm SelfCGP: Self-Configuring Genetic Programming PSOPB: Particle Swarm Optimisation with\nparasitic behaviour GT: LBP: low back pain vs healthy population\nTable 3.5: Audio-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'16 [228]audio (cry) - - - - 2D CNN - SL C P O ? k-fold other 78.50 ACC\n'19 [229]audio (cry) - - - - 2D CNN - SL C P O 31 LOSO:NPAD 96.77 ACC;\n'19 [230]audio\n(breathing)MFCCs,\nRASTA-\nPLP,\nDTD- FF E [2D CNN,\nLSTM]YRFc SL C P ST 40 LOSO Sense-\nEmotion64.39 ACC\n'17 [231]audio\n(voice)prosodic-\nspectral\nfeatures,\nSF- FF E LSTM`SVM UL,\nSLC P1, ID2S 63 LOSO other 72.301UAR,\n54.202UAR\nNon deep features: MFCCs: Mel Frequency Cepstral Coefficients RASTA-PLT: Relative Spectral Perceptual Linear Predictive DTD: descriptors from temporal domain SF: statistical features44 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.5 Multimodal studies\nSince pain is a multidimensional phenomenon, combining multiple modalities in a multi-\nmodal system offers a promising approach. Heterogeneous information sources can com-\nplement one another, enhancing specificity and sensitivity. As reported in [106], when in-\ndividual modalities demonstrate good predictive performance, their fusion tends to yield\nimproved outcomes. Moreover, integrating cues from various channels may be helpful and\nnecessary, especially in clinical settings where specific modalities may become unavailable\n(for instance, if the patient turns and their face is occluded). The information channels can\noriginate from (1) the same hardware sensor but focus on different regions of interest, such\nas RGB facial images and RGB body images [233], (2) different hardware sensors but the\nsame region of interest, like RGB facial images and thermal facial images [110], or (3)\ndifferent hardware sensors and information sources, such as RGB facial images and ECG\nsignals [234]. Table 3.6 lists the studies utilizing multimodal approaches.\n3.5.1 Static Analysis\nA commonly used biosignal combination is those of EDA, EMG, and ECG, as these channels\nare found in all main pain reference databases. Thiam et al. [235] applied an early fusion\nmethod by merging these signals into a 2D representation and inputting it into a 9-layer 2D\nCNN. Their results showed a strong correlation between EDA and pain intensity, and com-\nbining all three modalities did not outperform using EDA alone. Al-Qerem et al. [236] used\nleast generative adversarial networks (LSGANs) to enhance EMG, EDA, and ECG samples,\nreporting a notable improvement in classification when using an SVM on the augmented\ndataset. Haque et al. [110] introduced the MIntPAIN dataset, which includes RGB, depth,\nand thermal videos for multi-class ( 5levels) pain recognition. They combined these three vi-\nsual modalities into a 5D matrix (RGB+D+T) and used it to train the pre-trained VGG-Face\nmodel [157], leading to better classification performance in their experiments.\n3.5.2 Temporal Utilization\nZhiet al. [237] proposed a multimodal stream-integrated neural network that leverages video\nand biosignal data. They combined raw facial video frames with optical flow images to cap-\nture spatio-temporal dependencies via 3D CNNs, integrating these with biosignal features\nextracted using LSTMs. The entire network was trained end-to-end, achieving superior re-\nsults compared to their unimodal methods. Beyond facial analysis, Salekin et al. [233]\nfocused on assessing neonatal pain through body movements in videos. After identifying\nrelevant body regions, video frames were fed into a pre-trained VGG-16 [121], connected to\nan LSTM to capture temporal dynamics. In a follow-up study, Salekin et al. [238] fused three3.5. MULTIMODAL STUDIES 45\nmodalities\u2014facial expressions, body movements, and crying sounds\u2013demonstrating that this\nmultimodal approach outperformed unimodal techniques. Similarly, Wang et al. [239] ex-\nplored combining EMG, EDA, and ECG biosignals with handcrafted and learned features\nfrom a biLSTM model. They applied the minimum relevance method (MRMR) to reduce\nthe number of features, resulting in notable outcomes.\nIn addition to EDA, EMG, and ECG, other biosignal combinations have been explored.\nZhao et al. [240] integrated PPG, EDA, and temperature signals, using 2D convolutions for\nspatial feature extraction and time windows for capturing temporal information. Yuan et\nal.[241] successfully estimated pain using whole-body MoCap sensors and EMG, utilizing\nLSTM layers with an attention mechanism in an autoencoder, which reduced training time\nby leveraging latent space representations of raw data. Similarly, Li et al. [242] employed\nMoCap and EMG as data sources and tested various LSTM configurations to predict pain\nintensity, achieving the best performance with a 3-layer vanilla LSTM combined with a 3-\nlayer fully connected network.46 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.6: Multimodal-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [110]F (RGB,\nthermal,\ndepth)- RF - - 2D CNN`- SL C ID S 20 k-fold MIntPAIN 36.55 ACC\n'19 [233]F, B (RGB) - FF - E [2D CNN`,\nLSTM]Y- SL C P O 31 LOSO other 92.48 ACC;\n'19 [234]F (RGB),\nECG, EDAbiosignals\u2019\nfeaturesmFF FF - 2D CNN`RFc SL C I2 S 85 k-fold BioVid (A) 74.00 ACC\n'19 [235]EDA,\nEMG,\nECG- RF - - 2D CNN - SL C P1I2,\nID2S 87,\n86LOSO BioVid (A)1\nBioVid (B)84.4011ACC;,\n36.5412ACC;\n'20 [236]EDA,\nEMG,\nECGBoruta\nfeaturesFF - - LSGAN SVM UL,\nSLC I2, ID1S 85 hold-\noutBioVid (A) 82.801ACC\n'21 [237]F (RGB),\nEDA,\nEMG, ECGoptical\nflowFF FF NL,\nE, I[3D CNN,\nLSTM]Y- SL C, R P1, I2,\nID2S 87,\n40k-fold:BioVid (A)1,\nMIntPain68.2011ACC;,\n28.1021ACC\n'21 [238]F, B (RGB),\nsound- DF - E [2D CNN`,\nLSTM]Y- SL C P O 45 LOSO NPAD 78.95 ACC;\n'20 [239]EDA,\nEMG,\nECGMRMR,\nbiosig-\nnals\u2019\nfeaturesRF\nFFE biLSTM NN SL C P1, I2 S 87 LOSO BioVid (A) 83.301ACC\n'20 [243]EDA,\nEMG,\nECG- FF - I [DDCAE,\nNN]Y- UL,\nSLC P1, I2 S 87 LOSO BioVid (A) 83.991ACC;\n'21 [244]EDA,\nEMG,\nECG, RSP- FF - I [DDCAE,\nNN]Y- UL,\nSL,\nSSLC, R P1, ID2,\nICS 87,\n40LOSO BioVid (A)1,\nSense-\nEmotion84.2511ACC;,\n35.4421ACC;\n'21 [245]EDA, ECG - FF - E 1D CNN,\nLSTM- UL C P1, I2 S 67 hold-\noutBioVid (A) 81.711ACC\n'20 [240]PPG, EDA,\ntemperature- RF - I 2D CNN - SL R\u02ddP1, ID2S 21 k-fold other 96.301ACC,\n95.232ACC\n'20 [241]MoCap,\nEMG- RF - E AE, LSTM - UL,\nSLC ID O 23 LOSO:EmoPain 52.60 ACC;\n'20 [242]MoCap,\nEMG- RF - E LSTM, NN - UL C ID O 30 hold-\noutEmoPain 80.00 ACC;\n'21 [246]MoCap,\nEMG- RF - E LSTM, NN - SL C ID O 30 LOSO:EmoPain 54.60 ACC;\nm: Not specifically described \u02dd: Ordinal Modality F: face region B: body region EMG: electromyography Non deep features: MRMR: Minimum Redundancy Maximum Relevance method Deep models: LSGAN:\nLeast Square Generative Adversarial Networks3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 47\n3.6 Summary of Automatic Pain Assessment Methods\nThis section presents an analysis of the reviewed studies, summarizing the main conclusions\non current methods for automatic pain assessment, their advantages, and corresponding lim-\nitations. Additionally, it offers recommendations for future research directions that could\nadvance the field of pain research from a computational perspective.\n3.6.1 Input\nFirst, we observe a clear imbalance between unimodal and multimodal approaches in pain\nassessment studies. More than 86% of the reported research focuses on unimodal methods,\neven though the databases often contain multiple information channels. Notably, contact\nsensor-based and audio-based approaches are underrepresented, with only seven and four\nstudies, respectively, compared to 84studies that utilize a vision-based approach.\nMultimodal approaches are even less explored, with only 15studies falling into this\ncategory, making it difficult to draw strong conclusions about the effectiveness of specific\nmodality combinations. However, there are indications that EDA sensor data is particularly\nvaluable compared to other biopotentials. Researchers have primarily focused on visual data,\nlikely due to the complexity of implementing multimodal frameworks or the impracticality\nof contact sensors in non-laboratory settings. Further exploration of diverse modality com-\nbinations is necessary to evaluate their potential for pain assessment fully\u2014additionally, 28\nstudies employed non-deep features to enhance deep-learned representations.\nFinally, we identified three primary strategies in examining the approaches that utilize\ntemporal information: non-machine learning-based, machine learning-based (implicit), and\nmachine learning-based (explicit). Non-machine learning-based methods, such as motion\nhistory images [219] or temporal distillation [180], rely on traditional computer vision tech-\nniques. These methods tend to be more straightforward but are generally less sophisti-\ncated. In contrast, machine learning-based approaches [190] [217] offer richer temporal\ninformation and the flexibility to adapt to specific requirements, such as emphasizing certain\nvideo frames. Among the studies reviewed, 55% employed temporal features, with explicit\nmethods\u2014most commonly LSTM models\u2014being the predominant choice. Given that many\nstudies report superior performance when temporal information is incorporated, compared\nto non-temporal methods, it is evident that further emphasis on temporal approaches is war-\nranted.\n3.6.2 Processing\nRegarding machine learning approaches, various models and techniques have been employed\nfor pain estimation. CNN models remain the most widely used, with more than 75% of stud-48 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nies utilizing 1D, 2D, or 3D filters, highlighting the central role of convolution operations\nin deep learning. Sequential models, such as RNNs, GRUs, LSTMs, and biLSTMs, follow\nclosely behind in popularity. Almost half of the studies used pre-trained models to achieve\ntheir desired performance. This suggests that existing pain databases may not be adequate\nfor training deep-learning models from scratch. Non-deep learning models have also been\nemployed in 26studies as auxiliary decision components, with SVMs and shallow neural net-\nworks being the most common choices. There seems to be significant potential for adopting\nnewer deep learning architectures, especially transformer-based models, which have demon-\nstrated state-of-the-art results in various AI research fields and are particularly suited for\nexploiting temporal modality information [247].\nThe predominant learning method used across studies is supervised learning. How-\never, 16papers explored or adopted alternative methods such as unsupervised learning [119,\n133, 179, 189, 206, 223, 225, 231, 236, 241, 243], self-supervised [180, 244], self-supervised\nlearning [180, 244], semi-supervised learning [119], weakly supervised learning [190, 191],\nand federated learning [165]. Given the limited availability of pain data resources, self-\nsupervised learning appears to be the most appropriate method for future research and should\nbe further embraced by the community.\nLastly, it is notable that most studies\u2014approximately 70%\u2014treat pain assessment as a\nclassification problem rather than a regression problem. However, we believe that regres-\nsion more closely reflects the continuous nature of pain and is better suited to capturing the\ncomplexity of pain sensation.\n3.6.3 Evaluation\nThe primary objectives of the reviewed studies were (i)to estimate pain intensity on a dis-\ncrete scale (multi-class classification), (ii)to measure pain intensity on a continuous scale,\nand(iii)to determine the presence or absence of pain (binary classification). Notably, 25\nstudies focused on pain detection rather than pain intensity estimation, which, from a clin-\nical standpoint, is less informative as it does not provide sufficient data for effective pain\nmanagement. From an engineering perspective, detecting the presence or absence of pain is\nalso a more straightforward and less demanding task.\nA small subset of studies took a different approach to pain estimation. For instance, one\nstudy [179] sought to differentiate genuine pain from acted pain. Another [231] explored\npain events in emergency triage settings rather than controlled laboratory environments,\nwhile [234] examined the feasibility of real-time pain detection on IoT devices. Addition-\nally, [142] and [143] aimed to address the issue of occluded faces in pain estimation. So-\nciodemographic and psychological factors were also considered, as seen in studies like [245],\nwhich explored gender differences, and [194], which focused on pain assessment in elderly3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 49\npatients with dementia. The limited exploration of pain estimation in real-world settings\nor unconventional contexts suggests that current approaches may not be fully applicable in\npractical environments like clinics and hospitals.\nVarious annotation types are used regarding ground truth, such as self-reported ratings,\nFACS, and observer scales. Temporal features are critical for accurately estimating pain\nintensity, making the temporal granularity of the ground truth equally important. Several\nstudies have questioned the objectivity of PSPI scores, as noted in [248], which highlights\nthat PSPI scores can be zero even when pain is present or that there may be no visible facial\nexpressions in low-intensity pain. Pain expressions not captured by the FACS system, such\nas raising eyebrows or opening the mouth, further challenge the use of PSPI [249]. Addi-\ntionally, PSPI does not account for pain-related head and body movements, which are par-\nticularly valuable in newborn assessments [250]. For these reasons, we recommend moving\naway from PSPI as ground truth in favor of self-reports and observer scales at the video-\nsegment level.\nAround 54% of the studies employed the leave-one-subject-out (LOSO) validation method,\nwhich is widely regarded as more objective and better for assessing the generalizability of\nmodels. However, LOSO can be less practical due to the increased model size and longer\ntraining times. When researchers use other validation methods, such as k-fold or hold-out,\nit is essential to ensure that consecutive, highly correlated frames from the same subject do\nnot skew the training and validation results, leading to flawed estimations. Moreover, when\nresearchers define their own validation or testing sets, comparing results across studies\u2014\nespecially between classification and regression models\u2014becomes nearly impossible. We\nbelieve standardized evaluation protocols should be developed for each publicly available\ndatabase for these reasons.\n3.6.4 Pain Databases for Evaluation\nThe availability of suitable public databases is arguably the most crucial factor in addressing\nthe challenge of automatic pain assessment. Several aspects must be considered in evaluating\nthese datasets, including the number of subjects and their characteristics, such as age, sex,\nhealth status, and race. Moreover, the ground truth must be objective and offer meaningful\ninsights into the subject\u2019s pain experience [154].\nFig. 3.1 illustrates the number of papers corresponding to the pain database utilized in\neach study. It is clear from this figure that the UNBC andBioVid databases were the most\ncommonly used public datasets. However, the UNBC dataset does not record the subjects\u2019\nages, despite age being a known factor in pain expression [35,66]. While the BioVid dataset\ndoes document age, the oldest participants are only 65years old, which is notable since pain\nand its management are critical issues among individuals aged 65and older [251]. Simi-50 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nlar limitations are found in other pain datasets, such as X-ITE [117], EmoPain [115], and\nSenseEmotion [116].\nIt is well known that aging causes skin changes, including texture, rigidity, and elastic-\nity alterations, which can impact facial emotion recognition tasks [78]. Additionally, race-\nrelated factors can lead to inaccurate pain assessments due to variations in how pain is ex-\npressed [252]. Notably, one study by Nerella et al. [175] reported lower performance when\ntheir model was tested on African American patients. Furthermore, only one study [194]\nwas found that specifically addressed pain estimation in elderly individuals with dementia.\nIn summary, developing objective, automated, and generalizable deep learning-based\npain assessment systems will only be possible if balanced and representative datasets are\navailable for training and external validation.\n3.6.5 Interpretation of Results\nRecent advancements in AI have shown state-of-the-art performance across nearly every\nscientific discipline, often surpassing human accuracy in specific diagnostic tasks [253].\nHowever, a significant drawback of AI solutions, particularly deep neural networks, is their\nlack of transparency, commonly called \u201cblack box AI\u201d. This term highlights how these\nmodels learn intricate functions that are opaque and frequently incomprehensible to hu-\nmans [254]. This opacity is a primary reason for the criticism directed toward deep learning\ntechniques [255]. Various techniques, such as visualizations and gradients-backpropagation\nfocusing on specific units, have been developed to offer insights into how these models func-\ntion. For further reading, refer to the comprehensive review on explanatory techniques in\ndeep learning [256].\nTable 3.7 outlines the different approaches used to interpret model decisions. Only a\nsmall fraction of the reviewed studies\u2014 20out of 110\u2014implemented methods to explain\nhow their models work and which features or elements they focus on. It is important to\nnote that interpretable machine learning can be broadly defined as the \u201cextraction of rele-\nvant knowledge from a machine-learning model concerning relationships either contained\nin data or learned by the model\u201d [257]. To summarize: (i)18% of the reviewed studies\nprovided an approach to enhance the interpretability of the model\u2019s decision, (ii)all of these\nmethods were applied to studies using facial images as the input modality, and (iii)around\nhalf of these studies were conducted by just three specific research groups. These findings\nsuggest that the issue of interpretability and explainability within deep learning remains un-\nderexplored, particularly in the context of automatically classifying pain severity levels.3.7. CHALLENGES AND FUTURE DIRECTIONS 51\nTable 3.7: Interpretation approaches.\nPaper Year Modality Method\n[124] 2021 F (RGB) visualization (saliency maps)\n[128] 2018 F (RGB) visualization (heat maps)\n[130] 2021 F (RGB) visualization (saliency map)\n[133] 2016 F (RGB) visualization (learned filters)\n[134] 2021 F (RGB) visualization (learned filters)\n[135] 2019 F (RGB) visualization (heat maps),\nvalues of learned weights\n[138] 2018 F (RGB) visualization (saliency maps)\n[141] 2021 F (RGB) visualization (attention maps)\n[142] 2021 F (RGB) visualization (saliency map)\n[143] 2021 F (RGB) visualization (activation maps)\n[153] 2020 F (RGB) visualization (pixels contributions)\n[177] 2017 F (RGB) visualization (average saliency map)\n[179] 2019 F (RGB) visualization\n(generated intermediate representation)\n[194] 2020 F (RGB) visualization (saliency maps)\n[196] 2020 F (RGB) weights per AU (contribution of AUs)\n[173] 2019 F (RGB) visualization (feature maps)\n[174] 2021 F (RGB) visualization (integrated gradients)\n[210] 2021 F (RGB) visualization (heatmaps)\n[211] 2020 F (RGB) visualization (attention maps),\nvalues of learned weights\n[212] 2019 F (RGB) visualization (attention maps)\n3.7 Challenges and Future Directions\nThis section discusses the existing challenges in automatic pain assessment and proposes\nfuture research directions to further progress in the field.\n3.7.1 Current Challenges in Automatic Pain Assessment & Future Research Direc-\ntions\nSeveral limitations exist in the current pain databases. Important demographic factors such\nas sex, gender, and age are often missing, and there is an apparent lack of racial diversity\namong subjects. For example, facial structures and emotional expressions vary across Cau-\ncasian, Asian, and African populations [258]. Moreover, social interactions, such as the\npresence of a partner during assessments, could influence pain manifestation and should\nbe included in future datasets [69]. Estimating the location of pain, particularly for infants\nor individuals with communication impairments, is another vital aspect of pain assessment52 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsystems, which current databases largely overlook. Future datasets should incorporate stim-\nuli targeting various body locations. Furthermore, the videos in existing visual databases\noften have low to medium resolution and frame rates, which are inadequate for capturing\nfacial micro-expressions. Audio data is also sparsely represented, though it holds potential\nas a valuable modality. From an audio perspective, integrating natural language processing\n(NLP) methods to extract linguistic features and create multimodal systems is a promising\ndirection, as shown in affective computing research [259]. Finally, specific validation proto-\ncols should be provided with present and future datasets to ensure objective and consistent\ncomparisons across studies.\nFrom an engineering perspective, several issues must be addressed to advance automatic\npain assessment. Developing multimodal approaches is essential for creating robust systems\nwith enhanced capabilities. Not only do multimodal methods demonstrate better perfor-\nmance than unimodal ones, but they are also crucial in real-world scenarios where a specific\nmodality may become unavailable. Additionally, it is essential to exploit each modality\u2019s\ntemporal aspects fully. We encourage using machine learning models or other techniques\nthat can accommodate the dynamic nature of pain. More work is needed to improve the accu-\nracy of multi-level and low-intensity pain estimation. Another area of research involves the\nrelationship between pain and other affective states, such as negative emotions, which often\ncoexist during painful events. Detecting these emotions could improve pain assessment. Ad-\ndressing challenges like occlusions or poor lighting conditions in vision-based systems also\nrequires attention. Researchers should explore these scenarios, even if current databases do\nnot account for them. Real-time application of pain assessment systems is another critical\nfactor, so future studies should measure throughput, such as the number of images processed\nper second during inference. Generalization is another crucial concern for AI systems, and\nevaluating trained models across different pain databases could be valuable. Finally, to facil-\nitate the clinical adoption of AI-based pain assessment systems, the models\u2019 decisions need\ngreater explainability. Developing or adopting methods that improve interpretability will\nenhance their clinical viability.Chapter 4\nDemographic Variables: Their Role and\nImpact\nContents\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.1 Chapter Overview: Introduction & Related Work\nThis chapter includes the findings published in [35, 36]. As discussed in Section 2, research\nhas demonstrated that biological and psychological differences can lead to variations in how\npain is perceived between men and women. Regarding age, it is known that infants who\ncannot express themselves directly and older adults with health issues require specific care\ndue to their unique needs. However, a significant question remains unanswered in pain\nresearch, both from clinical and biological perspectives: Does the sensation of pain change\nas individuals age? Specifically, does a person in pain perceive their situation differently\n5354 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nas they grow older than others in different age groups? Is the sensation of pain evolving\nthroughout life? To the best of our knowledge, this question has not yet been definitively\nexplored thoroughly. This chapter investigates the differences among males and females, as\nwell as age groups, using ECG signals. In addition, it proposes a computational framework\nthat utilizes these two demographic elements to improve pain assessment performance. This\nchapter analyzes ECG signals to explore the differences in pain perception between males\nand females and various age groups. Additionally, it introduces a computational framework\nincorporating these two demographic factors to enhance the accuracy of pain assessments.\nFrom a computational standpoint, the literature concerning the use of demographic fac-\ntors in pain assessment is scarce. The study in [260] utilized a range of biosignals, including\nEDA, respiration rate, diastolic blood pressure, and facial action units, to demonstrate dif-\nferences in pain perception between males and females. Similarly, in research [245], the\nauthors utilized a hybrid CNN-LSTM model that processed ECG and EDA data, highlight-\ning gender-based variations in pain response. Following the publications of our research,\nanother study by Ricken et al. [261] was released, which explored the differences in adap-\ntation and habituation between men and women. This study extracted handcrafted features\nfrom various biosignals (including ECG and EDA) and employed random forest classifiers\nto analyze the data.\n4.2 ECG Analysis with Classical Machine Learning\nWe explore a pain estimation process using ECG signals and examine variations across dif-\nferent demographic groups, focusing on gender and age. Specifically, we analyze how pain\nmanifestation differs between males and females, investigate variations in pain perception\nacross different age groups, and consider the combined effects of age and gender on pain\nperception.\n4.2.1 Methodology\nThis section will describe the electrocardiography processing algorithm and the methods\nused for feature extraction and classification algorithms.\nECG signal Processing and Analysis\nAn ECG signal captures the heart\u2019s electrical activity over time. Typically, a normal ECG\ndisplays a sequence of waves, identified as P, Q, R, S, T, and occasionally U. These waves\nand their intervals provide crucial insights into heart function. The P wave indicates atrial\ndepolarization, the QRS complex signifies ventricular depolarization and contraction, and\nthe T wave corresponds to the repolarization of the ventricles. Each heartbeat is depicted4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 55\nQR\nST\nPQRS  \nComplex\nPR\nInterval\nQT Interval\nFigure 4.1: The PQRST waveform.\nLow Pass Filter ECG  High Pass Filter Differentiation\nAdaptive ThresholdsMoving W indow\nIntegrationSquaringQRS\nComplexBand-Pass Filter\nLow Pass Filter ECG  High Pass Filter\nDifferentiation\nAdaptive\nThresholdsMoving W indow\nIntegrationSquaring\nQRSBand-Pass Filter\nFigure 4.2: The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure.\nthrough the PQRST complex (refer to Figure 1). Accurately detecting the R wave within the\nQRS complex is especially critical as it is the most pronounced peak in the complex. Precise\ndetection of the R wave allows for the calculation of heart rate (HR) and heart rate variability\n(HRV), the latter of which measures the time intervals between successive R waves, known\nas the R-R or Interbeat interval. The Pan-Tompkins algorithm, developed in 1985, is one of\nthe most extensively used real-time QRS detection algorithms [262]. Over the years, both the\noriginal algorithm and its variations have been rigorously tested, consistently proving their\neffectiveness even with noisy and low-quality data [263,264]. The Pan-Tompkins Algorithm\nis frequently cited as a benchmark in the field due to its robust performance, making it a stan-\ndard against which new QRS detection methods are compared [265]. Our research employed\nthe original Pan-Tompkins Algorithm to identify the QRS complex. We integrated the algo-\nrithm in two primary phases: preprocessing and decision-making. The preprocessing stage\nis crucial for conditioning the ECG by eliminating noise and artifacts, smoothing the signal,\nand enhancing the QRS slope. The preprocessing steps of the Pan-Tompkins algorithm are\ndepicted in the flow diagram shown in Figure 4.2.56 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nFeature Extraction\nThe subsequent phase involves extracting specific features based on the inter-beat intervals\n(IBIs). In our study, we calculated several metrics, including the mean of IBIs, the root mean\nsquare of successive differences (RMSSD), the standard deviation of IBIs (SDNN), the slope\nof the linear regression of IBIs, the ratio of SDNN to RMSSD, and the heart rate, as outlined\nbelow:\n1. Mean of IBIs\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (4.1)\nwhere RRrepresents consecutive Rpeaks.\n2. Root mean square of successive differences\nRMSSD\u201cgffe1\nn\u00b41n\u00b41\u00ff\ni\u201c1pRRi`1\u00b4RRiq2 (4.2)\n3. Standard deviation of IBIs\nSDNN\u201cd\n1\nn\u00b41n\u00ff\ni\u201c1pRRi\u00b4\u00b5q2 (4.3)\n4. Slope of the linear regression of IBIs\nATAx\u201cATb, (4.4)\nwhere is calculated using the least-square approximation, where bis the vector of RR\npeak intervals and Ais the corresponding time series.\n5. Ratio of SDNN to RMSSD\nSR\u201cSDNN\nRMSSD(4.5)\n6. Heartbeat rate\nHR\u201c60\u00a8FS\n\u00b5, (4.6)\nwhere FSis the sampling frequency of the ECG recording, typically 512Hz. Figure\n4.3 illustrates the raw ECG signal and the algorithm\u2019s stages.4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 57\n0 500 1000 1500 2000 2500\nTime (ms)Raw Signal\n0 500 1000 1500 2000 2500\nTime (ms)Band Pass Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Derivative Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Squared\n500 1000 1500 2000 2500\nTime (ms)Moving Window Averaged\nSignal\nQRS\nNoise Level\nSignal Level\nAdaptive Threshold\nFigure 4.3: The signal preprocessing using the Pan-Tompkins algorithm.\nClassification Methods\nFor the classification phase, three widely recognized classifiers were utilized: Linear Dis-\ncriminant Analysis (LDA), Support Vector Machine (SVM) with a linear kernel, and SVM\nwith a Radial Basis Function (RBF) kernel.\n1. Linear Discriminant Analysis\nPpX|y\u201ckq\u201cexp\u00b4\n\u00b41\n2pX\u00b4\u00b5kqt\u03a3\u00b41\nkpX\u00b4\u00b5kqt\u00af\np2\u03c0qd{2|\u03a3k|1{2, (4.7)\nwhere Pdenotes the probability density function for the feature set X, conditional on\nthe target class y\u201ck.\n2. SVM with linear kernel\nKpx1, x2q\u201cxT\n1x2, (4.8)\nwhere x1andx2represent feature vectors from two separate classes.58 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n3. SVM with Radial Basis Function (RBF) kernel\nKpx1, x2q\u201cexp\u02dc\n\u00b4||x1\u00b4x2||2\n2\u03c32\u00b8\n, (4.9)\nwhere \u03c3is the parameter defining the width of the RBF kernel.\nDataset Details\nIn this study, we utilized the publicly available \u201cBioVid Heat Pain Database\u201d [109], which\ncontains facial videos and biosignals (ECG, EMG, EDA) from 87participants ( 44males and\n43females, aged 20\u00b465). This dataset is unique because it is the only publicly accessible\nresource that includes the subjects\u2019 age and gender. The data collection involved applying\na heat stimulus to the right arm of each participant using a thermode. Prior to recording,\nthe pain threshold (the temperature at which the participant first perceives heat as pain) and\npain tolerance (the temperature at which the pain becomes intolerable) were established for\neach participant. The study defined specific thresholds as the temperatures for the lowest\nand highest pain levels. Also, it included two intermediate levels, resulting in five pain\nconditions: No pain (NP), mild pain (P 1), moderate pain (P 2), severe pain (P 3), and very\nsevere pain (P 4). Each participant was exposed to 20stimulations for each intensity level,\ngenerating 100samples across the four modalities.\n4.2.2 Experiments\nIn the following experiments, we specifically used Part A of the BioVid , which includes\npre-processed ECG samples filtered through a Butterworth band-pass filter, totaling 8700\nsamples ( 87\u02c6100\u201c8700 ). All experiments were conducted in triplicate under identical\nconditions, using a distinct classifier for each iteration to compare their effectiveness. This\nwas based on the leave-one-subject-out (LOSO) cross-validation method, utilizing all avail-\nable subjects and ECG samples. The performance of each classifier was evaluated based on\naccuracy.\nUsing the previously mentioned classification algorithms, we conducted experiments to\nrecognize pain and its relationship with demographic factors. The classification tasks were\nstructured around the pain conditions in multi-class and binary classification formats. Specif-\nically, five distinct experiments were executed: (i)multi-class pain classification, (ii)NP vs.\nP1,(iii)NP vs. P 2,(iv)NP vs. P 3,(v)NP vs. P 4. In experiment (i), the goal was to cate-\ngorize an ECG signal into one of the five pain conditions, while experiments (ii)-(v) aimed\nto classify signals into one of two pain conditions, either no pain or the specified pain level.\nFurthermore, considering the gender and age of the subjects, we developed four different4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 59\nTable 4.1: Results for the Basic Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)AllMC 23.72 23.79 22.77\nNP vs P 1 50.97 52.38 49.97\nNP vs P 2 52.55 52.78 52.70\nNP vs P 3 55.20 55.37 53.87\nNP vs P 4 58.62 58.39 57.41\nMC: multi-classification NP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain\nP4: very severe pain LDA: Linear Discriminant Analysis LN:Linear RBF: Radial Basis\nFunction\nexperimental schemes: (i)theBasic Scheme , utilizing the entire dataset, (ii)theGender\nScheme , where data were segmented by the gender of the subjects into males and females,\n(iii) the Age Scheme , which grouped subjects into three age categories: \u201820-35\u2019 ,\u201836-50\u2019 ,\n\u201851-65\u2019 , and (iv) the Gender-Age Scheme , which combined both demographic factors, result-\ning in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\n\u2018Males 51-65\u2019 ,\u2018Females 51-65\u2019 . The most successful classification results are displayed\nin Figures 4.4-4.5 for each task and classification method, while Tables 4.1-4.5 detail the\noutcomes of each individual experiment.\n4.2.3 Results\nTable 4.1 shows the results from the entire dataset, where the multi-class pain classification\nachieved a 23.79% accuracy, and performance scores generally increased with pain intensity,\npeaking at 58.62% for NP vs. P 4. This progression highlights the difficulty in detecting\nlower levels of pain severity. Regarding the classification algorithms, SVM (linear) was\nmore effective, except for the highest pain level task, where SVM (RBF) was less successful.\nIn the Gender Scheme (see Table 4.2), notable differences were observed between males\nand females. Overall, females showed a 1.12% higher accuracy variation than males, with\nfemales achieving 60.69% in NP vs. P 4over males\u2019 56.07%. This 4.62% increase suggests\nthat females are more sensitive to higher levels of pain than males. Interestingly, in NP vs. P 1\nand NP vs. P 2, males outperformed females by 1.16% and1.78%, respectively. Consistent\nwith the first scheme, SVM (linear) yielded better results in most tasks. Figure 4.4 illustrates\nthe gender differences in classification accuracy.\nIn the Age-Scheme (refer to Table 4.3), the \u201820-35\u2019 age group achieved 25.06% accuracy\nin multi-level classification, compared to 23.27% and22.35% for the \u201836-50\u2019 and\u201851-65\u2019\ngroups, respectively, indicating that age significantly affects pain perception. The nearly 9%60 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.2: Results for the Gender Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)MalesMC 22.13 22.25 20.70\nNP vs P 1 51.53 52.61 47.72\nNP vs P 2 53.12 53.69 52.15\nNP vs P 3 54.94 54.71 51.36\nNP vs P 4 55.28 56.07 51.36FemalesMC 25.11 24.41 23.41\nNP vs P 1 50.23 51.45 49.06\nNP vs P 2 51.62 51.86 51.91\nNP vs P 3 55.98 55.87 55.29\nNP vs P 4 60.17 60.69 59.82\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.4: Results for the Gender Scheme .\ndifference in the NP vs P 4task between the youngest and oldest groups was particularly no-\ntable. Similar to the gender-based results, minor differences in low pain intensities among\nthe age groups became more pronounced as pain intensity increased. Specifically, the vari-\nance ( \u03c32) between the groups in NP vs. P 1was1.38%. At the same time, in the other tasks,\nit increased to 2.44%,6.35%, and 20.42%, respectively, showing that high pain intensities\nare necessary to discern perceptual differences among age groups. Regarding classification\naccuracy, the \u201820-35\u2019 group showed the highest sensitivity, followed by \u201836-50\u2019 and\u201851-\n65\u2019. Regarding classification methods, the SVM (RBF) performed best in the \u201851-65\u2019 group\nacross almost all tasks, while it underperformed in the \u201820-35\u2019 group, suggesting it is better\nsuited for more challenging, separable classes. Figure 4.5 displays the results from the age\nscheme.\nIn the final analysis, we examined the subjects more closely to gain deeper insights into\nthe relationship between pain and the demographic factors of gender and age. As shown4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 61\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.5: Results for the Age Scheme .\nin Tables 4.4-4.5, the \u2018Females 20-35\u2019 group achieved the highest accuracy in multi-class\npain classification at 24.80%, while \u2018Females 51-65\u2019 led in NP vs. P 1with 55.38%, again\nindicating higher pain sensitivity in females. Moreover, \u2018Females 51-65\u2019 followed by \u2018Males\n51-65\u2019 topped the performance in NP vs. P 2, and in NP vs. P 3,\u2018Females 36-50\u2019 surpassed\nthe next best group, \u2018Males 20-35\u2019 , by3.5%. In the final NP vs. P 4task, \u2018Females 20-\n35\u2019excelled with 67% accuracy, whereas \u2018Males 51-65\u2019 had the lowest at 54.50%, marking\nthem as the most and least pain-sensitive groups, respectively. It is noted that sometimes\nclassification accuracy decreases despite increased pain levels ( e.g.,\u2018Females 36-50\u2019 ). This\nmight be attributed to the subjects becoming accustomed to the stimulus over time during\nthe biosignal recording.\nFigure 4.6 illustrates the classification performance of the six groups in the Gender-Age\nScheme . Additionally, Table 7 compares our results with other studies that used ECG signals\nfrom the BioVid database and followed the same evaluation protocol, ensuring an objective\ncomparison. Our study achieved the best classification performance in both the multi-class\nsetting and the NP vs. P 1and NP vs. P 2tasks, with acceptable results in the remaining binary\nclassification tasks.\n4.2.4 Discussion\nWe analyzed ECG biosignals using the Pan-Tompkins algorithm to detect QRS complexes\nand extracted features about inter-beat intervals. We also evaluated three machine learning\ntechniques, assessing their performance in multi-class and binary pain classification across\nvarious pain intensities. We also examined the influence of gender and age on pain percep-\ntion, discovering significant differences: males generally showed lower sensitivity to high\npain levels. Regarding the age factor, significant variations suggest that pain sensitivity tends\nto diminish with age, potentially increasing the risk of further injury. In certain demographic\ngroups, the difference in pain perception exceeded 12%, underscoring the variability of pain\nsensation among individuals.62 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.3: Results for the Age Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)20-35MC 25.06 24.73 21.96\nNP vs P 1 52.83 52.83 49.90\nNP vs P 2 54.33 53.75 52.75\nNP vs P 3 55.58 56.16 54.66\nNP vs P 4 63.83 63.41 60.7536-50MC 23.27 22.06 23.03\nNP vs P 1 50.34 48.36 50.68\nNP vs P 2 49.13 51.20 50.17\nNP vs P 3 58.10 58.70 58.27\nNP vs P 4 58.10 57.75 55.9451-65MC 21.89 22.07 22.35\nNP vs P 1 52.23 51.87 52.58\nNP vs P 2 52.14 51.69 52.76\nNP vs P 3 53.66 53.39 54.10\nNP vs P 4 54.46 54.19 54.91\nTable 4.4: Results for the Gender-Age Scheme (Males) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Males 20-35MC 23.13 23.20 18.73\nNP vs P 1 52.50 52.83 45.83\nNP vs P 2 54.00 53.50 53.16\nNP vs P 3 56.33 56.50 54.83\nNP vs P 4 60.00 59.00 53.66Males 36-50MC 23.21 22.21 20.92\nNP vs P 1 50.53 50.53 46.42\nNP vs P 2 50.00 51.78 47.50\nNP vs P 3 54.64 56.25 47.32\nNP vs P 4 55.53 56.25 51.96Males 51-65MC 20.06 21.60 19.60\nNP vs P 1 52.66 51.66 50.66\nNP vs P 2 54.00 54.66 51.50\nNP vs P 3 53.00 54.66 51.50\nNP vs P 4 53.33 54.50 49.834.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 63\nTable 4.5: Results for the Gender-Age Scheme (Females) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Females 20-35MC 24.73 24.80 23.26\nNP vs P 1 49.83 51.50 52.00\nNP vs P 2 54.50 53.66 46.50\nNP vs P 3 53.50 52.83 49.00\nNP vs P 4 65.83 67.00 62.16Females 36-50MC 23.06 22.73 21.93\nNP vs P 1 48.16 49.33 48.33\nNP vs P 2 48.66 49.83 47.83\nNP vs P 3 57.50 60.00 55.00\nNP vs P 4 59.00 58.83 56.16Females 51-65MC 21.23 21.84 23.92\nNP vs P 1 48.84 49.80 55.38\nNP vs P 2 51.15 48.65 55.96\nNP vs P 3 53.07 53.07 50.96\nNP vs P 4 52.69 55.00 56.34\nTable 4.6:\nComparison of studies utilizing BioVid , ECG signals\nand LOSO validation (1).\nMethod Task Results\nMartinez and Picard [266] NP vs P 4 57.69\nWerner et al. [267]NP vs P 1 48.70\nNP vs P 2 51.60\nNP vs P 3 56.50\nNP vs P 4 62.00\nThiam et al. [235]MC 23.23\nNP vs P 1 49.71\nNP vs P 2 50.72\nNP vs P 3 52.87\nNP vs P 4 57.04\nOursMC 23.79\nNP vs P 1 52.38\nNP vs P 2 52.78\nNP vs P 3 55.37\nNP vs P 4 58.6264 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65MC232523232224BL vs P1535251495355BL vs P2545552505556BL vs P3575456605553BL vs P4606756595456018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65\n1\nFigure 4.6: Results for the Gender-Age Scheme .\n4.3 ECG Analysis with Multitask Neural Networks\nIn this section, we build on previous analysis 4.2 that explored variations in pain manifesta-\ntion across different demographic groups using ECG signals. It expands this investigation\nby implementing neural networks as the primary machine learning model and introduces a\nnovel multi-task learning (MTL) neural network. This network leverages demographic infor-\nmation to estimate age and gender in addition to pain levels, aiming to enhance the automatic\npain estimation system.\n4.3.1 Methodology\nThe following method we developed is a neural network-based approach. The feature extrac-\ntion process remains the same as previously described in 4.2.1, utilizing the Pan-Tompkins\nalgorithm for ECG signal processing.\nNeural Network\nThe proposed neural network was designed and trained using two distinct approaches: single-\ntask learning (STL) and multi-task learning (MTL). In the multi-task learning framework, the\nnetwork is simultaneous training for pain estimation and predicting age and/or gender.\nSingle-Task Neural Network: The proposed neural network comprises two components:\nthe encoder, which maps the original feature vectors into a higher dimensional space, and\nthe task-specific classifier. In our design, both the encoder and the classifier utilize fully-4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 65\nTable 4.7: Hyper-parameters used in our approach.\nEpochs Optimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsLabel\nsmoothEMA\n300 AdamW 1e-3 cosine 0.1 50 0.1 \u2713\nconnected (FC) layers, which are defined as follows:\nzipsq\u201cbi`nin\u00ff\nj\u201c1Wijsj for i\u201c1, .., n out, (4.10)\nwhere zirepresents the result of linearly combining the incoming inputs sj, with each input\nbeing weighted by Wijand adjusted by a bias bi. Each layer in the encoder is followed by a\nnonlinear activation function, specifically the rectified linear unit (ReLU), which is defined\nas:\n\u03c3pzq\u201c#\n1, z\u011b0\n0, z\u01030(4.11)\nThe classifier\u2019s layers are connected without nonlinearity, the encoder comprises four fully\nconnected (FC) layers with 256,512,1024 , and 1024 neurons respectively. The classifier\nincludes 2layers with 1024 andnneurons, where nrepresents the number of distinct pain\nclasses being classified. The hyperparameters of the network are detailed in Table 4.7.\nMulti-task neural network: This proposed machine learning method is based on the princi-\nple of sharing representations across related tasks, which helps the model better generalize\nto the primary task of pain estimation in this case. We kept the same encoder and pain\nclassifier in this configuration but introduced two additional auxiliary networks for age and\ngender estimation. The architecture of the proposed multi-task learning (MTL) neural net-\nwork is illustrated in Fig. 4.7. The objective of this network is to simultaneously minimize\nthree different losses. We adopt and expand upon the framework suggested by [268] for the\nmulti-task learning loss, where learned weights are applied to each loss function based on\nthe homoscedastic uncertainty of each task:\nLtotal\u201crew1LPain`w1sc1`rew2LAge`w2sc2`rew3LGender`w3sc3. (4.12)\nHere, Lrepresents the corresponding loss, wdenotes the weights, and care the coefficients\nthat modulate the losses LAgeandLGender to prioritize learning in the pain estimation task.\nIt should be noted that all tasks are treated as classification problems, utilizing cross-entropy66 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nL1\nL2\nL3 L4Feature\nvector\nPainAge\nGender6 x 1256 x 1512 x 11024 x 1 1024 x 1\n1024 x 1n x 1512 x 1256 x 1\nL1L2L1L2L336 x 1\n512 x 1256 x 1\nL1L2L32 x 1\nEncoderMTL LossPain\nEstimation\nFigure 4.7: The proposed MTL network: The sizes of the extracted vectors for the network are\nas follows: for the Pain classifier, n\u02c61, where nis the number of pain estimation\ntasks ( e.g.,2for binary classification, 5for multi-class classification); for the Age\nclassifier, 36\u02c61, where 36represents the possible age values of the subjects; for\nthe Gender classifier, 2\u02c61, corresponding to the two possible gender categories\n(i.e., males and females).\nloss with label smoothing:\nLD\u201c\u00b4\u00ff\n\u03b4PDnout\u00ff\ni\u201c1ppi|x\u03b4qlogrqpi|x\u03b4qs. (4.13)\nHere, Ddenotes the pain database, ppi|x\u03b4q \u201c1\u00b4\u03f5represents the probability of the true\nclass igiven the input x\u03b4, and ppi\u2030i\u03b4|x\u03b4q \u201c\u03f5{pnout\u00b41qis the probability distribution\nacross the other classes. This formulation spreads a small portion \u03f5of the probability across\nclasses other than the true class to implement label smoothing. Furthermore, qpi|x\u03b4qis the\nprobability distribution over the classes ias predicted by the network\u2019s output.\n4.3.2 Experiments\nSimilar to 4.2.2, we utilized the BioVid database, specifically focusing on its ECG signals.\nEmploying a single-task neural network (ST-NN), we conducted an initial series of experi-\nments to assess the impact of demographic factors. Building on the concept we proposed\nin the previous section, we devised five experimental schemes: (i)theBasic Scheme , which\nincluded all subjects from the database; (ii)theGender Scheme , which segregated subjects4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 67\ninto male and female groups; (iii)theAge Scheme , which categorized subjects into three age\ngroups\u2014 \u201820-35\u2019 ,\u201836-50\u2019 , and \u201851-65\u2019 ; and (iv)the\u2018Gender-Age Scheme\u2019 , which combined\nboth demographic factors, resulting in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\n\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\u2018Males 51-65\u2019 , and \u2018Females 51-65\u2019 . All experiments were\nconducted in both binary and multi-class classification formats. Specifically, the binary clas-\nsification tasks were (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3, and (4) NP vs. P 4, and the\nmulti-class task utilized all available pain classifications from the database.\n4.3.3 Results\nDemographic Groups\nTable 4.8 presents the classification results of the Basic Scheme , which utilized all subjects\nin the database. For the multi-class pain classification, we achieved an accuracy of 29.43%,\nwith NP vs. P 1scoring 61.15% and NP vs. P 4reaching 68.82%. These results indicate that\nas pain intensity increases, so performs, highlighting the difficulty in recognizing less severe\npain. According to the Gender Scheme (refer to Table 4.9), notable differences emerge\nbetween males and females, particularly at higher pain intensities. Specifically, in NP vs.\nP4, females achieved an accuracy of 69.48% compared to 66.48% for males, with an overall\nvariance of 1.63% between genders, suggesting that females exhibit higher pain sensitivity.\nFigure 4.8a illustrates these gender-based classification disparities. In the Age Scheme (see\nTable 4.10), the \u201820-35\u2019 age group outperformed the \u201836-50\u2019 and\u201851-65\u2019 groups in NP vs. P 4,\nwith accuracies of 72.58%,66.29%, and 64.91%, respectively. While the differences are less\npronounced at lower pain intensities, this scheme still shows that age significantly impacts\npain perception, particularly among the older population. Figure 4.8b shows the results from\nthe age scheme.\nIn the final scheme, by dividing subjects into more specific groups, we can analyze them\nmore precisely and gain better insights into the relationship between gender, age, and pain\nperception. Table 4.11 reveals that in the NP vs. P 4task, the group \u2018Females 20-35\u2019 reached\nthe highest accuracy of 71.67%, significantly outperforming the \u2018Males 51-65\u2019 group, which\nscored the lowest at 60.67%, marking them as the least sensitive group. This pattern is\nconsistent across the multi-class classification and other pain tasks, with \u2018Females 20-35\u2019\nand\u2018Males 51-65\u2019 exhibiting the highest and lowest accuracies, respectively. This supports\nthat females generally experience more pronounced pain responses, while older males have\na reduced pain sensation. It is noted that in some instances, such as with \u2018Males 20-35\u2019\nand\u2018Males 36-50\u2019 , higher pain levels do not necessarily correlate with higher classification\naccuracy, a phenomenon also noted in our previous experiments, in Section 4.2.3. A possible\nexplanation could be the subjects\u2019 habituation to pain stimuli, especially at lower intensities.\nFigure 4.8c visualizes the performance outcomes of the Gender-Age Scheme .68 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.8: Results for the Basic Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN 61.15 62.87 65.14 68.82 29.43\nST-NN: single-task neural network NP: no pain P1: mild pain P2: moderate pain P3: severe pain P4: very severe\npain MC: multi-classification\nTable 4.9: Results for the Gender Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales ST-NN 60.40 63.24 63.18 66.48 28.61\nFemales ST-NN 60.87 62.15 66.98 69.48 30.59\nTable 4.10: Results for the Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n20-35 ST-NN 61.58 64.08 66.08 72.58 31.07\n36-50 ST-NN 60.52 61.38 64.05 66.29 29.59\n51-65 ST-NN 61.70 60.80 62.50 64.91 27.82\nTable 4.11: Results for the Gender-Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales 20-35 ST-NN 62.83 62.33 65.50 71.33 29.73\nMales 36-50 ST-NN 61.79 60.00 59.64 64.11 27.14\nMales 51-65 ST-NN 59.50 58.67 57.33 60.67 26.07\nFemales 20-35 ST-NN 63.17 63.17 66.83 71.67 31.53\nFemales 36-50 ST-NN 59.50 61.00 65.83 67.00 29.13\nFemales 51-65 ST-NN 60.96 60.96 59.23 63.27 27.69\nAugmentation of Feature Vectors\nBuilding on the findings from the previous experiments about the impact of demographic\nfactors on pain perception, we explored the practical use of subjects\u2019 demographic data.\nExperiments were conducted using the Single-Task Neural Network (ST-NN) and feature4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 69\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(a) Gender\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(b) Age\nAge\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-Age\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\nBL vs P1 63 63 62 60 60 61\nBL vs P2 62 63 60 61 59 61\nBL vs P3 66 67 60 66 57 59\nBL vs P4 71 72 64 67 61 63\nMC 30 32 27 29 26 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales 20-35\n Females 20-35\n Males 36-50\n Females 36-50\n Males 51-65\n Females 51-65\nGender\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales\n Females\nAge-1\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-1\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales\n Females\n020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\n20-35 36-50 51-65018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales Females020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\n1\n(c) Gender-Age\nFigure 4.8: Results for the proposed Schemes.\nvectors enhanced with demographic attributes. Initially, the feature vectors, which originally\nconsisted of six features (see 4.3.1), were augmented by adding either one additional fea-\nture ( i.e., the subject\u2019s gender or age) or two additional features (both the subject\u2019s gender\nand age). We conducted the same pain estimation tasks using this enhanced set of features.\nAs shown in Table 4.12, the results demonstrate improved performance with the augmented\nfeature vectors. Specifically, the most effective augmentation involved combining gender70 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.12: Comparison of results adopting the feature augmentation approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAux: Auxiliary information -: original feature vectors F(G): feature vectors with the additional feature of gender F(A): feature\nvectors with the additional feature of age F(GA): feature vectors with the additional features of gender and age\nand age features, which increased the average pain estimation performance by 0.55%. Us-\ning these demographic features individually also improved classification accuracy, albeit\npartially.\nMulti-Task Neural Network\nThe final experiments utilized a multi-task learning framework with the proposed Multi-Task\nNeural Network (MT-NN) outlined in 4.3.1. The classification results for MT-NN, incorpo-\nrating additional tasks of (1) gender estimation, (2) age estimation, and (3) simultaneous\ngender and age estimation, are detailed in Table 4.13. For comparison, the results from ear-\nlier experiments using the Single-Task Neural Network (ST-NN) method are also included\nin Table 4.13. We noted that the task of gender estimation alone performed less effectively\nthan the other tasks. In contrast, the combined gender and age estimation delivered the high-\nest performance across four tasks. Specifically, in the multi-class classification, it achieved\n30.24%, and in NP vs. P 1, it reached 62.8%, marking the best results of any method pre-\nsented in this study. In NP vs. P 3and NP vs. P 4, the combined tasks outperformed the\nindividual gender and age tasks but were slightly less effective than the ST-NN approaches\nusing augmented features. Interestingly, in NP vs P 2, the age estimation task alone excelled,\nachieving 63.97%, the highest result recorded in this study.\nComparing the overall performances of MT-NN with the ST-NN approaches (using both\noriginal and augmented feature vectors), there is a noticeable improvement of 0.71% and\n0.39%, respectively, in average pain estimation accuracy across all tasks. Figure 4.9 visually\ncompare each neural network approach used in this study, encompassing multi-class and\nbinary classification tasks.4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 71\nTable 4.13: Comparison of results adopting the MT-NN approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAll MT-NN T(G) 61.72 63.39 65.95 68.99 30.00\nAll MT-NN T(A) 62.72 63.97 65.40 69.28 29.79\nAll MT-NN T(GA) 62.82 63.68 66.12 69.40 30.24\nT(G): MT-NN with the additional task of gender estimation T(A): MT-NN with the additional task of age estimation T(GA):\nMT-NN with the additional task of gender and age estimation\nComparison with Existing Approaches\nIn this section, we benchmark the results achieved using the Multi-Task Neural Network\n(MT-NN), which incorporated additional tasks of gender and age estimation against relevant\nstudies. These comparative studies also utilized electrocardiography signals from Part A of\ntheBioVid database with all 87participants. To ensure a fair comparison, they followed the\nsame evaluation protocol, specifically the leave-one-subject-out (LOSO) cross-validation.\nThe comparative results are detailed in Table 4.14 and include research that employed hand-\ncrafted features with traditional machine learning algorithms [35] [267], end-to-end deep\nlearning models [269] [235], and finally, hybrid approaches combine hand-crafted features\nwith deep learning classifiers [266]. Our approach, which leverages hand-crafted engineered\nECG features and a high-dimensional mapping from the encoder in combination with multi-\ntask learning neural networks, demonstrated superior performance across all pain estimation\ntasks, whether in binary or multi-class classification settings.\n4.3.4 Discussion\nWe explored multi-task learning neural networks for automatic pain estimation from electro-\ncardiography signals. By implementing the Pan-Tompkins algorithm to identify QRS com-\nplexes, we extracted features associated with inter-beat intervals (IBIs). Numerous experi-\nments were conducted to explore how gender and age influence pain perception, highlighting\ntheir significant impact. Additionally, we introduced two approaches to enhance pain esti-\nmation results by leveraging demographic information. Firstly, we augmented the original\nfeature vectors by incorporating the subjects\u2019 demographic data, improving classification\naccuracy. Secondly, we employed a multi-task learning neural network that combined the72 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(a) Binary classification\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(b) Multi-class classification\nFigure 4.9: Comparison of performances utilizing various neural networks approaches.\nTable 4.14: Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2).\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nGkikas et al. [35]:52.38 52.78 55.37 58.62 23.79\nHuang et al. [269]\u2039d- - - 65.00 28.50\nMartinez and Picard [270]\u00b8- - - 57.69 -\nThiam et al. [235]\u203949.71 50.72 52.87 57.04 23.23\nWernel et al. [267]:48.70 51.60 56.50 62.00 -\nThis study\u00b862.82 63.68 66.12 69.40 30.24\n::hand-crafted features and classic machine learning \u2039: end-to-end deep learning \u00b8: hand-crafted features with deep\nlearning classification algorithms d: pseudo heart rate gain extracted from visual modality\ntasks of pain, gender, and age estimation. This approach yielded superior results compared\nto methods previously discussed in this chapter and other related research. These findings\nindicate that domain-specific features can achieve excellent outcomes when combined with\nwell-designed deep-learning architectures and demographic factors.4.4. SUMMARY 73\n4.4 Summary\nIn this chapter, we examine the impact of age and gender on pain perception using ECG\nsignals to extract relevant features. Our study involved a series of experiments where subjects\nwere categorized into different groups based on gender (males and females) and age (20-35,\n36-50, and 51-65 years). Additionally, we created combined groups that segregated age\ngroups within each gender. Our findings from both approaches provided strong evidence\nof significant differences in pain perception among these groups. Notably, we observed\na12.5%disparity in pain sensitivity between young females and older males. Generally,\nour results confirm that females exhibit higher pain sensitivity than males, aligning with\nfindings from other studies in pain research. A critical discovery from our study is that pain\nsensitivity appears to decrease with age, which may increase the risk of unnoticed injuries.\nWe presented two methods of incorporating demographic information into our models from a\ncomputational perspective. First, we augmented the feature vectors derived from ECGs with\ndemographic data. Second, we utilized a multi-task neural network approach to estimate\npain, gender, and age simultaneously. Both methods demonstrated improved performance\ncompared to the standard approach, indicating that integrating demographic information can\nenhance the accuracy of automatic pain assessment systems.\nWe recommend that clinical pain assessment tools be designed for specific demographic\ngroups to account for the distinct ways pain manifests across different populations. Ad-\nditionally, we emphasize to researchers developing new pain databases the importance of\nincluding demographic factors and information on the social context and psychological con-\nditions of subjects to enhance the quality and applicability of the data collected. Our study\nfocused on analyzing pain sensation through biosignals, specifically ECGs. We propose that\nfurther research should explore pain expressivity through visual mediums such as video. As\npreviously discussed in Section 2, the expression of pain is a crucial and complex issue. Peo-\nple vary in expressiveness; for various reasons, they might exaggerate or even feign pain,\nmaking accurate assessment challenging.74Chapter 5\nOptimization: Balancing Efficiency and Per-\nformance\nContents\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the studies published in [37, 38]. As outlined in\n3.6.3, research in automatic pain assessment has rarely considered real-world situations. For\nexample, [231] implemented their study in an emergency triage setting, while [234] tested\ntheir approach within IoT devices. Additionally, we highlighted that the scarcity of stud-\nies exploring pain estimation in real-world settings or unconventional contexts suggests that\ncurrent methodologies might not be entirely suitable for practical environments like clinics\nor hospitals due to issues with generalization or operational factors such as efficiency and\ninference time. For these reasons, this chapter\u2019s objective is to explore approaches that (i)\n7576 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nutilize modalities readily available and applicable in the market and (ii)examine the impact\nof model size and computational cost on performance. In this context, our methodologies\nand experiments exclusively utilize RGB videos, a universally available modality, particu-\nlarly on mobile devices. Additionally, we incorporate heart rate data, which is commonly\naccessible from various types of wearable technology. It is important to note that although\nwe employ established pain datasets in our experiments, the videos and heart rate data ex-\ntracted from ECGs are akin to those that could be obtained from smartphones and wearables,\nserving as a proof of concept for application in real-world environments. Furthermore, with\nrespect to efficiency and the speed of inference, our goal is to develop the most compact pain\nassessment frameworks possible, ensuring they maintain adequate performance levels.\nNumerous studies highlight the capabilities of automated systems that utilize behavioral\nor physiological modalities for pain assessment [271]. Sario et al. [34] demonstrate the fea-\nsibility of accurately detecting and quantifying pain through facial expressions, establishing\ntheir value in clinical settings. The use of multimodal sensing appears especially promising,\noffering increased accuracy in pain monitoring systems [22]. An important aspect of pain\nmonitoring involves wearable devices that record biopotentials to estimate pain levels. Few\nstudies have investigated the use of mainstream, wearable technology for this purpose, possi-\nbly due to a research preference for more costly, highly precise medical equipment. Leroux\net al. [32] state, \u201cThe challenge is not whether wearable devices will provide useful clinical\ninformation but rather when we will start to use them in practice to enhance the field of pain. \u201d\nAdditionally, Claret et al. [272] explore the potential of using cardiac signals from wearable\nsensors for automatic emotion recognition, confirming the effectiveness of such methods.\nIn this chapter, our deep learning approaches are founded on transformer-based archi-\ntectures. Convolutional Neural Networks (CNNs) have been the cornerstone of mainstream\nneural architectures in computer vision (CV), especially in the field of automatic pain as-\nsessment using images and videos, as we discussed in Section 3. Inspired by the success\nof transformer architecture in natural language processing (NLP), where the self-attention\nmechanism is a fundamental element [273], researchers have developed similar models for\nvisual tasks. The introduction of Vision Transformers (ViT) [274] has established a new\nparadigm in the computer vision domain. This has led to a plethora of new approaches based\non ViT, such as the Transformer in Transformer (TNT) [275], which enhances local feature\nrepresentation by subdividing image patches into smaller sub-patches. While transformer-\nbased models have shown impressive results and offer great flexibility, they tend to scale\npoorly with input size and incur higher computational costs due to the self-attention layers\nthat compute interactions between all input pairs. Efforts to mitigate these challenges in-\nclude replacing self-attention with cross-attention [276] or combining both techniques [277]\nto improve the efficiency and reduce the complexity of these architectures.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 77\nFigure 5.1: The application of face alignment illustrates landmarks in 2D (left) and 3D (right) space.\n5.2 Video Analysis with Vision Transformers\nWe introduce a framework incorporating a vision transformer as a module extracting spatial\nfeatures for individual video frames, combined with a transformer-based model equipped\nwith cross and self-attention blocks, extracting temporal features from the video feature\nsequences. This configuration enables the effective utilization of the temporal dimensions of\nvideo data to deliver more accurate and reliable estimation of the continuous nature of pain.\n5.2.1 Methodology\nThis section outlines the preprocessing methods employed, the design of our framework, the\nimplementation details concerning the training procedure, and the database used.\nPre-processing\nBefore processing videos for pain estimation, applying face detection and alignment was\ncrucial to enhance performance and computational efficiency. We utilized the well-known\nface detector MTCNN [278] in combination with the Face Alignment Network (FAN) [279],\nwhich leverages 3D landmarks. This 3D approach is critical for addressing our specific chal-\nlenges, as head movements tend to increase, particularly during instances of high-intensity\npain, which can lead to inaccurate alignments with 2D methods. Additionally, it should be\nnoted that all experiments were carried out using video frames with a resolution of 224\u02c6224\npixels. Figure 5.1 illustrates the facial alignment process applied to a video frame.\nTransformer-based Framework\nOur framework is composed of two primary components: the \u201cspatial feature extraction\nmodule\u201d , specifically a TNT (Transformer in Transformer) model, and the \u201ctemporal fea-78 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nture extraction module\u201d , which is a transformer with both cross and self-attention blocks.\nThis framework, illustrated in Figure 5.2, includes approximately 24million parameters and\nperforms operations at 4.2GFLOPS.\nSpatial feature extraction module :Each frame is initially divided into npatches represented\nasFk\u201crF1\nk, F2\nk, . . . , Fn\nksPRn\u02c6p\u02c6p\u02c63, where p\u02c6pdenotes the resolution of each patch ( i.e.,\n16\u02c616) and 3represents the number of color channels. These patches are then subdivided\nintomsub-patches to facilitate the model\u2019s learning of global and local features. Each frame\nis thus transformed into a sequence of patches and sub-patches:\nFk\u00d1rFk,n,1, Fk,n,2, . . . , F k,n,ms, (5.1)\nwhere Fk,n,mPRs\u02c6s\u02c63is the m-th sub-patch of the n-th patch of the k-th frame, with each\nsub-patch having a resolution of s\u02c6s(i.e.,4\u02c64). Following this, the patches and sub-\npatches undergo a linear projection and are transformed into embeddings ZandY. Position\nembeddings are then added to retain spatial information:\nZ0\u00d0Z0`Epatch, (5.2)\nwhere Epatchare the position encodings for the patches. Correspondingly, for each sub-patch\nwithin a patch, a position encoding is also added:\nY0\ni\u00d0Y0\ni`Esub-patch , (5.3)\nwhere Esub-patch are the sub-patch position encodings and i\u201c1,2, . . . , m denotes the index of\na sub-patch. These sub-patches are then processed through an \u201cInner Transformer Encoder\u201d ,\nwhich consists of two multi-head self-attention blocks, crucial for dot product attention. The\nattention mechanism is defined as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n, (5.4)\nwhere QPRM\u02c6D, KPRM\u02c6C,andVPRM\u02c6C(Mis the input dimension, CandDare\nchannel dimensions) are projections of the corresponding input and represent the Query, Key,\nand Value matrices. They defined as Q\u201cXW Q,K\u201cXW K, and V\u201cXW V, where W\nare the learnable weight matrices and Xis the input. The output embedding from the \u201cInner\nTransformer Encoder\u201d is then added to the patch embedding and forwarded to the \u201cOuter\nTransformer Encoder\u201d . This encoder comprises three multi-head self-attention blocks, and\nits output is a feature vector d\u201c192. The \u201cspatial feature extraction module\u201d as a whole\nencompasses a depth of 12blocks.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 79\nTable 5.1: Training details for the automatic pain assessment.\nEpochs Optimizer Learning\nRateLR decay Weight\ndecayWarmup\nepochs\n200 AdamW 1e-4 cosine 0.1 5\nLabel\nSmoothingDropPath Attention\nDropOutLoss\nfunctionAugmentation methods\n0.1 0.1 0.1 Cross\nEntropyAugMix [281] &\nTrivialAugment [282]\nDropPath applied to the \u201cspatial feature extraction module\u201d , Attention DropOut applied to the \u201ctemporal\nfeature extraction module\u201d\nTemporal feature extraction module :The extracted embeddings of each input video frame\nare concatenated into a unified vector D, representing the entire video as V\u00f1D\u201c\npd1\"d2\", . . . , dkq. This vector is then processed through the temporal module, a transformer\narchitecture consisting of 1cross-attention and 2self-attention mechanisms, each followed\nby a fully connected neural network (FCN). The introduction of cross-attention, which em-\nploys asymmetry in the attention mechanism, helps reduce computational complexity and\nincrease the model\u2019s efficiency. Specifically, rather than projecting the input with dimen-\nsions M\u02c6D, theQin cross-attention is a learned matrix with dimensions N\u02c6D, where\nN\u0103M. This module\u2019s self-attention components function as detailed in Equation 7.3, with\nthe cross and self-attention units comprising 1and8heads, respectively. In addition, we\nincorporate Fourier feature position encoding [277].\nTraining Details: Before starting the automatic pain estimation training process, we pre-\ntrained the \u201cspatial feature extraction module\u201d using the VGGFace2 dataset [280], incorpo-\nrating over three million facial images from more than nine thousand individuals. Table 7.3\ndetails the hyperparameters of our method and the applied augmentation techniques.\nDatabase Details: For this approach, we used the publicly available BioVid dataset [109],\nas described in the previous chapters.\n5.2.2 Experiments\nIn this section, we detail the experiments conducted for pain estimation. Our experiments\nwere carried out in both binary and multi-level classification formats. Specifically, we con-\nducted binary classification tasks: (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3,(iv)NP vs. P 4,\nand(v)a multi-level pain classification utilizing all available pain classes from the database.\nWe employed the leave-one-subject-out (LOSO) cross-validation method as our evaluation80 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nprotocol. Additionally, the classification metrics used in this study include micro-average ac-\ncuracy, macro-average precision, macro-average recall (sensitivity), and macro-average F1\nscore.\n5.2.3 Results\nPain Estimation\nIn evaluating pain estimation tasks, we noted the following results: For the NP vs. P 1task,\naccuracy reached 65.95%, with precision almost identical at 65.90%. The F1 score was\nFigure 5.2: An overview of our proposed transformer-based framework for automatic pain as-\nsessment.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 81\nTable 5.2: Results on the pain estimation tasks.\nMetricTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAccuracy 65.95 66.87 69.22 73.28 31.52\nPrecision 65.90 66.89 69.18 73.31 31.48\nRecall 67.85 68.34 70.84 74.75 29.94\nF1 65.04 66.19 68.54 72.75 27.82\nNP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain P 4: very severe pain MC: multi-level\nclassification\nslightly lower at 65.04%, and the recall stood out at 67.85%. In the NP vs. P 2task, accuracy\nincreased to 66.87%, and all related metrics improved, with the F1 score climbing by over\n1.15%, highlighting enhanced detection of true positives. The results were notably better\nfor the NP vs. P 3task, with an accuracy of 69.22% and a sensitivity of 70.84%. This is\nexpected as the pain at this level is considered severe, eliciting more pronounced responses\nfrom subjects. In the highest pain task, NP vs. P 4, the recall was particularly high at 74.75%,\nwith an accuracy of 73.28%, demonstrating that the detection of very severe pain is relatively\nmore straightforward due to the pain reaching tolerance thresholds, making it more visibly\nevident through subjects\u2019 facial expressions. However, in the multi-level classification task,\nperformance metrics were lower, illustrating the complexity of estimating all pain levels\nconcurrently; accuracy was only 31.52%, with a recall of 29.94%, pointing to significant\nchallenges in accurately identifying true positives across multiple pain levels.\nIt should be noted that our framework, encompassing both the architectural and procedu-\nral aspects of training, was consistent across all binary and multi-level classification tasks.\nThis was done to evaluate the generalization potential of our approach across all possible\nscenarios provided by the database, akin to real-world clinical settings. The detailed classifi-\ncation outcomes are presented in Table 5.2.\nVideo Sampling\nIn this section, we explore the impact of video frame sampling on automatic pain estimation.\nExperiments detailed in Section 5.2.3 utilized all available frames ( 138) from each video.\nSubsequent experiments employed frame sampling with strides of 2,3, and 4. Starting with\nall138frames, the video feature representation Dhas dimensions 138\u02c6192, totaling 26,496.\nA stride of 2reduces this to 69frames, with Dhaving dimensions 69\u02c6192and totaling\n13,248. With strides 3and4, the frame counts reduce to 46and35, resulting in Dsizes82 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.3: Results for the pain estimation tasks using various numbers of input frames.\nNumber of\nFramesTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n138 65.95 66.87 69.22 73.28 31.52\n69 65.76 66.74 69.15 73.25 31.29\n46 65.66 66.70 68.50 71.78 31.20\n35 65.40 66.12 68.32 72.01 30.80\nFigure 5.3: The impact of the number of input frames on accuracy (left) and on runtime in\nmilliseconds (right). Runtime calculated during inference on a NVIDIA RTX-3090 .\nof8,832and6,720, respectively. Table 5.3 displays the classification accuracies achieved\nwith these varying frame counts for each pain estimation task. Concurrently, Figure 5.3\ndemonstrates how the number of frames affects mean accuracy across the five tasks and\nmean runtime during inference. We noted a performance increase of approximately 1.38%\nwhen using 138frames compared to 35frames. Additionally, the runtime increased by a\nfactor of 3. Despite the longer runtime, each sampling rate allows for real-time automatic\npain estimation when necessary.\nInterpretation\nResearch in deep learning, particularly relevant to healthcare, increasingly focuses on model\ninterpretability to explain decision-making processes. This is crucial for enhancing the trans-\nparency of models, a key factor for their acceptance and integration into clinical settings. In\nour study, we implemented the technique described in [283] to generate relevant maps illus-\ntrating which facial areas our model\u2014the \u201cspatial feature extraction module\u201d \u2014focuses on.\nAs shown in Figure 5.4, the model\u2019s attention is distributed over \u201carbitrary\u201d areas at the on-\nset of a facial expression sequence. However, as the expression of pain intensifies, the focus\nsharpens on specific regions indicative of pain. It is important to note from our relevance5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 83\nFigure 5.4: Relevance Maps.\nmaps that no universal facial expressions are unique to pain. However, there is a noticeable\nconcentration on areas like the mouth and eyes.\nComparison with existing methods\nIn this section, we present a comparison of our results achieved using a transformer-based\nframework that utilizes all available video frames against other studies that also employed\nPart A of the BioVid database with all 87subjects, following the same leave-one-subject-out\n(LOSO) cross-validation protocol. This ensures objective and accurate comparisons, with\nresults detailed in Table 5.4. The studies compared fall into three main categories: i) those\nfocusing exclusively on pain detection (NP vs. P 4),ii) those examining both pain detection\nand multi-level pain estimation, and iii) those that cover all major pain-related tasks.\nOur method, tested across all tasks, recorded the highest performance metrics in bi-\nnary and multi-level pain estimations. Studies limited to specific aspects of pain detec-\ntion or multi-level pain estimation often yielded comparable or superior results, as indicated\nin [219], [180], and [284]. This highlights that while focused studies often show high perfor-\nmance, the broader impact lies in developing systems that perform well across all potential\nscenarios.\n5.2.4 Discussion\nThis research examined the application of transformer-based architectures for automatic pain\nestimation through video analysis. Our framework employed exclusively transformer mod-\nels, leveraging the spatial and temporal aspects of the video frames. The experiments demon-\nstrated the effectiveness of our approach in assessing pain, showing strong generalization\nacross various pain estimation tasks with notable results, particularly with low-intensity pain\nwhere facial expressions are less apparent. Additionally, the framework demonstrated high\nefficiency, suitable for real-time applications. A significant contribution of our work includes\ndeveloping relevance maps highlighting facial areas the model focuses on. We advocate for\ncontinued efforts within the affective computing field to enhance the interpretability of these\ndeep-learning methods.84 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.4: Comparison of studies utilizing BioVid , RGB videos, and LOSO validation.\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nThiam et al. [219] - 69.25 -\nTavakolian et al. [180] - - - 71.02 -\nPatania et al. [284] - - - 73.20 -\nHuang et al. [269] - - - 77.50 34.30\nXinet al. [141] - - - 86.65 40.40\nZhi and Wan [217] 56.50 57.10 59.60 61.70 29.70\nWerner et al. [248] 53.30 56.00 64.00 72.40 30.80\nOur approach 65.95 66.87 69.22 73.28 31.52\n5.3 Video & Heart Rate Analysis with Transformer Architectures\nWe introduce a proof of concept for an automatic pain assessment framework that integrates\nfacial video data captured by an RGB camera with heart rate signals. We build and extend\nour previous analysis in 5.2. Our main objectives include (1) evaluating the effectiveness and\nlimitations of video and heart rate data as standalone modalities in an unimodal setting, (2)\nexploring the efficacy of combining behavioral (video) and physiological (heart rate) markers\nto overcome challenges associated with their reliance on different sensing technologies and\ninformation representations, and (3) analyzing the performance and efficiency of recently\nintroduced transformer-based architectures.\n5.3.1 Methodology\nThis section details the preprocessing methods for video and ECG, the design of the proposed\nframework, the augmentation techniques developed, and the specifics of implementing the\npretraining process.\nPre-processing\nPreparatory processing was essential before feeding data into the pain assessment framework,\nparticularly for the raw ECG data used to compute heart rate. We focused on exploring heart\nrate as the primary feature due to its benefits: It\u2019s readily obtainable from wearable devices,\ncost-effective, and easily accessible. These advantages position heart rate as a potentially\nvaluable feature for automated pain assessment.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 85\nVideo Preprocessing: Video preprocessing included face detection to isolate the facial re-\ngion using the MTCNN face detector [278], which employs multitask cascaded convolu-\ntional networks to predict facial and landmark locations. Predicting landmarks facilitates\nface alignment, which is crucial for accurate facial analysis. However, we observed that\nface alignment reduced the expressiveness linked to head movements, a common behavioral\nindicator of pain. Consequently, face alignment was omitted from our proposed pipeline.\nAdditionally, the resolution of frames post-face detection was standardized at 448\u02c6448\npixels.\nECG preprocessing & analysis: Similar to Section 4, we utilize the Pan-Tompkins [262] al-\ngorithm to detect the QRS complex, the most prominent wave complex in an ECG signal.\nThis algorithm operates in two phases: preprocessing and decision-making. The preprocess-\ning phase focuses on noise removal, artifact elimination, signal smoothing, and enhancing\nthe QRS slope. The decision-making phase involves initial QRS detection using adaptive\nthresholds, a retrospective search to identify any missed QRS complexes, and a method for\ndistinguishing T waves. Following the accurate detection of R waves, the estimation of inter-\nbeat intervals (IBIs) was conducted, leading to the extraction of key features. Specifically,\nwe calculated the mean of the IBIs as follows:\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (5.5)\nwhere nis the total number of IBIs, and RRidenotes the consecutive R\u00b4Rintervals.\nSubsequently, the heart rate was calculated using the following formula:\nHR\u201c60\u00a8FS\n\u00b5, (5.6)\nwhere FSdenotes the sampling frequency of the ECG recording.\nFramework architecture\nThe proposed framework, as illustrated in Figure 5.5, consists of four key components: the\nSpatial-Module that extracts embeddings from video data, the Heart Rate Encoder which\nmaps heart rate signals into a higher dimensional space, the AugmNet that generates aug-\nmentations in the latent space, and the Temporal-Module performs with the final assessment\nof pain.\nSpatial-Module: The architecture for this module draws inspiration from the Transformer\nin Transformer approach as detailed by [275]. The process begins with the initial video86 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(a) Video analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(b) ECG analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(c) Fusion analysis pipeline.\nFigure 5.5: Outline of the proposed framework.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 87\nframe at a resolution of 448\u02c6448pixels, segmented into 4quadrants, each at 224\u02c6224\npixels. This tiling method, which maximizes the utilization of the frame\u2019s resolution, is\ninfluenced by approaches seen in satellite imaging analysis. Our framework incorporates the\n4tiles and the original frame\u2014resized to 224\u02c6224pixels\u2014into our analysis pipeline. Thus,\neach video frame transforms into 5distinct images, denoted as:\nFk\u201crFk,1, Fk,2, . . . , Fk,ts, (5.7)\nwhere kstands for the frame number, and tencompasses the tile count, including the resized\nfull frame. Each tile is initially split into npatches, expressed as:\nFk,t\u201crFk,t,1, Fk,t,2, . . . , Fk,t,nsPRn\u02c6p\u02c6p\u02c63, (5.8)\nwhere p\u02c6pspecifies the resolution of each patch ( 16\u02c616), and 3denotes the RGB channels.\nThese patches are further segmented into msub-patches, allowing the model to capture the\nimage\u2019s global and localized features. Each tile from a frame thus transitions into a sequence\nof patches and sub-patches, represented as\nFk,t\u201crFk,t,n, 1, Fk,t,n, 2, . . . , Fk,t,n,ms.\nConsequently, each video frame is characterized by:\nFk\u00d1\u201c\nFk,t,n,m|tPr1,5s, nPr1,196s, mPr1,16s\u2030\n, (5.9)\nwhere Fk,t,n,mPRs\u02c6s\u02c63defines the m-th sub-patch within the n-th patch of the t-th tile\nfor the k-th frame, with each sub-patch having a resolution of s\u02c6s(4\u02c64). Each frame\nconsists of 5image representations, encompassing 196patches, and each patch contains 16\nsub-patches. The patches and sub-patches are then linearly projected into embeddings Zand\nY. Positional embedding is applied to maintain spatial information, employing 1D learnable\nposition encodings:\nZ0\u00d0Z0`Epatch, (5.10)\nwhere Epatch indicates the position encoding. Each sub-patch also receives its specific posi-\ntional encoding:\nYi\n0\u00d0Yi\n0`Esub\u00b4patch, (5.11)\nwhere Esub\u00b4patch denotes the positional encodings for sub-patches, and irepresents the index\nof a sub-patch within a patch. The sub-patches are processed in the Inner Encoder , which88 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nconsists of four self-attention heads [285], utilizing dot product attention:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n. (5.12)\nThe output from the Inner Encoder integrates into the patch embedding, advancing to the\nOuter Encoder , which mimics the Inner Encoder with ten self-attention heads. The Spatial-\nModule consists of twelve parallel blocks, generating embeddings of dimensionality d\u201c100.\nFor each input video frame, 5distinct output embeddings of dimensionality 100are produced\nand combined to form a comprehensive frame representation:\nD\u201cdFullFrame`pdTile1`dTile2`dTile3`dTile4q\u00a8c, DPR100, (5.13)\nwhere cadjusts the contribution from the tile embeddings, subsequently, the embedding for\neach frame, D, is concatenated with those of other frames to construct a complete video\nrepresentation:\nVD\u201crD1}D2}. . .}Dfs, VDPRN, (5.14)\nwhere frepresents the total number of frames in the video, and Ndenotes the dimensionality\nof the final video embedding.\nHeart Rate Encoder: As mentioned in Section 5.3.1, heart rate is computed from the origi-\nnal ECG every second, producing a heart rate vector of length h\u201c\u03b8for recordings lasting\n\u03b8seconds. It should be noted that when beats per minute (BPM) fall below 60within any\n1-second segment of the ECG, making direct heart rate calculation impractical, the method\naverages heart rates from the data points immediately before and after to maintain a uniform\nset of \u03b8data points. The Heart Rate Encoder , which is part of a transformer-based archi-\ntecture similar to the Inner andOuter Encoders , utilizes one cross-attention head instead\nof self-attention followed by a fully connected neural network (FCN). This use of cross-\nattention introduces an asymmetry that reduces computational load and increases efficiency.\nUnlike traditional input projections that are M\u02c6Din dimension, as detailed in Section\n5.3.1, the Qmatrix in cross-attention is a learnable matrix sized N\u02c6Dwhere N\u0103M. The\nencoder\u2019s internal embeddings are set to a dimensionality of 512and contain only a single\nblock depth. Fourier feature position encodings [277] are also implemented to handle posi-\ntional information. The main goal of this encoder is to transform the initial heart rate vector\nhinto a more complex and richer feature space,\nhPR\u03b8\u00d1EhPR2048,5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 89\nwhere Ehrepresents the enhanced output embedding of this encoder. In the next step, the\noutput from the heart rate encoder is expanded dimensionally via a bicubic interpolation\nmodule. This process enhances the original heart rate\u2019s feature representation, allowing it to\nintegrate smoothly with the video\u2019s embedding representation through addition. The need\nfor identical dimensions in both embedding vectors is critical and is adeptly addressed by this\ninterpolation module. This non-learning-based approach proves to be efficient and effective\nfor encoding purposes. Additionally, interpolation provides the flexibility to dynamically set\nthe dimensionality of the final output embedding, unlike the fixed dimensions typically seen\nin neural network-based methods. Specifically:\nBh\u201c3\u00ff\ni\u201c03\u00ff\nj\u201c0aijpEhq\u00a8px\u00b4x0pEhqqi\u00a8py\u00b4y0pEhqqj, (5.15)\nwhere aijare the coefficients used for interpolation, and Bhis the resulting vector from the\nbicubic interpolation process. The dimension of BhisN, which is the same as that of VD.\nAugmNet: Inspired by recent developments in the augmentation literature [286], employs\na learning-based technique to identify augmentation patterns within the latent space. Un-\nlike conventional methods that perform image augmentations ( e.g., rotation, cropping) in the\npixel space, AugmNet universally applies transformations to the embeddings. This method\neliminates the necessity for specific transformations tailored individually to each modality,\ne.g., image, signal, and text. Incorporating this module within the automatic pain assessment\nframework helps to regularize the learning process and address overfitting issues. Moreover,\ncorrupting the input embeddings compels the following model, especially the Temporal-\nModel , to derive more precise and representative features, thereby improving performance\nin the pain assessment task. The modality-agnostic method effectively applies to embedding\nrepresentations from any original modality, including video and heart rate signals. AugmNet\nadopts an encoder-decoder architecture, where both the encoder and decoder consist of only\n2fully connected layers with the ELU nonlinear activation function applied after each layer.\nFor a session lasting \u03b8seconds, it produces \u03b8\u02c6frames per second frames and \u03b8\u02c6FS\ndata points for video and ECG, respectively. In the video analysis pipeline, the Spatial-\nModule constructs an embedding representation, VD(5.14), from the original video, dimen-\nsioned at d\u02c6FPS\u201cN. In the ECG analysis pipeline, a feature vector with dimension \u03b8\nis created after extracting the heart rate, one data point per second. The Heart Rate Encoder\nand bicubic interpolation then produce an embedding, Bh(5.15), with dimension N. The fu-\nsion of video and heart rate embeddings at the session level is performed, where VDandBh\nare merged by addition, integrating the data from the initial input modalities. The resulting90 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncombined embedding is then processed by AugmNet :\nAD\u00d0AugmNetpVD`Bhq (5.16)\nP\u00d0AD`pVD`Bhq, (5.17)\nwhere Prepresents the transformed embedding vector, serving as input for the final module,\ntheTemporal-Module .AugmNet is active only during training as a standard augmentation\nmethod and remains inactive during inference.\nTemporal-Module: Like the Heart Rate Encoder , this component operates on a transformer-\nbased architecture. It employs a combination of multi-head cross-attention and multi-head\nself-attention mechanisms. The architecture consists of one multi-head cross-attention block\nwith a single attention head and three subsequent multi-head self-attention blocks, each fea-\nturing eight attention heads. An FCN follows each attention block. The internal embed-\ndings in this module have a dimensionality of 128and encompass a single block in depth.\nThe position encoding method used here mirrors that of the Heart Rate Encoder , utilizing\nFourier feature position encoding. This module processes the input embedding Por the sum\npVD`BhqifAugmNet is inactive, to derive the final classification outcome. The learning\nerror is computed during this phase, and the framework undergoes further training.\nSupplementary augmentation methods\nWe also introduced additional augmentation strategies alongside the AugmNet module, which\napplies learned transformations to embeddings. The initial technique, dubbed Basic , com-\nbines polarity inversion and noise addition to manipulate the original inputs by reversing\ntheir polarity and injecting noise. Another technique, named Masking , involves nullifying el-\nements within the embeddings using randomly sized and placed masks that zero out 10\u00b420%\nof the embedding elements. These methods function within the latent space, similar to Augm-\nNet.\nPre-training\nBefore starting the training for automatic pain assessment, we individually pretrained all\nmodules except AugmNet . The Spatial-Module underwent a dual-phase pretraining process.\nInitially, it was pre-trained on VGGFace2 [280], a dataset designed for facial recognition\nto learn basic facial features. This was followed by an advanced training phase involving\nemotion recognition datasets in a multi-task learning framework. These datasets include the\nwidely used AffectNet [287], Compound Facial Expressions of Emotions Database [288],\nRAF Face Database basic [289], and RAF Face Database compound [289], enabling the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 91\nTable 5.5: Datasets utilized for the pre-training process of the framework.\nDataset # samples # classes Task\nVGGFace2 [280] 3.31M 9,131 Face\nAffectNet [287] 0.40M 8 Emotion\nCompound FEE-DB [288] 6,000 26 Emotion\nRAF-DB basic [289] 15,000 7 Emotion\nRAF-DB compound [289] 4,000 11 Emotion\nECG HBC Dataset [291] 0.45M 5 Arrhythmia\nTask: all tasks involve classification\nmodule to adapt to specific emotional expressions related to pain manifestations. During\nthis phase, the model is trained across these datasets simultaneously, employing a multi-task\nlearning loss approach as suggested by [290], where learned weights scale the individual\nlosses to account for the homoscedastic uncertainty of each task:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s`rew3LS3`w3s`rew4LS4`w4s,\nwhere LSdenotes the loss for each dataset and ware the weights adjusting the learning focus\nto optimize the overall loss Ltotal. The Temporal-Module is trained solely on the VGGFace2\ndataset. For this module, images are converted into 1D vectors prior to processing. The\nHeart Rate Encoder is pre-trained using the ECG Heartbeat Categorization Dataset [291],\nwhich includes heartbeat signals from the MIT-BIH Arrhythmia Dataset [292] and the PTB\nDiagnostic ECG Database [293] [294]. Table 5.5 provides a detailed list of the datasets used\nin our training process.\nDataset Details\nIn this study, to assess the developed framework, we utilized the BioVid Heat Pain Database\n[109], which includes facial videos, electrocardiograms, electromyograms, and skin conduc-\ntance levels from 87healthy individuals ( 44males and 43females, aged 20\u00b465). The\ndataset employs a thermode to induce varying pain levels in the participants\u2019 right arm. Ini-\ntially, each participant\u2019s pain threshold (the transition from heat sensation to pain) and pain\ntolerance (the point at which pain becomes unbearable) were determined. These thresholds\ndelineated the minimum and maximum pain levels, along with two intermediary levels, form-\ning five distinct pain intensities: No Pain (NP), Mild Pain (P 1), Moderate Pain (P 2), Severe\nPain (P 3), and Very Severe Pain (P 4). Pain stimuli temperatures ranged from P 1to P 4, capped\nat50.5\u00b0C. Each subject experienced 20stimulations at each of the four intensities (P 1to P 4).\nEach stimulation lasted 4seconds, interspersed with recovery intervals of 8to12seconds.92 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.6: Training details for the automatic pain assessment.\nOptimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsBatch\nsize\nAdamW 1e-4 cosine 0.1 50 32\nThis protocol, along with 20baseline measurements at NP ( 32\u00b0C), culminated in randomly\nordered 100stimulations per participant. The data was preprocessed to capture 5.5-second\nwindows starting 1-second after reaching the target temperature for each stimulation. This\nprocess produced 8,700samples, each 5.5seconds in duration, distributed evenly across the\nfive classes and modalities for all 87subjects.\n5.3.2 Experiments\nThe study leveraged videos and electrocardiograms from Part A of the BioVid dataset, using\nall available samples from the 87participants. The videos were recorded at a rate of 25\nframes per second (FPS), and the electrocardiogram (ECG) signals were sampled at 512\nHertz (Hz). Each recording session lasted 5.5seconds, generating 138video frames and\nECG vectors containing 2,816elements each, then converted into heart rate vectors of 5\ndata points. The complete set of frames and data points from both videos and cardiac signals\nwas utilized in the experiments. The experimental approach included iterative refinement\nof techniques, with the most effective combinations selected for extended training periods\nranging from 500to800epochs to improve feature extraction and performance outcomes.\nTable 5.6 details the training configurations for the automatic pain assessment tasks.\nPain assessment experiments were structured around binary and multi-level classification\nsetups, testing each modality individually and in combination. The binary classification task\ndifferentiated between No Pain (NP) and Very Severe Pain (P 4), whereas the multi-level\nclassification (MC) involved categorizing all pain intensities available in the dataset. The\nevaluation strategy adopted was the leave-one-subject-out (LOSO) cross-validation, and the\nassessment metrics included accuracy, precision, recall (sensitivity), and F1 score. Notably,\na consistent training regimen was applied across both the binary (NP vs. P 4) and multi-level\n(MC) classification tasks without varying the training schedule or optimization strategies.\n5.3.3 Results\nVideo modality\nExperiments related to the video modality explored the effects of pretraining on the Spatial-\nModule , the video analysis pipeline\u2019s performance, particularly the impact of tiling, and the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 93\nTable 5.7: Results utilizing the video modality.\nEpochsPretraining stage Pipeline Augmentations Task\n1st2ndFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n500 \u2713 - \u2713 - \u2713 - - 72.56 31.22\n500 - \u2713 \u2713 - \u2713 - - 74.25 33.34\n500 - \u2713 - \u2713 \u2713 - - 68.07 31.49\n500 - \u2713 \u2713 \u2713 \u2713 - - 65.11 27.84\n500 - \u2713 \u2713 \u2713c\u2713 - - 74.86 33.86\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 - 73.05 32.14\n500 - \u2713 \u2713 \u2713c\u2713 - \u2713 74.83 33.73\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 73.16 32.87\n800 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 77.10 35.39\nStage: referring to pretraining process for Spatial-Module Mask: Masking c: constant-coefficient applied exclusively to the tiles NP:\nNo Pain P 4: Very Severe Pain MC: multiclass pain level\nimplementation of new augmentation techniques. These experiments are detailed in Table\n5.7. Performance enhancements are evident when comparing the first and second pretraining\nstages of the Spatial-Module . For instance, in the NP vs. P 4task, initial pretraining alone\nachieved 72.56% accuracy, while including the second emotion-focused pretraining stage\nincreased accuracy to 74.25%. This trend is also notable in the multi-level classification,\nwhere the second stage added 1.12% to the performance, totaling 33.34%. Further experi-\nments assessed the effect of using tiles in the video representation. Initially, employing four\ntiles led to a performance decrease of over 6%in the binary classification task and 1.85% in\nthe multi-level task. This reduction likely results from the localized information in each tile,\nwhich may capture irrelevant details like non-expressive facial areas or background elements.\nIncluding the resized full-frame ( 224\u02c6224pixels) alongside tiles further decreased accuracy\nto65.11% and27.84% for binary and multi-level tasks, respectively. However, introducing\na coefficient ( c\u201c0.1) to adjust the tile embeddings restored some performance, achieving\n74.86% and33.86% in respective tasks.\nThe integration of two augmentation techniques, Masking andAugmNet , along with the\nBasic method, was then tested. Masking reduced performance by 1.81% and1.72%, and\nAugmNet showed smaller declines of 0.03% and0.13%. Using both techniques together\nresulted in better outcomes than Masking alone but did not independently surpass the per-\nformance of AugmNet . Despite these initial results, combining all augmentation methods\nproved advantageous for extended training periods. This approach addresses the risk of\noverfitting through a robust regularization strategy. Ultimately, this comprehensive strategy\nled to final accuracy rates of 77.10% and35.39% for binary and multi-level classifications,\ndemonstrating its effectiveness in an unimodal, vision-based pain assessment framework.94 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nHeart rate modality\nThe experiments concerning the heart rate modality explore the use of the encoder and vari-\nous augmentation methods. Table 5.8 details all experiments involving the heart rate modal-\nity. Initially, using the original heart rate vectors with a dimensionality of h\u201c5, we achieved\nclassification scores of 61.70% for NP vs P 4and27.60% for the multi-level task. After apply-\ning the Heart Rate Encoder to map these vectors to a higher-dimensional space of h\u201c2048 ,\nwe noted a slight improvement: a 0.23% increase for the binary task and 0.08% for the\nmulti-level task. Despite the considerable increase in embedding size, this modest enhance-\nment suggests that the intrinsic information within the limited heart rate data points does not\nsignificantly enhance the feature representation. Nonetheless, the encoder\u2019s use is vital for\nproducing larger embeddings, especially for our multimodal approach integrating video and\nheart rate data, which will be discussed in subsequent sections.\nWe also tested augmentation methods on the heart rate data. Applying Masking yielded\na slight improvement of 0.02% for the binary task and 0.05% for the multi-level task. Imple-\nmenting AugmNet further enhanced performance to 62.09% and28.11% for binary and multi-\nlevel tasks, respectively. However, combining all augmentations decreased performance to\n61.87% and27.96%. During an extended 800-epoch training period, we achieved 64.87%\naccuracy for the binary task and 29.81% accuracy for the multi-level task using all augmen-\ntations. Despite these gains, we found that augmentations pose more challenges for accurate\nheart rate classification than video. Therefore, we repeated the extended training without\nBasic andMasking , keeping only AugmNet , which improved binary task performance to\n67.04% and multi-level to 31.22%. This reduction in heart rate embedding corruption sig-\nnificantly enhanced performance. The differing effects of augmentations between heart rate\nand video modalities highlight the challenges of using a single, isolated feature in a machine\nlearning system. We infer that heart rate embeddings with limited informational content\nare more vulnerable to significant performance degradation from augmentations than richer\nvideo embeddings.\nMultimodality\nThe results of integrating video and heart rate modalities are detailed in Table 5.9. Based on\nthe insights gained from separate experiments with each modality, we extended the training\nduration to 800epochs. For this integrated approach, we utilized the tiles with a coefficient\nc\u201c0.1and applied AugmNet as the sole augmentation method. This fusion strategy re-\nsulted in a classification accuracy of 82.74% for NP vs. P 4and39.77% for the multi-level\nclassification task. These results mark a substantial enhancement, with improvements of\n5.64% and15.70% over the individual performances of the video and heart rate modalities,\nrespectively, for the binary classification. Similarly, for the multi-level classification, the in-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 95\nTable 5.8: Results utilizing the heart rate modality.\nEpochsHR\nEncoderAugmentations Task\nBasic Mask AugmNet NP vs P 4 MC\n500 - \u2713 - - 61.70 27.60\n500 \u2713 \u2713 - - 61.93 27.68\n500 \u2713 \u2713 \u2713 - 61.95 27.73\n500 \u2713 \u2713 - \u2713 62.09 28.11\n500 \u2713 \u2713 \u2713 \u2713 61.87 27.96\n800 \u2713 \u2713 \u2713 \u2713 64.84 29.81\n800 \u2713 - - \u2713 67.04 31.22\nTable 5.9: Results utilizing the video & the heart rate modality.\nEpochsHR\nEncoderPipeline Augmentations Task\nFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n800 \u2713 \u2713 \u2713c- - \u2713 82.74 39.77\ntegrated approach shows a 4.38% and8.55% increase compared to the standalone modalities.\nThe combination of these two pivotal modalities significantly boosts the efficacy of the pain\nassessment process, outperforming the results obtained by each modality on its own.\nComparison with existing methods\nIn this section, a comparative analysis is performed to evaluate the performance of our\nmethod against other existing approaches documented in the literature. This evaluation\nis based on Part A of the BioVid dataset, including all 87participants. The same evalua-\ntion protocol\u2014leave-one-subject-out (LOSO) cross-validation\u2014is adhered to for all com-\nparisons to ensure fairness and accuracy. Our method is contrasted with both unimodal and\nmultimodal approaches, divided into (1) video-based, (2) ECG-based, and (3) multimodal\nstudies regardless of the modalities involved. The outcomes are summarized in Table 5.10.\nFor video-based studies, our approach, achieving 77.10% in binary and 35.39% in multi-\nlevel classification tasks, is recognized as one of the highest-performing methods. It exceeds\nthe average results of comparative studies by about 4.7%for binary and 3.4%for multi-level\ntasks. Regarding ECG-based studies, our method shows superior performance, exceeding\nthe average by 8.5%and18.1%for binary and multi-level classifications, respectively. Re-\nmarkably, it records the highest classification accuracy in the multi-level task at 31.22%.96 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nThese findings underscore the effectiveness of using heart rate data extracted from ECG\nas a standalone feature, establishing our method\u2019s capability to assess pain accurately and\nachieve state-of-the-art results. In multimodal studies, our approach records an impressive\n82.74% accuracy in the NP vs. P 4task, making it one of the top-performing methods. It\nis slightly outperformed by studies [269] and [243], which achieved 88.10% and83.99%,\nrespectively. For multi-level tasks, comparisons are scarce; however, study [269] reached\n42.20%, and [235] reported 36.54%, positioning our method favorably within this context.\nInference time\nWe explored several methodologies in this study, including a video-based approach, video\nincorporating tiles, a heart rate-based approach, heart rate analysis with an encoder, and a\ncombined multimodal strategy. Figure 5.6 illustrates each method\u2019s inference time in sec-\nonds and the corresponding average accuracy performances across binary and multi-level\ntasks. Table 5.11 details the number of parameters and the computational cost of floating-\npoint operations (FLOPS) for each component. Inference tests were conducted on an Intel\nCore i7-8750H CPU, including the time for face detection in each frame but excluding the\nextraction of heart rate from electrocardiography, focusing on the potential use of automati-\ncally provided cardiac features from wearables.\nThe inference time for the video modality employing the standard pipeline is approxi-\nmately 26seconds. Utilizing the tile pipeline increases inference time significantly, soaring\nto about 130seconds due to processing five image representations per frame\u2014one full frame\nand four tiles\u2014compared to a single image representation in the non-tiled approach. In the\ncontext of heart rate signals, completing a pain assessment requires only 1.2seconds. With\nthe integration of the Heart Rate Encoder , the processing time remains virtually unchanged,\nshowing a negligible increase of less than half a second, highlighting this specific encoder\u2019s\nefficiency. Lastly, the comprehensive multimodal framework incorporating the tiles and the\nHeart Rate Encoder demands about 131seconds, illustrating the increased complexity and\ncomputational requirements when combining multiple modalities.\nInterpretation\nImproving model interpretability is essential for their acceptance and integration into clin-\nical settings. This study generates attention maps from both the Spatial-Module and the\nTemporal-Module , as illustrated in Figure 5.7. For the Spatial-Module , attention maps are de-\nrived from the last fully connected layer\u2019s weight contributions, which are then interpolated\nonto the images to highlight the model\u2019s focal areas. Figure 5.7a displays an original frame\nsequence along with three attention map variations: (1) post-initial pretraining, (2) after the\nsecond pretraining phase, and (3) post-training on the BioVid . Based on face recognition5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 97\nTable 5.10: Comparison of studies utilizing BioVid & LOSO validation, reported on accuracy %.\nStudy ModalityMethod Task\nFeatures Machine Learning Params (M) FLOPS (G) NP vs P 4 MC\n[295] Video optical flow RF - - 70.20 -\n[217] Video raw SLSTM - - 61.70 29.70\n[269] Video raw 3D CNN, - - 77.50 34.30\n[219] Video raw 2D CNN, biLSTM - - 69.25 -\n[180] Video raw 2D CNN 25.00\u00154.00 71.00 -\n[296] Video facial action\ndescriptorsDeep RF - - 72.40 30.80\n[296] Video facial 3D distances Deep RF - - 72.10 30.30\n[284] Video fiducial points GNN - - 73.20 -\n[135]:Video raw 2D CNN - - 71.30 37.60\n[211]:Video raw 2D CNN, GRU 150.00\u0015- 73.90 39.10\n[37] Video raw Transformer 24.00 4.20 73.28 31.52\n[267] Video facial landmarks,\n3D distancesRF 71.60 -\nOur Video raw Transformer 4.20\u00101.62 77.10 35.39\n[235] ECG raw 1D CNN 1.80\u0015- 57.04 23.23\n[270] ECG domain-specific\u00b8LR - - 57.69 -\n[35] ECG domain-specific\u00b8SVM - - 58.39 23.79\n[269] ECG heart rate\u20393D CNN - - 65.00 28.50\n[267] ECG domain-specific RF - - 62.00 -\n[36] ECG domain-specific FCN 4.09\u00120.40 69.40 30.24\n[297] ECG domain-specific\u00b8SVM - - 63.50 -\nOur ECG heart rate Transformer 6.03\u00101.25 67.04 31.22\n[235] ECG, EMG, GSR raw 2D CNN 10.00\u0015- 76.72 36.54\n[270] ECG, GSR domain-specific\u00b8SVM - - 72.20 -\n[269] Video1, ECG2raw1, heart rate2\u20393D CNN - - 88.10 42.20\n[267] ECG1, EMG1,\nGSR1domain-specific1\u00b8RF - - 74.10 -\n[267] Video1,ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 77.80 -\n[297] Video1, ECG2,\nGSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 78.90 -\n[297] Video1, ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8SVM - - 76.60 -\n[243] ECG, EMG, GSR raw DDCAE 4.00\u0015- 83.99 -\nOur Video1,ECG2raw1, heart rate2Transformer 8.60\u00102.44 82.74 39.77\nM: Millions G: Giga :: reimplemented for pain intensity estimation on BioVid by [269]\u2039: pseudo heart rate gain \u00b8: numerous\nfeatures \u0015: parameter count estimated from provided paper details \u0010:AugmNet excluded from parameter count, not used in inference\n\u0012: parameter count not mentioned in study, provided directly by authors -: missing value RF: Random Forest AE-ATT: Autoencoder\nAttention SVM: Support Vector Machines LR: Logistic Regression98 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nVideo Video\n(Tiles)Heart rate Heart rate\n(Encoder)Multimodal0102030405060Accuracy (%)Average Accuracy\n020406080100120\nInference Time (s)\nInference Time\nFigure 5.6: Comparison of mean accuracy and inference period for unimodal and multimodal\nstrategies across NP versus P 4and MC tasks. The diagram adopts a dual-y-axis\nconfiguration\u2014accuracy measurements on the left and time metrics on the right\u2014\nto outline the balance between performance efficacy and computational load, cate-\ngorizing the methodologies along the x-axis.\nTable 5.11: Module parameters and computational cost in FLOPS for the proposed framework.\nModule Params (M) FLOPS (G)\nSpatial-Module 2.57 1.19\nHeart Rate Encoder 4.40 0.82\nAugmNet 1.02 0.02\nTemporal-Module 1.63 0.43\nTotal 9.62 2.46\ntasks, the Spatial-Module produces maps focusing broadly on the facial area, particularly\nthe zygomatic, buccal, oral, mental, and nasal regions. The second stage, oriented towards\nmulti-task emotion recognition, refines the focus, sharpening attention on specific facial ar-\neas, highlighted in the first stage but with greater clarity and emphasis. After training on the\nBioVid for pain assessment, the attention maps show further refined focus on specific facial\nareas, with reduced attention to less relevant regions, ensuring concentrated focus on key\nareas. These maps consistently demonstrate the model\u2019s capability to adjust its focus based\non pain-related facial expressions.\nAttention maps from the Temporal-Module , based on input embeddings, illustrate the\nweight contributions in the module\u2019s final layer, forming easy-to-visualize rectangular pat-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 99\nterns. Figure 5.7b shows examples for three scenarios: (1) video embedding, (2) heart rate\nembedding, and (3) a combined embedding of video and heart rate. The attention maps ex-\nhibit a grid-like pattern, possibly due to the Fourier position encoding used, akin to those\nseen in perceiver-like architectures. The video embedding map shows intense attention\nacross the input. In contrast, the heart rate embedding map focuses attention more sparsely,\nwith a notable concentration in specific areas indicated by intense red coloration. The com-\nbined embedding map displays moderate intensity, consistent with the blended nature of the\ninput. These maps tend to emphasize the latter part of the session, aligning with the timing\nof pain manifestation towards the session\u2019s end, indicating the model\u2019s responsiveness to\nreal-time pain expressions.\n5.3.4 Discussion\nThis research introduced a multimodal framework that integrates video and heart rate sig-\nnals to assess pain automatically. Our innovative approach includes four main modules,\neach characterized by effectiveness and efficiency. The Spatial Module , particularly no-\ntable for its compact size of only 2.57million parameters, ranks as one of the most efficient\nvision-based models in automatic pain assessment literature. Despite limited comparative\nstudies, our model has shown it can match or exceed the performance of larger models. Its\nhigh efficiency and robust performance are primarily due to a thorough pretraining regime\non datasets related to affective responses, crucial for enhancing model capabilities in pain\nestimation tasks. The Heart Rate Encoder , with 4.40million parameters, excels at transform-\ning heart rate data into complex, high-dimensional embeddings, which integrate seamlessly\nwith video data during inference, all within under half a second. This quick processing\nunderscores the encoder\u2019s efficiency, supported by bicubic interpolation that modifies input\ndynamically to achieve variable output dimensions without predefined constraints. AugmNet ,\na novel augmentation module, learns to modify latent space representations directly, prevent-\ning the need for specific augmentation techniques designed for each data type. However, this\nmodule requires careful application to avoid overfitting and other training challenges. The\nTemporal-Module , consisting of 1.63million parameters, is crucial for final pain level as-\nsessments. It leverages a mix of cross- and self-attention mechanisms to enhance efficiency\nand accuracy, demonstrating the potential of transformers in streamlined settings contrary to\ntheir typical use in large-scale applications.\nOur experiments demonstrate that videos are invaluable for discerning individual pain ex-\nperiences by capturing diverse behavioral indicators like facial expressions, eye movements,\nand even slight color changes under stress. Utilizing video data, our methodology reached\nan accuracy of 77.10% in binary classification, effectively distinguishing between no pain\nand very severe pain scenarios. Moreover, it achieved 35.39% accuracy in a multi-level clas-100 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n(a) Attention maps from the Spatial-Module .\n(b) Attention maps from the Temporal-Module .\nFigure 5.7: Regions highlighted in yellow and red denote areas of significant attention. (a)\n(1strow) Sequence of original frames. (2ndrow) Derived from the Spatial-Module\nafter initial stage pretraining. (3rdrow) Derived from the Spatial-Module post sec-\nond stage pretraining. (4throw) Derived from the Spatial-Module trained on the\nBioVid dataset. (b) (1strow) Derived from the Temporal-Module incorporating\nvideo embeddings. (2ndrow) Derived from the Temporal-Module with heart rate\nembeddings. (3rdrow) Derived from the Temporal-Module using a combined em-\nbedding of video and heart rate.5.4. SUMMARY 101\nsification spanning five distinct pain intensities. The heart rate signal, tested as a standalone\nfeature from electrocardiography, showed that remarkable outcomes are possible with this\nsingle data feature, which is pivotal for validating the feasibility of heart rate as a viable\npain indicator. This is crucial as heart rate data is readily accessible from most wearable\ntechnologies, reducing the need for specialized algorithms to handle cardiac signals or raw\nbiosignals, thereby conserving both time and computational resources. Solely using heart\nrate, our model excelled, registering accuracies of 67.04% and31.22% for binary and multi-\nlevel classifications, respectively, among the highest reported. Incorporating video and heart\nrate data, our multimodal method yielded superior results\u2014 82.74% and39.77% for binary\nand multi-level classifications, respectively. These figures significantly enhance video-only\nresults by roughly 9%and heart rate-only outcomes by about 24%. Furthermore, with a total\nparameter count of just 9.62million, our approach stands out for its efficiency and effec-\ntiveness. Overall, this study showcases the synergy achievable by merging video and heart\nrate data, leading to a system outperforming its unimodal counterparts and emphasizing the\npotential of integrated multimodal pain assessment tools.\nUsing attention maps generated by the Spatial-Module , our framework analysis iden-\ntified crucial facial areas like the zygomatic and oral regions as significant for automatic\npain assessment. These maps demonstrated that different pretraining stages refine the focus,\nshowing more targeted attention with specialized training. Similarly, attention maps from\ntheTemporal-Module focused on the latter part of the input image, corresponding to where\npain manifestations are typically observed in the particular dataset.\n5.4 Summary\nThis chapter explores the interplay between model efficiency and performance in automatic\npain assessment tasks. We also aimed to mirror real-world conditions by leveraging read-\nily accessible and applicable modalities without relying on costly precision medical devices.\nConsequently, we utilized RGB videos with a resolution of approximately 1080x1080 and\nheart rate data. The videos utilized are of medium quality, comparable to those captured with\nmobile phone cameras, and the heart rate data simulates readings from wearable devices. It\nis important to note that wearables across various price ranges automatically provide heart\nrate information. Consequently, exploring the potential of using this readily available modal-\nity for pain assessment is crucial. This proof of concept is significant as it could enable\ncost-effective and accessible pain assessment solutions without dependence on specialized\nmedical equipment. Additionally, our study focuses on developing compact and efficient\nmodels that maintain robust performance.\nThe experiments detailed in this chapter reveal that reducing the number of frames used\nin a video-based pipeline by a factor of four minimizes the accuracy loss, under 1.5%, while102 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncutting inference time threefold, facilitating near real-time pain assessment. Furthermore,\nour initially proposed framework, which incorporates 24million parameters and operates at\n4.3GFLOPS, demonstrated exceptionally high performance. In the subsequent experiments,\nwe showcased that heart rate data alone can be effectively used for pain assessment, achiev-\ning impressive results. This finding underscores the practical utility of data available from\nwearables. Additionally, combining video data with heart rate information yielded the high-\nest accuracy in our tests, illustrating that an integrated approach using both behavioral and\nphysiological modalities can significantly enhance performance. Additionally, we demon-\nstrated that creating one of the smallest models documented, with only 9.62million param-\neters and 2.46GFLOPS, allowed us to achieve top-tier results. This highlights that large\nmodels, commonly favored in the current era of AI, are not always necessary for effective\nperformance. However, it is important to note that extensive multi-stage pretraining across\nvarious datasets greatly aided this framework\u2019s success, which was critical in achieving such\nhigh efficiency and effectiveness.\nIn this chapter, we aimed to explore the effectiveness of compact models in achieving\nhigh performance with rapid inference times. We exclusively used heart rate data, mimicking\nthe information typically available from wearable devices, though our experiments did not\nextend to real-world conditions. Instead, they relied on the sole publicly accessible dataset\nexplicitly designed for pain assessment, including facial videos and cardiac signals collected\nin highly controlled laboratory settings. Participants were positioned facing forward un-\nder optimal lighting conditions, with physiological sensors precisely affixed. Recognizing\nthe potential challenges of applying these findings to clinical settings is essential. Issues\nlike inconsistent lighting, unforeseen facial movements, occlusions, or difficulties with sen-\nsor placement must be meticulously addressed to tailor these systems for real-world use.\nMoreover, depending solely on heart rate as a cardiac feature could be restrictive in more\ndemanding scenarios, highlighting the necessity to integrate multiple extracted features or\nuutilizeraw biosignals for comprehensive assessments.Chapter 6\nSynthetic Data: The Role of Thermal Imag-\ning\nContents\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n6.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the study published in [39]. In recent years, syn-\nthetic data generation has gained traction as a viable approach to addressing data scarcity\nand privacy issues while also meeting the requirements for training AI algorithms on unbi-\nased data with adequate sample size and statistical robustness. Additionally, synthetic data\ncan increase the availability and diversity of real data, particularly in rare modalities, which\ncan be essential for training AI-driven diagnostic and predictive models. This enhance-\nment supports healthcare research and improves patient outcomes. [298]. In the literature\non automatic pain assessment, no studies have been reported concerning creating synthetic\nmodalities. This chapter introduces the generation process of synthetic thermal videos using\nGenerative Adversarial Networks (GANs).\nRegarding thermal modality, in recent years, the field of affective computing research\nhas increasingly adopted thermal imaging techniques [299]. This shift was motivated by\n103104 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nstudies showing that stress and cognitive load significantly affect skin temperature [300], at-\ntributable to the autonomic nervous system\u2019s (ANS) regulation of physiological signals such\nas heart rate, respiration rate, blood perfusion, and body temperature. These signals are vital\nindicators of human emotions and affect [299]. Moreover, muscle contractions can alter fa-\ncial temperature by transferring heat to the facial skin [301]. Consequently, thermal imaging\nhas emerged as a viable method for recording transient facial temperatures [302]. Research\nby the authors in [303] on thermal imaging and facial action units to evaluate emotions, such\nas frustration, boredom, and enjoyment, indicated that a multimodal approach yielded the\nmost accurate results. Within pain research, thermal imaging has been explored in limited\nstudies. For instance, [304] reported increased facial temperature in response to painful stim-\nuli, suggesting thermal cameras as effective tools for monitoring pain. Another study [305]\nintroduced a pain dataset consisting of RGB, thermal, and depth videos, finding that while\nthe RGB modality slightly outperformed the others, integrating all modalities provided the\nbest results. This prompted us to explore the specific modality of thermal imagery through\nthe prism of synthesis using generative deep learning.\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks\nWe present a process of creating synthetic thermal videos using GANs in unimodal and\nmultimodal settings alongside RGB video modalities. Integrating a Vision Multilayer Per-\nceptron (MLP) model with a transformer-based module is at the core of our automatic pain\nassessment framework. Key contributions of this research include (1) generating synthetic\nthermal videos to enhance pain assessment as an additional vision modality, (2) assessing\nthe effectiveness of RGB and synthetic thermal videos as standalone modalities, (3) exam-\nining the utility of thermal-related information for pain assessment, and (4) evaluating the\nperformance and implications of the newly developed Vision-MLP architectures.\n6.2.1 Methodology\nThis section outlines the generation of synthetic thermal videos, which will be utilized sub-\nsequently and incorporated into an automatic assessment pipeline.\nSynthetic Thermal Videos\nAn image-to-image translation (I2I) approach has been utilized to create synthetic thermal\nvideos. I2I generative models are designed to bridge different image domains by learning the\ndata distributions inherent to each domain. Here, the source domain comprises RGB images;\nthe target domain is thermal images. In this research, conditional generative adversarial\nnetworks (cGANs) [306] were employed and trained in a supervised manner using paired6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 105\nimages. Fig. 6.1 provides a high-level overview of the method. The generator Gproduces\nimages that appear realistic, whereas the discriminator Dworks to differentiate between\ngenuine and synthetic images through the following minimax game:\nmin\nGmax\nDLcGANpG, Dq, (6.1)\nwhere the objective function LcGANpG, Dqis defined as:\nEx,yrlogDpx, yqs`Ex,zrlogp1\u00b4Dpx, Gpx, zqqqs, (6.2)\nwithxdenoting the actual data, yindicating the target data, and zrepresenting the random\nnoise vector. Here, Gseeks to minimize the objective function, whereas Doperates in\nopposition, striving to maximize it. Additionally, we incorporated the Wasserstein gradient\npenalty (WGAN-GP) [307] to enhance the stability of training. The overall objective is\narticulated as:\nLcGANpG, Dq`\u03bbE\u02c6x,yrp}\u2207\u02c6xDp\u02c6x, yq}2\u00b41q2s, (6.3)\nwhere \u03bbis the penalty coefficient. In the architectural design of our proposed method, which\ndraws inspiration from [308], the generator Gis divided into three distinct modules: an en-\ncoder, which includes two convolutional layers that downsample the input; a middle ResNet\nmodule, featuring nine residual blocks, each equipped with two convolutional layers; and a\ndecoder that upsamples the feature maps back to the final resolution ( i.e.,256\u02c6256) for the\nsynthetic sample. The discriminator D, based on the approach outlined in [309], employs a\npixel-level PatchGAN strategy using 1\u02c61kernels and consists of two convolutional layers.\n6.3 Combination of RGB and Synthetic Thermal Videos\n6.3.1 Methodology\nThis section presents the structure of the proposed automatic pain assessment framework,\nthe augmentation techniques developed, the pre-processing methods employed, and the pre-\ntraining strategy for the modules.\nFramework Architecture\nThe proposed framework consists of two primary modules: a Vision-MLP model that acts\nas a spatial embedding extractor for individual video frames and a transformer-based model\nthat functions as a temporal module, using the embedded representations of the videos for\ntemporal analysis and final pain assessment. Fig. 6.2 displays the modules and their main\ncomponents.106 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nDecoder\n\u2026Encoder\n\u2026Residual blocks\nDiscriminator\nGenerator\nAuthentic/Synthetic?\nFigure 6.1: Illustration of the procedure for creating thermal images, featuring the architecture\nof the Generator G(Encoder, mid-stage ResNet, Decoder), and the Discriminator\nD.\nVision-MLP: MLP-like models have recently emerged as a novel class of vision models,\nproviding an alternative to traditional Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViT). These models are characterized by their straightforward architectures,\nwhich consist of fully connected layers combined with activation functions. They possess\na lower level of inductive bias and rely on basic matrix multiplication operations. Our\nmethodology is grounded in the principles outlined in [310] that introduced the Vision-\nMLP, and [311] that incorporates a wave representation for the patches (also known as\ntokens). Each video frame is initially partitioned into nnon-overlapping tokens Fm\u201c\nrfm,1, fm,2, . . . , f m,nsPRn\u02c6p\u02c6p\u02c63, where pspecifies the resolution of each token, i.e.,16\u02c616\npixels, and 3represents the number of color channels. Each token is subsequently linearly\nprojected into a dimension d\u201c768prior to entering the Vision-MLP (refer to Fig. 2a). The\nfirst principal sub-module, the Channel-Mixer (Fig. 2c), operates independently on each\ntoken fj, enabling interactions among different channels, and is formulated as:\nChannel-Mixer pfj, Wcq\u201cWcfj (6.4)\nwhere Wcdenotes the weight matrix with learnable parameters, and j\u201c1,2, . . . , n . Fol-\nlowing this, the next significant sub-module, the Token-Mixer (Fig. 2b), facilitates commu-\nnication among various tokens, aiding in the extraction of features from different spatial6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 107\nVision-MLPLinear ProjectionToken MixerChannel Mixer\nNormNorm2x\n3xStage-1Token MixerChannel Mixer\nNormNorm4xStage-2Token MixerChannel Mixer\nNormNorm18xStage-3Token MixerChannel Mixer\nNormNorm 3xStage-4\nInput Imagea\nChannel-MixerMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLPChannelsTokensSkip-connectionscMLP\nFully-connectedGELUFully-connectedd\nChannel-MixerWave-BlockWave-Block\nToken-MixerTokensChannels\n......FCNb\nTransformer\n\u2026\nVision-MLP\nWeighted Fusion\nPain Assessment\nCross-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN1x\n8x\n8x4x\n8xe\nFigure 6.2: Representation of the proposed framework, illustrating its components and their\nmain functions: (a)The Vision-MLP module, tasked with extracting feature em-\nbeddings from video frames. (b)The Token-Mixer , an important sub-module of\nVision-MLP , generates the wave representation for the tokens. (c)The Channel-\nMixer , a crucial sub-module within Vision-MLP .(d)The MLP, a core component\nof the Channel-Mixer .(e)The fusion procedure that combines RGB and synthetic\nthermal embeddings, succeeded by the Transformer module, which conducts the\nfinal pain assessment.\nlocations. Typically, in MLP-based models, the token mixers are defined as:\nToken-MixerpF, Wtqj\u201c\u00ff\nkWt\njkdfk, (6.5)\nwhere Wtis the corresponding weight matrix for the tokens, and drepresents element-\nwise multiplication. Our proposed approach modifies tokens into wave-like representations\nto dynamically adjust the interactions between tokens and weights based on their semantic\ncontent. To depict a token fjas a wave \u02dcfjvia a wave function, both amplitude and phase\ninformation is necessary:\n\u02dcfj\u201c|fj|dei\u03b8j. (6.6)\nHere, iis the imaginary unit satisfying i2\u201c \u00b41. The term|fj|indicates the amplitude of\nthe signal, while ei\u03b8jis a periodic function, with \u03b8jrepresenting the phase of the signal. The108 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nabsolute value operation is typically omitted and substituted with 6.4 for simplicity. Each to-\nken\u2019s phase \u03b8jreflects its position within the wave cycle and can, therefore, be characterized\nusing fixed parameters, which are adjustable during the training phase. As such, 6.4 is also\napplied for phase estimation. Given that 6.6 characterizes a wave within the complex domain,\nthe Euler formula is employed to embed tokens within the neural network architecture:\n\u02dcfj\u201c|fj|dcos\u03b8j`i|fj|dsin\u03b8j. (6.7)\nCombining 6.5 and 6.7, a token is represented as:\nfj\u201c\u00ff\nWt\njkfkdcos\u03b8k`Wi\njkfkdsin\u03b8k (6.8)\n\u00f9\u00f1\u00ff\nWt\njkfkdcospWcfkq`Wi\njkfkdsinpWcfkq (6.9)\nwhere Wt,Wc, and Wiare the learnable weight matrices. The described process focuses\non wave-like representations and is conducted within the Token-Mixer , particularly in the\nWave-Block . The Token-Mixer architecture consists of three blocks: two Wave-Blocks and\noneChannel-Mixer operating in parallel. The Vision-MLP module is organized into four\nstages, each featuring a sequence of a Token-Mixer and a Channel-Mixer block, preceded\nby a normalization layer. The depth of parallel blocks in each stage is 3,4,18, and 3, re-\nspectively, allowing for hierarchical embeddings extraction with corresponding dimensions\nacross stages 64,128,320, and 100.\nFusion: For each input frame, the Vision-MLP extracts an embedding of dimensionality d\u201c\n100. The embeddings from the individual frames of a specific video are then concatenated\nto form a comprehensive embedding representation of the video:\nVD\u201crd1}d2}\u00a8\u00a8\u00a8} dms,VDPRN, (6.10)\nwhere mrepresents the number of frames in the video, and Nis the dimensionality of the\nultimate embedding. Following this, the embeddings from the RGB and synthetic thermal\nvideos are combined through a weighted fusion process:\nVFused\u201cw1\u00a8VRGB`w2\u00a8VThermal ,VFusedPRN. (6.11)\nThis fusion strategy integrates the respective embeddings using learned weights w1andw2,\nwhich adjust the influence of the RGB and thermal embeddings, respectively. This weighted\nsummation achieves an effective integration, capturing the relevance of each modality in the\nresultant fused representation VFused .6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 109\nTransformer: The fused embeddings are then input into a transformer-based module that\nincludes self-attention and cross-attention blocks (Fig. 2e). The self-attention mechanism is\nexpressed as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dk\u02d9\nV. (6.12)\nIn this formulation, QPRM\u02c6C,KPRM\u02c6C, and VPRM\u02c6Care the Query, Key, and Value\nmatrices respectively, where Mindicates the input dimension, and Crepresents the channel\ndimension. The cross-attention mechanism also utilizes a dot product operation; however,\nQfor cross-attention is dimensioned N\u02c6Crather than M\u02c6C, with N\u0103Mproviding\na reduction in computational cost. Each self-attention and cross-attention block features 1\nand8attention heads, respectively, and the entire Transformer module consists of 4parallel\nblocks. The output embeddings, with a dimensionality of 340, are used to perform the final\npain assessment via a fully connected neural network.\nAugmentation Methods: Two augmentation techniques have been implemented within the\nframework. Firstly, the method known as Basic is utilized, which combines polarity in-\nversion with the addition of noise. This approach modifies the original input embedding by\ninverting the polarity of data elements and adding random noise from a Gaussian distribution,\nthus introducing variability and perturbations. Secondly, the Masking technique involves ap-\nplying zero-valued masks to the embeddings, effectively eliminating sections of the vectors.\nThe dimensions of these masks are randomly determined, ranging from 10% to 50% of the\nembedding\u2019s total dimensions, and are randomly positioned within the embeddings.\nPre-processing: The pre-processing involves face detection to isolate the facial region. The\nMTCNN face detector [278] is used, which employs multitask cascaded convolutional neural\nnetworks to identify faces and landmarks. It is essential to highlight that the face detector\nwas applied exclusively to the individual RGB frames, and the coordinates of the detected\nface were then applied to the corresponding synthetic thermal frames. The resolution of all\nframes was standardized at 224\u02c6224pixels.\nPre-training: For the I2I approach, the SpeakingFaces dataset [312] was employed to train\nthe proposed GAN model for translating RGB to synthetic thermal videos. Additionally, be-\nfore commencing the automatic pain assessment training, the Vision-MLP andTransformer\nmodules underwent pre-training. The Vision-MLP was subject to a three-stage pre-training\nstrategy: initially, it was trained on DigiFace-1M [313] to acquire basic facial features. It\nwas then trained on AffectNet [287] and RAF Face Database basic [289] to learn features\nrelated to basic emotions through multi-task learning. Lastly, the Compound Facial Expres-\nsions of Emotions Database [288] and the RAF Face Database compound [289] were used110 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.1: Datasets utilized for the pretraining process of the framework.\nDataset # samples # classes Task\nSpeakingFaces [312] 4.58M 142 Face\u0015\nDigiFace-1M [313] 1.00M 10,000 Face\u0010\nAffectNet [287] 0.40M 8 Emotion\u0010\nCompound FEE-DB [288] 6,000 26 Emotion\u0010\nRAF-DB basic [289] 15,000 7 Emotion\u0010\nRAF-DB compound [289] 4,000 11 Emotion\u0010\n\u0015: includes face image pairs for the I2I task \u0010: includes images for face or emotion\nrecognition tasks\nto learn features of compound emotions in a similar multi-task framework. The multi-task\nlearning process is represented as:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s, (6.13)\nwhere LSis the loss for the specific task associated with different datasets, and ware the\nlearned weights that guide the learning process in minimizing the collective loss Ltotal, in-\ntegrating all the individual losses. The Transformer was pre-trained solely on the DigiFace-\n1M[313], adapting the input images into 1D vectors to suit its architectural needs. Table 6.1\noutlines the datasets utilized in the pre-training phase.\n6.3.2 Experiments\nThe proposed framework was assessed using the BioVid Heat Pain Database [109], which\ncomprises facial videos, electrocardiograms, electromyograms, and skin conductance levels\nfrom 87healthy subjects. Participants underwent heat-induced pain via a thermode on their\nright arm at five different intensities: no pain (NP), mild pain (P 1), moderate pain (P 2), se-\nvere pain (P 3), and very severe pain (P 4). Each level was applied 20times to each subject,\nresulting in 100samples per modality and a total of 1740 samples per class. Experiments\nwere structured around binary and multi-level classification schemes to assess pain, analyz-\ning each modality individually and collectively. Binary classification aimed to distinguish\nbetween no pain (NP) and very severe pain (P 4), while multi-level classification (MC) was\ntasked with categorizing all levels of pain intensity present in the dataset. The leave-one-\nsubject-out (LOSO) method was utilized for validation, and accuracy served as the metric\nfor performance evaluation. Table 6.2 outlines the training details for the automatic pain\nassessment, including parameter number and the computational cost measured in floating-\npoint operations (FLOPS) for each module.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 111\nTable 6.2: Training specifications, and number of parameters and FLOPS for each module.\nTraining Details Vision-MLP Transformer\nOptimizer: AdamW Params: 7.35 M Params: 7.96 M\nLearning rate: 2e-5 FLOPS: 30.95 G FLOPS: 30.90 G\nLR decay: cosine\nWeight decay: 0.1\nWarmup epochs: 5\nBatch size: 32\nTotal Params: 15.31 Millions FLOPS: 61.85 Giga\nTable 6.3: Results utilizing the RGB video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.37 30.23\n200 \u2713 20-50 0.7 70.26 28.50\n300 \u2713 30-50 0.9 70.05 30.02\nMasking: indicates the percentage of the input embedding to which zero-value masking is applied\nP(Aug): represents the probability of applying the augmentation methods of Basic & Masking NP: No\nPain P 4: Very Severe Pain MC: multiclass pain level\n6.3.3 Results\nRGB Videos\nWithin the context of the RGB video modality, we recorded an accuracy of 69.37% for the\nbinary classification task (NP vs. P 4) and 30.23% for the multi-class classification (MC).\nThe use of the Masking augmentation method, which obscured 20\u00b450% of the input em-\nbeddings, yielded a slight increase in binary classification accuracy by 0.89%. However, it\nled to a reduction in multi-class classification accuracy. By extending the training period to\n300epochs, modifying the Masking method to cover 30\u00b450% of the embeddings, and ap-\nplying a 90% probability to both augmentation methods, the accuracies improved to 70.05%\nand30.02% for the binary and multi-class tasks, respectively. This represents an average\naccuracy gain of just under 0.5%. The classification outcomes are detailed in Table 6.3.\nSynthetic Thermal Videos\nIn the synthetic thermal modality experiments conducted under identical conditions, the ini-\ntial accuracies were 69.97% for the binary classification and 30.04% for the multi-class clas-\nsification. Enhancing the intensity of the masking method yielded modest gains in accuracy112 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.4: Results utilizing the synthetic thermal video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.97 30.04\n200 \u2713 20-50 0.7 70.20 30.50\n300 \u2713 30-50 0.9 70.69 29.60\nof0.23% for the binary classification and 0.46% for the multi-class classification. Subse-\nquent final accuracies were 70.69% for the binary classification and 29.60% for the multi-\nclass classification, reflecting an average increment of 0.28%. This discrepancy likely arises\nfrom the challenge of detecting nuanced facial changes linked to low-level pain, exacerbated\nby more aggressive augmentation that potentially reduces performance. The summarized\nresults are presented in Table 6.4.\nAdditional Analysis on RGB & Synthetic Thermal Videos\nThe results from the previous section showed a surprising similarity in performance between\nthe RGB and synthetic thermal modalities. Specifically, the RGB modality achieved maxi-\nmum accuracies of 70.26% for the NP vs. P4 task and 30.23% for the MC task. Similarly,\nthe synthetic thermal modality reached top accuracies of 70.69% and30.50% for the same\ntasks, respectively. On average, the thermal video performances were about 1%higher than\nthose for the RGB modality. This was unexpected, considering the synthetic modality was\ninitially considered inferior to the original. This prompted further investigation into why\nsynthetic modalities might perform comparably to or better than the original RGB modality.\nA key question involved the relevance and efficacy of the thermal-related data featured in the\nsynthetic videos. The theory proposed that minimizing facial expressions in thermal videos\ncould enhance the clarity of thermal data assessment.\nGaussian blurring was incrementally applied to RGB and synthetic thermal videos, as\nshown in Fig. 6.3, with kernel sizes increasing from 0to191. According to Table 6.5, at\na kernel size of k\u201c0, a performance differential of 0.47%, favoring the thermal modality,\nconfirms prior findings. This gap slightly expands to 0.49% atk\u201c41. Remarkably, at\nk\u201c91, the disparity enlarges to 2.13% and increases to 5.90% atk\u201c191, where blurring\nis most intense. The results reveal that as facial expressions become less visible through\nblurring, synthetic thermal videos consistently outperform RGB videos, with respective ac-\ncuracies of 66.24% versus 60.34%. As blurring intensifies from k\u201c0tok\u201c191, accuracy\nrates for synthetic thermal and RGB modalities decrease by 1.81% and7.13%, respectively.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 113\nFigure 6.3: Gradual blurring of RGB and synthetic thermal facial images: a series displaying\nvarying levels of Gaussian blur applied, with kernel sizes gradually increased from\nk\u201c0(no blur) to k\u201c191(extensively blurred).\nTable 6.5: Results utilizing the RGB & the synthetic thermal video.\nEpochs Modality BlurAugmentations Task\nBasic Masking P(Aug) NP vs P 4\n100 RGB 0 \u2713 10-20 0.7 67.47\n100 Thermal 0 \u2713 10-20 0.7 68.05\n100 RGB 41 \u2713 10-20 0.7 66.61\n100 Thermal 41 \u2713 10-20 0.7 67.10\n100 RGB 91 \u2713 10-20 0.7 64.80\n100 Thermal 91 \u2713 10-20 0.7 66.93\n100 RGB 191 \u2713 10-20 0.7 60.34\n100 Thermal 191 \u2713 10-20 0.7 66.24\nBlur: Gaussian blurring with kernel sizes k\nThis suggests that critical information, such as visually represented facial temperature in\nthe synthetic modality, is unaffected or slightly impacted. Fig. 6.4 displays the embedding\ndistribution for the RGB and synthetic thermal modalities at k\u201c0andk\u201c191, highlight-\ning a distinct variation in distribution patterns, albeit with ambiguous data point separation.\nFork\u201c191, while RGB embeddings tend to cluster and potentially overlap, many points\nconspicuously stray from the central mass without any distinct arrangement. Conversely, the\ndata points for the synthetic modality spread more uniformly, possibly indicating better class\ndifferentiation.114 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nRGB-0\nThermal-0 Thermal-191RGB-191\nNP\nP4\nFigure 6.4: Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain) classes\nin RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191(heavily\nblurred).\nFusion\nThree fusion strategies were assessed for multimodal analysis involving RGB and synthetic\nthermal video data. Initially, the fusion strategy referenced in 6.11 utilized learned weights\nw1andw2to scale the contributions of each modality. A second strategy incorporated an\nadditional weight, w3, modifying the formula to w3\u00a8pw1\u00a8VRGB`w2\u00a8VThermalq. The\nthird method bypassed learned weights altogether, directly combining the embedding vectors\nfrom the modalities. The results, detailed in Table 6.6, indicate that omitting learned weights\nachieved accuracies of 64.92% and26.40% for the binary and multi-class tasks, respectively.\nThe introduction of w3reduced 0.5%in accuracy for both tasks. The strategy using only\nweights w1andw2yielded the best outcomes, with accuracies of 65.08% and26.50% for\nthe binary and multi-class tasks, respectively. By maintaining the use of weights w1and\nw2and increasing the training duration from 100to300epochs, while consistent with the\naugmentation settings, accuracies improved to 69.50% and29.80% for the binary and multi-\nclass tasks, respectively. Further prolonging the training to 500epochs, with no evidence\nof overfitting, led to further improved performances, with final accuracies of 71.03% and\n30.70% for the respective tasks.6.4. SUMMARY 115\nTable 6.6: Results utilizing the fusion of RGB & synthetic thermal video.\nEpochsFusion\nweightsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n100 \u2013 \u2713 10-20 0.7 64.92 26.40\n100 W2 \u2713 10-20 0.7 65.08 26.50\n100 W3 \u2713 10-20 0.7 64.42 25.90\n300 W2 \u2713 10-20 0.7 69.50 29.80\n500 W2 \u2713 10-20 0.7 71.03 30.70\nW2: utilization of [w 1,w2] W3: utilization of [w 1,w2,w3]\nComparison with Existing Methods\nThis section compares the proposed method with other methodologies reported in the liter-\nature. We utilize Part A of the BioVid dataset, which includes all 87subjects, employing\nthe LOSO cross-validation protocol for validation. The results are displayed in Table 6.7.\nOur vision-based approach, which leverages RGB and synthetic thermal modalities, shows\nperformances comparable to or surpass those of prior studies. Relative to results in the lit-\nerature from [180, 217, 219, 295], our method achieved higher accuracy in both binary and\nmulti-level tasks. Notably, the research in [296] recorded accuracies of 72.40% and30.80%,\nmarking an improvement of 1.37% and0.10% over our method, respectively. The highest\nreported results are from [37], which utilized a transformer-based architecture.\nAdditionally, Table 6.8 compares our results with those from [305], where the authors\nintroduced the MIntPAIN dataset comprising both RGB and thermal videos for automatic\npain assessment across five intensity levels. Our analysis revealed that the accuracies of\nthe RGB and thermal modalities were closely matched at 18.55% and18.33%, respectively,\nwhich parallels our observations of similar performance between RGB and synthetic ther-\nmal modalities. By integrating these modalities, the authors in [305] reported a significant\nperformance increase of 30.77%, surpassing our modest gains. It should be emphasized that\nin [305], the performance levels of the individual modalities were below the random guess\nprediction threshold of 20%. It was only through their combination that performance was\nelevated above this threshold.\n6.4 Summary\nThis chapter investigated the creation of synthetic thermal imagery via GAN models to as-\nsess its utility in automatic pain evaluation. Additionally, a novel framework incorporating\naVision-MLP and supported by a Transformer module as the core of the assessment sys-\ntem was introduced. The experiments demonstrated the effectiveness of the synthetic ther-116 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.7: Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.\nStudy MethodTask\nNP vs P 4 MC\nWerner et al. [296] Deep RF 72.40 30.80\nWerner et al. [295] RF 70.20 \u2013\nZhiet al. [217] SLSTM 61.70 29.70\nThiam et al. [219] 2D CNN, biLSTM 69.25 \u2013\nTavakolian et al. [180] 2D CNN 71.00 \u2013\nGkikas et al. [37] Vision-Transformer 73.28 31.52\nOur Vision-MLP 71.03 30.70\nTable 6.8: Comparison with the MIntPAIN dataset.\nStudy Dataset ModalityTask\nMC\nHaque et al. [305] MIntPAINRGB 18.55\nThermal\u02dd18.33\nFusion 30.77\nOur BioVidRGB 30.02\nThermal\u203929.69\nFusion 30.70\n\u02dd:real\u2039: synthetic\nmal modality, which achieved performances comparable to or better than the original RGB\nmodality. This research also delved into the factors contributing to this effectiveness, par-\nticularly the role of temperature color representations in the analysis. Furthermore, various\nfusion techniques were used to evaluate the combination of the two vision modalities, high-\nlighting the potential for performance improvements over single-modality approaches. It\nis important to note that further enhancements and experimental work, especially with the\nmultimodal approach, could improve outcomes. The generation and integration of synthetic\nmodalities, such as thermal imagery, within an automatic pain assessment framework exhibit\nconsiderable promise, warranting additional exploration and research.Chapter 7\nGeneral-Purpose Models\nContents\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n7.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n7.1 Chapter Overview: Introduction & Related Work\nThis chapter discusses the approaches and findings from [40] and [41]. In recent years,\nthe literature on deep learning has shown a trend towards adopting general-purpose models.\nThese models are characterized by architectures not tied to specific modalities or pre-training\non datasets derived solely from a single domain. We will explore two approaches: first, a\nmodality-agnostic pipeline for automatic pain assessment, and second, the development of a\nfoundation model explicitly applied for automatic pain assessment tasks.\nRegarding modality-agnostic approaches, in 5.3, we introduced the concept specifically\nfor augmentations. These augmentations were designed not directly for the image or biosig-\nnal space but for the latent space. Thus, regardless of the original input modality, whether\n117118 CHAPTER 7. GENERAL-PURPOSE MODELS\nimages, biosignals, or others, the augmentations were applied to their feature representa-\ntions. This chapter advances our work by developing a modality-agnostic multimodal fusion\npipeline evaluated in pain assessment tasks. The literature on modality-agnostic approaches\nremains limited. In [314], the researchers pursued a novel approach by exploring modality-\nagnostic representations through knowledge distillation for semantic segmentation. Their\ngoal was to reduce the modality gap and diminish semantic ambiguity, enabling the com-\nbination of various modalities under any visual conditions. In [315], the authors addressed\nthe persistent challenges of temporal asynchrony and modality heterogeneity in multimodal\nsequence fusion, often leading to performance bottlenecks. To overcome these issues, they\ndeveloped a strategy integrating modality-exclusive and modality-agnostic representations\nfor multimodal ideo sequence fusion. This approach enabled them to capture reliable con-\ntext dynamics within individual modalities and enhance distinctive features across modality-\nexclusive spaces. Additionally, they designed a hierarchical cross-modal attention module\nto uncover significant element correlations across different modalities within the modality-\nagnostic space.\nThe literature on foundation models is considerably more extensive. With the emerging\nparadigm of building AI systems around foundation models, there has been a shift toward\ncreating more adaptable and scalable systems that generalize across various tasks and do-\nmains. A foundation model is defined as any model trained on vast datasets, often through\nextensive self-supervision, which can then be adapted\u2014such as through fine-tuning\u2014to a\nwide range of downstream tasks. Despite their reliance on traditional deep learning and\ntransfer learning techniques, the extensive scale of foundation models fosters the develop-\nment of new capabilities and enhances effectiveness across many tasks [316]. Numerous\nexamples have surfaced recently in academic literature. For instance, the SAM model [317],\na foundation model for image segmentation, was initially trained from scratch on 11million\nimages. In later studies [318, 319], researchers have adapted SAM for medical imaging by\noptimizing it for smaller, specialized datasets. Additionally, a notable paradigm shift has oc-\ncurred with the introduction of generalist models [320], a novel class of foundation models\ntrained simultaneously on various tasks under a unified learning strategy, typically super-\nvised. This approach is particularly advantageous in computer vision, enabling handling\ndiffering embedding representations across tasks and various visual modalities [321].\nTo the best of our knowledge, there are no modality-agnostic or foundation models in\nthe field of automatic pain assessment. Currently, the majority of the approaches utilize\npre-trained models; however, these models typically adhere to the traditional methodology\nof pre-training on a general, large-scale dataset and then fine-tuning for the specific task of\npain assessment. Studies such as those detailed in [305, 322] rely on transfer learning tech-\nniques derived from facial recognition datasets. The majority of these studies are reviewed\nin Chapter 3.7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 119\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment\nWe present a modality-agnostic multimodal framework that leverages both video and fNIRS\ndata. The pipeline operates on a dual Vision Transformer (ViT) format, which obviates the\nnecessity for modality-specific architectures or expansive feature engineering. It interprets\nthe inputs as unified images through 2D waveform representations.\n7.2.1 Methodology\nThis section outlines the pipeline for the proposed multi-modal automatic pain assessment\nframework, describing the model architectures, pre-processing methods, pre-training strate-\ngies, and augmentation techniques.\nFramework Architecture\nThe proposed framework, Twins-PainViT , includes two modules: PainViT\u20131 andPainViT\u20132 .\nBoth models share uniform architectures and parameters and follow to identical pre-training\nprotocol. PainViT\u20131 is tasked with processing the individual input video frames and the\nvisualized fNIRS waveforms, serving as an embedding extractor. Meanwhile, PainViT\u20132\nmanages the visual representation of these embeddings and performs the final pain classifi-\ncation task.\nPainViT: Vision Transformers (ViTs) [323] have established themselves as a leading frame-\nwork in computer vision tasks, recognized for their impressive performance. However,\ndespite their effectiveness, transformer-based models encounter scalability challenges with\nlarger input sizes, significantly increasing computational demands. This inefficiency mainly\nstems from the element-wise operations in the multi-head self-attention mechanism. Efforts\nto improve the efficiency and simplify the architecture of transformer-based models have\nincluded modifications to the self-attention module and overall structural adjustments [324]\n[325]. Our methodology builds on the principles of [326], which incorporates hierarchical\narchitectures into vision transformers, and [327], which introduces mechanisms to enhance\nefficiency and processing speed.\nPainViT\u2013block: A block consists of two key elements: the Token-Mixer and the Cascaded-\nAttention . The architecture places the Cascaded-Attention module centrally, combined with\naToken-Mixer module before and after it. For every input image I, overlapping patch em-\nbedding is utilized, generating 16\u02c616patches. Each patch is then projected into a token\nwith a dimensionality of d.120 CHAPTER 7. GENERAL-PURPOSE MODELS\nToken-Mixer: To better integrate local structural information, the token Tis processed\nthrough a depthwise convolution layer:\nYc\u201cKc\u02daTc`bc. (7.1)\nHere, Ycdenotes the output of the depthwise convolution for channel cof the token Tc.Kc\nrepresents the convolutional kernel for channel c,Tcindicates the c-th channel of the token\nT, and bcis the bias term added to the convolution output for channel c. The symbol \u02da\nindicates the convolution operation. After the depthwise convolution, batch normalization is\nthen applied to the output:\nZc\u201c\u03b3c\u02dc\nYc\u00b4\u00b5Ba\n\u03c32\nB`\u03f5\u00b8\n`\u03b2c. (7.2)\nHere, Zcrepresents the batch-normalized output for channel cof the token T. The learnable\nparameters \u03b3cand\u03b2care specific to channel c, adjusting the scale and shift of the normalized\ndata. \u00b5Bdenotes the batch mean of Yc,\u03c32\nBindicates the batch variance of Yc, and \u03f5is a\nsmall constant included for numerical stability to prevent division by zero. Subsequently, a\nfeed-forward network (FFN) is used to enhance communication between different feature\nchannels:\n\u03a6FpZcq\u201cW2\u00a8ReLUpW1\u00a8Zc`b1q`b2. (7.3)\nHere, \u03a6FpZcqrepresents the output of the feed-forward network for the input Zc. The weight\nmatrices for the first and second linear layers are denoted by W1andW2, respectively; b1\nandb2are the corresponding bias terms for these layers. The activation function employed\nhere is ReLU.\nCascaded-Attention Respecting the attention mechanism, it includes a single self-attention\nlayer. For each input embedding:\nXi`1\u201c\u03a6ApXiq. (7.4)\nTheXirefers to the entire input embedding for the i-thPainViT-block . The Cascaded-\nAttention module uses a cascaded mechanism that splits the full input embedding into smaller\nsegments, with each segment directed to a specific attention head. This method distributes\nthe computational load across the heads, improving efficiency by managing long input em-\nbeddings more effectively. The attention mechanism operates as follows:\nrXij\u201cAttnpXijWQ\nij, XijWK\nij, XijWV\nijq, (7.5)\nrXi`1\u201cConcatrrXijsj\u201c1:hWP\ni, (7.6)7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 121\nTable 7.1: Number of parameters and FLOPS for the components of the proposed Twins-PainViT .\nModule Params (M) FLOPS (G)\nPainViT\u20131 16.46 0.59\nPainViT\u20132 16.46 0.59\nTotal 32.92 1.18\nwhere each j-th head is responsible for computing the self-attention of Xi,j, thej-th segment\nof the full input embedding Xi. This embedding is organized as rXi1, Xi2, . . . , X ihs, with j\nranging from 1 to h, where his the total number of heads. The projection layers WQ\nij,WK\nij,\nandWV\nijtransform each segment of the input embedding into distinct subspaces for queries,\nkeys, and values, respectively. The process concludes with WP\ni, a linear layer that combines\nthe outputs of all heads, restoring them to the original input dimensionality. Additionally, the\ncascaded design aids in developing more complex representations for the Q,K, andVlayers.\nThis enhancement occurs as the output from each head is fed into the subsequent head,\nallowing for a progressive accumulation of information throughout the layers. Specifically:\nX1\nij\u201cXij`rXipj\u00b41q. (7.7)\nHere, X1\nijdenotes the sum of the j-th input segment Xijand the output \u02dcXipj\u00b41qfrom thepj\u00b4\n1q-th head. This summation is the new input embedding for the j-th head in the self-attention\ncomputation. Additionally, depthwise convolution is applied to each Qin every attention\nhead, enhancing the self-attention process\u2019s ability to capture both global representations\nand local details.\nThe architecture consists of three PainViT\u2013blocks with depths of 1,3, and 4. This hier-\narchical design progressively reduces the number of tokens by subsampling the resolution\nby2\u02c6at each stage, enabling the extraction of embeddings with dimensions dacross the\nblocks, particularly 192,288, and 500. Each block also features a multihead self-attention\nmechanism, employing 3,3, and 4heads, respectively. Fig. 1(a-d) depicts the PainViT archi-\ntecture and its core components, while Table 7.1 details the number of parameters and the\ncomputational costs measured in floating-point operations (FLOPS).\nEmbedding extraction & Fusion\nFor every frame in a video, V\u201crv1, v2, . . . , v ns,PainViT\u20131 generates a corresponding em-\nbedding. These embeddings are combined to produce a composite feature representation of\nthe video. Similarly, for each fNIRS signal channel, C\u201c rc1, c2, . . . , c ms,PainViT\u20131 ex-\ntracts embeddings which are then compiled to form a complete representation of the fNIRS122 CHAPTER 7. GENERAL-PURPOSE MODELS\nsignal. The following equations outline this procedure:\nEV\u00d0n\u00ff\ni\u201c1PainViT\u20131pviq, (7.8)\nEC\u00d0m\u00ff\ni\u201c1PainViT\u20131pciq, (7.9)\nwhere EVandECrepresent the embedding representations for the video and fNIRS, re-\nspectively. After these embeddings are extracted, EVandECare visualized as waveform\ndiagrams. The waveforms from both modalities\u2014video and fNIRS\u2014are then combined into\na single image with a resolution of 224\u02c6224. This integrated visual representation is input\ninto PainViT\u20132 for final pain assessment. (Fig. 1e) provides a high-level overview of the\nmultimodal proposed pipeline.\nPre-processing\nThe preprocessing steps include face detection in video frames and generating waveform\ndiagrams from the original fNIRS data. The MTCNN face detector [278], which uses a\nseries of cascaded convolutional neural networks, was employed to identify faces and facial\nlandmarks with the faces set at a resolution of 224\u02c6224pixels. All fNIRS channels are used\nto create waveform diagrams, which visually represent the signal\u2019s wave shape and form\nover time, displaying amplitude, frequency, and phase. This method offers a straightforward\napproach to visualizing a signal without requiring transformations or complex computations\ntypical of spectrograms, scalograms, or recurrence plots. Similarly, embeddings extracted\nbyPainViT\u20131 are visualized using waveform diagrams. Although these embeddings are not\nsignals per se, the 1D vectors can still be plotted in a 2D space for further analysis or use by\ndeep-learning vision models. Waveform diagrams created from fNIRS data and embeddings\nare formatted as images with a 224\u02c6224pixels resolution. Fig. 7.2 shows waveform\nrepresentations of channel-specific fNIRS signals, an embedding extracted from a video,\nand an embedding derived from a channel-specific fNIRS sample.\nPre-training\nBefore the automatic pain assessment training, the Twins-PainViT models underwent pre-\ntraining using a multi-task learning approach. This pre-training incorporated four datasets de-\nsigned for emotion recognition tasks. The AffectNet [287] and RAF-DB basic [289] datasets\nsupplied facial images to train on basic emotions, whereas the Compound FEE-DB [288]\nandRAF-DB compound [289] datasets focused on complex emotional states. In addition,\nfive datasets containing various biosignals were utilized. The EEG-BST-SZ [328] dataset in-7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 123\nTable 7.2: Datasets utilized for the pretraining process of the framework.\nDataset # samples # classes Modality\nAffectNet [287] 0.40M 8 Facial Images\nRAF-DB basic [289] 15,000 7 Facial Images\nRAF-DB compound [289] 4,000 11 Facial Images\nCompound FEE-DB [288] 6,000 26 Facial Images\nEEG-BST-SZ [328] 1.5M 2 EEG\nSilent-EMG [329] 190,816 8 EMG\nBioVid [109] 8,700 5 ECG\nBioVid [109] 8,700 5 EMG\nBioVid [109] 8,700 5 GSR\nEEG: electroencephalogram EMG: electromyogram ECG: electrocardiogram GSR: galvanic\nskin response\ncludes electroencephalograms for schizophrenia analysis, and the Silent-EMG [329] contains\nelectromyograms that help pinpoint the origin of EMG activities, such as from the throat or\nmid-jaw areas. The BioVid [109] dataset provided electrocardiogram, electromyogram, and\ngalvanic skin response samples for pain assessment. All biosignals were represented as\nwaveforms, as detailed in 7.2.1. The equation defines the multi-task learning framework:\nLtotal\u201c9\u00ff\ni\u201c1rewiLSi`wis, (7.10)\nwhere LSirepresents the loss for each specific task from the various datasets, and wiare\nthe learned weights that drive the learning process to minimize the combined loss Ltotal,\nconsidering all individual losses. Table 7.2 outlines the datasets involved in the pre-training\nphase.\nAugmentation Methods & Regularization\nSeveral augmentation methods have been employed to train the proposed framework. For the\npre-training process, RandAugment [330] and TrivialAugment [282] were used, along with\nauxiliary noise from a uniform distribution and a custom MaskOut technique that masks\nout random square sections of input images. In the automatic pain assessment task, Aug-\nMix[281] is used in addition to the previously mentioned methods. Moreover, Label Smooth-\ning[331] and DropOut [332] are implemented as regularization techniques to optimize the\ntraining outcome.124 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.3: Training details for the automatic pain assessment.\nOptimizer LR LR\ndecayWeight\ndecayEpochs Warmup\nepochsCooldown\nepochsBatch\nsize\nAdamW 2e-5 cosine 0.1 100 10 10 32\nLR: learning rate\n7.2.2 Experimental Evaluation & Results\nWe employ the dataset provided by the challenge organizers [118,333], which includes facial\nvideos and fNIRS data from 65participants. The dataset is partitioned into 41 training, 12\nvalidation, and 12 testing subjects, all recorded at the Human-Machine Interface Laboratory,\nUniversity of Canberra, Australia. Electrodes for transcutaneous electrical nerve stimula-\ntion, used as pain stimuli, were placed on the right hand\u2019s inner forearm and back. The study\nmeasures both pain threshold\u2014the lowest stimulus intensity perceived as painful\u2014and pain\ntolerance\u2014the maximum intensity of pain a participant can tolerate. For the fNIRS mea-\nsurements, 24channels were utilized for both HbO and HbR, and each video in the dataset\ncontains 30frames. This paper focuses on the results from the validation segment of the\ndataset, which are structured into a multi-level classification setting for No Pain ,Low Pain ,\nandHigh Pain . Table 7.3 details the training framework for the automatic pain assessment. It\nshould be noted that while numerous experiments were conducted across different modalities\nand their combinations, only the most successful outcomes are discussed in the subsequent\nsections and detailed in the corresponding tables.\nFacial Videos\nFor facial videos, two embedding fusion methods were implemented: the Addition tech-\nnique, which aggregates 30embeddings into a single vector of dimension d\u201c500, and the\nConcatenation approach, which merges the embeddings into a larger vector of d\u201c15,000.\nWith the Addition method, the initial accuracy reached 41.90% under basic augmentation\nand regularization settings. Enhancing the augmentation intensity and adjusting MaskOut\nimproved the accuracy incrementally, achieving a peak of 44.91%. Adjusting DropOut and\nother augmentation parameters refined the performance to 43.52%. These findings are de-\ntailed in Table 7.4. Employing the Concatenation method with initial uniform augmentation\nprobabilities resulted in a starting accuracy of 40.28%. Strategic increases in MaskOut and\nmaintaining other augmentations at moderate levels led to a gradual accuracy improvement,\nculminating in a high of 43.75% when all augmentations were maximized except MaskOut ,\npaired with high regularization settings. The results of this approach are outlined in Table\n7.5.7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 125\nTable 7.4: Results utilizing the video modality & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.1 0.1 0.1 0.1 |3 0.1 0.5 41.90\n0.5 0.5 0.5 0.7 |3 0.0 0.5 44.91\n0.5 0.5 0.5 0.7 |10 0.0 0.5 42.13\n0.5 0.5 0.5 0.7 |3 0.0 0.6 42.36\n0.9 0.9 0.9 0.7 |3 0.3 0.7 43.52\nRand :RandAugment Trivial : TrivialAugment LS:Label Smoothing MS: multiclass pain\nassessment. For Augmentation & Regularization the first number represents the probability\nof application, while in MaskOut the number followed |indicates the number of square\nsections applied.\nTable 7.5: Results utilizing the video modality & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.3 0.3 0.3 0.3 |3 0.1 0.5 40.28\n0.5 0.5 0.5 0.8 |5 0.0 0.5 41.44\n0.9 0.9 0.9 0.7 |3 0.2 0.7 42.13\n0.9 0.9 0.9 0.7 |1 0.4 0.5 41.90\n0.9 0.9 0.9 0.6 |3 0.4 0.5 43.75\nfNIRS\nSimilar to facial videos, the Addition andConcatenation methods were also applied to the\nfNIRS channels, excluding two faulty ones from the original 24. For the HbR with the\nAddition method, the initial accuracy was 39.35%, set with uniform probabilities of 0.5for\nAugMix ,Rand , and Trivial , and MaskOut adjusted to 0.6|5. Modifying MaskOut to0.7|3and\nincreasing LSslightly reduced accuracy, while subsequent adjustments in LSandDropOut\nimproved it to 41.20% (refer to Table 7.6). In the HbR with the Concatenation method, start-\ning with MaskOut at0.7|3led to an accuracy of 40.97%. Escalating all augmentations to\n0.9, while keeping MaskOut at0.7|3, achieved a peak accuracy of 42.13% (refer to Table\n7.7). For the HbO with the Addition method, accuracies started at 43.06% with uniform aug-\nmentation probabilities of 0.3andMaskOut at0.3|3. Elevating MaskOut to0.7|3with minor\nadjustments in LSandDropOut maintained similar accuracies, while optimizing MaskOut\nto0.8|3enhanced performance to 44.68% (refer to Table 7.8). The HbO with the Concate-\nnation method began with an accuracy of 42.13% under an augmentation probability of\n0.1. A balanced augmentation setup at 0.9andMaskOut at0.7|3lifted the peak accuracy\nto44.44%, demonstrating the effectiveness of increased overall augmentation coupled with\nhigh regularization. Further adjustments slightly reduced accuracy, highlighting the critical126 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.6: Results utilizing the HbR & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.6 |5 0.0 0.5 39.35\n0.5 0.5 0.5 0.7 |3 0.4 0.5 38.89\n0.9 0.9 0.9 0.7 |3 0.1 0.9 40.05\n0.9 0.9 0.9 0.7 |5 0.4 0.5 41.20\n0.5 0.5 0.5 0.7 |3 0.0 0.4 40.51\nTable 7.7: Results utilizing the HbR & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.0 0.0 0.7 0.7 |3 0.0 0.5 40.97\n0.5 0.5 0.5 0.7 |1 0.0 0.5 41.44\n0.9 0.9 0.9 0.7 |3 0.1 0.8 42.13\n0.9 0.9 0.9 0.7 |3 0.4 0.5 41.20\n0.5 0.5 0.5 0.7 |3 0.0 0.3 39.81\nTable 7.8: Results utilizing the HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.3 0.3 0.3 0.3 |3 0.1 0.5 43.06\n0.5 0.5 0.5 0.7 |3 0.2 0.5 42.82\n0.9 0.9 0.9 0.7 |3 0.4 0.8 43.29\n0.9 0.9 0.9 0.7 |9 0.4 0.5 44.44\n0.9 0.9 0.9 0.8 |3 0.4 0.5 44.68\nnature of optimal augmentation settings (refer to Table 7.9). Generally, enhanced perfor-\nmance is observed with HbO compared to HbR, as noted in other studies [334], attributed\nto its superior signal-to-noise ratio. Combining HbR and HbO using the Addition method\ninitially showed an accuracy of 42.82% with all augmentations at zero except for MaskOut\nat0.7|3. Increasing AugMix ,Rand , and Trivial to0.5while raising MaskOut to0.7|7slightly\nimproved accuracy to 43.29%. Adjustments to MaskOut back to 0.7|3with a slight increase\ninLSled to a minor reduction in accuracy to 42.59%. However, further increasing all aug-\nmentations to 0.9andLSto0.3while maintaining MaskOut at0.7|3maximized accuracy\nto43.75%. A reduction in DropOut to0.1in the final setup slightly reduced accuracy to\n43.06%, emphasizing the importance of optimizing regularization alongside augmentation\nstrategies for achieving optimal results (refer to Table 7.10).7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 127\nTable 7.9: Results utilizing the HbO & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.1 0.1 0.1 0.1 |3 0.1 0.5 42.13\n0.5 0.5 0.5 0.0 |0 0.0 0.5 43.98\n0.5 0.5 0.5 0.7 |1 0.0 0.5 42.36\n0.9 0.9 0.9 0.7 |3 0.4 0.9 44.44\n0.5 0.5 0.5 0.7 |3 0.0 0.8 43.52\nTable 7.10: Results utilizing the HbR, HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.0 0.0 0.7 0.7 |3 0.0 0.5 42.82\n0.5 0.5 0.5 0.7 |7 0.0 0.5 43.29\n0.5 0.5 0.5 0.7 |3 0.1 0.5 42.59\n0.9 0.9 0.9 0.7 |3 0.3 0.9 43.75\n0.5 0.5 0.5 0.7 |3 0.0 0.1 43.06\nTable 7.11: Results utilizing the videos, HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.4 |5 0.0 0.5 42.36\n0.5 0.5 0.5 0.7 |9 0.0 0.5 41.67\n0.9 0.9 09. 0.7 |3 0.1 0.6 42.59\n0.9 0.9 0.9 0.7 |3 0.3 0.9 43.06\n0.9 0.9 0.9 0.7 |5 0.4 0.5 43.75\nFusion\nThis section explores the fusion of facial videos and fNIRS, explicitly utilizing HbO due\nto its demonstrated superior performance over HbR. Two fusion methods were employed:\ntheAddition method, which aggregates embeddings from video frames and fNIRS channels\ninto a unified vector, and the Single-Diagram method, where aggregated embeddings from\nboth modalities are visualized simultaneously in a single image. For the Addition method,\ninitial configurations with moderate augmentation levels ( 0.5forAugMix ,Rand ,Trivial ) and\nMaskOut at0.4|5achieved an accuracy of 42.36%. Increasing the augmentation levels to 0.9\nand adjusting the regularization parameters ( LSup to 0.4andDropOut up to 0.9) enhanced\nthe accuracy, peaking at 43.75% (refer to Table 7.11). For the Single Diagram method, accu-\nracy improvements were noted, as shown in Table 7.12. Starting with lower MaskOut levels128 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.12: Results utilizing the videos, HbO & Single Diagram method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.3 |5 0.0 0.5 45.83\n0.9 0.9 0.9 0.7 |3 0.1 0.6 46.76\n0.9 0.9 0.9 0.7 |3 0.3 0.6 46.53\n0.9 0.9 0.9 0.9 |3 0.4 0.5 45.83\n0.5 0.5 0.5 0.7 |3 0.0 0.7 45.14\nTable 7.13: Comparison with the validation baseline provided by the AI4PAIN challenge organizers.\nApproachModality\nVideo fNIRS Fusion\nBaseline 40.00 43.20 40.20\nOur 44.91 44.68 46.76\nat0.3|5and standard augmentation probabilities ( 0.5), the accuracy was 45.83%. Utilizing\naugmentation probabilities to 0.9andMaskOut adjustments to 0.7|3significantly improved\nperformance, achieving a high of 46.76%.\nInterpretation & Comparison\nIn the framework\u2019s analysis, attention maps from the last layer of PainViT\u20132 were generated,\nillustrating the processed unified image that integrates both the video and HbO embedding\nwaveforms. This layer consists of 500neurons, each specifically engaging with different in-\nput aspects. Figure 7.3 displays four examples demonstrating how specific neurons predom-\ninantly focus on the video embedding waveform. In contrast, others concentrate on the HbO\nwaveform, and some attend to both, highlighting various details. Table 7.13 compares the\nproposed pipeline and the baseline results set by the challenge organizers. The video-based\napproach utilizing the Addition method exceeded the baseline by 4.91%. Implementing the\nHbO with the Addition method showed a minor improvement of 1.48%. However, the fu-\nsion of modalities through the Single Diagram method achieved a more substantial gain of\n6.56%.\n7.2.3 Discussion\nThis chapter contributes to the First Multimodal Sensing Grand Challenge for Next-Gen\nPain Assessment (AI4PAIN) , utilizing facial videos and fNIRS in a modality-agnostic frame-\nwork. The Twins-PainViT , a dual Vision Transformer configuration, was pre-trained across7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 129\nmultiple datasets using a multi-task learning approach. A key feature of our approach is the\nwaveform representation applied to the original fNIRS data and the learned embeddings, al-\nlowing for their integration into a single image diagram. This method effectively eliminates\nthe need for domain-specific models for each modality. Our experiments demonstrated high\nperformance in both unimodal and multimodal configurations, outperforming the established\nbaselines. The analysis of PainViT\u20132 through attention maps further revealed that specific\nneurons specifically target different modalities or distinct aspects within them, suggesting a\ncomprehensive analytical approach. Future research should continue to explore multimodal\nstrategies, as they have shown superior efficacy in real-world pain assessment settings. De-\nveloping interpretative methods is crucial for integrating these advanced frameworks into\nclinical practice.\n7.3 A Foundation Model for Automatic Pain Assessment\nWe introduce PainFormer , a multi-task learning vision foundation model tailored for auto-\nmatic pain assessment. This initiative is the first to develop and deploy a foundation model\nfor pain recognition, inspired by the frameworks discussed in [320]. Our method involves\ntraining across various datasets and tasks, leveraging large-scale corpora to enhance repre-\nsentation learning for pain assessment applications. This research makes three key contri-\nbutions: (1) it introduces a foundation model capable of extracting robust embeddings from\ndiverse modalities, (2) it incorporates synthetic thermal and estimated depth videos as inno-\nvative modalities, and (3) it evaluates the performance of these modalities in both unimodal\nand multimodal settings.\n7.3.1 Methodology\nThis section outlines the structure and components of the suggested framework. It covers\nthe foundation model\u2019s pretraining based on multi-task learning, the methods for augmen-\ntation, and the training configurations for pretraining and pain evaluation tasks. It also de-\nscribes how synthetic thermal and depth videos are generated and details the visualization\ntechniques for biosignal modalities employed in this research.\nFramework Architecture\nThe framework integrates three models: the PainFormer , a foundation model that extracts\nembeddings from input data; the Embedding-Mixer , which applies these embeddings, either\nsingly or in combination, to classify pain; and the Video-Encoder , which reduces video data\ninto a lower-dimensional latent space for use in the multimodal approaches that are explained\nlater. This framework operates in two separate stages: initially extracting embeddings and130 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.14: Number of parameters and FLOPS for the modules of the proposed framework.\nModule Params\n(Millions)FLOPS\n(Giga)\nPainFormer 19.60 5.82\nEmbedding-Mixer 9.85 2.94\nVideo-Encoder 3.37 0.86\nTotal 32.82 9.62\nthen deploying them according to the demands of specific modality pipelines. Table 7.14\ndetails the parameters and computational costs for each module\u2019s floating-point operations\n(FLOPS).\nPainFormer: Vision Transformers (ViT) have become increasingly widespread for various\nimage-processing tasks, demonstrating the effectiveness of their self-attention mechanisms.\nMoreover, developing Vision Multilayer Perceptron (Vision-MLP) models that use spectral\nmixing techniques\u2014substituting self-attention layers with Fourier transformation layers\u2014\nillustrates that simpler structures with fewer inductive biases can achieve similar outcomes.\nOur strategy incorporates two key concepts: hierarchical Vision Transformers (ViT) [326],\nwhich use multiple embedding extraction stages to boost performance and scalability, and\nthe Fourier transform\u2019s efficient token information mixing as shown in [335]. PainFormer\nintegrates spectral layers using the Fast Fourier Transform (FFT) with self-attention layers.\nBoth spectral and self-attention layers are initially applied, whereas later stages rely solely\non self-attention. The architecture of PainFormer is illustrated in Fig. 1(a). Each 2D input\nimage Iis segmented into nnon-overlapping patches, each patch PRn\u02c6h\u02c6w\u02c63, where hand\nware the patch resolution set at 16\u02c616, and 3represents the RGB channels. Each patch\nis linearly projected into a dimension d\u201c768, followed by positional encoding. Applying\nDiscrete Fourier Transform (DFT) to a 1D sequence of Nelements, xrns, ranging from 0to\nN\u00b41, yields:\nXrks\u201cN\u00b41\u00ff\nn\u201c0xrns\u00a8e\u00b4i2\u03c0k\nNn:\u201cN\u00b41\u00ff\nn\u201c0xrns\u00a8Wkn\nN, (7.11)\nwhere iis the imaginary unit, and WNis defined as e\u00b4i2\u03c0\nN. The sequence can be transformed\nback to the time domain by applying the inverse Discrete Fourier Transform (IDFT):\nxrns\u201c1\nNN\u00b41\u00ff\nk\u201c0Xrks\u00a8ei2\u03c0k\nNn, (7.12)7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 131\nwhere xrnsrepresents the original sequence. Furthermore, for two-dimensional inputs, xrm, ns,\nwith0\u010fm\u010fM\u00b41and0\u010fn\u010fN\u00b41, the formula extends to:\nXru, vs\u201cM\u00b41\u00ff\nm\u201c0N\u00b41\u00ff\nn\u201c0xrm, ns\u00a8e\u00b4i2\u03c0`um\nM`vn\nN\u02d8\n, (7.13)\nwhere Xru, vsis the frequency-domain representation of the input xrm, ns.\nSpectral Layer: For the tokens xfrom image I, a 2D FFT is applied across the spatial\ndimensions to transform xinto the frequency domain:\nX\u201cFrxsPCh\u02c6w\u02c6d. (7.14)\nAfter applying the FFT to extract the various frequency components of the image, we employ\na learnable filter, KPCh\u02c6w\u02c6dacts as a gate to regulate the significance of each frequency\ncomponent. This spectrum modulation allows for the identification and learning of features\nsuch as lines and edges. Specifically:\n\u02dcX\u201cKdX, (7.15)\nwhereddefines the element-wise multiplication. Afterward, the inverse Fast Fourier Trans-\nform (IFFT) is applied, which converts the spectral space back into the physical space:\nx\u00d0F\u00b41r\u02dcXs, (7.16)\nwhere the physical space is referred to as the spatial domain in this case. The final component\nof a spectrum layer is an MLP module, which enables efficient channel mixing communica-\ntion:\n\u03a6pxq\u201cW2\u00a8GELUpDWConvpW1\u00a8x`b1qq`b2, (7.17)\nwhere DWConv denotes a depthwise convolution layer. In addition, layer normalization is\nemployed before and after the FFT and IFFT processes, refer to Fig. 1(b).\nSelf-Attention Layer: In this layer, the standard self-attention mechanism characteristic\nof transformers is utilized. For a given token sequence X, the attention mechanism is defined\nas:\nAttpXq:\u201csoftmax\u02c6XW qpXW kqT\n?\nd\u02d9\nXW v, (7.18)\nwhere Att maps RN\u02c6dtoRN\u02c6d, with Nrepresenting hw. The matrices Wq,Wk, and Wvin\nRd\u02c6dcorrespond to the query, key, and value weights, respectively. Layer normalization is132 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.15: Details of the PainFormer\u2019s architecture.\nStage # Spectral\nLayers# Self-Attention\nLayers# Self-Attention\nHeadsDimension\nd\n1 2 1 2 64\n2 2 2 4 128\n3 \u2013 12 10 320\n4 \u2013 3 16 160\nd: token dimensions\napplied both before and after the attention mechanism, mirroring the approach used in the\nspectral layer. Additionally, the MLP component within this layer is expressed as:\n\u03a6pxq\u201cW2\u00a8GELUpW1\u00a8x`b1q`b2. (7.19)\nFig. 1(c) illustrates the design of this layer.\nStages: A stage-based architecture was developed to create a hierarchical representa-\ntion. PainFormer is organized into four stages, each followed by a single-layer 2D CNN that\ndownsamples the resolution by a factor of 2to reduce the token dimensions. Additionally,\neach stage incorporates a distinct combination of spectral and self-attention layers, with vary-\ning numbers of heads in the self-attention layers and different dimensions for the extracted\ntokens. Table 7.15 presents the relevant details.\nEmbedding-Mixer: The model is built on a transformer architecture, leveraging cross- and\nself-attention mechanisms. Echoing insights from prior research [277], it employs an asym-\nmetrical attention scheme using cross-attention with fewer latent variables to reduce compu-\ntational demands and boost efficiency. Cross-attention operation parallels self-attention, as\ndetailed in Eq. (7.18). Nonetheless, the dimensions for Wq,Wk, and Wvadjust to n\u02c6d\nfrom d\u02c6d, with n\u0103dandnspecifically set at 256. The Embedding-Mixer includes 2\nlayers, each equipped with 1cross-attention and 2self-attention modules. The head counts\nfor cross- and self-attention are 1and8, respectively. The final classification task utilizes an\noutput embedding of length 512, as shown in Fig. 1(d).\nVideo-Encoder: The design of this specific module mirrors the Embedding-Mixer . How-\never, it is simplified for efficiency by including only a 1layer that contains a single cross-\nattention module with a 1head. The number of latent variables, n, is maintained at 256,\nand the output embedding length is set at 40. This module functions exclusively within a7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 133\nparticular framework as part of one of the multimodal strategies, integrating video and GSR\nembeddings. The module\u2019s architecture is depicted in Fig. 1(e).\nSynthetic Thermal & Depth Videos\nThis section incorporates thermal and depth vision modalities alandGB videos into our pain\nassessment frameworks. For the thermal modality, we utilize thermal videos from our ear-\nlier research [39], described in Chapter 6, which introduced an image-to-image translation\n(I2I) method using a conditional generative adversarial network (cGAN). This network was\ndesigned and trained to map the data distribution from the RGB to the thermal domain, fa-\ncilitating the creation of synthetic thermal images from new RGB videos. For the depth\nvideos, we employ the \u201cDepth Anything\u201d technique [336], which is a pioneering model for\nmonocular depth estimation (MDE) that uses a vision transformer-based encoder-decoder\narchitecture with semi-supervised learning. Fig. 7.5 displays a frame sample from the RGB,\nsynthetic thermal, and depth modalities.\nBiosignal Visualization\nGiven that the core model in this research is vision-based, it necessitates using 2D represen-\ntations for physiological modalities. We explore four distinct visualizations: (1) waveform\ndiagrams, which outline the signal\u2019s progression over time, showcasing its amplitude, fre-\nquency, and phase characteristics; (2) spectrogram-angle , which displays the phase angles\nassociated with different frequencies; (3) spectrogram-phase , which reveals phase details\nand incorporates unwrapping to rectify discontinuities; and (4) spectrogram-PSD , which de-\nlineates the power spectral density, indicating the distribution of power across frequencies\nover time. Fig. 7.6 provides an example of each visualization type.\nFoundation Training\nPainFormer , our proposed foundation model, serves as an embedding extractor as previously\noutlined. It has undergone extensive training on 14datasets, which include a total of 10.9\nmillion samples; for further details, see Table 7.16. The training datasets cover a variety\nof human-centric data, ranging from facial recognition datasets such as VGGFace2 [280]\nandDigiFace-1M [313] to datasets aimed at recognizing basic and compound emotions,\nlike AffectNet [287] and RAF-DB [289]. Furthermore, datasets based on biosignals like\nEEG, EMG, and ECG have been incorporated. Regarding training methodology, PainFormer\nemploys a multi-task learning strategy, with each dataset representing a separate supervised\nlearning task. Architecturally, the model adheres to its initial design as specified in 7.3.1\nbut now includes additional task-specific auxiliary classifiers. These classifiers comprise\na single-layer, fully connected network with an ELU (Exponential Linear Unit) activation134 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.16: Datasets utilized for the multitask learning-based pretraining process of the framework.\nDataset # samples # classes Modality\nVGGFace2 [280] 3.31M 9,131 Facial Images\nSpeakingFaces RGB [312]\u00150.76M 142 Facial Images\nSpeakingFaces Thermal [312]\u00150.76M 142 Facial Images\nDigiFace-1M [313] 0.72M 10,000 Facial Images\nDigiFace-1M [313] 0.50M 100,000 Facial Images\nAffectNet [287] 0.40M 8 Facial Images\nSFace [337] 1.84M 10,341 Facial Images\nCACIA-WebFace [129] 0.50M 10,575 Facial Images\nRAF-DB basic [289] 15,000 7 Facial Images\nRAF-DB compound [289] 4,000 11 Facial Images\nCompound FEE-DB [288] 6,000 26 Facial Images\nEEG-BST-SZ [328]\u00121.50M 2 EEG signals\nSilent-EMG [329]\u00120.19M 8 EMG signals\nECG HBC Dataset [291]\u00120.45M 5 ECG signals\nTotal: 14 datasets\u2013tasks 10.9M\nEEG: electroencephalogram EMG: electromyography ECG \u0015: The datasets were also used for the I2I process\ndescribed in 7.3.1, in addition to the training of the PainFormer \u0012: The samples were transformed into spectrograms\nbefore being employed.\nfunction. The objective during training is to simultaneously learn from all 14datasets/tasks.\nThe following equation formalizes this approach:\nLtotal\u201c14\u00ff\ni\u201c1rewiLSi`wis, (7.20)\nwhere LSiindicates the loss linked to each dataset/task, and wiare the adaptive weights that\naim to minimize the overall loss Ltotal, encompassing all individual task losses. The model\nwas trained using this methodology over 200epochs.\nAugmentation & Regularization Methods\nVarious augmentation and regularization strategies were applied during the pre-training of\nPainFormer and in the downstream pain assessment tasks. For foundational training, Triv-\nialAugment [282] and AugMix [281] were used. A customized augmentation method that\nmodifies brightness, contrast, and saturation and involves image cropping was also imple-\nmented. The pre-training regime included adding random noise sourced from a Gaussian\ndistribution. Moreover, a technique was devised to obscure random square portions of the\ninput images. Regularization during pre-training was achieved using DropPath [338] and\nLabel Smoothing [331]. Within the pain assessment framework, two specific augmentation7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 135\nTable 7.17: Training details of the proposed framework.\nTask Optimizer LR LR decay Weight\ndecayWarmup\nepochsCooldown\nepochsBatch size\nMTL AdamW 2e-5 cosine 0.1 5 10 126\u0010\nPain AdamW 2e-5 cosine 0.1 10 10 32\nMTL: multi-task learning for pre-training the foundation model Pain: pain assessment task LR: learning rate \u0010: batch\nsize is proportionally distributed across the 14 tasks\nmethods were integrated. The first, termed Basic , involves polarity inversion and the addition\nof noise, which alters the original input embeddings by reversing data elements\u2019 polarity and\nadding random noise from a Gaussian distribution, thereby inducing variability. The second\nmethod, Masking , implements zero-valued masks on the embeddings, effectively nullify-\ning sections of the vectors. These masks are randomly sized and placed, obscuring 10%\nto20% of the embedding\u2019s total dimensions. For further regularization, techniques such as\nDropOut [332] and Label Smoothing [331] were employed. Additional specifics on the two\ntraining methodologies are detailed in Table 7.17.\nDataset Details\nTo evaluate the effectiveness and resilience of our proposed framework, we performed tests\non two specific pain datasets, BioVid [109] and AI4Pain [118]. These datasets offer a varied\nand solid foundation for validating the performance of our model in pain assessment tasks.\nBioVid Heat Pain Database: This dataset is recognized and well-established within the do-\nmain of pain research. It encompasses facial videos, electrocardiograms, electromyograms,\nand galvanic skin response measurements from eighty-seven pn=87qhealthy participants ( 44\nmales and 43females, aged between 20and65). The experiment involved applying a ther-\nmode to the participants\u2019 right arm to induce pain. Before data collection, each participant\u2019s\npain and tolerance thresholds were determined, defining the minimum and maximum levels\nof pain experienced. This setup included two additional intermediate levels, culminating in\nfive distinct pain intensities: No Pain (NP), Mild Pain (P 1), Moderate Pain (P 2), Severe Pain\n(P3), and Very Severe Pain (P 4). The temperature for inducing these pain levels ranged from\nP1to P 4but did not exceed 50.5\u02ddC. Participants underwent 20inductions at each of the four\nspecified intensity levels (P 1to P 4), with each stimulus lasting 4s, followed by a recovery\ninterval of 8to12s. Additionally, 20baseline measurements at 32\u02ddC (NP) were conducted,\nresulting in 100total stimulations per participant, administered randomly. After reaching\nthe target temperature for each induction, the data was subsequently segmented into 5.5s\nintervals starting at 1s. This segmentation generated 8,700samples, each 5.5slong, evenly\ndistributed across the five pain intensity classes for each modality, encompassing all 87sub-136 CHAPTER 7. GENERAL-PURPOSE MODELS\njects. The video recordings were captured at a frame rate of 25frames per second (FPS),\nwhile the biosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\nAI4Pain Dataset: The AI4Pain Grand Challenge 2024 dataset is a recent addition tailored\nfor advanced pain recognition tasks using fNIRS and facial video data. Sixty-five pn=65q\nvolunteers participated, including 23females, with ages ranging from 17to52years. The\ndataset additionally includes physiological signals like photoplethysmography (PPG), elec-\ntrodermal activity (EDA), and respiration (RESP), though these are not currently publicly\navailable. The dataset is segmented into three parts: training ( 41volunteers), validation\n(12volunteers), and testing ( 12volunteers). The data collection setup for this dataset in-\nvolves comprehensive fNIRS and video recording to capture both brain activity and facial\nexpressions. The fNIRS recordings were conducted using an Artinis device (Artinis Medical\nSystems, Gelderland, the Netherlands), measuring fluctuations in oxygenated haemoglobin\n(HBO2) and deoxygenated haemoglobin (HHB) concentrations (in \u00b5mol/L). This fNIRS sys-\ntem uses 24channels to cover the prefrontal cortex with optodes ( 10sources and 8detectors)\nplaced 30mm apart. It emits near-infrared light at wavelengths of 760nm and 840nm and\nsamples at a rate of 50Hz. The video data is recorded using a Logitech StreamCam at a\nframe rate of 30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low\nPain, and High Pain . It includes 65instances of No Pain (lasting 60s each), 780instances\nofLow Pain (lasting 10s each), and 780instances of High Pain (also lasting 10s each). The\nNo Pain instances represent baseline data. In contrast, Low Pain andHigh Pain are derived\nfrom the pain tolerance tests, capturing subtle and significant changes in neurological and\nbehavioral responses via fNIRS and video data.\n7.3.2 Experimental Evaluation & Results\nThis research devised various testing scenarios, including unimodal and multimodal settings,\nto assess the effectiveness of the proposed foundational model. The aim is to utilize a variety\nof behavioral and physiological modalities to ascertain the capability of PainFormer to gener-\nate and provide high-quality embeddings for pain assessment. The experimental framework\nutilizes a comprehensive set of modalities, encompassing RGB, synthetic thermal imaging,\ndepth videos, and physiological measurements such as ECG, EMG, GSR, and fNIRS, with\nwaveform and spectrogram representations. Additionally, specific pipelines were tailored to\nsingle modalities or their combinations based on each integration need. This adaptability\nis a cornerstone of our approach, as different pipelines might be required depending on the\nspecific demands, data availability, or intended application. We aim to offer robust feature\nrepresentations for any given input modality and excel in performance across all modalities\nand testing scenarios. Figure 7.7 displays a high-level view of the proposed framework. Note7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 137\nthat all images, including video frames and biosignal visual representations, are standardized\nto a resolution of 224\u02c6224pixels.\nThis research employed Part A of the BioVid dataset, focusing on pain assessment in\nbinary terms, differentiating between No Pain (NP) and Very Severe Pain (P4). Validation\nwas conducted using the leave-one-subject-out (LOSO) cross-validation technique. In the\ncase of AI4Pain , a multilevel classification scheme was applied, categorizing pain into three\nlevels: No Pain ,Low Pain , and High Pain . The challenge organizers\u2019 hold-out method\n(training, validation, testing) was utilized for validation. For both datasets, the evaluation\nmetrics included accuracy, recall (sensitivity), and F1 score. It is also important to mention\nthat all experiments used a deterministic approach, ensuring no influence from random ini-\ntializations. This practice assures that any differences in performance observed are strictly\nattributable to specific optimization parameters, modalities, and other controlled variations\nrather than any random factors.\nBioVid\nNumerous experiments were performed using the BioVid dataset. Beyond the original RGB\nvideos, synthetic thermal and depth videos were developed to provide additional visual con-\ntexts, as detailed in 7.3.1. As specified in 7.3.1, four distinct representations of ECGs, EMGs,\nand GSRs were assessed for biosignals. Combinations of these representations were also ex-\nplored.\nVideo: For behavioral modalities within the BioVid dataset, PainFormer generates an em-\nbedding of dimension d\u201c160for each video frame. These embeddings are concatenated to\ncreate a comprehensive representation of each video:\nVD\u201crd1}d2}\u00a8\u00a8\u00a8} dms, DPRN1, (7.21)\nwhere mrepresents the number of frames per video, and N1is the dimensionality of the total\nembedding, computed as m\u02c6d\u00d1138\u02c6160\u201c22,080. This unified embedding is fed\ninto the Embedding-Mixer for final pain assessment. Starting with a training duration of 200\nepochs and using augmentation only on RGB videos, an accuracy of 71.83% and a recall\nof74.52% were recorded. Thermal and depth videos achieved accuracies of 69.83% and\n69.00%, respectively. When the training was extended to 300epochs with intensified aug-\nmentations and incorporating Label Smoothing for regularization, RGB accuracy improved\nto72.50%, although recall decreased slightly by 0.46%. Thermal modality performance de-\ncreased overall, highlighting its sensitivity to augmentations and regularization techniques.\nConversely, depth modalities responded well to these changes, showing improved metrics\nwith an accuracy of 70.08%, a recall of 71.27%, and an F1 score of 69.63%. In the final138 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.18: Results utilizing the video modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1RGB200 0.5 0.5 |10-20| 0.0 0.0 71.83 74.52 70.29\n300 0.7 0.7 |15-20| 0.1 0.0 72.50 74.06 70.93\n600 0.5 0.5 |15-20| 0.1 0.5 76.29 77.56 75.56Thermal200 0.5 0.5 |10-20| 0.0 0.0 69.83 71.51 69.17\n300 0.7 0.7 |15-20| 0.1 0.0 68.83 69.77 68.41\n600 0.5 0.5 |15-20| 0.1 0.5 71.55 72.83 71.12Depth200 0.5 0.5 |10-20| 0.0 0.0 69.00 69.44 67.94\n300 0.7 0.7 |15-20| 0.1 0.0 70.08 71.27 69.63\n600 0.5 0.5 |15-20| 0.1 0.5 71.67 72.84 71.26\nLS: Label Smoothing For Augmentation and Regularization, the number denotes the probability of application,\nwhile in Masking , the number in | |indicates the size of the mask applied.\nexperimental phase, training was extended to 600epochs, employing lighter augmentations\nat a0.7probability, alongside 0.1Label Smoothing and0.5DropOut . This regimen resulted\nin the highest performance for RGB videos, achieving an accuracy of 76.29% and a recall\nof77.56%. The F1 score also significantly increased, rising over 5%to75.56%. Similar pat-\nterns were observed for the thermal and depth videos in this final experimental setup, albeit\nwith minor improvements. Accuracy for thermal videos was 71.55% and for depth videos\n71.67%, with recall rates closely matching at 72.83% and72.84%, respectively. These find-\nings demonstrate consistent enhancement across all visual modalities with refined training\nparameters and extended training durations. Table 7.18 consolidates these experimental out-\ncomes, indicating that the RGB modality consistently surpasses others, while the thermal\nand depth modalities show comparable performance levels. Moreover, although thermal and\ndepth enhancements are modest, they suggest a plateau in potential performance increases.\nECG: The training configuration used for the video data was similarly applied to the ECG\nsignals. As previously indicated, four visual representations were utilized. Each represen-\ntation corresponds to an image dimension of 224\u02c6224pixels, from which embeddings of\ndimensionality d\u201c160are extracted and then inputted into the Embedding-Mixer . Starting\nwith200epochs and employing minimal augmentation without regularization, the waveform\nrepresentation reached an accuracy of 69.58%, with recall and F1 scores of 72.67% and\n68.10%, respectively. The spectrogram-angle had lower performance in all metrics, achiev-\ning an accuracy of 65.58%. Meanwhile, the spectrogram-phase showed better accuracy,7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 139\nTable 7.19: Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 69.58 72.67 68.10\n300 0.7 0.7 |15-20| 0.1 0.0 71.08 72.74 70.22\n600 0.5 0.5 |15-20| 0.1 0.5 73.36 74.75 72.52Angle200 0.5 0.5 |10-20| 0.0 0.0 65.58 66.68 64.89\n300 0.7 0.7 |15-20| 0.1 0.0 66.33 68.22 65.22\n600 0.5 0.5 |15-20| 0.1 0.5 68.25 71.24 66.99Phase200 0.5 0.5 |10-20| 0.0 0.0 70.08 71.54 69.40\n300 0.7 0.7 |15-20| 0.1 0.0 72.33 73.73 71.69\n600 0.5 0.5 |15-20| 0.1 0.5 72.70 74.19 72.14PSD200 0.5 0.5 |10-20| 0.0 0.0 71.08 73.13 70.19\n300 0.7 0.7 |15-20| 0.1 0.0 71.50 73.14 70.18\n600 0.5 0.5 |15-20| 0.1 0.5 75.49 77.15 74.90\nsurpassing the prior two by 0.5%and4.5%, respectively. The spectrogram-PSD achieved\nthe highest results, recording 71.08% accuracy, 73.13% recall, and 70.19% F1 score. Fur-\nther improvements were seen in the 300-epoch configuration across all visual representations\nand metrics. In the ultimate experimental setup extending to 600epochs, enhancements were\nnoted universally, but the spectrogram-PSD showed the most considerable gains, nearly 4%,\nachieving 75.49% accuracy, 77.15% recall, and 74.90% F1 score. This indicates that integrat-\ning amplitude and frequency information, as the PSD representation provides, is particularly\neffective and valuable for analyzing ECG signals. Table 7.19 documents the outcomes for\nthe ECG modality.\nEMG: For EMG signals, the initial training configuration of 200 epochs demonstrated\ncomparable accuracy across the waveform ,spectrogram-phase , and spectrogram-PSD rep-\nresentations, recording scores of 68.75%,68.33%, and 69.25% respectively. However, the\nspectrogram-angle representation underperformed with an accuracy of 66.42%, mirroring\nits lower performance in the ECG modality. In subsequent training sessions with increased\nepochs and enhanced augmentation and regularization, the spectrogram-angle representa-\ntion showed a notable decline in performance across all metrics. Despite some marginal\nimprovements in the 300-epoch configuration, it still trailed behind its initial results, posting\nan accuracy of 65.32%, with recall and F1 scores of 68.15% and63.17%, respectively. This\npattern suggests that the angle representation, which lacks phase unwrapping, is less effec-140 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.20: Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 68.75 70.55 67.93\n300 0.7 0.7 |15-20| 0.1 0.0 69.83 72.52 68.68\n600 0.5 0.5 |15-20| 0.1 0.5 72.07 73.64 71.48Angle200 0.5 0.5 |10-20| 0.0 0.0 66.42 68.57 65.26\n300 0.7 0.7 |15-20| 0.1 0.0 63.92 66.33 62.67\n600 0.5 0.5 |15-20| 0.1 0.5 65.32 68.15 63.77Phase200 0.5 0.5 |10-20| 0.0 0.0 68.33 69.75 67.68\n300 0.7 0.7 |15-20| 0.1 0.0 68.58 70.00 67.97\n600 0.5 0.5 |15-20| 0.1 0.5 69.37 71.17 68.66PSD200 0.5 0.5 |10-20| 0.0 0.0 69.25 70.38 68.84\n300 0.7 0.7 |15-20| 0.1 0.0 69.67 71.06 69.12\n600 0.5 0.5 |15-20| 0.1 0.5 72.10 72.82 71.82\ntive for pain assessment tasks in EMG signals. Conversely, the other visual representations\ndemonstrated consistent improvements in each training configuration. The spectrogram-\nPSD achieved the highest accuracy at 72.10% and an F1 score of 71.82%. The waveform\nrepresentation obtained the highest recall at 73.64%. These results are presented in Table\n7.20 for the EMG modality.\nGSR: For the GSR modality, distinct performance variations among the four representa-\ntions are evident. The waveform -based representations significantly outshine the others,\nstarting with an initial accuracy of 87.75% in the 200-epoch configuration, which is over\n14% higher than other metrics. With extended training sessions, a modest improvement is\nnoted across all representations, indicating that the GSR modality might have reached its\nmaximum potential performance. Among the spectrograms, the spectrogram-phase proves\nto be the most informative, culminating in final accuracy, recall, and F1 scores of 76.41%,\n77.23%, and 76.47%, respectively. The waveform representation emerges as the most effec-\ntive, achieving the highest metrics with an accuracy of 88.99%, recall of 89.55%, and an\nF1 score of 88.88%. The distinct performance of these representations can be related to the\ninherent characteristics of the GSR signal. As depicted in Fig. 7.7, GSR typically presented\nas a smooth curve with gradual slopes, indicative of slow and steady changes in skin conduc-\ntivity due to variations in sweat gland activity triggered by stress or arousal. In comparison,\nEMG signals are marked by sharp spikes and erratic fluctuations, reflecting rapid electrical\nactivities from skeletal muscle contractions. On the other hand, ECG signals display distinct7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 141\nTable 7.21: Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 87.75 88.68 87.56\n300 0.7 0.7 |15-20| 0.1 0.0 88.50 89.16 88.34\n600 0.5 0.5 |15-20| 0.1 0.5 88.99 89.55 88.88Angle200 0.5 0.5 |10-20| 0.0 0.0 73.67 75.00 73.26\n300 0.7 0.7 |15-20| 0.1 0.0 73.08 74.60 72.66\n600 0.5 0.5 |15-20| 0.1 0.5 73.24 75.02 72.83Phase200 0.5 0.5 |10-20| 0.0 0.0 75.17 76.13 74.79\n300 0.7 0.7 |15-20| 0.1 0.0 75.92 76.60 75.57\n600 0.5 0.5 |15-20| 0.1 0.5 76.41 77.23 76.47PSD200 0.5 0.5 |10-20| 0.0 0.0 72.83 73.91 72.34\n300 0.7 0.7 |15-20| 0.1 0.0 73.08 73.96 72.68\n600 0.5 0.5 |15-20| 0.1 0.5 73.96 74.81 73.50\ncyclical patterns, including the P and T waves and the QRS complex. These observations\nimply that the simpler patterns in GSR are not as well suited for spectral and frequency do-\nmain analyses, which are more effectively captured by spectrograms. However, waveform\nrepresentations excel in capturing critical physiological data from GSR signals, outperform-\ning all other modalities and visual representations due to their ability to effectively represent\nthe essential dynamics of GSR activity. The results for the GSR modality are summarized in\nTable 7.21.\nFusion: Various fusion techniques were tested to evaluate whether combining different rep-\nresentations or modalities could enhance performance. In this research, using inputs from\nthe same sensor type, such as RGB with depth-estimation videos or ECG waveforms with\nECG spectrogram-PSD , was considered an unimodal fusion approach. On the other hand,\ncombining inputs from different sensor types, like GSR with EMG, was treated as a multi-\nmodal fusion. Three primary methods of fusion were explored: feature fusion and decision\nfusion. Feature fusion includes strategies such as addition, where embeddings from various\ninputs are summed before progressing to the following processing stage, and concatenation,\nwhich aligns them along the y-axis . Decision fusion, meanwhile, involves processing each\nembedding through the Embedding-Mixer , which then aggregates the predictions from each\ninput to generate a final decision. All related experiments were conducted under the previ-\nously detailed 600-epoch training configuration, with results compiled in Table 7.22.\nIn video modality fusion, we assessed combinations of RGB with thermal, RGB with142 CHAPTER 7. GENERAL-PURPOSE MODELS\ndepth, and thermal with depth, plus a three-input amalgamation of RGB, thermal, and depth.\nThe RGB and thermal blend underperformed compared to RGB alone, with the best perfor-\nmance ( 75.66% accuracy) achieved through decision fusion. The RGB and depth combina-\ntion similarly yielded optimal results through decision fusion, achieving 75.53% accuracy\nbut falling short of RGB-only performance. Notably, merging thermal and depth videos im-\nproved upon using depth alone, particularly via decision fusion, which attained a 73.02%\naccuracy rate. The combination of RGB, thermal, and depth inputs was the sole group that\noutperformed the standalone RGB setup, with decision fusion delivering the highest metrics:\n76.55% accuracy, 77.91% recall, and 76.11% F1 score, indicating marginal improvements\nacross all measures. Decision fusion consistently outperformed the addition method in all\nvideo-based experiments.\nFor biosignals, experiments focused on ECG and EMG using the waveform and the rep-\nresentations of spectrogram-PSD . No fusion experiments were conducted for GSR due to\nthe waveform\u2019s dominance in performance. For ECG, all fusion methods were less effective\nthan the spectrogram-PSD alone, except for the addition method, which slightly improved\nrecall by 0.21%. EMG results were enhanced by all fusion techniques, with concatenation\nproving to be the most beneficial, leading to increases in accuracy, recall, and F1 score by\n0.74%,0.36%, and 0.64%, respectively.\nPhysiological and behavioral modalities were integrated into our multimodal setup, com-\nbining GSR signals with RGB, synthetic thermal, and estimated depth videos. The GSR\u2019s\nwaveform representation and video features, described in 7.21, were merged into a unified\nvector of dimension 22,080. This vector was then processed through the Video-Encoder into\na smaller space of 40. The resulting combined vector of 160`40\u201c200dimensions was\nformed by concatenating the GSR and video embeddings, represented as:\nMh\u201cGd}Enc\u201c\npVRGB\nD`VThermal\nD`VDepth\nDq\u2030\n, hPRN2, (7.22)\nwhere Gdenotes the GSR embedding and Mthe fused vector with N2equal to 200. This\napproach, visualized in Fig. 7.7 (bottom right), achieved the highest performance in the\nstudy, with accuracy, recall, and F1 scores of 89.08%,89.88%, and 88.87%, respectively.\nThis method slightly surpassed the performance of GSR used independently, especially in\naccuracy and recall.\nAI4Pain\nIn the AI4Pain dataset, experiments were conducted utilizing both unimodal and multimodal\napproaches. The original RGB videos were employed for the behavioral modality, while\nwaveforms from the fNIRS\u2019s HBO2 channels were used for the physiological modality. It\nis important to note that out of the 24available HBO2 channels, 2were excluded due to7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 143\nTable 7.22: Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and F1 score.\nModality Input FusionMetric\nAcc Rec F1\nVideoRGB, ThermalAdd 75.09\n-1.2076.97\n-0.5973.98\n-1.58\nDF 75.66\n-0.6377.23\n-0.3375.08\n-0.48\nRGB, DepthAdd 74.93\n-1.3676.41\n-1.1573.38\n-2.18\nDF 75.53\n-0.7677.18\n-0.3875.00\n-0.56\nThermal, DepthAdd 71.44\n-0.2373.15\n+0.3170.73\n-0.50\nDF 73.02\n+1.3574.46\n+1.6272.59\n+1.33\nRGB, Thermal, DepthAdd 76.26\n-0.0377.70\n+0.1475.78\n+0.22\nDF 76.55\n+0.2677.91\n+0.3576.11\n+0.55\nECG Wave, PSDAdd 75.43\n-0.0677.36\n+0.2174.75\n-0.15\nConcat 74.74\n-0.7576.77\n-0.3874.00\n-0.90\nEMG Wave, PSDAdd 72.79\n+0.6974.15\n+0.5172.28\n+0.46\nConcat 72.84\n+0.7474.00\n+0.3672.46\n+0.64\nVideo, GSRRGB, Thermal, Depth,\nWaveAdd &\nConcat89.08\n+0.0989.88\n+0.3388.87\n-0.01\n\u02da: All experiments follow the augmentation and regularization settings for the 600 epoch con-\nfiguration outlined in the unimodal experiments. + and - indicate an increase or decrease in\nperformance, respectively, compared to the best unimodal input approach. DF: Decision Fusion\nAdd: Addition Concat: Concatenation\nmalfunctions. Table 7.23 presents the corresponding results.\nVideo: Similar to 7.3.2, an embedding of dimension d\u201c160is extracted for every frame\nin the AI4Pain dataset. However, in this instance, the extracted embeddings are aggregated\ninto a unified vector:\nVd\u201crd1`d2`\u00a8\u00a8\u00a8` dms, dPRN3, (7.23)\nwhere mrepresents the number of frames in a video, and N3is the dimensionality of the\nunified embedding, set at 160. After processing the embedding through the Embedding-\nMixer and employing the same 600-epoch training configuration as used in prior experi-\nments, this setup achieved an accuracy of 49.77%, with recall and F1 scores of 50.11% and\n49.77%, respectively. Increasing the DropOut rate to 0.3improved the accuracy and F1\nscores to 51.39% and51.31%. Further elevating the DropOut rate to 0.8enhanced the recall\nto52.74%.144 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.23: Results on the validation set of AI4Pain dataset, multilevel classification task, re-\nported on accuracy, recall and F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Video600 0.5 0.5 |15-20| 0.1 0.5 49.77 50.11 49.77\n600 0.5 0.5 |15-20| 0.1 0.3 51.39 51.50 51.31\n600 0.5 0.5 |15-20| 0.1 0.8 48.38 52.74 46.69fNIRS600 0.5 0.5 |15-20| 0.1 0.5 43.06 42.80 42.07\n600 0.5 0.5 |15-20| 0.1 0.3 44.44 45.55 43.74\n600 0.4 0.4 |15-20| 0.1 0.1 43.06 44.18 42.44Fusion600 0.5 0.5 |15-20| 0.1 0.5 50.00 51.01 48.54\n600 0.1 0.1 |15-20| 0.1 0.8 50.23 50.25 50.24\n600 0.4 0.4 |15-20| 0.1 0.6 51.85 51.87 51.35\nFusion: the Addition method of the modalities applied\nfNIRS: For the fNIRS modality, embeddings were aggregated across the 22HBO2 chan-\nnels to produce a feature representation of Od\u201c160. The 600-epoch training setup initially\nyielded 43.06% accuracy, 42.80% recall, and 42.07% F1 score. By increasing the DropOut\nrate to 0.3, peak performance metrics of 44.44% accuracy, 45.55% recall, and 43.74% F1\nscore were achieved.\nFusion: For the fusion of video and fNIRS data, the following aggregation approach was\nutilized:\nFd\u201cVd`Od, dPRN3, (7.24)\nwhere Fdrepresents the combined feature representation. Starting with the same 600-epoch\ntraining configuration, the initial results were 50.00% accuracy, 51.01% recall, and 48.54%\nF1 score. Increasing the DropOut rate to 0.8slightly improved the accuracy and F1 score\nby0.23% and1.7%, respectively, though recall decreased by 0.75%. The optimal DropOut\nsetting of 0.6achieved peak performances of 51.85% accuracy, 51.57% recall, and 51.35%\nF1 score.\n7.3.3 Comparison with existing methods\nTo evaluate PainFormer , we compared it against studies from the literature that utilized the\nBioVid dataset ( Part A ), included all available subjects ( 87), conducted the same task, ad-\nhered to the leave-one-subject-out (LOSO) validation protocol, and reported accuracy met-\nrics. For the AI4Pain dataset, our comparisons were made with studies that strictly followed\nthe evaluation guidelines outlined in the corresponding challenge.7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 145\nInBioVid , the proposed approach using RGB, thermal, and depth video inputs is among\nthe top performers in video-based studies, achieving an accuracy of 76.55%. This surpasses\nall methods utilizing hand-crafted features, as referenced in [284,295,339] and outperforms\nmost deep learning-based methods cited in [37, 180, 217, 219]. Exceptions include results\nfrom [38] at 77.10% and [340] at 78.90%, with the study in [269] achieving 77.50% using a\n3D CNN approach, which, when combined with pseudo heart rate data extracted from videos,\nreached the highest reported result of 88.10%. These results are documented in Table 7.24.\nRegarding biosignals, in ECG-based studies, PainFormer achieved the highest accuracy\nin the literature at 75.49% using the spectrogram-PSD representation, significantly outper-\nforming subsequent studies [36,38] by over 6%and8%, respectively. In EMG-based studies\nutilizing waveform andspectrogram-PSD representations, we achieved a 72.84% accuracy,\nsignificantly exceeding the nearest study [339] at 63.10%. For GSR-based studies, utiliz-\ning solely waveform representation led to the highest performance with an 88.99% accuracy.\nStudies using raw biosignals instead of extracting domain-specific features generally exhib-\nited better results, with the second [341] and third [342] ranked studies achieving 85.56%\nand84.80% accuracy, respectively. Table 7.25 presents these biosignal results.\nIn multimodal scenarios, our approach combining video inputs and GSR achieved the\nhighest reported accuracy of 89.08% (refer to Table 7.26). Notably, with one exception [38],\nall studies incorporated the GSR signal. GSR is consistently recognized as the most effective\nmodality for pain assessment, with the second-highest-performing study [343] using a GSR\nand ECG combination achieving 87.06%. A study including videos, ECG, EMG, and GSR\n[344] reached 86.00% accuracy.\nFor the AI4Pain dataset, PainFormer achieved a 53.67% accuracy using the RGB video\nmodality, outperforming [345] at 49.00% but falling behind [346] at 55.00% utilizing a\ntransformer-based masked autoencoder. Using only fNIRS, an accuracy of 52.60% was\nachieved. In a multimodal approach combining videos and waveform representations, an\naccuracy of 55.69% was attained, surpassing [40] by over 9%and establishing the highest\nperformance on this dataset to date.\n7.3.4 Interpretation\nEnhancing the interpretability of models is crucial for their acceptance and effective integra-\ntion into clinical settings. In this study, PainFormer generates attention maps, as illustrated\nin Fig. 7.8. The weights from the \u201cStage 4\u201d self-attention heads are applied by interpolating\nthem onto the input images, enabling visualization of the model\u2019s focal areas.\nIn Fig. 7.8(a), (1strow), we display examples from the RGB, thermal, and depth modali-\nties, and in Fig. 7.8(a), (2ndrow), the corresponding attention maps are presented. Observa-\ntions indicate that the model primarily focuses on the glabella region (the area between the146 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.24: Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task and\nLOSO validation.\nStudyMethod\nAcc%\nFeatures ML\n[217] raw SLSTM 61.70\n[219] raw 2D CNN, biLSTM 69.25\n[295] optical flow RF 70.20\n[180] raw 2D CNN 71.00\n[39] raw\u0015Vision-MLP 71.03\n[135]:raw 2D CNN 71.30\n[267] facial landmarks, 3D distances RF 71.60\n[296] facial 3D distances Deep RF 72.10\n[296] facial action descriptors Deep RF 72.40\n[297] facial landmarks, 3D distances RF 72.70\n[284] fiducial points GNN 73.20\n[37] raw Transformer 73.28\n[211]:raw 2D CNN, GRU 73.90\n[339] facial landmarks, head pose RF 76.60\n[38] raw Transformer 77.10\n[269] raw 3D CNN, 77.50\n[340] raw, rPPG\u27263D CNN 78.90\n[269] raw, heart rate\u26053D CNN 88.10\nOur raw\u273fTransformer 76.55\n:: reimplemented for pain intensity estimation on BioVid by [269] \u0015: RGB, syn-\nthetic thermal videos \u2726: remote photo plethysmography (estimated from videos)\n\u2605: pseudo heart rate gain (estimated from videos) \u273f: RGB-thermal-depth (DF)\nRF: Random Forest GNN: Graph Neural Networks MLP: Multi-Layer Percep-\ntron\neyebrows) in the RGB frame, a key area for facial expressions. Additional focus is observed\non the mental protuberance area (the chin), which is also associated with expressions of pain.\nFor the thermal frame, the model concentrates on areas around the eyes and the sides of\nthe mouth, where brighter colors in the thermal imagery suggest higher temperatures, indi-\ncating that temperature variations rather than facial expressions drive the model\u2019s attention\nin the thermal modality. In the depth frame, the model targets areas showing variations in\ndepth, particularly across the horizontal eye region, with slight attention to the frame\u2019s lower\nleft and right edges, highlighting depth differences in body parts beyond the face, which\nillustrates a nuanced understanding of the model\u2019s representation of depth.\nThe ECG attention maps in Fig. 7.8(b), (top left), primarily emphasizes a distinct R peak\nin the trace\u2019s center. Significant attention is also directed towards the T waves, especially\nthose following the central R peak, highlighting the model\u2019s sensitivity to these elements in\nthe signal. In the EMG attention maps of Fig. 7.8(b), (top right), PainFormer mainly focuses\non the initial and middle sections of the signals. Despite a muscle contraction burst appearing7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 147\nTable 7.25: Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation.\nStudy ModalityMethod\nAcc%\nFeatures ML\n[235] ECG raw 1D CNN 57.04\n[347] ECG domain-specific\u00b8LR 57.40\n[270] ECG domain-specific\u00b8LR 57.69\n[35] ECG domain-specific\u00b8SVM 58.39\n[342] ECG raw 1D CNN, biLSTM 61.20\n[267] ECG domain-specific\u00b8RF 62.00\n[297] ECG domain-specific\u00b8SVM 62.40\n[297] ECG domain-specific\u00b8SVM 62.40\n[348] ECG raw 2D CNN, biLSTM 63.20\n[339] ECG domain-specific\u00b8RF 64.00\n[269] ECG heart rate\u26053D CNN 65.00\n[38] ECG heart rate Transformer 67.04\n[36] ECG domain-specific\u00b8FCN 69.40\nOur ECG raw\u2726Transformer 75.49\n[347] EMG domain-specific\u00b8LR 58.59\n[235] EMG raw 2D CNN 58.65\n[339] EMG domain-specific\u00b8RF 63.10\nOur EMG raw\u2740Transformer 72.84\n[339] GSR domain-specific\u00b8RF 71.90\n[270] GSR domain-specific\u00b8LR 74.21\n[297] GSR domain-specific\u00b8RF 74.40\n[349] GSR domain-specific\u00b8RF 80.40\n[350] GSR domain-specific\u00b8RF 81.90\n[347] GSR domain-specific\u00b8LR 82.36\n[351] GSR domain-specific\u00b8SVM 83.30\n[348] GSR raw 1D CNN, biLSTM 83.60\n[232] GSR domain-specific\u00b8MLP 84.22\n[235] GSR raw 1D CNN 84.57\n[342] GSR raw 1D CNN, biLSTM 84.80\n[341] GSR raw 1D CNN,\nTransformer85.56\nOur GSR raw\u273fTransformer 88.99\n\u00b8: numerous features \u2605: pseudo heart rate gain (estimated from videos) \u2726: PSD\n\u2740: waveform-PSD (Concat) \u273f: waveform SVM: Support Vector Machines LR:\nLogistic Regression\nlater in the sequence, the model exhibits less attention to this portion. This observation may\nbe related to the PainFormer \u2019s pre-training on the Silent-EMG dataset [329], which might\ninfluence its responsiveness to specific sections of the EMG signals.\nFor the GSR signal in Fig. 7.8(b), (bottom left), mild attention is noted at the onset of the\nresponse, marking the start of the conductance increase, with the most intense attention near\nthe peak amplitude, where conductance reaches its maximum level. In the fNIRS signal\nshown in Fig. 7.8(b) (bottom right), the attention map predominantly highlights regions148 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.26: Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation.\nStudy ModalityMethod\nAcc%\nFeatures ML\n[270] ECG, GSR domain-specific\u00b8SVM 72.20\n[267] ECG, EMG, GSR domain-specific\u00b8RF 74.10\n[267] Video1,ECG2, EMG2,\nGSR2facial landmarks1, 3D distances1,\ndomain-specific2\u00b8RF 77.80\n[297] Video1, ECG2, GSR2facial landmarks1, 3D distances1,\ndomain-specific2\u00b8RF 78.90\n[339] Video1, ECG2, EMG2,\nGSR2facial landmarks1, head pose1,\ndomain-specific2RF 80.60\n[38] Video1, ECG2raw1, heart rate2Transformer 82.74\n[350] Video1, ECG2, EMG2,\nGSR2geometric1, appearance1,\ndomain-specific2RF 83.10\n[347] ECG, EMG, GSR domain-specific LR 83.20\n[239] ECG, EMG, GSR domain-specific biLSTM 83.30\n[243] ECG, EMG, GSR raw DDCAE 83.99\n[352] ECG, EMG, GSR raw DDCAE, NN 84.25\n[235] ECG, EMG, GSR raw 2D CNN 84.40\n[353] GSR, ECG domain-specific\u00b8NN 84.58\n[348] Video, GSR raw 2D CNN,\nbiLSTM84.80\n[342] ECG, GSR raw 1D CNN,\nbiLSTM84.80\n[354] ECG, EMG, GSR domain-specific RF 85.70\n[355] ECG, EMG, GSR domain-specific RF 85.80\n[344] Video1, ECG2, EMG2,\nGSR2facial descriptors1, domain-specific2RF 86.00\n[343] GSR, ECG domain-specific\u00b8NN 87.06\nOur Video\u2722, GSR\u273draw Transformer 89.08\n\u2722: RGB-thermal-depth \u273d: waveform \u00b8: numerous features DDCAE: deep denoising convolutional autoencoders\nNN: neural network\nTable 7.27: Comparison of studies on the testing set of AI4Pain dataset.\nStudyModality\nML Acc%\nVideo fNIRS Fusion\n[40] \u2013 \u2013 \u2713 Transformer 46.67\n[345] \u2713 \u2013 \u2013 2D CNN 49.00\n[346] \u2713 \u2013 \u2013 Transformer 55.00\nOur\u2713 \u2013 \u2013\nTransformer53.67\n\u2013 \u2713 \u2013 52.60\n\u2013 \u2013 \u2713 55.697.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 149\naligning with peaks and rapid changes in HbO2 levels. Significant attention is concentrated\nin the map\u2019s left, middle, and right sections, where distinct peaks and dips in the signal are\nobserved, indicating that PainFormer consistently focuses on substantial fluctuations in the\nHbO2 signal, likely associated with pain conditions. Areas with lower or moderate attention\ncorrespond to segments of the time series with stable or minor variations in HbO2, reflecting\nlower levels of brain activation typically associated with mild or no pain responses.\n7.3.5 Discussion\nThis research introduces PainFormer , a foundation model utilizing a vision-transformer ar-\nchitecture tailored for pain assessment across diverse modalities. The model was pre-trained\non14datasets, totaling 10.9million samples, using a multi-task learning framework to en-\nhance its capability in processing behavioral and physiological inputs. PainFormer is com-\nplemented by the Embedding-Mixer for analyzing embeddings and the Video-Encoder for re-\nducing the dimensionality of video embeddings. We tested our model using the BioVid and\nAI4Pain datasets, experimenting with modalities including RGB, synthetic thermal, depth\nvideos, and biosignal representations such as ECG, EMG, GSR, and fNIRS. The evaluation\ndemonstrated that RGB videos provided the highest accuracy among behavioral modalities,\nrecording 76.29% accuracy in the BioVid dataset. Thermal and depth modalities also per-\nformed well, with accuracies of 71.55% and71.67%, respectively. Interestingly, integrat-\ning thermal and depth modalities improved accuracy by 1.35%, suggesting their potential\nto match the efficacy of RGB while addressing privacy concerns associated with direct fa-\ncial recordings. ECG was particularly effective regarding physiological signals, with the\nspectrogram-PSD achieving the highest accuracy at 75.49%. Although combining differ-\nent ECG representations did not enhance performance, EMG signals showed exceptional\naccuracy above 72% when combining waveform andspectrogram-PSD . GSR, known for\nits efficacy in pain assessment, achieved an impressive accuracy of 88.9%using waveform\nrepresentations alone. Our multimodal approach that integrated GSR with RGB, thermal,\nand depth video embeddings led to a notable accuracy of 89.08% in the BioVid dataset, un-\nderscoring the strength of combining multiple modalities. Further, the creation of attention\nmaps revealed PainFormer \u2019s consistent focus on key areas indicative of pain across all tested\nmodalities. This ability highlights the model\u2019s utility in clinical settings, where understand-\ning pain through various indicators is crucial. However, the influence of pre-training on\nspecific areas requires further investigation to ensure the model\u2019s generalizability and accu-\nracy in real-world applications. Our findings place PainFormer at the forefront of current\nmethodologies, achieving state-of-the-art results across the board. While our model excels\nwith video-based and biosignal inputs in unimodal and multimodal settings, continuous ad-\nvancements and further explorations are needed, especially with the newer AI4Pain dataset.150 CHAPTER 7. GENERAL-PURPOSE MODELS\n7.4 Summary\nThis chapter has introduced general-purpose models and pipelines for automatic pain as-\nsessment. Such approaches have attained popularity recently, driven by the development of\nadvanced architectures and the availability of substantial data and computing resources nec-\nessary for training these models. While this combination has led to notable achievements\nin the broader fields of deep learning and AI, its effect on pain research has been virtu-\nally nonexistent. This distinction motivated our exploration of these methodologies in pain\nrecognition tasks. We presented a modality-agnostic method that homogenizes the pipeline\nirrespective of the input type. Our experiments with RGB videos and fNIRs showed promis-\ning results in both unimodal and multimodal settings. However, there is potential for further\nimprovement. We believe increasing the pre-training data for models within a modality-\nagnostic framework could significantly improve performance. It is generally accepted that\nfoundation models deliver superior outcomes. Our study introduced the first foundation\nmodel specifically developed for and applied to pain assessment. The results demonstrated\nthat this approach is highly effective, not only for well-established applications such as lan-\nguage understanding but also for automatic pain assessment. While further investigation\nis necessary, it is essential to acknowledge a fundamental challenge: the generally limited\navailability of pain-related data, which could restrict the effectiveness of these models.7.4. SUMMARY 151\n\u2026\nVideo\n\u2026\nfNIRSaddwaveform \ngeneration embedding \nextraction# frames \naddwaveform \ngeneration embedding \nextraction# channels \n PainViT-1\nPainViT-2\nPain \nAssessmenteFFN\nFully-connectedRELUFully-connectedc\nOutput\nInput\nHead h\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 2\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 1\nK\nQ\nV\nDWConv\nSelf-attention\nSegment\u2026\nConcatenation & Projectiond\nCascaded Attention PainViTLinear ProjectionToken Mixer\nCascaded Attention\nToken Mixer\n1xBlock-1+Token Mixer\nCascaded Attention\nToken Mixer\n3xBlock-2+Token Mixer\nCascaded Attention\nToken Mixer\n4xBlock-3+a\nInput ImageFFN\nDWConvBatchNorm++\nToken-Mixerb\nFFN\nFully-connectedRELUFully-connectedc\nOutput\nInput\nHead h\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 2\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 1\nK\nQ\nV\nDWConv\nSelf-attention\nSegment\u2026\nConcatenation & Projectiond\nCascaded Attention PainViTLinear ProjectionToken Mixer\nCascaded Attention\nToken Mixer\n1xBlock-1+Token Mixer\nCascaded Attention\nToken Mixer\n3xBlock-2+Token Mixer\nCascaded Attention\nToken Mixer\n4xBlock-3+a\nInput ImageFFN\nDWConvBatchNorm++\nToken-Mixerb\n\u2026\nVideo\n\u2026\nfNIRSaddwaveform \ngeneration embedding \nextraction# frames \naddwaveform \ngeneration embedding \nextraction# channels \nPainViT-1\nPainViT-2\nPain \nAssessment\nFigure 7.1: PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer having\nvarying depths, showcasing how token resolution decreases at each stage; (b)Com-\nposition of the Token-Mixer module, featuring elements like depthwise convolu-\ntion (DWConv) and batch normalization; (c)Architecture of the Feed-Forward\nNetwork (FFN) within the Token-Mixer ;(d)The Cascaded Attention mechanism\nimplemented across multiple heads, illustrating how outputs from preceding heads\nare incorporated to refine the self-attention process, culminating in the final out-\nput projection; (e)Configuration of the proposed multimodal pipeline, employing\nvideos and fNIRS. The embeddings from PainViT\u20131 are represented as waveform\ndiagrams, which are merged into a single diagram that illustrates both modalities\nbefore entering PainViT\u20132 for final pain evaluation.152 CHAPTER 7. GENERAL-PURPOSE MODELS\nabc\nFigure 7.2: Waveform illustrations for various data types: (a)original fNIRS signal, (b)video\nembedding derived from PainViT\u20131 , and (c)fNIRS embedding obtained from\nPainViT\u20131 .\nabc\nFigure 7.3: Attention maps from the PainViT\u20132 .7.4. SUMMARY 153\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nEmbedding-Mixerd\n++++++\nx2\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nVideo-Encodere\n++\nx1MLP-3\nFully-connectedGeGLUFully-connectedhMLP-1\nFully-connectedGELUFully-connected\nDWConvf\nLayer Norm\nLayer NormMLP-2\nMHSA x(2-4-10-16)\nSelf-Attention Layer+\n+c\nLayer Norm\nLayer NormMLP-1\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+ba\nPainFormerInput ImagePosition Embedding\nLinear ProjectionStage-1x2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2x2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3x12 Self-Attention LayerDownsamplerStage-4x3 Self-Attention LayerDownsampler\nMLP-2\nFully-connectedGELUFully-connectedg\nFigure 7.4: Overview of primary models and their components outlined in this research: (a)\nPainFormer is structured hierarchically into four stages, incorporating Spectral\nandSelf-Attention Layers to extract embeddings from the inputs; (b)The Spec-\ntral Layer , a key element of PainFormer , uses FFT to analyze frequency-specific\ndata along with a learnable filter Kto highlight critical frequencies; (c)TheSelf-\nAttention Layer , crucial for PainFormer , enables parallel processing of features and\ntheir interconnections; (d)TheEmbedding-Mixer , employing both cross and self-\nattention mechanisms, functions as the component for the final classification of\nembeddings in pain assessment; (e)TheVideo-Encoder , designed for compact and\nefficient encoding, compresses video data into a reduced dimensional form; (f)The\nMLP-1 is part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)The MLP-3 configuration is integrated into the Embedding-Mixer and\nVideo-Encoder .\nabc\nabcd\na\nb\nc\nd\nFigure 7.5: Examples of different vision modalities in frame samples: (a)RGB frame, (b)\nsynthetic thermal frame, and (c)depth estimation frame.154 CHAPTER 7. GENERAL-PURPOSE MODELS\na\nb\nc\nd\nFigure 7.6: Examples of different visual representations for biosignals: (a)waveform ,(b)\nspectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD .\nFusion N\nEmbedding-Mixer\nadd\nadd\nadd\nVideo-Encoder\nconcatFusion 2DF\n Embedding-Mixer\n add\nadd\nadd\nFusion 1\nadd\nEmbedding-Mixer\naddFusion Pipelines\nadd\nEmbedding-Mixer Modality 1\nconcat\nModality 2\n Embedding-MixerUnimodal Pipelines\nPainFormer\n\u2026Depth\n\u2026Thermal\n\u2026\nRGB\nECG\nEMG\nGSR\n\u2026\nfNIRS\nEmbeddingsadd:      addition\nconcat: concatenation\nDF:      decision fusion\nFusion N\nEmbedding-Mixer\nadd\nadd\nadd\nVideo-Encoder\nconcat\nadd\nFusion 1\nadd\n Embedding-Mixer\naddMultimodal Pipelines\nadd\nEmbedding-MixerModality 1\nconcat\nModality 2\n Embedding-Mixer\nadd\nEmbedding-Mixer Modality NUnimodal Pipelines\nPainFormer\n\u2026Depth\n\u2026Thermal\n\u2026\nRGB\nECG\nEMG\nGSR\n\u2026\nfNIRS\nEmbeddings\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nEmbedding-Mixerd\n++++++\nx2\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nVideo-Encodere\n++\nx1MLP-3\nFully-connectedGeGLUFully-connectedhMLP-1\nFully-connectedGELUFully-connected\nDWConvf\nLayer Norm\nLayer NormMLP-2\nMHSA x(2-4-10-16)\nSelf-Attention Layer+\n+c\nLayer Norm\nLayer NormMLP-1\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+ba\nPainFormerInput ImagePosition Embedding\nLinear ProjectionStage-1x2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2x2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3x12 Self-Attention LayerDownsamplerStage-4x3 Self-Attention LayerDownsampler\nMLP-2\nFully-connectedGELUFully-connectedg\nLayer Norm\nLayer Norm\nCross-AttentionMLP-1\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-1\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-1\nx8\nEmbedding-Mixerb\n++++++\nx2\nLayer Norm\nLayer Norm\nCross-AttentionMLP-1\nx1\nVideo-Encoderc\n++\nx1MLP-3\nFully-connectedGELUFully-connectedhMLP-1\nFully-connectedGeGLUFully-connectedf\nMLP-2\nFully-connectedGELUFully-connected\nDWConvg\nLayer Norm\nLayer NormMLP-3\nMHSA\nSelf-Attention Layer+\n+e\nLayer Norm\nLayer NormMLP-2\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+d a\nPainFormerInput ImagePosition Embedding\nLinear ProjectionStage-1\nx2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2\nx2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3 x12 Self-Attention LayerDownsamplerStage-4 x3 Self-Attention LayerDownsampler\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nVideo-Encoderf\n++\nx1\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nEmbedding-Mixerf\n++++++\nx2MLP-2\nFully-connectedGELUFully-connectede\nMLP-3\nFully-connectedGeGLUFully-connectedgMLP-1\nFully-connectedGELUFully-connected\nDWConvc\nLayer Norm\nLayer NormMLP-2\nMHSA\nSelf-Attention Layer+\n+d\nLayer Norm\nLayer NormMLP-1\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+ba\nPainFormerInput ImagePotition Embedding\nLinear ProjectionStage-1\nx2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2\nx2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3 x12 Self-Attention LayerDownsamplerStage-4 x3 Self-Attention LayerDownsamplerFFN\nFully-connectedRELUFully-connectedc\nOutput\nInput\nHead h\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 2\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 1\nK\nQ\nV\nDWConv\nSelf-attention\nSegment\u2026\nConcatenation & Projectiond\nCascaded Attention PainViTLinear ProjectionToken Mixer\nCascaded Attention\nToken Mixer\n1xBlock-1+Token Mixer\nCascaded Attention\nToken Mixer\n3xBlock-2+Token Mixer\nCascaded Attention\nToken Mixer\n4xBlock-3+a\nInput ImageFFN\nDWConvBatchNorm++\nToken-Mixerb\n\u2026\nVideo\n\u2026\nfNIRSaddwaveform \ngeneration embedding \nextraction# frames \naddwaveform \ngeneration embedding \nextraction# channels \n PainViT-1\nPainViT-2\nPain \nAssessmente\nFigure 7.7: An overview of the presented framework. PainFormer , the foundational model,\nexcels in deriving high-quality embeddings from a diverse array of behavioral and\nphysiological modalities. The evaluation of RGB, thermal, and depth videos, along-\nside various representations of ECG, EMG, GSR, and fNIRS such as waveforms\nand spectrograms, underscores the rich information captured within these embed-\ndings. Leveraging the embeddings from PainFormer facilitates the creation of var-\nious and diverse unimodal and multimodal pipelines designed for the pain assess-\nment task. Each pipeline can be customized to suit the specific modalities involved,\ndataset characteristics, and the demands of the intended application or clinical set-\nting. Our assessments included the development and implementation of several\npipelines in both unimodal and multimodal contexts, achieving leading-edge re-\nsults across various modalities and data representations.7.4. SUMMARY 155\na\nb\nFigure 7.8: Attention maps from the PainFormer :(a)(1strow) frames from RGB, thermal, and\ndepth video modalities; (a)(2ndrow) corresponding attention maps; (b)(1strow)\nattention maps for ECG and EMG; (b)(2ndrow) attention maps for EDA and fNIRS\nmodalities.156Chapter 8\nConclusions, Perspectives and Future Work\nContents\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n8.1 Summary of Thesis Achievements\nThe primary objective of this thesis was to explore and improve methods for automatic pain\nassessment. We developed innovative methods that either improved assessment accuracy\nor introduced new approaches, potentially paving the way for advanced methodologies in\nthe future. Additionally, this thesis aimed to integrate ideas and insights from psychology,\nbiology, and nursing, translating them into engineering concepts.\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment\nChapter 3 presented a comprehensive review of deep learning-based approaches in the field.\nThis systematic review, conducted at the beginning of this Ph.D. project, laid the groundwork\nfor our understanding of the domain. We believe that this foundational work will continue\nto benefit other researchers interested in pain assessment from a machine learning and AI\nperspective. In addition to documenting existing approaches, this work identified emerging\ntrends and suggested potential improvements, offering valuable insights for future research.\n157158 CHAPTER 8. CONCLUSIONS, PERSPECTIVES AND FUTURE WORK\n8.1.2 Insights from Gender and Age Analysis\nIn Chapter 4, we explored the impact of demographic factors on an automatic pain assess-\nment pipeline, focusing specifically on gender and age. For decades, it has been recognized\nthat these factors significantly affect pain expression, sensation, and perception in diverse\nand intriguing ways. Our study utilized ECG signals to explore variations in pain sensation\namong people. Our findings confirmed significant differences between males and females,\nwith the latter group exhibiting higher sensitivity\u2014a phenomenon well-documented in pain\nliterature from psychological and biological perspectives. Moreover, substantial variations\nwere observed across different age groups. A critical discovery was that pain sensation\ntends to decrease with age. This observation is particularly concerning as it implies that\nolder individuals might sustain injuries without adequate perception of pain due to neurolog-\nical reasons, potentially leading to further harm. To our knowledge, such explorations and\nfindings have not been previously addressed in automatic pain assessment. We expect our re-\nsearch will inspire more studies into these phenomena from computational and engineering\nperspectives.\n8.1.3 Pain Assessment with Compact, High-Performance Models\nIn Chapter 5, we introduced methods aimed at developing effective and efficient approaches\nsuitable for real-world applications, focusing on computational cost and efficiency. A pri-\nmary concern addressed is the tendency in both automatic pain research and the broader\nfield of deep learning to rely on large models that require high-end GPUs for basic infer-\nence. We investigated whether achieving comparable performance with more efficient and\nfaster models is feasible. Additionally, we conducted one of the first studies to use heart\nrate as the sole feature for pain assessment. This choice was motivated by the widespread\navailability of heart rate data from consumer wearables, prompting us to examine its viabil-\nity for pain assessment. Our findings indicate that well-designed and optimized models can\ndeliver performance on par with, or even superior, systems that use more complex features\nor raw ECG signals. This is significant, as effective real-world applications must balance\npeak performance with manageable computational demands, particularly in clinical or home\nmonitoring settings.\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy\nIn Chapter 6, we explored the creation of synthetic data and its potential utility within an\nautomatic pain assessment framework. Motivated by literature suggesting that thermal im-\nagery can reflect skin temperature increases during arousal events like pain, we generated\nsynthetic thermal videos. Our efforts led to the development of this new synthetic modality,8.1. SUMMARY OF THESIS ACHIEVEMENTS 159\nand we assessed its effectiveness compared to authentic RGB videos. The results demon-\nstrated that the synthetic videos we produced were not only of high quality but also reached\nthe performance of RGB videos. This significant outcome may pave the way for new au-\ntomatic systems that rely on synthetic data. Such systems could ( i) utilize modalities that\nare rare, challenging, or costly to record and ( ii) enhance privacy by concealing or partially\nconcealing the identities of individuals involved. Furthermore, it opens the possibility for\nsystems that integrate both authentic and synthetic modalities, leveraging the strengths of\neach.\n8.1.5 Universal Modeling for Automatic Pain Assessment\nIn Chapter 7, we explored and presented general-purpose models and pipelines for auto-\nmatic pain assessment tasks. Our initial focus was on conceiving a single pipeline applica-\nble across all input modalities, simplifying the development process by eliminating the need\nfor specialized models and modules for each modality. Our strategy employed vision-based\nmodels, which are highly effective since all inputs can be transformed into image formats\u2014\nfor instance, a video frame remains an image. Signals can be visualized as images, such\nas spectrograms or waveforms. This approach proved straightforward and effective in our\nexperiments with videos and FNIRs, applying waveforms in unimodal and multimodal sce-\nnarios. A major advancement in our research was the development of the PainFormer , a\nfoundation model specifically crafted for automatic pain assessment. This model is notable\nfor being the first in its field and represents a significant step forward in pain assessment tech-\nnology. The most important finding from this study is the exceptional performance of the\nfoundation model across a range of modalities, including RGB, thermal, depth videos, ECG,\nEMG, GSR, and fNIRs. It consistently delivered state-of-the-art results, demonstrating its\nversatility and effectiveness. We believe that this research will encourage further investiga-\ntions into similar models, enhancing their performance in benchmarks and possibly leading\nto broader real-world applications by proving their effectiveness and utility.\n8.1.6 Explainable Deep Learning\nAlthough explainability was not the primary focus of the thesis, our research consistently\naimed to provide insights and explanations on how the model performs and makes decisions.\nUsing attention maps, we aimed to deliver these explanations, which were often clear and\nvaluable. This approach is crucial, especially if such models are to be implemented in clinical\nsettings in the future.160 CHAPTER 8. CONCLUSIONS, PERSPECTIVES AND FUTURE WORK\n8.2 Perspectives for Automatic Pain Assessment Methods\nAutomatic pain assessment is a fascinating subject from engineering and research perspec-\ntives and a critical matter in healthcare and clinical environments. Our observations and\ncritiques focus on the datasets, the cornerstone of any computational science research. In\nSection 3.7.1, we discussed the challenges in automatic pain assessment. The observations\nand statements remain largely unchanged a few years after this writing.\nThe comments provided here cannot be addressed with existing datasets; they are in-\ntended for future ones. The most superficial data to incorporate involves demographic fac-\ntors such as age and gender. As explored in our research, both are crucial and including\nthem should be considered a minimum standard for future datasets. Another factor criti-\ncal to the generalization of future models is the inclusion of individuals from various racial\nbackgrounds. We have observed notable improvements in this area; for instance, the AI4Pain\ndataset, a recent addition, features participants from diverse backgrounds. One important el-\nement not yet considered in any dataset is social interactions, which, as discussed, influence\npain perception. Future datasets should incorporate this factor, designing experiments to\nassess responses to stimuli when individuals are alone, accompanied by a female and male\nperson. Additionally, attractiveness also affects pain perception. Incorporating these aspects\ninto experiments is feasible and should be relatively straightforward. Recording sound and\nspeech is essential in terms of modalities. This data will benefit signal processing and lan-\nguage analysis and be valuable for assessing pain.\nLastly, we observe that even though significant funds are invested in equipment for\nrecording biosignals, video capture quality in some datasets is only medium to low. Fu-\nture research must utilize cameras capable of recording high-resolution videos, such as 4K.\nIt is important to note that while mobile phones may meet these specifications on paper,\nthe quality of their sensors and lenses is often inadequate. More professional video record-\ning equipment is necessary. Additionally, a high frame rate is essential for capturing facial\nmicro-expressions. Furthermore, as we have discussed, incorporating visual modalities be-\nyond RGB, such as thermal and depth imaging, can be highly valuable. The cost of these\ntechnologies is comparable to that of biosignal recording equipment and should not be pro-\nhibitively expensive.\n8.3 Future Work\nBased on the work and findings presented in this thesis, we propose recommendations for\nfuture research. As emphasized earlier, any modality must incorporate and extract features\nthat capture the temporal dimension. This is vital because pain is a dynamic condition,\nexpressed and evolving over time, not static. This principle applies equally to many other8.3. FUTURE WORK 161\naffective-related tasks.\nWe have a primary recommendation regarding the computational methods used in auto-\nmatic pain assessment. We have observed that researchers in this field often recycle the same\nmethods repeatedly. This repetition does not appear to stem from efforts to enhance or refine\nexisting approaches. We encourage future researchers to be more adventurous and explore\nnew concepts and techniques. The literature on deep learning, which is readily accessible\nand constantly evolving, provides a solid foundation. Researchers should strive to adopt and\nadapt these ideas to their specific problems and tasks and seek to innovate and improve upon\nthem.\nA more specific technical recommendation regarding methods concerns the use of foun-\ndation models. In our research, we explored this concept and developed PainFormer . It is\nimportant to note that the specific model is not large compared to others in the literature,\nespecially the well-known language models; it can be considered reasonably compact. This\ndemonstrates that small, efficient models can be developed for tasks like pain assessment and\nbeing effective. We believe greater efforts should be directed toward creating foundational,\ngeneral-purpose models incorporating multimodality. This approach represents a promising\nand significant path for future research. In addition, we believe that methods related to gener-\nation processes hold valuable potential. In particular, until new, high-quality pain datasets are\ndeveloped, techniques like super-resolution (upscaling) to produce higher-resolution video\nframes and methods for generating auxiliary frames to increase FPS are promising areas for\nexploration.162Bibliography\n[1] HAFD Merskey. Pain terms: a list with definitions and notes on usage. recommended\nby the iasp subcommittee on taxonomy. Pain, 6:249\u2013252, 1979.\n[2] Amanda C de C Williams and Kenneth D Craig. Updating the definition of pain. Pain,\n157(11):2420\u20132423, nov 2016.\n[3] Rebecca Pillai Riddell and Kenneth D Craig. Developmental Dimensions in Under-\nstanding Interpersonal Features of Pain , pages 43\u201355. Springer International Publish-\ning, Cham, 2018.\n[4] Amanda Williams and Judith Kappesser. Why Do We Care? Evolutionary Mecha-\nnisms in the Social Dimension of Pain , pages 3\u201322. 2018.\n[5] Steven J. Mithen. The Prehistory of the Mind: A Search for the Origins of Art, Reli-\ngion and Science . Orion Publishing Group, 1996.\n[6] Rebecca Redfern. A regional examination of surgery and fracture treatment in iron\nage and roman britain. International Journal of Osteoarchaeology , 20(4):443\u2013471,\n2010.\n[7] Steven P Cohen, Lene Vase, and William M Hooten. Chronic pain: an update on\nburden, best practices, and new advances. The Lancet , 397(10289):2082\u20132097, 2021.\n[8] Aza Abdulla, Nicola Adams, Margaret Bone, Alison M Elliott, Jean Gaffin, Derek\nJones, Roger Knaggs, Denis Martin, Liz Sampson, Pat Schofield, and British Geriatric\nSociety. Guidance on the management of pain in older people. Age and ageing , 42\nSuppl 1:1\u201357, March 2013.\n[9] Global, regional, and national incidence, prevalence, and years lived with disabil-\nity for 354 Diseases and Injuries for 195 countries and territories, 1990-2017: A\nsystematic analysis for the Global Burden of Disease Study 2017. The Lancet ,\n392(10159):1789\u20131858, nov 2018.\n[10] US Burden of Disease Collaborators. The State of US Health, 1990-2010: Burden of\nDiseases, Injuries, and Risk Factors. JAMA , 310(6):591\u2013606, 08 2013.\n163164 BIBLIOGRAPHY\n[11] Darrell J. Gaskin and Patrick Richard. The Economic Costs of Pain in the United\nStates. The Journal of Pain , 13(8):715\u2013724, aug 2012.\n[12] Harald Breivik, Elon Eisenberg, and Tony O\u2019Brien. The individual and societal bur-\nden of chronic pain in europe: the case for strategic prioritisation and action to im-\nprove knowledge and availability of appropriate care. BMC public health , 13:1\u201314,\n2013.\n[13] Deloitte Access Economics. The cost of pain in australia, 2019.\n[14] Pradeep Dinakar and Alexandra Marion Stillman. Pathogenesis of Pain. Seminars in\nPediatric Neurology , 23(3):201\u2013208, aug 2016.\n[15] Puja Seth, Rose A. Rudd, Rita K. Noonan, and Tamara M. Haegerich. Quantifying the\nepidemic of prescription opioid overdose deaths. American Journal of Public Health ,\n108(4):500\u2013502, 2018.\n[16] Ramsin Benyamin, Andrea M Trescot, Sukdeb Datta, Ricardo M Buenaventura, Ra-\njive Adlaka, Nalini Sehgal, Scott E Glaser, and Ricardo Vallejo. Opioid complications\nand side effects. Pain Physician , 2s;11(3;2s):S105\u2013S120, 03 2008.\n[17] Stefanos Gkikas and Manolis Tsiknakis. Automatic assessment of pain based on\ndeep learning methods: A systematic review. Computer Methods and Programs in\nBiomedicine , 231:107365, 2023.\n[18] Lucille A Joel. The fifth vital sign: pain. AJN The American Journal of Nursing ,\n99(2):9, 1999.\n[19] Aleksandra Badura, Aleksandra Maslowska, Andrzej My \u00b4sliwiec, and Ewa Pietka.\nMultimodal signal analysis for pain recognition in physiotherapy using wavelet scat-\ntering transform. Sensors , 21(4), 2021.\n[20] Kyle Vader, Geoff P. Bostick, Lisa C. Carlesso, Judith Hunter, Giulia Mesaroli, Kadija\nPerreault, Yannick Tousignant-Laflamme, Susan Tupper, David M. Walton, Timo-\nthy H. Wideman, and Jordan Miller. The revised iasp definition of pain and accompa-\nnying notes: Considerations for the physiotherapy profession. Physiotherapy Canada ,\n73(2):103\u2013106, 2021.\n[21] Lauren N Straatman, Michael J Lukacs, Joshua Y Lee, Maryam Ghodrati, Emily A\nLalone, and David M Walton. Are people good prognosticators of their own pain?\nan exploration of the relationship between sex-specific pain beliefs and clinical pain\nevaluation. Musculoskeletal Science and Practice , 62:102667, December 2022.BIBLIOGRAPHY 165\n[22] Raul Fernandez Rojas, Nicholas Brown, Gordon Waddington, and Roland Goecke.\nA systematic review of neurophysiological sensing for the assessment of acute pain.\nNPJ Digital Medicine , 6(1):76, 2023.\n[23] Raul Fernandez Rojas, Mingyu Liao, Julio Romero, Xu Huang, and Keng-Liang Ou.\nCortical network response to acupuncture and the effect of the hegu point: An fnirs\nstudy. Sensors , 19(2), 2019.\n[24] Seyed Amir Hossein Aqajari, Rui Cao, Emad Kasaeyan Naeini, Michael-David\nCalderon, Kai Zheng, Nikil Dutt, Pasi Liljeberg, Sanna Salanter \u00a8a, Ariana M Nelson,\nand Amir M Rahmani. Pain assessment tool with electrodermal activity for postop-\nerative patients: method validation study. JMIR mHealth and uHealth , 9(5):e25258,\n2021.\n[25] H H Yong, S J Gibson, D J Horne, and R D Helme. Development of a pain atti-\ntudes questionnaire to assess stoicism and cautiousness for possible age differences.\nThe journals of gerontology. Series B, Psychological sciences and social sciences ,\n56(5):P279\u201384, sep 2001.\n[26] E J Bartley and R B Fillingim. Sex differences in pain: a brief review of clinical and\nexperimental findings. British journal of anaesthesia , 111(1):52\u201358, jul 2013.\n[27] Jean-Michel Rou \u00b4e, Iris Morag, Wassim M Haddad, Behnood Gholami, and Kanwal-\njeet JS Anand. Using sensor-fusion and machine-learning algorithms to assess acute\npain in non-verbal infants: a study protocol. BMJ open , 11(1):e039292, 2021.\n[28] Boaz Gedaliahu Samolsky Dekel, Alberto Gori, Alessio Vasarri, Maria Cristina\nSorella, Gianfranco Di Nino, and Rita Maria Melotti. Medical evidence influence\non inpatients and nurses pain ratings agreement. Pain Research and Management ,\n2016.\n[29] Kelly M. Hoffman, Sophie Trawalter, Jordan R. Axt, and M. Norman Oliver. Racial\nbias in pain assessment and treatment recommendations, and false beliefs about bio-\nlogical differences between blacks and whites. Proceedings of the National Academy\nof Sciences , 113(16):4296\u20134301, apr 2016.\n[30] Francis J Keefe, Tamara J Somers, David A Williams, and Suzanne J Smith. Assess-\nment of pain behaviors. In Handbook of pain assessment, 3rd ed. , pages 134\u2013150.\nThe Guilford Press, New York, NY , US, 2011.\n[31] Nicole Miglio and Jessica Stanier. Beyond pain scales: A critical phenomenology of\nthe expression of pain. Frontiers in Pain Research , 3, 2022.166 BIBLIOGRAPHY\n[32] Andrew Leroux, Rachael Rzasa-Lynn, Ciprian Crainiceanu, and Tushar Sharma.\nWearable Devices: Current Status and Opportunities in Pain Assessment and Man-\nagement. Digital Biomarkers , 5(1):89\u2013102, 04 2021.\n[33] Philipp Werner, Daniel Lopez-Martinez, Steffen Walter, Ayoub Al-Hamadi, Sascha\nGruss, and Rosalind Picard. Automatic recognition methods supporting pain assess-\nment: A survey. IEEE Transactions on Affective Computing , 2019.\n[34] Gioacchino D. De Sario, Clifton R. Haider, Karla C. Maita, Ricardo A. Torres-\nGuzman, Omar S. Emam, Francisco R. Avila, John P. Garcia, Sahar Borna, Christo-\npher J. McLeod, Charles J. Bruce, Rickey E. Carter, and Antonio J. Forte. Using ai to\ndetect pain through facial expressions: A review. Bioengineering , 10(5), 2023.\n[35] Stefanos Gkikas., Chariklia Chatzaki., Elisavet Pavlidou., Foteini Verigou., Kyriakos\nKalkanis., and Manolis Tsiknakis. Automatic pain intensity estimation based on\nelectrocardiogram and demographic factors. In Proceedings of the 8th International\nConference on Information and Communication Technologies for Ageing Well and\ne-Health - ICT4AWE, , pages 155\u2013162. INSTICC, SciTePress, 2022.\n[36] Stefanos Gkikas, Chariklia Chatzaki, and Manolis Tsiknakis. Multi-task neural net-\nworks for pain intensity estimation using electrocardiogram and demographic factors.\nInInformation and Communication Technologies for Ageing Well and e-Health , pages\n324\u2013337. Springer Nature Switzerland, 2023.\n[37] Stefanos Gkikas and Manolis Tsiknakis. A full transformer-based framework for au-\ntomatic pain estimation using videos. In 2023 45th Annual International Conference\nof the IEEE Engineering in Medicine & Biology Society (EMBC) , pages 1\u20136, 2023.\n[38] Stefanos Gkikas, Nikolaos S. Tachos, Stelios Andreadis, Vasileios C. Pezoulas, Dim-\nitrios Zaridis, George Gkois, Anastasia Matonaki, Thanos G. Stavropoulos, and Dim-\nitrios I. Fotiadis. Multimodal automatic assessment of acute pain through facial videos\nand heart rate signals utilizing transformer-based architectures. Frontiers in Pain Re-\nsearch , 5, 2024.\n[39] Stefanos Gkikas and Manolis Tsiknakis. Synthetic thermal and rgb videos for auto-\nmatic pain assessment utilizing a vision-mlp architecture. In 2024 12th International\nConference on Affective Computing and Intelligent Interaction Workshops and Demos\n(ACIIW) , pages 4\u201312, 2024.\n[40] Stefanos Gkikas and Manolis Tsiknakis. Twins-painvit: Towards a modality-agnostic\nvision transformer framework for multimodal automatic pain assessment using facialBIBLIOGRAPHY 167\nvideos and fnirs. In 2024 12th International Conference on Affective Computing and\nIntelligent Interaction Workshops and Demos (ACIIW) , pages 13\u201321, 2024.\n[41] Stefanos Gkikas, Raul Fernandez Rojas, and Manolis Tsiknakis. Painformer: a\nvision foundation model for automatic pain assessment, 2025. arXiv preprint\narXiv:2505.01571.\n[42] Srinivasa N Raja, Daniel B Carr, Milton Cohen, Nanna B Finnerup, Herta Flor,\nStephen Gibson, Francis J Keefe, Jeffrey S Mogil, Matthias Ringkamp, Kathleen A\nSluka, Xue-Jun Song, Bonnie Stevens, Mark D Sullivan, Perri R Tutelman, Takahiro\nUshida, and Kyle Vader. The revised international association for the study of pain\ndefinition of pain: concepts, challenges, and compromises. Pain, 161(9):1976\u20131982,\nSeptember 2020.\n[43] Khalid S and Tubbs RS. Neuroanatomy and Neuropsychology of Pain. Cureus , 9(10),\n2017.\n[44] Eric L. Garland. Pain Processing in the Human Nervous System: A Selective Review\nof Nociceptive and Biobehavioral Pathways. Primary Care: Clinics in Office Practice ,\n39(3):561\u2013571, sep 2012.\n[45] Tatiana F. Almeida, Suely Roizenblatt, and Sergio Tufik. Afferent pain pathways:\na neuroanatomical review. Brain Research , 1000(1):40\u201356, 2004. Brain Research\nV olume 1000.\n[46] A. Vania Apkarian, M. Catherine Bushnell, Rolf-Detlef Treede, and Jon-Kar Zubieta.\nHuman brain mechanisms of pain perception and regulation in health and disease.\nEuropean Journal of Pain , 9(4):463\u2013463, 2005.\n[47] Mary Beth Babos, Brittany Grady, Warren Wisnoff, and Christy McGhee. Pathophys-\niology of pain. Disease-a-Month , 59(10):330\u2013358, 2013. Pathophysiology of pain.\n[48] Clifford J. Woolf. What is this thing called pain? The Journal of Clinical Investigation ,\n120(11):3742\u20133744, 11 2010.\n[49] Allan I Basbaum, Diana M Bautista, Gr \u00b4egory Scherrer, and David Julius. Cellular and\nmolecular mechanisms of pain. Cell, 139(2):267\u2013284, 2009.\n[50] Rolf-Detlef Treede, Winfried Rief, Antonia Barke, Qasim Aziz, Michael I Bennett,\nRafael Benoliel, Milton Cohen, Stefan Evers, Nanna B Finnerup, Michael B First,\net al. A classification of chronic pain for icd-11. Pain, 156(6):1003\u20131007, 2015.168 BIBLIOGRAPHY\n[51] Joy Onyekachukwu Egede. Automatic pain assessment from face video (continuous\npain intensity estimation in adults and newborns) . PhD thesis, University of Notting-\nham, 2019.\n[52] Raymond Sinatra. Causes and consequences of inadequate management of acute pain.\nPain Medicine , 11(12):1859\u20131871, 12 2010.\n[53] Lies De Ruddere and Raymond Tait. Facing Others in Pain: Why Context Matters ,\npages 241\u2013269. Springer International Publishing, Cham, 2018.\n[54] MVM De Oliveira, JAL De Jesus, and RM Tristao. Psychophysical parameters of\na multidimensional pain scale in newborns. Physiological measurement , 33(1):39,\n2011.\n[55] K Feldh. The checklist of nonverbal pain indicators. Pain Management Nursing, I ,\n1:13\u201321, 2000.\n[56] JC Evans, DG V ogelpohl, CM Bourguignon, and CS Morcott. Pain behaviors in lbw\ninfants accompany some\u201d nonpainful\u201d caregiving procedures. Neonatal network: NN ,\n16(3):33\u201340, 1997.\n[57] Huda Huijer Abu-Saad, Gerrie JJW Bours, Bonnie Stevens, and Jan PH Hamers. As-\nsessment of pain in the neonate. In Seminars in perinatology , volume 22, pages 402\u2013\n416. Elsevier, 1998.\n[58] Richard Stephens, John Atkins, and Andrew Kingston. Swearing as a response to\npain. Neuroreport , 20(12):1056\u20131060, 2009.\n[59] Richard Stephens and Claudia Umland. Swearing as a response to pain\u2013effect of daily\nswearing frequency. The Journal of Pain , 12(12):1274\u20131281, 2011.\n[60] Jacob Greisen, Claus B Juhl, Thorbj\u00f8rn Gr\u00f8fte, Hendrik Vilstrup, Troels S Jensen,\nand Ole Schmitz. Acute pain induces insulin resistance in humans. The Journal of the\nAmerican Society of Anesthesiologists , 95(3):578\u2013584, 2001.\n[61] Bonnie J Stevens and C Celeste Johnston. Physiological responses of premature in-\nfants to a painful stimulus. Nursing research , 43(4):226\u2013231, 1994.\n[62] Hunter G Hoffman, Todd L Richards, Barbara Coda, Aric R Bills, David Blough,\nAnne L Richards, and Sam R Sharar. Modulation of thermal pain-related brain activity\nwith virtual reality: evidence from fmri. Neuroreport , 15(8):1245\u20131248, 2004.\n[63] PJ Mathew and Joseph L Mathew. Assessment and management of pain in infants.\nPostgraduate medical journal , 79(934):438\u2013443, 2003.BIBLIOGRAPHY 169\n[64] R Melzack and P D Wall. Pain mechanisms: a new theory. Science (New York, N.Y.) ,\n150(3699):971\u2013979, nov 1965.\n[65] Thomas Hadjistavropoulos, Kenneth D Craig, Steve Duck, Annmarie Cano, Liesbet\nGoubert, Philip L Jackson, Jeffrey S Mogil, Pierre Rainville, Michael J L Sullivan,\nAmanda C de C Williams, Tine Vervoort, and Theresa Dever Fitzgerald. A biopsy-\nchosocial formulation of pain communication. Psychological bulletin , 137(6):910\u2013\n939, nov 2011.\n[66] Katelynn E Boerner, Kathryn A Birnie, Line Caes, Meghan Schinkel, and Christine T\nChambers. Sex differences in experimental pain among healthy children: a systematic\nreview and meta-analysis. Pain, 155(5):983\u2013993, may 2014.\n[67] Edmund Keogh. Men, masculinity, and pain. Pain, 156(12):2408\u20132412, dec 2015.\n[68] Fredric M Levine and Laura Lee De Simone. The effects of experimenter gender on\npain report in male and female subjects. Pain, 44(1):69\u201372, jan 1991.\n[69] Laura E McClelland and James A McCubbin. Social influence and pain response in\nwomen and men. Journal of behavioral medicine , 31(5):413\u2013420, oct 2008.\n[70] Jacob M Vigil and Joe Alcock. Tough guys or sensitive guys? Disentangling the role\nof examiner sex on patient pain reports. Pain Research & Management : The Journal\nof the Canadian Pain Society , 19(1):e9, 2014.\n[71] K A Raftery, R Smith-Coggins, and A H Chen. Gender-associated differences in\nemergency department pain management. Annals of emergency medicine , 26(4):414\u2013\n421, oct 1995.\n[72] E M Hooper, L M Comstock, J M Goodwin, and J S Goodwin. Patient characteristics\nthat influence physician behavior. Medical care , 20(6):630\u2013638, jun 1982.\n[73] Edmund Keogh. Sex and Gender as Social-Contextual Factors in Pain , pages 433\u2013\n453. Springer International Publishing, 2018.\n[74] Thomas Hadjistavropoulos and Natasha L Gallant. Pain in Older Adults: Caregiver\nChallenges , pages 415\u2013429. Springer International Publishing, 2018.\n[75] Christine J McPherson, Thomas Hadjistavropoulos, Alana Devereaux, and\nMichelle M Lobchuk. A qualitative investigation of the roles and perspectives of\nolder patients with advanced cancer and their family caregivers in managing pain in\nthe home. BMC palliative care , 13:39, 2014.170 BIBLIOGRAPHY\n[76] T Hadjistavropoulos, D L LaChapelle, F K MacLeod, B Snider, and K D Craig. Mea-\nsuring movement-exacerbated pain in cognitively impaired frail elders. The Clinical\njournal of pain , 16(1):54\u201363, mar 2000.\n[77] Clive Ballard, Maria Luisa Hanney, Megan Theodoulou, Simon Douglas, Rupert Mc-\nShane, Katja Kossakowski, Randeep Gill, Edmund Juszczak, Ly-Mee Yu, and Robin\nJacoby. The dementia antipsychotic withdrawal trial (DART-AD): long-term follow-\nup of a randomised placebo-controlled trial. The Lancet Neurology , 8(2):151\u2013157,\nfeb 2009.\n[78] Ryuta Ochi and Akira Midorikawa. Decline in Emotional Face Recognition Among\nElderly People May Reflect Mild Cognitive Impairment. Frontiers in Psychology , 12,\n2021.\n[79] R Pillai Riddell and Nicole Racine. Assessing pain in infancy: The caregiver context.\nPain Research & Management : The Journal of the Canadian Pain Society , 14(1):27,\n2009.\n[80] Tine Vervoort, Line Caes, Zina Trost, Michael Sullivan, Karoline Vangronsveld, and\nLiesbet Goubert. Social modulation of facial pain display in high-catastrophizing chil-\ndren: an observational study in schoolchildren and their parents. Pain, 152(7):1591\u2013\n1599, jul 2011.\n[81] Francis J. Keefe, Robert H. Wilkins, Wesley A. Cook, James E. Crisson, and\nLawrence H. Muhlbaier. Depression, Pain, and Pain Behavior. Journal of Consulting\nand Clinical Psychology , 54(5):665\u2013669, oct 1986.\n[82] John W. Burns, Phillip Quartana, Wesley Gilliam, Erika Gray, Justin Matsuura, Carla\nNappi, Brandy Wolfe, and Kenneth Lofland. Effects of Anger Suppression on Pain\nSeverity and Pain Behaviors Among Chronic Pain Patients: Evaluation of an Ironic\nProcess Model. Health Psychology , 27(5):645\u2013652, sep 2008.\n[83] Lies De Ruddere, Liesbet Goubert, Micha \u00a8el Andr \u00b4e Louis Stevens, Myriam Deveugele,\nKenneth Denton Craig, and Geert Crombez. Health Care Professionals\u2019 Reactions\nto Patient Pain: Impact of Knowledge About Medical Evidence and Psychosocial\nInfluences. The Journal of Pain , 15(3):262\u2013270, mar 2014.\n[84] Lisa J Staton, Mukta Panda, Ian Chen, Inginia Genao, James Kurz, Mark Pasanen,\nAlex J Mechaber, Madhusudan Menon, Jane O\u2019Rorke, JoAnn Wood, Eric Rosenberg,\nCharles Faeslis, Tim Carey, Diane Calleson, and Sam Cykert. When race matters:\ndisagreement in pain perception between patients and their physicians in primary care.\nJournal of the National Medical Association , 99(5):532\u2013538, may 2007.BIBLIOGRAPHY 171\n[85] Samantha M. Meints, Madison Stout, Samuel Abplanalp, and Adam T. Hirsh. Pain-\nRelated Rumination, But Not Magnification or Helplessness, Mediates Race and Sex\nDifferences in Experimental Pain. The Journal of Pain , 18(3):332\u2013339, mar 2017.\n[86] Brian Blake Drwecki. Race and Pain: A Dual Injustice , pages 455\u2013480. Springer\nInternational Publishing, 2018.\n[87] C S Cleeland, R Gonin, A K Hatfield, J H Edmonson, R H Blum, J A Stewart, and\nK J Pandya. Pain and its treatment in outpatients with metastatic cancer. The New\nEngland journal of medicine , 330(9):592\u2013596, mar 1994.\n[88] Lies De Ruddere and Raymond Tait. Facing Others in Pain: Why Context Matters ,\npages 241\u2013269. Springer International Publishing, Cham, 2018.\n[89] Raymond C. Tait, John T. Chibnall, Laura Miller, and Chas A. Werner. Judging pain\nand disability: effects of pain severity and physician specialty. Journal of Behavioral\nMedicine 2010 34:3 , 34(3):218\u2013224, nov 2010.\n[90] Relieving pain in America: A blueprint for transforming prevention, care, education,\nand research . National Academies Press, oct 2011.\n[91] Line Caes, Liesbet Goubert, and Laura Simons. An ecological and lifespan approach\nof social influences on childhood pain experiences. Social and Interpersonal Dynam-\nics in Pain , pages 395\u2013413, may 2018.\n[92] Geert Crombez, Patricia Bijttebier, Chris Eccleston, Tamara Mascagni, Gustaaf\nMertens, Liesbet Goubert, and Katrien Verstraeten. The child version of the pain\ncatastrophizing scale (PCS-C): a preliminary validation. Pain, 104(3):639\u2013646, aug\n2003.\n[93] Paula A Forgeron, Patrick McGrath, Bonnie Stevens, Joan Evans, Bruce Dick,\nAllen G Finley, and Torie Carlson. Social information processing in adolescents with\nchronic pain: my friends don\u2019t really understand me. Pain, 152(12):2773\u20132780, dec\n2011.\n[94] Liesbet Goubert and Laura E. Simons. Cognitive styles and processes in paediatric\npain. In Oxford Textbook of Paediatric Pain , pages 95\u2013101. Oxford University Press,\nnov 2013.\n[95] Maria Fitzgerald and Suellen M Walker. Infant pain management: a developmental\nneurobiological approach. Nature clinical practice. Neurology , 5(1):35\u201350, jan 2009.172 BIBLIOGRAPHY\n[96] David Kang, Negin Hesam-Shariati, James H. McAuley, Monzurul Alam, Zina Trost,\nCaroline D. Rae, and Sylvia M. Gustin. Disruption to normal excitatory and inhibitory\nfunction within the medial prefrontal cortex in people with chronic pain. European\nJournal of Pain , jul 2021.\n[97] Domenica A Delgado, Bradley S Lambert, Nickolas Boutris, Patrick C McCulloch,\nAndrew B Robbins, Michael R Moreno, and Joshua D Harris. Validation of Digi-\ntal Visual Analog Scale Pain Scoring With a Traditional Paper-based Visual Analog\nScale in Adults. Journal of the American Academy of Orthopaedic Surgeons. Global\nresearch & reviews , 2(3):e088, mar 2018.\n[98] Mathias Haefeli and Achim Elfering. Pain assessment. European spine journal :\nofficial publication of the European Spine Society, the European Spinal Deformity\nSociety, and the European Section of the Cervical Spine Research Society , 15 Suppl\n1(Suppl 1):S17\u201324, jan 2006.\n[99] Kenneth M. Prkachin and Patricia E. Solomon. The structure, reliability and validity\nof pain expression: Evidence from patients with shoulder pain. Pain, 139(2):267\u2013274,\noct 2008.\n[100] J Lawrence, D Alcock, P McGrath, J Kay, S B MacMurray, and C Dulberg. The\ndevelopment of a tool to assess neonatal pain. Neonatal network : NN , 12(6):59\u201366,\nsep 1993.\n[101] David E Weissman and David J Haddox. Opioid pseudoaddiction\u2013an iatrogenic syn-\ndrome. Pain, 36(3):363\u2013366, mar 1989.\n[102] Kenneth M. Prkachin. Assessing pain by facial expression: Facial expression as nexus.\nPain Research and Management , 14(1):53\u201358, 2009.\n[103] Ghada Zamzmi, Rangachar Kasturi, Dmitry Goldgof, Ruicong Zhi, Terri Ashmeade,\nand Yu Sun. A Review of Automated Pain Assessment in Infants: Features, Classi-\nfication Tasks, and Databases. IEEE Reviews in Biomedical Engineering , 11:77\u201396,\nnov 2018.\n[104] Zhanli Chen, Rashid Ansari, and Diana Wilkie. Automated Pain Detection from Fa-\ncial Expressions using FACS: A Review. arXiv , 2018.\n[105] Teena Hassan, Dominik Seus, Johannes Wollenberg, Katharina Weitz, Miriam Kunz,\nStefan Lautenbacher, Jens-Uwe Garbas, and Ute Schmid. Automatic Detection of\nPain from Facial Expressions: A Survey. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , pages 1\u20131, apr 2019.BIBLIOGRAPHY 173\n[106] Philipp Werner, Daniel Lopez-Martinez, Steffen Walter, Ayoub Al-Hamadi, Sascha\nGruss, and Rosalind Picard. Automatic Recognition Methods Supporting Pain As-\nsessment: A Survey. IEEE Transactions on Affective Computing , 2019.\n[107] Rasha M. Al-Eidan, Hend Al-Khalifa, and AbdulMalik Al-Salman. Deep-Learning-\nBased Models for Pain Recognition: A Systematic Review. Applied Sciences ,\n10(17):5984, aug 2020.\n[108] Patrick Lucey, Jeffrey F. Cohn, Kenneth M. Prkachin, Patricia E. Solomon, and Iain\nMatthews. Painful data: The UNBC-McMaster shoulder pain expression archive\ndatabase. In 2011 IEEE International Conference on Automatic Face and Gesture\nRecognition and Workshops, FG 2011 , pages 57\u201364, 2011.\n[109] Steffen Walter, Sascha Gruss, Hagen Ehleiter, Junwen Tan, Harald C. Traue, Stephen\nCrawcour, Philipp Werner, Ayoub Al-Hamadi, Adriano O. Andrade, and Gustavo Mor-\neira Da Silva. The biovid heat pain database: Data for the advancement and systematic\nvalidation of an automated pain recognition. In 2013 IEEE International Conference\non Cybernetics , pages 128\u2013131, 2013.\n[110] Mohammad A. Haque, Ruben B. Bautista, Fatemeh Noroozi, Kaustubh Kulkarni,\nChristian B. Laursen, Ramin Irani, Marco Bellantonio, Sergio Escalera, Golamreza\nAnbarjafari, Kamal Nasrollahi, Ole K. Andersen, Erika G. Spaich, and Thomas B.\nMoeslund. Deep multimodal pain recognition: A database and comparison of spatio-\ntemporal visual modalities. In 13th IEEE International Conference on Automatic\nFace and Gesture Recognition, FG 2018 , pages 250\u2013257. Institute of Electrical and\nElectronics Engineers Inc., jun 2018.\n[111] Sheryl Brahnam, Chao-Fa Chuang, Frank Y Shih, and Melinda R Slack. SVM Clas-\nsification of Neonatal Facial Images of Pain. In Isabelle Bloch, Alfredo Petrosino,\nand Andrea G B Tettamanzi, editors, Fuzzy Logic and Applications , pages 121\u2013128,\nBerlin, Heidelberg, 2006. Springer Berlin Heidelberg.\n[112] Sheryl Brahnam, Loris Nanni, Shannon McMurtrey, Alessandra Lumini, Rick Brat-\ntin, Melinda Slack, and Tonya Barrier. Neonatal pain detection in videos using the\niCOPEvid dataset and an ensemble of descriptors extracted from Gaussian of Local\nDescriptors. Applied Computing and Informatics , 2019.\n[113] Ghada Zamzmi, Pai Chih-Yun, Dmitry Goldgof, R. Kasturi, Terri Ashmeade, and\nYu Sun. A Comprehensive and Context-Sensitive Neonatal Pain Assessment Using\nComputer Vision. IEEE Transactions on Affective Computing , 2019.174 BIBLIOGRAPHY\n[114] Joy Egede, Michel Valstar, Mercedes Torres Torres, and Don Sharkey. Automatic\nNeonatal Pain Estimation: An Acute Pain in Neonates Database. 2019 8th Inter-\nnational Conference on Affective Computing and Intelligent Interaction, ACII 2019 ,\npages 475\u2013481, 2019.\n[115] Min S H Aung, Sebastian Kaltwang, Bernardino Romera-Paredes, Brais Martinez,\nAneesha Singh, Matteo Cella, Michel Valstar, Hongying Meng, Andrew Kemp,\nMoshen Shafizadeh, Aaron C Elkins, Natalie Kanakam, Amschel de Rothschild, Nick\nTyler, Paul J Watson, Amanda C de C Williams, Maja Pantic, and Nadia Bianchi-\nBerthouze. The Automatic Detection of Chronic Pain-Related Expression: Require-\nments, Challenges and the Multimodal EmoPain Dataset. IEEE transactions on affec-\ntive computing , 7(4):435\u2013451, 2016.\n[116] Maria Velana, Sascha Gruss, Georg Layher, Patrick Thiam, Yan Zhang, Daniel\nSchork, Viktor Kessler, Sascha Meudt, Heiko Neumann, Jonghwa Kim, Friedhelm\nSchwenker, Elisabeth Andr \u00b4e, Harald C. Traue, and Steffen Walter. The senseemo-\ntion database: A multimodal database for the development and systematic validation\nof an automatic pain and emotion-recognition system. In Friedhelm Schwenker and\nStefan Scherer, editors, Multimodal Pattern Recognition of Social Signals in Human-\nComputer-Interaction , pages 127\u2013139, Cham, 2017. Springer International Publish-\ning.\n[117] Sascha Gruss, Mattis Geiger, Philipp Werner, Oliver Wilhelm, Harald C Traue, Ayoub\nAl-Hamadi, and Steffen Walter. Multi-Modal Signals for Analyzing Pain Responses\nto Thermal and Electrical Stimuli. Journal of visualized experiments : JoVE , (146),\napr 2019.\n[118] Raul Fernandez Rojas, Niraj Hirachan, Calvin Joseph, Ben Seymour, and Roland\nGoecke. The ai4pain grand challenge 2024: Advancing pain assessment with mul-\ntimodal fnirs and facial video analysis. In 2024 12th International Conference on\nAffective Computing and Intelligent Interaction . IEEE, 2024.\n[119] Henrik Pedersen. Learning appearance features for pain detection using the UNBC-\nMcMaster shoulder pain expression archive database. Lecture Notes in Computer\nScience (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes\nin Bioinformatics) , 9163:128\u2013136, 2015.\n[120] Joy O. Egede, Siyang Song, Temitayo A. Olugbade, Chongyang Wang, Amanda C.\nDe C. Williams, Hongying Meng, Min Aung, Nicholas D. Lane, Michel Valstar, and\nNadia Bianchi-Berthouze. Emopain challenge 2020: Multimodal pain evaluationBIBLIOGRAPHY 175\nfrom facial and bodily expressions. In 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition (FG 2020) , pages 849\u2013856, 2020.\n[121] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-\nscale image recognition. In 3rd International Conference on Learning Representa-\ntions, ICLR 2015 - Conference Track Proceedings . International Conference on Learn-\ning Representations, ICLR, sep 2015.\n[122] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE Computer Society Conference\non Computer Vision and Pattern Recognition , volume 2016-Decem, pages 770\u2013778.\nIEEE Computer Society, dec 2016.\n[123] Ruijing Yang, Xiaopeng Hong, Jinye Peng, Xiaoyi Feng, and Guoying Zhao. Incor-\nporating high-level and low-level cues for pain intensity estimation. In Proceedings -\nInternational Conference on Pattern Recognition , volume 2018-Augus, pages 3495\u2013\n3500. Institute of Electrical and Electronics Engineers Inc., 2018.\n[124] Ashish Semwal and Narendra D Londhe. Computer aided pain detection and inten-\nsity estimation using compact CNN based fusion network. Applied Soft Computing ,\n112:107780, 2021.\n[125] Saandeep Aathreya Sidhapur Lakshminarayan, Saurabh Hinduja, and Shaun Cana-\nvan. Three-level Training of Multi-Head Architecture for Pain Detection. In Gomez-\nFernandez F Struc V ., editor, Proceedings - 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition, FG 2020 , pages 839\u2013843. Institute of\nElectrical and Electronics Engineers Inc., 2020.\n[126] Van Thong Huynh, Hyung Jeong Yang, Guee Sang Lee, and Soo Hyung Kim. Mul-\ntimodality Pain and related Behaviors Recognition based on Attention Learning. In\nGomez-Fernandez F Struc V ., editor, Proceedings - 2020 15th IEEE International\nConference on Automatic Face and Gesture Recognition, FG 2020 , pages 814\u2013818.\nInstitute of Electrical and Electronics Engineers Inc., 2020.\n[127] Ashish Semwal and Narendra D. Londhe. Automated Pain Severity Detection Using\nConvolutional Neural Network. In Rajpurohit V S Nadkatti M N Niranjan S.K. Desai\nV ., editor, Proceedings of the International Conference on Computational Techniques,\nElectronics and Mechanical Systems, CTEMS 2018 , pages 66\u201370. Institute of Electri-\ncal and Electronics Engineers Inc., 2018.\n[128] Mohammad Tavakolian and Abdenour Hadid. Deep binary representation of facial\nexpressions: A novel framework for automatic pain intensity recognition. In Proceed-176 BIBLIOGRAPHY\nings - International Conference on Image Processing, ICIP , pages 1952\u20131956. IEEE\nComputer Society, 2018.\n[129] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning Face Representation from\nScratch, 2014.\n[130] Ashish Semwal and Narendra D. Londhe. ECCNET: An ensemble of compact convo-\nlution neural network for pain severity assessment from face images. In Proceedings\nof the Confluence 2021: 11th International Conference on Cloud Computing, Data\nScience and Engineering , pages 761\u2013766, 2021.\n[131] Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon,\nChou P. Hung, and Brent J. Lance. EEGNet: A compact convolutional neural network\nfor EEG-based brain-computer interfaces. Journal of Neural Engineering , 15(5), nov\n2018.\n[132] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi.\nInception-v4, inception-ResNet and the impact of residual connections on learning.\nIn31st AAAI Conference on Artificial Intelligence, AAAI 2017 , pages 4278\u20134284.\nAAAI press, feb 2017.\n[133] Reza Kharghanian, Ali Peiravi, and Farshad Moradi. Pain detection from facial im-\nages using unsupervised feature learning approach. In Proceedings of the Annual\nInternational Conference of the IEEE Engineering in Medicine and Biology Society,\nEMBS , volume 2016-Octob, pages 419\u2013422. Institute of Electrical and Electronics\nEngineers Inc., 2016.\n[134] Reza Kharghanian, Ali Peiravi, Farshad Moradi, and Alexandros Iosifidis. Pain detec-\ntion using batch normalized discriminant restricted Boltzmann machine layers. Jour-\nnal of Visual Communication and Image Representation , 76, 2021.\n[135] Dong Huang, Zhaoqiang Xia, Lei Li, Kunwei Wang, and Xiaoyi Feng. Pain-\nawareness multistream convolutional neural network for pain estimation. Journal\nof Electronic Imaging , 28(04):1, 2019.\n[136] Xuwu Xin, Xiaoyan Lin, Shengfu Yang, and Xin Zheng. Pain intensity estimation\nbased on a spatial transformation and attention CNN. PLoS ONE , 15(8 August\n2020):1\u201315, aug 2020.\n[137] S Cui, D Huang, Y Ni, and X Feng. Multi-scale regional attention networks for pain\nestimation. In 2021 13th International Conference on Bioinformatics and Biomedical\nTechnology , pages 1\u20138. Association for Computing Machinery, 2021.BIBLIOGRAPHY 177\n[138] Conghui Li, Zhaocheng Zhu, and Yuming Zhao. Saliency Supervision: An Intuitive\nand Effective Approach for Pain Intensity Regression. Lecture Notes in Computer\nScience (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes\nin Bioinformatics) , 11307 LNCS:455\u2013464, 2018.\n[139] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified em-\nbedding for face recognition and clustering. In 2015 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 815\u2013823. IEEE Computer Society, mar\n2015.\n[140] Xianlin Peng, Dong Huang, and Haixi Zhang. Pain intensity recognition via multi-\nscale deep network. IET Image Processing , 14(8):1645\u20131652, 2020.\n[141] X Xin, X Li, S Yang, X Lin, and X Zheng. Pain expression assessment based on a\nlocality and identity aware network. IET Image Processing , 15(12):2948\u20132958, 2021.\n[142] Ashish Semwal and Narendra D. Londhe. S-PANET: A Shallow Convolutional Neural\nNetwork for Pain Severity Assessment in Uncontrolled Environment. In 2021 IEEE\n11th Annual Computing and Communication Workshop and Conference, CCWC 2021 ,\npages 800\u2013806. Institute of Electrical and Electronics Engineers Inc., jan 2021.\n[143] Ashish Semwal and Narendra D. Londhe. MVFNet: A multi-view fusion network for\npain intensity assessment in unconstrained environment. Biomedical Signal Process-\ning and Control , 67, may 2021.\n[144] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going\ndeeper with convolutions. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , volume 07-12-June, pages 1\u20139. IEEE Computer Society, oct\n2015.\n[145] Jiann Shu Lee and Chuan Wei Wang. Facial pain intensity estimation for ICU patient\nwith partial occlusion coming from treatment. In 3rd International Conference on Bi-\nological Information and Biomedical Engineering, BIBE 2019 , pages 106\u2013109. VDE\nVerlag GmbH, 2019.\n[146] Reneiro Andal Virrey, Wahyu Caesarendra, Muhammad Iskandar Bin Pg Hj Petra,\nEmeroylariffion Abas, Asmah Husaini, and Chandratilak De Silva Liyanage. Mile-\nstone of Pain Intensity Evaluation from Facial Action Units. In ICECOS 2019 - 3rd\nInternational Conference on Electrical Engineering and Computer Science, Proceed-\ning, pages 54\u201357. Institute of Electrical and Electronics Engineers Inc., 2019.178 BIBLIOGRAPHY\n[147] Hermawan Nugroho, Dani Harmanto, and Hamada Rasheed Hassan Al-Absi. On the\nDevelopment of Smart Home Care: Application of Deep Learning for Pain Detection.\nIn2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) ,\npages 612\u2013616, 2019.\n[148] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Unified Em-\nbedding for Face Recognition and Clustering. Proceedings of the IEEE Computer So-\nciety Conference on Computer Vision and Pattern Recognition , 07-12-June:815\u2013823,\nmar 2015.\n[149] Laduona Dai, Joost Broekens, and Khiet P. Truong. Real-time pain detection in facial\nexpressions for health robotics. In 2019 8th International Conference on Affective\nComputing and Intelligent Interaction Workshops and Demos, ACIIW 2019 , pages\n277\u2013283. Institute of Electrical and Electronics Engineers Inc., 2019.\n[150] Guglielmo Menchetti, Zhanli Chen, DIana J. Wilkie, Rashid Ansari, Yasemin\nYardimci, and A. Enis Cetin. Pain detection from facial videos using two-stage deep\nlearning. In GlobalSIP 2019 - 7th IEEE Global Conference on Signal and Information\nProcessing, Proceedings . Institute of Electrical and Electronics Engineers Inc., 2019.\n[151] Chittaranjan Andrade. Internal, external, and ecological validity in research design,\nconduct, and evaluation. Indian Journal of Psychological Medicine , 40(5):498\u2013499,\n2018. PMID: 30275631.\n[152] Dianbo Liu, Peng Fengjiao, Ognjen (Oggi) Rudovic, and Rosalind Picard. Deep-\nfacelift: Interpretable personalized models for automatic estimation of self-reported\npain. In Neil Lawrence and Mark Reid, editors, Proceedings of IJCAI 2017 Work-\nshop on Artificial Intelligence in Affective Computing , volume 66 of Proceedings of\nMachine Learning Research , pages 1\u201316. PMLR, 20 Aug 2017.\n[153] Xiaojing Xu, Jeannie S Huang, and Virginia R De Sa. Pain Evaluation in Video using\nExtended Multitask Learning from Multidimensional Measurements. In Proceedings\nof the Machine Learning for Health NeurIPS Workshop , volume 116, pages 141\u2013154.\nPMLR, apr 2020.\n[154] Paola Casti, Arianna Mencattini, Maria Colomba Comes, Giuseppina Callari, Davide\nDi Giuseppe, Silvia Natoli, Mauro Dauri, Elena Daprati, and Eugenio Martinelli. Cal-\nibration of Vision-Based Measurement of Pain Intensity with Multiple Expert Ob-\nservers. IEEE Transactions on Instrumentation and Measurement , 68(7):2442\u20132450,\n2019.BIBLIOGRAPHY 179\n[155] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification\nwith deep convolutional neural networks. In Advances in Neural Information Process-\ning Systems 25 (NIPS 2012) , 2012.\n[156] Luigi Celona and Luca Manoni. Neonatal Facial Pain Assessment Combining Hand-\nCrafted and Deep Features. Lecture Notes in Computer Science , 10590 LNCS:197\u2013\n204, 2017.\n[157] Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep Face Recognition.\nInProceedings of the British Machine Vision Conference (BMVC) , pages 41.1\u201341.12.\nBritish Machine Vision Association and Society for Pattern Recognition, dec 2015.\n[158] Gil Levi and Tal Hassner. Emotion recognition in the wild via convolutional neural\nnetworks and mapped binary patterns. In ICMI 2015 - Proceedings of the 2015 ACM\nInternational Conference on Multimodal Interaction , pages 503\u2013510, New York, NY ,\nUSA, 2015. ACM.\n[159] Guanming Lu, Qiang Hao, Kaiting Kong, Jingjie Yan, Haibo Li, and Xiaonan Li.\nDeep convolutional neural networks with transfer learning for neonatal pain expres-\nsion recognition. In Xiao G Ning X Li K Li M Xiao Z. Wang L., editor, 2018 14th\nInternational Conference on Natural Computation, Fuzzy Systems and Knowledge\nDiscovery (ICNC-FSKD) , pages 251\u2013256. Institute of Electrical and Electronics En-\ngineers Inc., 2018.\n[160] Ghada Zamzmi, Rahul Paul, Md. Sirajus Salekin, Dmitry Goldgof, Rangachar Kas-\nturi, Thao Ho, and Yu Sun. Convolutional Neural Networks for Neonatal Pain Assess-\nment. IEEE Transactions on Biometrics, Behavior, and Identity Science , 1(3):192\u2013\n200, 2019.\n[161] Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, and Yu Sun. Neonatal Pain\nExpression Recognition Using Transfer Learning. arXiv , jul 2018.\n[162] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of\nthe devil in the details: Delving deep into convolutional nets, 2014.\n[163] Luigi Celona, Sheryl Brahnam, and Simone Bianco. Getting the most of few data\nfor neonatal pain assessment. In ACM International Conference Proceeding Series ,\npages 298\u2013301. Association for Computing Machinery, 2019.\n[164] Martin Arjovsky, Soumith Chintala, and L \u00b4eon Bottou. Wasserstein GAN. arXiv , jan\n2017.180 BIBLIOGRAPHY\n[165] Ognjen Rudovic, Nicolas Tobis, Sebastian Kaltwang, Bj \u00a8orn Schuller, Daniel Rueckert,\nJeffrey F. Cohn, and Rosalind W. Picard. Personalized Federated Deep Learning for\nPain Estimation From Face Images. arXiv , jan 2021.\n[166] Kornprom Pikulkaew, Ekkarat Boonchieng, Waraporn Boonchieng, and Varin Chou-\nvatut. Pain detection using deep learning with evaluation system. In Advances in In-\ntelligent Systems and Computing , volume 1184, pages 426\u2013435. Springer Singapore,\n2021.\n[167] S El Morabit, A Rivenq, M.-E.-N. Zighem, A Hadid, A Ouahabi, and A Taleb-Ahmed.\nAutomatic pain estimation from facial expressions: A comparative analysis using off-\nthe-shelf cnn architectures. Electronics , 10(16), 2021.\n[168] C Li, A Pourtaherian, L Van Onzenoort, W.E.T.A. Ten, and P H N De With. Infant\nFacial Expression Analysis: Towards a Real-Time Video Monitoring System Using\nR-CNN and HMM. IEEE Journal of Biomedical and Health Informatics , 25(5):1429\u2013\n1440, 2021.\n[169] Feng Wang, Xiang Xiang, Chang Liu, Trac D. Tran, Austin Reiter, Gregory D. Hager,\nHarry Quon, Jian Cheng, and Alan L. Yuille. Regularizing face verification nets for\npain intensity regression. In Proceedings - International Conference on Image Pro-\ncessing, ICIP , volume 2017-Septe, pages 1087\u20131091. IEEE Computer Society, 2018.\n[170] Marilena Ctlina Dragomir, Corneliu Florea, and Valentin Pupezescu. Automatic Sub-\nject Independent Pain Intensity Estimation using a Deep Learning Approach. In 2020\n8th E-Health and Bioengineering Conference, EHB 2020 , pages 1\u20134, 2020.\n[171] Ashish Semwal and Narendra D. Londhe. Automated facial expression based pain\nassessment using deep convolutional neural network. In Proceedings of the 3rd Inter-\nnational Conference on Intelligent Sustainable Systems, ICISS 2020 , pages 366\u2013370.\nInstitute of Electrical and Electronics Engineers Inc., 2020.\n[172] N Rathee, S Pahal, and P Sheoran. Pain detection from facial expressions using do-\nmain adaptation technique. Pattern Analysis and Applications , 2021.\n[173] Ghada Zamzmi, Rahul Paul, Dmitry Goldgof, Rangachar Kasturi, and Yu Sun. Pain\nAssessment from Facial Expression: Neonatal Convolutional Neural Network (N-\nCNN). In Proceedings of the International Joint Conference on Neural Networks ,\nvolume 2019-July. Institute of Electrical and Electronics Engineers Inc., 2019.\n[174] L P Carlini, L A Ferreira, G S Coutrin, V V Varoto, T M Heiderich, R X Balda,\nM M Barros, R Guinsburg, and C E Thomaz. A Convolutional Neural Network-\nbased Mobile Application to Bedside Neonatal Pain Assessment. In Conference onBIBLIOGRAPHY 181\nGraphics, Patterns and Images (SIBGRAPI) , pages 394\u2013401, Los Alamitos, CA, USA,\n2021. IEEE Computer Society.\n[175] Subhash Nerella, Julie Cupka, Matthew Ruppert, Patrick Tighe, Azra Bihorac, and\nParisa Rashidi. Pain action unit detection in critically ill patients. In 2021 IEEE\n45th Annual Computers, Software, and Applications Conference (COMPSAC) , pages\n645\u2013651, 2021.\n[176] Joy Egede, Michel Valstar, and Brais Martinez. Fusing deep learned and hand-crafted\nfeatures of appearance, shape, and dynamics for automatic pain estimation. In 12th\nIEEE International Conference on Automatic Face & Gesture Recognition , pages 689\u2013\n696, 2017.\n[177] Joy O. Egede and Michel Valstar. Cumulative attributes for pain intensity estimation.\nInICMI 2017 - Proceedings of the 19th ACM International Conference on Multi-\nmodal Interaction , volume 2017-Janua, pages 146\u2013153. Association for Computing\nMachinery, Inc, nov 2017.\n[178] Shashank Jaiswal, Joy Egede, and Michel Valstar. Deep learned cumulative attribute\nregression. In Proceedings - 13th IEEE International Conference on Automatic Face\nand Gesture Recognition, FG 2018 , pages 715\u2013722. Institute of Electrical and Elec-\ntronics Engineers Inc., 2018.\n[179] Mohammad Tavakolian, Carlos Guillermo Bermudez Cruces, and Abdenour Hadid.\nLearning to detect genuine versus posed pain from facial expressions using residual\ngenerative adversarial networks. In Proceedings - 14th IEEE International Confer-\nence on Automatic Face and Gesture Recognition, FG 2019 . Institute of Electrical\nand Electronics Engineers Inc., may 2019.\n[180] Mohammad Tavakolian, Miguel Bordallo Lopez, and Li Liu. Self-supervised pain in-\ntensity estimation from facial videos via statistical spatiotemporal distillation. Pattern\nRecognition Letters , 140:26\u201333, 2020.\n[181] Ehsan Othman, Philipp Werner, Frerk Saxen, Ayoub Al-Hamadi, Sascha Gruss, and\nSteffen Walter. Automatic vs. Human recognition of pain intensity from facial expres-\nsion on the x-ite pain database. Sensors , 21(9), may 2021.\n[182] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-\nChieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. Proceedings\nof the IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\ntion, pages 4510\u20134520, jan 2018.182 BIBLIOGRAPHY\n[183] Ehsan Othman, Philipp Werner, Frerk Saxen, Ayoub Al-Hamadi, and Steffen Walter.\nCross-database evaluation of pain recognition from facial video. In International\nSymposium on Image and Signal Processing and Analysis, ISPA , volume 2019-Septe,\npages 181\u2013186. IEEE Computer Society, sep 2019.\n[184] Mohammad Tavakolian and Abdenour Hadid. Deep Spatiotemporal Representation of\nthe Face for Automatic Pain Intensity Estimation. In 2018 24th International Confer-\nence on Pattern Recognition (ICPR) , volume 2018-Augus, pages 350\u2013354. Institute\nof Electrical and Electronics Engineers Inc., 2018.\n[185] Jinwei Wang and Huazhi Sun. Pain intensity estimation using deep spatiotem-\nporal and handcrafted features. IEICE Transactions on Information and Systems ,\nE101D(6):1572\u20131580, 2018.\n[186] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.\nLearning Spatiotemporal Features with 3D Convolutional Networks. In 2015 IEEE\nInternational Conference on Computer Vision (ICCV) , pages 4489\u20134497, 2015.\n[187] Yibo Huang, Linbo Qing, Shengyu Xu, Lu Wang, and Yonghong Peng. HybNet: a\nhybrid network structure for pain intensity estimation. Visual Computer , 2021.\n[188] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethink-\ning spatiotemporal feature learning: Speed-accuracy trade-offs in video classification.\nInComputer Vision \u2013 ECCV 2018 , volume 11219 LNCS, pages 318\u2013335, dec 2018.\n[189] Mohammad Tavakolian and Abdenour Hadid. A Spatiotemporal Convolutional Neu-\nral Network for Automatic Pain Intensity Estimation from Facial Dynamics. Interna-\ntional Journal of Computer Vision , 127(10):1413\u20131425, oct 2019.\n[190] Gnana Praveen R, Eric Granger, and Patrick Cardinal. Deep Domain Adaptation\nfor Ordinal Regression of Pain Intensity Estimation Using Weakly-Labeled Videos.\nCoRR , aug 2020.\n[191] R. Gnana Praveen, Eric Granger, and Patrick Cardinal. Deep Weakly Supervised Do-\nmain Adaptation for Pain Localization in Videos. In Gomez-Fernandez F Struc V .,\neditor, 15th IEEE International Conference on Automatic Face and Gesture Recogni-\ntion, pages 473\u2013480. Institute of Electrical and Electronics Engineers Inc., 2020.\n[192] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model\nand the kinetics dataset. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , July 2017.BIBLIOGRAPHY 183\n[193] Ghazal Bargshady, Xujuan Zhou, Ravinesh C. Deo, Jeffrey Soar, Frank Whittaker,\nand Hua Wang. The modeling of human facial pain intensity based on Temporal\nConvolutional Networks trained with video frames in HSV color space. Applied Soft\nComputing Journal , 97, 2020.\n[194] Siavash Rezaei, Abhishek Moturu, Shun Zhao, Kenneth M. Prkachin, Thomas Had-\njistavropoulos, and Babak Taati. Unobtrusive pain monitoring in older adults with\ndementia using pairwise and contrastive training. IEEE Journal of Biomedical and\nHealth Informatics , 25(5):1450\u20131462, 2021.\n[195] Hinton GE. Training products of experts by minimizing contrastive divergence. Neu-\nral computation , 14(8):1771\u20131800, aug 2002.\n[196] Vedhas Pandit, Maximilian Schmitt, Nicholas Cummins, and Bj \u00a8orn Schuller. I see it\nin your eyes: Training the shallowest-possible CNN to recognise emotions and pain\nfrom muted web-assisted in-the-wild video-chats in real-time. Information Processing\nand Management , 57(6):102347, nov 2020.\n[197] Jing Zhou, Xiaopeng Hong, Fei Su, and Guoying Zhao. Recurrent Convolutional\nNeural Network Regression for Continuous Pain Intensity Estimation in Video. IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition Work-\nshops , pages 1535\u20131543, may 2016.\n[198] Pau Rodriguez, Guillem Cucurull, Jordi Gonalez, Josep M. Gonfaus, Kamal Nasrol-\nlahi, Thomas B. Moeslund, and F. Xavier Roca. Deep Pain: Exploiting Long Short-\nTerm Memory Networks for Facial Expression Classification. IEEE Transactions on\nCybernetics , feb 2017.\n[199] Marco Bellantonio, Mohammad A. Haque, Pau Rodriguez, Kamal Nasrollahi, Taisi\nTelve, Sergio Escarela, Jordi Gonzalez, Thomas B. Moeslund, Pejman Rasti, and Gho-\nlamreza Anbarjafari. Spatio-temporal pain recognition in CNN-based super-resolved\nfacial images. Lecture Notes in Computer Science (including subseries Lecture Notes\nin Artificial Intelligence and Lecture Notes in Bioinformatics) , 10165 LNCS:151\u2013162,\n2017.\n[200] Ghazal Bargshady, Jeffrey Soar, Xujuan Zhou, Ravinesh C. Deo, Frank Whittaker,\nand Hua Wang. A joint deep neural network model for pain recognition from face.\n2019 IEEE 4th International Conference on Computer and Communication Systems,\nICCCS 2019 , pages 52\u201356, 2019.184 BIBLIOGRAPHY\n[201] Ghazal Bargshady, Xujuan Zhou, Ravinesh C. Deo, Jeffrey Soar, Frank Whittaker, and\nHua Wang. Enhanced deep learning algorithm development to detect pain intensity\nfrom facial expression images. Expert Systems with Applications , 149, 2020.\n[202] Antoni Mauricio, F \u00b4abio Cappabianco, Adriano Veloso, and Guillermo C \u00b4amara. A\nSequential Approach for Pain Recognition Based on Facial Representations. Lecture\nNotes in Computer Science (including subseries Lecture Notes in Artificial Intelli-\ngence and Lecture Notes in Bioinformatics) , 11754 LNCS:295\u2013304, 2019.\n[203] Selvarajah Thuseethan, Sutharshan Rajasegarar, and John Yearwood. Deep hybrid\nspatiotemporal networks for continuous pain intensity estimation. Lecture Notes in\nComputer Science (including subseries Lecture Notes in Artificial Intelligence and\nLecture Notes in Bioinformatics) , 11955 LNCS:449\u2013461, 2019.\n[204] Ghazal Bargshady, Xujuan Zhou, Ravinesh C. Deo, Jeffrey Soar, Frank Whittaker,\nand Hua Wang. Ensemble neural network approach detecting pain intensity from\nfacial expressions. Artificial Intelligence in Medicine , 109, 2020.\n[205] Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho,\nand Yu Sun. First Investigation into the Use of Deep Learning for Continuous As-\nsessment of Neonatal Postoperative Pain. In Gomez-Fernandez F Struc V ., editor,\nProceedings - 2020 15th IEEE International Conference on Automatic Face and Ges-\nture Recognition, FG 2020 , pages 415\u2013419. Institute of Electrical and Electronics\nEngineers Inc., 2020.\n[206] Nikolai Kalischek, Patrick Thiam, Peter Bellmann, and Friedhelm Schwenker. Deep\nDomain Adaptation for Facial Expression Analysis. In 2019 8th International Con-\nference on Affective Computing and Intelligent Interaction Workshops and Demos,\nACIIW 2019 , pages 317\u2013323. Institute of Electrical and Electronics Engineers Inc.,\n2019.\n[207] Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual\ndomain adaptation. In 6th International Conference on Learning Representations,\nICLR 2018 - Conference Track Proceedings , jun 2018.\n[208] Daniel Lopez Martinez, Ognjen Rudovic, and Rosalind Picard. Personalized Auto-\nmatic Estimation of Self-Reported Pain Intensity from Facial Expressions. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition Work-\nshops , volume 2017-July, pages 2318\u20132327. IEEE Computer Society, jun 2017.\n[209] Diyala Erekat, Zakia Hammal, Maimoon Siddiqui, and Hamdi Dibeklioglu. Enforcing\nmultilabel consistency for automatic spatio-temporal assessment of shoulder pain in-BIBLIOGRAPHY 185\ntensity. In ICMI 2020 Companion - Companion Publication of the 2020 International\nConference on Multimodal Interaction , pages 156\u2013164. Association for Computing\nMachinery, Inc, 2020.\n[210] Manh Tu Vu, Marie Beurton-Aimar, Pierre-yves Dezaunay, and Marine Cotty Es-\nlous. Automated Pain Estimation based on Facial Action Units from Multi-Databases.\nInJoint International Conference on Informatics, Electronics Vision (ICIEV) and In-\nternational Conference on Imaging, Vision Pattern Recognition (icIVPR) , pages 1\u20138,\n2021.\n[211] Dong Huang, Zhaoqiang Xia, Joshua Mwesigye, and Xiaoyi Feng. Pain-attentive\nnetwork: a deep spatio-temporal attention model for pain estimation. Multimedia\nTools and Applications , 79(37-38):28329\u201328354, 2020.\n[212] Jun Yu, Toru Kurihara, and Shu Zhan. Frame by Frame Pain Estimation Using Locally\nSpatial Attention Learning. Lecture Notes in Computer Science (including subseries\nLecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) , 11868\nLNCS:229\u2013238, 2019.\n[213] Haochen Xu and Manhua Liu. A Deep Attention Transformer Network for Pain Es-\ntimation with Facial Expression Video. In Jianjiang Feng, Junping Zhang, Manhua\nLiu, and Yuchun Fang, editors, Biometric Recognition , pages 112\u2013119, Cham, 2021.\nSpringer International Publishing.\n[214] Adria Mallol-Ragolta, Shuo Liu, Nicholas Cummins, and Bjorn Schuller. A Cur-\nriculum Learning Approach for Pain Intensity Recognition from Facial Expressions.\nIn Gomez-Fernandez F Struc V ., editor, 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition (FG 2020) , pages 829\u2013833. Institute of\nElectrical and Electronics Engineers Inc., 2020.\n[215] Yikang Guo, Li Wang, Yan Xiao, and Yingzi Lin. A Personalized Spatial-Temporal\nCold Pain Intensity Estimation Model Based on Facial Expression. IEEE journal of\ntranslational engineering in health and medicine , 9, 2021.\n[216] Sowmya Rasipuram, Bukka Nikhil Sai, Dinesh Babu Jayagopi, and Anutosh Maitra.\nUsing Deep 3D Features and an LSTM Based Sequence Model for Automatic Pain\nDetection in the Wild. In Gomez-Fernandez F Struc V ., editor, 2020 15th IEEE Inter-\nnational Conference on Automatic Face and Gesture Recognition (FG 2020) , pages\n781\u2013785. Institute of Electrical and Electronics Engineers Inc., 2020.\n[217] Ruicong Zhi and Ming Wan. Dynamic facial expression feature learning based on\nsparse RNN. In Proceedings of 2019 IEEE 8th Joint International Information Tech-186 BIBLIOGRAPHY\nnology and Artificial Intelligence Conference, ITAIC 2019 , pages 1373\u20131377. Institute\nof Electrical and Electronics Engineers Inc., may 2019.\n[218] Thomas Blumensath and Mike Davies. Iterative Thresholding for Sparse Approxima-\ntions. Journal of Fourier Analysis and Applications , 14:629\u2013654, 2008.\n[219] Patrick Thiam, Hans A. Kestler, and Friedhelm Schwenker. Two-stream attention\nnetwork for pain recognition from video sequences. Sensors (Switzerland) , 20(3):839,\nfeb 2020.\n[220] Antoni Mauricio, Jonathan Pe \u02dcna, Erwin Dianderas, Leonidas Mauricio, Jose D \u00b4\u0131az, and\nAntonio Mor \u00b4an. Chronic Pain Estimation Through Deep Facial Descriptors Analysis.\nCommunications in Computer and Information Science , 1070 CCIS:173\u2013185, 2020.\n[221] Jie Ting, Yi-Cheng Yang, Li-Chen Fu, Chu-Lin Tsai, and Chien-Hua Huang. Distance\nOrdering: A Deep Supervised Metric Learning for Pain Intensity Estimation. In 2021\n20th IEEE International Conference on Machine Learning and Applications (ICMLA) ,\npages 1083\u20131088, 2021.\n[222] Mingxin Yu, Yichen Sun, Bofei Zhu, Lianqing Zhu, Yingzi Lin, Xiaoying Tang,\nYikang Guo, Guangkai Sun, and Mingli Dong. Diverse frequency band-based convo-\nlutional neural networks for tonic cold pain assessment using EEG. Neurocomputing ,\n378:270\u2013282, 2020.\n[223] Jiahao Wang, Mengying Wei, Li Zhang, Gan Huang, Zhen Liang, Linling Li, and\nZhiguo Zhang. An Autoencoder-based Approach to Predict Subjective Pain Percep-\ntion from High-density Evoked EEG Potentials. In International Conference of the\nIEEE Engineering in Medicine Biology Society , volume 2020-July, pages 1507\u20131511.\nInstitute of Electrical and Electronics Engineers Inc., 2020.\n[224] Raul Fernandez Rojas, Julio Romero, Jehu Lopez-Aparicio, and Keng-Liang Ou.\nPain Assessment based on fNIRS using Bi-LSTM RNNs. In 10th International\nIEEE/EMBS Conference on Neural Engineering (NER) , pages 399\u2013402. IEEE, may\n2021.\n[225] Hyunjun Lim, Byeongnam Kim, Gyu Jeong Noh, and Sun K. Yoo. A deep neural\nnetwork-based pain classifier using a photoplethysmography signal. Sensors (Switzer-\nland) , 19(2), jan 2019.\n[226] Boyi Hu, Chong Kim, Xiaopeng Ning, and Xu Xu. Using a deep learning network to\nrecognise low back pain in static standing. Ergonomics , 61(10):1374\u20131381, oct 2018.BIBLIOGRAPHY 187\n[227] Danila Mamontov, Iana Polonskaia, Alina Skorokhod, Eugene Semenkin, Viktor\nKessler, and Friedhelm Schwenker. Evolutionary algorithms for the design of neu-\nral network classifiers for the classification of pain intensity. Multimodal Pattern\nRecognition of Social Signals in Human-Computer-Interaction , 11377 LNAI:84\u2013100,\n2019.\n[228] Chuan Yu Chang and Jia Jing Li. Application of deep learning for recognizing in-\nfant cries. In 2016 IEEE International Conference on Consumer Electronics-Taiwan,\nICCE-TW 2016 . Institute of Electrical and Electronics Engineers Inc., jul 2016.\n[229] Md Sirajus Salekin, Ghada Zamzmi, Rahul Paul, Dmitry Goldgof, Rangachar Kas-\nturi, Thao Ho, and Yu Sun. Harnessing the Power of Deep Learning Methods in\nHealthcare: Neonatal Pain Assessment from Crying Sound. In 2019 IEEE Healthcare\nInnovations and Point of Care Technologies, HI-POCT 2019 , pages 127\u2013130. Institute\nof Electrical and Electronics Engineers Inc., 2019.\n[230] Patrick Thiam and Friedhelm Schwenker. Combining deep and hand-crafted features\nfor audio-based pain intensity classification. Lecture Notes in Computer Science (in-\ncluding subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioin-\nformatics) , 11377 LNAI:49\u201358, 2019.\n[231] Fu Sheng Tsai, Yi Ming Weng, Chip Jin Ng, and Chi Chun Lee. Embedding stacked\nbottleneck vocal features in a LSTM architecture for automatic pain level classifica-\ntion during emergency triage. In 2017 7th International Conference on Affective Com-\nputing and Intelligent Interaction, ACII 2017 , volume 2018-Janua, pages 313\u2013318.\nInstitute of Electrical and Electronics Engineers Inc., 2018.\n[232] P Gouverneur, F Li, W M Adamczyk, T M Szikszay, K Luedtke, and M Grzegorzek.\nComparison of feature extraction methods for physiological signals for heat-based\npain recognition. Sensors , 21(14), 2021.\n[233] Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho,\nand Yu Sun. Multi-channel neural network for assessing neonatal pain from videos.\nInIEEE International Conference on Systems, Man and Cybernetics , volume 2019-\nOctob, pages 1551\u20131556. Institute of Electrical and Electronics Engineers Inc., oct\n2019.\n[234] Emad Kasaeyan Naeini, Sina Shahhosseini, Ajan Subramanian, Tingjue Yin, Amir M.\nRahmani, and Nikil Dutt. An Edge-Assisted and Smart System for Real-Time Pain\nMonitoring. In Proceedings - 4th IEEE/ACM Conference on Connected Health: Appli-188 BIBLIOGRAPHY\ncations, Systems and Engineering Technologies, CHASE 2019 , pages 47\u201352. Institute\nof Electrical and Electronics Engineers Inc., 2019.\n[235] Patrick Thiam, Peter Bellmann, Hans A. Kestler, and Friedhelm Schwenker. Explor-\ning deep physiological models for nociceptive pain recognition. Sensors , 19(20):4503,\noct 2019.\n[236] Ahmad Al-Qerem. An efficient machine-learning model based on data augmentation\nfor pain intensity recognition. Egyptian Informatics Journal , 21(4):241\u2013257, 2020.\n[237] R Zhi, C Zhou, J Yu, T Li, and G Zamzmi. Multimodal-based stream integrated\nneural networks for pain assessment. IEICE Transactions on Information and Systems ,\nE104D(12):2184\u20132194, 2021.\n[238] Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho,\nand Yu Sun. Multimodal spatio-temporal deep learning approach for neonatal postop-\nerative pain assessment. Computers in Biology and Medicine , 129, 2021.\n[239] Run Wang, Ke Xu, Hui Feng, and Wei Chen. Hybrid RNN-ANN Based Deep Phys-\niological Network for Pain Recognition. In Proceedings of the Annual International\nConference of the IEEE Engineering in Medicine and Biology Society, EMBS , volume\n2020-July, pages 5584\u20135587. Institute of Electrical and Electronics Engineers Inc., jul\n2020.\n[240] Yun Zhao, Franklin Ly, Qinghang Hong, Zhuowei Cheng, Tyler Santander, Henry T.\nYang, Paul K. Hansma, and Linda Petzold. How Much Does It Hurt: A Deep Learning\nFramework for Chronic Pain Score Assessment. In Cuzzocrea A Zaniolo C Wu X Di\nFatta G. Sheng V ., editor, IEEE International Conference on Data Mining Workshops,\nICDMW , volume 2020-Novem, pages 651\u2013660. IEEE Computer Society, 2020.\n[241] Xinhui Yuan and Marwa Mahmoud. ALANet:Autoencoder-LSTM for pain and pro-\ntective behaviour detection. In Proceedings - 2020 15th IEEE International Confer-\nence on Automatic Face and Gesture Recognition, FG 2020 , pages 824\u2013828, 2020.\n[242] Yi Li, Shreya Ghosh, Jyoti Joshi, and Sharon Oviatt. LSTM-DNN based Approach\nfor Pain Intensity and Protective Behaviour Prediction. In Gomez-Fernandez F Struc\nV ., editor, Proceedings - 2020 15th IEEE International Conference on Automatic Face\nand Gesture Recognition, FG 2020 , pages 819\u2013823. Institute of Electrical and Elec-\ntronics Engineers Inc., 2020.\n[243] Patrick Thiam, Hans A. Kestler, and Friedhelm Schwenker. Multimodal deep denois-\ning convolutional autoencoders for pain intensity classification based on physiologicalBIBLIOGRAPHY 189\nsignals. In ICPRAM 2020 - Proceedings of the 9th International Conference on Pat-\ntern Recognition Applications and Methods , pages 289\u2013296. SCITEPRESS - Science\nand Technology Publications, 2020.\n[244] Patrick Thiam, Heinke Hihn, Daniel A Braun, Hans A Kestler, and Friedhelm\nSchwenker. Multi-Modal Pain Intensity Assessment Based on Physiological Signals:\nA Deep Learning Perspective. Frontiers in Physiology , 12, 2021.\n[245] Saranya Devi Subramaniam and Brindha Dass. Automated Nociceptive Pain Assess-\nment Using Physiological Signals and a Hybrid Deep Learning Network. IEEE Sen-\nsors Journal , 21(3):3335\u20133343, 2021.\n[246] Yi Li, Shreya Ghosh, and Jyoti Joshi. PLAAN: Pain Level Assessment with Anomaly-\ndetection based Network. Journal on Multimodal User Interfaces , 2021.\n[247] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transform-\ners.AI Open , 3:111\u2013132, 2022.\n[248] Philipp Werner, Ayoub Al-Hamadi, Kerstin Limbrecht-Ecklundt, Steffen Walter,\nSascha Gruss, and Harald C. Traue. Automatic Pain Assessment with Facial Activity\nDescriptors. IEEE Transactions on Affective Computing , 8(3):286\u2013299, jul 2017.\n[249] Kunz M and Lautenbacher S. The faces of pain: a cluster analysis of individual\ndifferences in facial activity patterns of pain. European journal of pain (London,\nEngland) , 18(6):813\u2013823, 2014.\n[250] Manon Ranger, Celeste Johnston, and Karthika Anand. Current controversies regard-\ning pain assessment in neonates. Seminars in perinatology , 31:283\u20138, 11 2007.\n[251] Mark R. Jones, Ken P. Ehrhardt, Juan G. Ripoll, Bharat Sharma, Ira W. Padnos,\nRachel J. Kaye, and Alan D. Kaye. Pain in the Elderly, feb 2016.\n[252] Laura Pence Forsythe, Beverly Thorn, Melissa Day, and Grace Shelby. Race and Sex\nDifferences in Primary Appraisals, Catastrophizing, and Experimental Pain Outcomes.\nThe Journal of Pain , 12(5):563\u2013572, may 2011.\n[253] Philipp Tschandl, Noel Codella, Beng \u00a8u Nisa Akay, Giuseppe Argenziano, Ralph P\nBraun, Horacio Cabo, David Gutman, Allan Halpern, Brian Helba, Rainer Hofmann-\nWellenhof, Aimilios Lallas, Jan Lapins, Caterina Longo, Josep Malvehy, Michael A\nMarchetti, Ashfaq Marghoob, Scott Menzies, Amanda Oakley, John Paoli, Susana\nPuig, Christoph Rinner, Cliff Rosendahl, Alon Scope, Christoph Sinz, H Peter Soyer,\nLuc Thomas, Iris Zalaudek, and Harald Kittler. Comparison of the accuracy of human190 BIBLIOGRAPHY\nreaders versus machine-learning algorithms for pigmented skin lesion classification:\nan open, web-based, international, diagnostic study. The Lancet Oncology , 20(7):938\u2013\n947, 2019.\n[254] Guang Yang, Qinghao Ye, and Jun Xia. Unbox the black-box for the medical explain-\nable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases\nand beyond. Information Fusion , 77:29\u201352, 2022.\n[255] Karim Lekadir, Richard Osuala, Catherine Gallin, Noussair Lazrak, Kaisar Kushibar,\nGianna Tsakou, Susanna Ausso, Leonor Cerda Alberich, Kostas Marias, Manolis\nTsiknakis, Sara Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, Henry C\nWoodruff, Philippe Lambin, and Luis Marti-Bonmati. Future-ai: Guiding princi-\nples and consensus recommendations for trustworthy artificial intelligence in medical\nimaging, 2021.\n[256] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable ai:\nA review of machine learning interpretability methods. Entropy , 23(1):1\u201345, 2021.\n[257] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu.\nDefinitions, methods, and applications in interpretable machine learning. Proceedings\nof the National Academy of Sciences , 116(44):22071\u201322080, 2019.\n[258] Rachael E. Jack. Culture and facial expressions of emotion. Visual Cognition , 21(9-\n10):1248\u20131286, 2013.\n[259] Erik Cambria, Newton Howard, Jane Hsu, and Amir Hussain. Sentic blending: Scal-\nable multimodal fusion for the continuous interpretation of semantics and sentics. In\n2013 IEEE Symposium on Computational Intelligence for Human-like Intelligence\n(CIHLI) , pages 108\u2013117, 2013.\n[260] Saurabh Hinduja, Shaun Canavan, and Gurmeet Kaur. Multimodal Fusion of Physio-\nlogical Signals and Facial Action Units for Pain Recognition. In Gomez-Fernandez F\nStruc V ., editor, IEEE International Conference on Automatic Face and Gesture\nRecognition , pages 577\u2013581. Institute of Electrical and Electronics Engineers Inc.,\n2020.\n[261] Tobias B. Ricken, Peter Bellmann, Sascha Gruss, Hans A. Kestler, Steffen Walter,\nand Friedhelm Schwenker. Pain recognition differences between female and male\nsubjects: An analysis based on the physiological signals of the x-ite pain database. In\nCompanion Publication of the 25th International Conference on Multimodal Interac-\ntion, ICMI \u201923 Companion, pages 121\u2013130, New York, NY , USA, 2023. Association\nfor Computing Machinery.BIBLIOGRAPHY 191\n[262] A real-time qrs detection algorithm. IEEE Transactions on Biomedical Engineering ,\nBME-32:230\u2013236, 1985.\n[263] M A Z Fariha, R Ikeura, S Hayakawa, and S Tsutsumi. Analysis of Pan-Tompkins\nAlgorithm Performance with Noisy ECG Signals. Journal of Physics: Conference\nSeries , 1532(1):12022, 2020.\n[264] Feifei Liu, Shoushui Wei, Yibin Li, Xinge Jiang, Zhimin Zhang, Ling Zhang, and\nChengyu Liu. The accuracy on the common Pan-Tompkins based QRS detection\nmethods through low-quality electrocardiogram database. Journal of Medical Imag-\ning and Health Informatics , 7(5):1039\u20131043, 2017.\n[265] Kai Zhao, Yongfu Li, Guoxing Wang, Yu Pu, and Yong Lian. A robust QRS detec-\ntion and accurate R-peak identification algorithm for wearable ECG sensors. Science\nChina Information Sciences , 64(8):182401, 2021.\n[266] Daniel Lopez-Martinez and Rosalind Picard. Continuous Pain Intensity Estimation\nfrom Autonomic Signals with Recurrent Neural Networks. Conference proceedings\n: ... Annual International Conference of the IEEE Engineering in Medicine and Biol-\nogy Society. IEEE Engineering in Medicine and Biology Society. Annual Conference ,\n2018:5624\u20135627, jul 2018.\n[267] Philipp Werner, Ayoub Al-Hamadi, Robert Niese, Steffen Walter, Sascha Gruss, and\nHarald C. Traue. Automatic pain recognition from video and biomedical signals. In\nInternational Conference on Pattern Recognition , pages 4582\u20134587. Institute of Elec-\ntrical and Electronics Engineers Inc., 2014.\n[268] Roberto Cipolla, Yarin Gal, and Alex Kendall. Multi-task learning using uncertainty\nto weigh losses for scene geometry and semantics. In 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 7482\u20137491, 2018.\n[269] Dong Huang, Xiaoyi Feng, Haixi Zhang, Zitong Yu, Jinye Peng, Guoying Zhao, and\nZhaoqiang Xia. Spatio-temporal pain estimation network with measuring pseudo\nheart rate gain. IEEE Transactions on Multimedia , 24:3300\u20133313, 2022.\n[270] Daniel Lopez-Martinez and Rosalind Picard. Continuous Pain Intensity Estimation\nfrom Autonomic Signals with Recurrent Neural Networks. In International Confer-\nence of the IEEE Engineering in Medicine and Biology Society. , volume 2018, pages\n5624\u20135627, 2018.\n[271] Philipp Werner, Daniel Lopez-Martinez, Steffen Walter, Ayoub Al-Hamadi, Sascha\nGruss, and Rosalind Picard. Automatic Recognition Methods Supporting Pain As-\nsessment: A Survey. IEEE Transactions on Affective Computing , 2019.192 BIBLIOGRAPHY\n[272] Anderson Faria Claret, Karina Rabello Casali, Tatiana Sousa Cunha, and Matheus Car-\ndoso Moraes. Automatic classification of emotions based on cardiac signals: A\nsystematic literature review. Annals of Biomedical Engineering , 51(11):2393\u20132414,\n2023.\n[273] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceed-\nings of the 31st International Conference on Neural Information Processing Systems ,\nNIPS\u201917, pages 5998\u20136008, Red Hook, NY , USA, 2017. Curran Associates Inc.\n[274] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale, 2021.\n[275] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing XU, and Yunhe Wang. Trans-\nformer in transformer. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems ,\nvolume 34, pages 15908\u201315919. Curran Associates, Inc., 2021.\n[276] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye\nTeh. Set transformer: A framework for attention-based permutation-invariant neural\nnetworks. In International conference on machine learning , pages 3744\u20133753. PMLR,\n2019.\n[277] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and\nJoao Carreira. Perceiver: General perception with iterative attention. In International\nconference on machine learning , pages 4651\u20134664. PMLR, 2021.\n[278] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint Face Detection\nand Alignment using Multi-task Cascaded Convolutional Networks. IEEE Signal\nProcessing Letters , 23(10):1499\u20131503, apr 2016.\n[279] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d\nface alignment problem? (and a dataset of 230,000 3d facial landmarks). In Proceed-\nings of the IEEE International Conference on Computer Vision (ICCV) , Oct 2017.\n[280] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman. VG-\nGFace2: A dataset for recognising faces across pose and age. In Proceedings -\n13th IEEE International Conference on Automatic Face and Gesture Recognition, FG\n2018 , pages 67\u201374. Institute of Electrical and Electronics Engineers Inc., oct 2018.BIBLIOGRAPHY 193\n[281] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji\nLakshminarayanan. Augmix: A simple data processing method to improve robustness\nand uncertainty. arXiv preprint arXiv:1912.02781 , 2019.\n[282] Samuel G. MG\u2019Oller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-\nart data augmentation. In 2021 IEEE/CVF International Conference on Computer\nVision (ICCV) , pages 754\u2013762, 2021.\n[283] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention\nvisualization. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 782\u2013791, 2021.\n[284] Sabrina Patania, Giuseppe Boccignone, Sathya Bur \u02c7si\u00b4c, Alessandro D\u2019Amelio, and\nRaffaella Lanzarotti. Deep graph neural network for video-based facial pain expres-\nsion assessment. SAC \u201922, pages 585\u2013591, New York, NY , USA, 2022. Association\nfor Computing Machinery.\n[285] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,\nU. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran\nAssociates, Inc., 2017.\n[286] Tsz-Him Cheung and Dit-Yan Yeung. {MODALS }: Modality-agnostic automated\ndata augmentation in the latent space. In International Conference on Learning Rep-\nresentations , 2021.\n[287] Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. Affectnet: A\ndatabase for facial expression, valence, and arousal computing in the wild. IEEE\nTransactions on Affective Computing , 10(1):18\u201331, 2019.\n[288] Shichuan Du, Yong Tao, and Aleix M. Martinez. Compound facial expressions of\nemotion. Proceedings of the National Academy of Sciences , 111(15):E1454\u2013E1462,\n2014.\n[289] Shan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep locality-\npreserving learning for expression recognition in the wild. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , July 2017.\n[290] Roberto Cipolla, Yarin Gal, and Alex Kendall. Multi-task learning using uncertainty\nto weigh losses for scene geometry and semantics. In 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 7482\u20137491, 2018.194 BIBLIOGRAPHY\n[291] Mohammad Kachuee, Shayan Fazeli, and Majid Sarrafzadeh. Ecg heartbeat classifi-\ncation: A deep transferable representation. In 2018 IEEE International Conference\non Healthcare Informatics (ICHI) , pages 443\u2013444, 2018.\n[292] G.B. Moody and R.G. Mark. The impact of the mit-bih arrhythmia database. IEEE\nEngineering in Medicine and Biology Magazine , 20(3):45\u201350, 2001.\n[293] R. Bousseljot, D. Kreiseler, and A. Schnabel. Nutzung der ekg-signaldatenbank car-\ndiodat der ptb G\u2019Ober das internet. Biomedical Engineering / Biomedizinische Tech-\nnik, 40(s1):317\u2013318, 1995.\n[294] AL Goldberger, LA Amaral, L Glass, JM Hausdorff, PC Ivanov, RG Mark, JE Mietus,\nGB Moody, CK Peng, and HE Stanley. Physiobank, physiotoolkit, and physionet:\ncomponents of a new research resource for complex physiologic signals. Circulation ,\n101(23):E215\u201320, June 2000.\n[295] Philipp Werner, Ayoub Al-Hamadi, and Steffen Walter. Analysis of facial expressive-\nness during experimentally induced heat pain. In 2017 Seventh International Con-\nference on Affective Computing and Intelligent Interaction Workshops and Demos\n(ACIIW) , pages 176\u2013180, 2017.\n[296] Philipp Werner, Ayoub Al-Hamadi, Kerstin Limbrecht-Ecklundt, Steffen Walter,\nSascha Gruss, and Harald C Traue. Automatic pain assessment with facial activity\ndescriptors. IEEE Transactions on Affective Computing , 8(3):286\u2013299, 2016.\n[297] Markus K \u00a8achele, Philipp Werner, Ayoub Al-Hamadi, G \u00a8unther Palm, Steffen Walter,\nand Friedhelm Schwenker. Bio-visual fusion for person-independent recognition of\npain intensity. In Multiple Classifier Systems , pages 220\u2013230. Springer International\nPublishing, 2015.\n[298] Vasileios C. Pezoulas, Dimitrios I. Zaridis, Eugenia Mylona, Christos Androutsos,\nKosmas Apostolidis, Nikolaos S. Tachos, and Dimitrios I. Fotiadis. Synthetic data\ngeneration methods in healthcare: A review on open-source tools and methods. Com-\nputational and Structural Biotechnology Journal , 23:2892\u20132910, 2024.\n[299] Mustafa MM Al Qudah, Ahmad SA Mohamed, and Syaheerah L Lutfi. Affective\nstate recognition using thermal-based imaging: A survey. Computer Systems Science\n& Engineering , 37(1), 2021.\n[300] Stephanos Ioannou, Vittorio Gallese, and Arcangelo Merla. Thermal infrared imaging\nin psychophysiology: Potentialities and limits. Psychophysiology , 51(10):951\u2013963,\n2014.BIBLIOGRAPHY 195\n[301] Sophie Jarlier, Didier Grandjean, Sylvain Delplanque, Karim N\u2019Diaye, Isabelle\nCayeux, Maria Ines Velazco, David Sander, Patrik Vuilleumier, and Klaus R. Scherer.\nThermal analysis of facial muscles contractions. IEEE Transactions on Affective Com-\nputing , 2(1):2\u20139, 2011.\n[302] A Merla, L Di Donato, PM Rossini, and GL Romani. Emotion detection through func-\ntional infrared imaging: preliminary results. Biomedizinische Technick , 48(2):284\u2013\n286, 2004.\n[303] Youssef Mohamed, Arzu G \u00a8uneysu, S \u00b4everin Lemaignan, and Iolanda Leite. Multi-\nmodal affect detection using thermal and optical imaging in a gamified robotic exer-\ncise. International Journal of Social Robotics , Oct 2023.\n[304] Varlik K Erel and Heval Selman Ozkan. Thermal camera as a pain monitor. Journal\nof Pain Research , 10:2827\u20132832, 2017.\n[305] Mohammad A. Haque, Ruben B. Bautista, Fatemeh Noroozi, Kaustubh Kulkarni,\nChristian B. Laursen, Ramin Irani, Marco Bellantonio, Sergio Escalera, Golamreza\nAnbarjafari, Kamal Nasrollahi, Ole K. Andersen, Erika G. Spaich, and Thomas B.\nMoeslund. Deep multimodal pain recognition: A database and comparison of spatio-\ntemporal visual modalities. In 2018 13th IEEE International Conference on Automatic\nFace & Gesture Recognition (FG 2018) , pages 250\u2013257, 2018.\n[306] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv\npreprint arXiv:1411.1784 , 2014.\n[307] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C\nCourville. Improved training of wasserstein gans. In I. Guyon, U. V on Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.\n[308] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan\nCatanzaro. High-resolution image synthesis and semantic manipulation with condi-\ntional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , June 2018.\n[309] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image trans-\nlation with conditional adversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 1125\u20131134, 2017.\n[310] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,\nThomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkor-\neit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for196 BIBLIOGRAPHY\nvision. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Processing Systems , volume 34,\npages 24261\u201324272. Curran Associates, Inc., 2021.\n[311] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang.\nAn image patch is a wave: Phase-aware vision mlp. In 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , pages 10925\u201310934, 2022.\n[312] Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat Khassanov,\nMichael Lewis, and Huseyin Atakan Varol. Speakingfaces: A large-scale multimodal\ndataset of voice commands with visual and thermal video streams. Sensors , 21(10),\n2021.\n[313] Gwangbin Bae, Martin de La Gorce, Tadas Baltru \u02c7saitis, Charlie Hewitt, Dong Chen,\nJulien Valentin, Roberto Cipolla, and Jingjing Shen. Digiface-1m: 1 million digital\nface images for face recognition. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV) , pages 3526\u20133535, January 2023.\n[314] Xu Zheng, Yuanhuiyi Lyu, and Lin Wang. Learning modality-agnostic representation\nfor semantic segmentation from any modalities, 2024.\n[315] Dingkang Yang, Mingcheng Li, Linhao Qu, Kun Yang, Peng Zhai, Song Wang, and\nLihua Zhang. Asynchronous multimodal video sequence fusion via learning modality-\nexclusive and -agnostic representations, 2024.\n[316] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Syd-\nney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brun-\nskill, et al. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258 , 2021.\n[317] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr\nDollar, and Ross Girshick. Segment anything. In 2023 IEEE/CVF International Con-\nference on Computer Vision (ICCV) , pages 3992\u20134003, 2023.\n[318] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything\nin medical images. Nature Communications , 15(1):654, 2024.\n[319] Junde Wu, Wei Ji, Yuanpei Liu, Huazhu Fu, Min Xu, Yanwu Xu, and Yueming Jin.\nMedical sam adapter: Adapting segment anything model for medical image segmen-\ntation. arXiv preprint arXiv:2304.12620 , 2023.BIBLIOGRAPHY 197\n[320] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander\nNovikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias\nSpringenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175 , 2022.\n[321] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer,\nHisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan.\nFoundational models defining a new era in vision: A survey and outlook. arXiv\npreprint arXiv:2307.13721 , 2023.\n[322] Pau Rodriguez, Guillem Cucurull, Jordi Gonzalez, Josep M. Gonfaus, Kamal Nasrol-\nlahi, Thomas B. Moeslund, and F. Xavier Roca. Deep pain: Exploiting long short-term\nmemory networks for facial expression classification. IEEE Transactions on Cyber-\nnetics , 52(5):3314\u20133324, 2022.\n[323] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recog-\nnition at scale. arXiv preprint arXiv:2010.11929 , 2020.\n[324] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan,\nand Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. In 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages\n5260\u20135269, 2022.\n[325] Sitong Wu, Tianyi Wu, Haoru Tan, and Guodong Guo. Pale transformer: A general\nvision transformer backbone with pale-shaped attention. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 36, pages 2731\u20132739, 2022.\n[326] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. Swin transformer: Hierarchical vision transformer using shifted\nwindows. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV) ,\npages 9992\u201310002, 2021.\n[327] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan.\nEfficientvit: Memory efficient vision transformer with cascaded group attention. In\n2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,\npages 14420\u201314430, 2023.\n[328] Judith M. Ford, Vanessa A. Palzes, Brian J. Roach, and Daniel H. Mathalon. Did I\nDo That? Abnormal Predictive Processes in Schizophrenia When Button Pressing to\nDeliver a Tone. Schizophrenia Bulletin , 40(4):804\u2013812, 07 2013.198 BIBLIOGRAPHY\n[329] David Gaddy and Dan Klein. Digital voicing of silent speech. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) ,\npages 5521\u20135530, Online, 2020. Association for Computational Linguistics.\n[330] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V . Le. Randaugment: Prac-\ntical automated data augmentation with a reduced search space. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops (CVPRW) , pages\n3008\u20133017, 2020.\n[331] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wo-\njna. Rethinking the inception architecture for computer vision. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 2818\u20132826,\n2016.\n[332] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R\nSalakhutdinov. Improving neural networks by preventing co-adaptation of feature\ndetectors. arXiv preprint arXiv:1207.0580 , 2012.\n[333] Raul Fernandez Rojas, Niraj Hirachan, Nicholas Brown, Gordon Waddington, Luke\nMurtagh, Ben Seymour, and Roland Goecke. Multimodal physiological sensing for\nthe assessment of acute pain. Frontiers in Pain Research , 4, 2023.\n[334] Raul Fernandez Rojas, Xu Huang, Jesus Hernandez-Juarez, and Keng-Liang Ou.\nPhysiological fluctuations show frequency-specific networks in fnirs signals during\nresting state. In 2017 39th Annual International Conference of the IEEE Engineering\nin Medicine and Biology Society (EMBC) , pages 2550\u20132553, 2017.\n[335] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and\nBryan Catanzaro. Efficient token mixing for transformers via adaptive fourier neural\noperators. In International Conference on Learning Representations , 2021.\n[336] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang\nZhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 10371\u201310381, 2024.\n[337] Fadi Boutros, Marco Huber, Patrick Siebke, Tim Rieber, and Naser Damer. Sface:\nPrivacy-friendly and accurate face recognition using synthetic data. In 2022 IEEE\nInternational Joint Conference on Biometrics (IJCB) , pages 1\u201311, 2022.\n[338] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep\nneural networks without residuals. arXiv preprint arXiv:1605.07648 , 2016.BIBLIOGRAPHY 199\n[339] Philipp Werner, Ayoub Al-Hamadi, Robert Niese, Steffen Walter, Sascha Gruss, and\nHarald C. Traue. Automatic pain recognition from video and biomedical signals. In\n2014 22nd International Conference on Pattern Recognition , pages 4582\u20134587, 2014.\n[340] Ruijing Yang, Ziyu Guan, Zitong Yu, Xiaoyi Feng, Jinye Peng, and Guoying Zhao.\nNon-contact pain recognition from video sequences with remote physiological mea-\nsurements prediction. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth Interna-\ntional Joint Conference on Artificial Intelligence, IJCAI-21 , pages 1231\u20131237. Inter-\nnational Joint Conferences on Artificial Intelligence Organization, 8 2021.\n[341] Zhenyuan Lu, Burcu Ozek, and Sagar Kamarthi. Transformer encoder with multi-\nscale deep learning for pain classification using physiological signals. Frontiers in\nPhysiology , 14, 2023.\n[342] Kim Ngan Phan, Ngumimi Karen Iyortsuun, Sudarshan Pant, Hyung-Jeong Yang, and\nSoo-Hyung Kim. Pain recognition with physiological signals using multi-level con-\ntext information. IEEE Access , 11:20114\u201320127, 2023.\n[343] Mingzhe Jiang, Yufei Li, Jiangshan He, Yuqiang Yang, Hui Xie, and Xueli Chen.\nPhysiological time-series fusion with hybrid attention for adaptive recognition of pain.\nIEEE Journal of Biomedical and Health Informatics , pages 1\u20139, 2024.\n[344] Ruicong Zhi and Junwei Yu. Multi-modal fusion based automatic pain assessment.\nInProceedings of 2019 IEEE 8th Joint International Information Technology and Ar-\ntificial Intelligence Conference, ITAIC 2019 , pages 1378\u20131382. Institute of Electrical\nand Electronics Engineers Inc., may 2019.\n[345] Pooja Prajod, Dominik Schiller, Daksitha Withanage Don, and Elisabeth Andre. Faces\nof experimental pain: Transferability of deep learned heat pain features to electrical\npain, 2024.\n[346] Minh-Duc Nguyen, Hyung-Jeong Yang, Soo-Hyung Kim, Ji-Eun Shin, and Seung-\nWon Kim. Transformer with leveraged masked autoencoder for video-based pain\nassessment, 2024.\n[347] Manisha Shantaram Patil and Hitendra Dhansing Patil. Logistic regression based\nmodel for pain intensity level detection from biomedical signal. International Re-\nsearch Journal of Multidisciplinary Scope , 2024.\n[348] Manisha S. Patil and Hitendra D. Patil. Ensemble neural networks for multimodal\nacute pain intensity evaluation using video and physiological signals. Journal of Com-\nputational Analysis and Applications (JoCAAA) , 33(05):779\u2013791, Sep. 2024.200 BIBLIOGRAPHY\n[349] Xinwei Ji, Tianming Zhao, Wei Li, and Albert Zomaya. Automatic pain assess-\nment with ultra-short electrodermal activity signal. In Proceedings of the 38th\nACM/SIGAPP Symposium on Applied Computing , SAC \u201923, pages 618\u2013625, New\nYork, NY , USA, 2023. Association for Computing Machinery.\n[350] Markus K \u00a8achele, Patrick Thiam, Mohammadreza Amirian, Philipp Werner, Steffen\nWalter, Friedhelm Schwenker, and G \u00a8unther Palm. Multimodal data fusion for person-\nindependent, continuous estimation of pain intensity. In Lazaros Iliadis and Chrisina\nJayne, editors, Engineering Applications of Neural Networks , pages 275\u2013285, Cham,\n2015. Springer International Publishing.\n[351] Fatemeh Pouromran, Srinivasan Radhakrishnan, and Sagar Kamarthi. Exploration\nof physiological sensors, features, and machine learning models for pain intensity\nestimation. PLOS ONE , 16(7):1\u201317, 07 2021.\n[352] Patrick Thiam, Heinke Hihn, Daniel A. Braun, Hans A. Kestler, and Friedhelm\nSchwenker. Multi-modal pain intensity assessment based on physiological signals:\nA deep learning perspective. Frontiers in Physiology , 12, 2021.\n[353] Mingzhe Jiang, Riitta Rosio, Sanna Salantera, Amir M. Rahmani, Pasi Liljeberg,\nDaniel S. da Silva, Victor Hugo C. de Albuquerque, and Wanqing Wu. Personal-\nized and adaptive neural networks for pain detection from multi-modal physiological\nfeatures. Expert Systems with Applications , 235:121082, 2024.\n[354] Markus K \u00a8achele, Patrick Thiam, Mohammadreza Amirian, Friedhelm Schwenker,\nand GG\u2019Onther Palm. Methods for person-centered continuous pain intensity assess-\nment from bio-physiological channels. IEEE Journal of Selected Topics in Signal\nProcessing , 10(5):854\u2013864, 2016.\n[355] Peter Bellmann and Friedhelm Schwenker. Automated pain assessment: Is it use-\nful to combine person-specific data samples? In 2020 IEEE Symposium Series on\nComputational Intelligence (SSCI) , pages 1588\u20131593, 2020.Appendix\nSupplementary Metrics\nTable 1: Results utilizing the video modality reported on precision, recall, and F1 score\n(refer to Section 5.3).\nEpochs MetricPretraining stage Pipeline Augmentations Task\n1st2ndFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n500Precision \u2713 - \u2713 -\u2713 - - 72.53 31.24\nRecall \u2713 - \u2713 -\u2713 - - 74.31 29.61\nF1 \u2713 - \u2713 -\u2713 - - 71.95 27.16\n500Precision - \u2713 \u2713 -\u2713 - - 74.21 33.36\nRecall - \u2713 \u2713 -\u2713 - - 76.74 33.41\nF1 - \u2713 \u2713 -\u2713 - - 72.24 28.77\n500Precision - \u2713 - \u2713 \u2713 - - 68.11 31.50\nRecall - \u2713 - \u2713 \u2713 - - 72.15 27.99\nF1 - \u2713 - \u2713 \u2713 - - 65.92 25.14\n500Precision - \u2713 \u2713 \u2713 \u2713 - - 65.14 27.78\nRecall - \u2713 \u2713 \u2713 \u2713 - - 70.36 18.42\nF1 - \u2713 \u2713 \u2713 \u2713 - - 61.93 18.86\n500Precision - \u2713 \u2713 \u2713c\u2713 - - 74.88 33.96\nRecall - \u2713 \u2713 \u2713c\u2713 - - 77.41 34.31\nF1 - \u2713 \u2713 \u2713c\u2713 - - 73.90 29.20\n500Precision - \u2713 \u2713 \u2713c\u2713 \u2713 - 73.09 32.17\nRecall - \u2713 \u2713 \u2713c\u2713 \u2713 - 75.72 28.41\nF1 - \u2713 \u2713 \u2713c\u2713 \u2713 - 71.92 26.02\n500Precision - \u2713 \u2713 \u2713c\u2713 - \u2713 74.87 33.88\nRecall - \u2713 \u2713 \u2713c\u2713 - \u2713 77.80 29.30\nF1 - \u2713 \u2713 \u2713c\u2713 - \u2713 73.59 27.74\n500Precision - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 73.12 32.79\nRecall - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 76.18 28.51\nF1 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 71.91 26.57\n800Precision - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 77.15 35.39\nRecall - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 79.35 35.11\nF1 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 76.33 31.70\n201202 Appendix\nTable 2: Results utilizing the heart rate modality reported on precision, recall, and F1 score\n(refer to Section 5.3).\nEpochs MetricHR\nEncoderAugmentations Task\nBasic Mask AugmNet NP vs P 4 MC\n500Precision \u2713 \u2713 - - 61.73 27.66\nRecall \u2713 \u2713 - - 65.04 20.91\nF1 \u2713 \u2713 - - 57.74 19.73\n500Precision - \u2713 - - 61.97 27.71\nRecall - \u2713 - - 66.01 22.13\nF1 - \u2713 - - 57.79 20.61\n500Precision \u2713 \u2713 \u2713 - 61.97 27.80\nRecall \u2713 \u2713 \u2713 - 65.27 20.98\nF1 \u2713 \u2713 \u2713 - 57.38 20.97\n500Precision \u2713 \u2713 - \u2713 62.09 28.00\nRecall \u2713 \u2713 - \u2713 65.73 21.27\nF1 \u2713 \u2713 - \u2713 58.04 21.61\n500Precision \u2713 \u2713 \u2713 \u2713 61.63 27.86\nRecall \u2713 \u2713 \u2713 \u2713 65.08 21.24\nF1 \u2713 \u2713 \u2713 \u2713 56.78 21.17\n800Precision \u2713 \u2713 \u2713 \u2713 65.44 29.73\nRecall \u2713 \u2713 \u2713 \u2713 69.85 27.40\nF1 \u2713 \u2713 \u2713 \u2713 62.07 23.71\n800Precision \u2713 - - \u2713 67.07 31.11\nRecall \u2713 - - \u2713 71.24 29.33\nF1 \u2713 - - \u2713 63.97 25.83\nTable 3: Results utilizing the video & the heart rate modality reported on precision, recall\nand F1 score (refer to Section 5.3).\nEpochs MetricHR\nEncoderPipeline Augmentations Task\nFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n800Precision \u2713 \u2713 \u2713c- - \u2713 82.69 39.13\nRecall \u2713 \u2713 \u2713c- - \u2713 84.71 37.67\nF1 \u2713 \u2713 \u2713c- - \u2713 81.44 36.31203\nTable 4: Results utilizing the RGB video modality, reported on recall and F1 score (refer to\nSection 6.2).\nEpochsAugmentations\nMetricTask\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 30-50 0.9Recall 71.29 29.61\nF1 68.53 27.22\n200 \u2713 30-50 0.9Recall 71.93 24.43\nF1 69.61 23.78\n300 \u2713 30-50 0.9Recall 71.34 30.64\nF1 69.65 26.12\nTable 5: Results utilizing the synthetic thermal video modality, reported on recall and F1\nscore (refer to Section 6.2).\nEpochsAugmentations\nMetricTask\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 30-50 0.9Recall 72.04 28.80\nF1 69.16 26.45\n200 \u2713 30-50 0.9Recall 72.18 30.89\nF1 69.44 26.45\n300 \u2713 30-50 0.9Recall 72.52 24.96\nF1 70.01 23.43204 Appendix\nTable 6: Results utilizing the fusion of RGB & synthetic thermal video modality, reported\non recall and F1 score (refer to Section 6.2).\nEpochsFusion\nweightsAugmentations\nMetricTask\nBasic Masking P(Aug) NP vs P 4 MC\n100 \u2013 \u2713 30-50 0.9Recall 67.05 21.68\nF1 62.96 18.29\n100 W2 \u2713 30-50 0.9Recall 68.72 21.69\nF1 62.98 19.35\n100 W3 \u2713 30-50 0.9Recall 66.12 23.12\nF1 59.72 19.67\n300 W2 \u2713 30-50 0.9Recall 71.40 26.39\nF1 68.82 26.18\n500 W2 \u2713 10-20 0.7Recall 73.20 29.69\nF1 70.30 27.84\nTable 7: Results of the proposed approaches, reported on macro-averaged precision, recall\nand F1 score (refer to Section 7.2).\nModality ApproachMetrics\nPrecision Recall F1\nVideo Addition 44.91 44.97 44.60\nfNIRS HbO & Addition 44.68 45.08 43.60\nFusion Single Diagram 46.76 47.29 46.70205\nSupplementary Figures\nFigure 1: Attention maps generated by the Spatial-Module .Yellow and red colors signify\nintense focus on specific areas. (1strow) Sequence of original frames. (2ndrow)\nDerived from the Spatial-Module after initial stage pretraining. (3rdrow) Derived\nfrom the Spatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module following training on BioVid (refer to Section 5.3).206 Appendix\nFigure 2: Attention maps generated by the Spatial-Module .Yellow and red colors signify\nintense focus on specific areas. (1strow) Sequence of original frames. (2ndrow)\nDerived from the Spatial-Module after initial stage pretraining. (3rdrow) Derived\nfrom the Spatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module following training on BioVid (refer to Section 5.3).207\nabc\nFigure 3: Additional attention maps from the PainViT\u20132 (refer to Section 7.2).208Acronyms\nAI A rtificial Intelligence\nECG E lectrocardio graphy\nEDA E lectro dermal Activity\nEEG E lectro encephalo graphy\nEMG E lectro myography\nFACS F acial Action Coding System\nFLOPS Fl oating-point Operations perSecond\nfMRI F unctional Magnetic Resonance Imaging\nfNIRS F unctional Near-Infrared Spectroscopy\nGSR G alvanic SkinResponse\nML M achine Learning\nNIPS N eonatal/ Infant PainScale\nNIRS N ear-Infrared Spectroscopy\nPPG P hotoplethysmo graphy\nPSPI P rkachin and Solomon PainIntensity Scale\nRGB R edGreen Blue\nSpO2 S aturation of Peripheral Oxygen\nV AS V isual Analog Scale\nVRS V erbal Rating Scale\n209", "Demographic Variables: Their Role and Impact": "53\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5 Optimization: Balancing Efficiency and Performance 75\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n6 Synthetic Data: The Role of Thermal Imaging 103\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7 General-Purpose Models 117\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n8 Conclusions, Perspectives and Future Work 157\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nBibliography 163\nAppendix 201\nAcronyms 209List of Figures\n2.1 The spinothalamic tract (STT) [43]. Pain, temperature, and some touch affer-\nents end in the posterior horn, where second-order fibers cross the midline\nto form the spinothalamic tract, ascending to the thalamus and projecting to\nvarious cortical areas. Along the way, collaterals connect to the reticular for-\nmation. Due to the rostral inclination of fibers in Lissauer\u2019s tract, cordotomy\nmust be performed several segments above the pain level for effective relief. 12\n2.2 Pain classification [48]: (A)Nociceptive pain , which results from detecting\npotentially harmful stimuli and serves a protective function. (B)Inflamma-\ntory pain is linked to tissue damage and immune cell infiltration, increas-\ning pain sensitivity during healing. (C)Pathological pain is a disease state\ncaused by either nervous system damage (neuropathic) or abnormal nervous\nsystem function (dysfunctional). . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 The number of studies utilizing these specific datasets. Note that various\nstudies used multiple datasets to conduct their experiments. . . . . . . . . . 25\n4.1 The PQRST waveform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure. . 55\n4.3 The signal preprocessing using the Pan-Tompkins algorithm. . . . . . . . . 57\n4.4 Results for the Gender Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Results for the Age Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.6 Results for the Gender-Age Scheme . . . . . . . . . . . . . . . . . . . . . . 64\n4.7 The proposed MTL network: The sizes of the extracted vectors for the net-\nwork are as follows: for the Pain classifier, n\u02c61, where nis the number of\npain estimation tasks ( e.g.,2for binary classification, 5for multi-class clas-\nsification); for the Age classifier, 36\u02c61, where 36represents the possible\nage values of the subjects; for the Gender classifier, 2\u02c61, corresponding to\nthe two possible gender categories ( i.e., males and females). . . . . . . . . 66\n4.8 Results for the proposed Schemes. . . . . . . . . . . . . . . . . . . . . . . 69\n4.9 Comparison of performances utilizing various neural networks approaches. 72\nxix5.1 The application of face alignment illustrates landmarks in 2D (left) and 3D\n(right) space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 An overview of our proposed transformer-based framework for automatic\npain assessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 The impact of the number of input frames on accuracy (left) and on runtime\nin milliseconds (right). Runtime calculated during inference on a NVIDIA\nRTX-3090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Relevance Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5 Outline of the proposed framework. . . . . . . . . . . . . . . . . . . . . . 86\n5.6 Comparison of mean accuracy and inference period for unimodal and multi-\nmodal strategies across NP versus P 4and MC tasks. The diagram adopts a\ndual-y-axis configuration\u2014accuracy measurements on the left and time met-\nrics on the right\u2014to outline the balance between performance efficacy and\ncomputational load, categorizing the methodologies along the x-axis. . . . . 98\n5.7 Regions highlighted in yellow and red denote areas of significant attention.\n(a) (1strow) Sequence of original frames. (2ndrow) Derived from the\nSpatial-Module after initial stage pretraining. (3rdrow) Derived from the\nSpatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module trained on the BioVid dataset. (b) (1strow) Derived from\ntheTemporal-Module incorporating video embeddings. (2ndrow) Derived\nfrom the Temporal-Module with heart rate embeddings. (3rdrow) Derived\nfrom the Temporal-Module using a combined embedding of video and heart\nrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.1 Illustration of the procedure for creating thermal images, featuring the archi-\ntecture of the Generator G(Encoder, mid-stage ResNet, Decoder), and the\nDiscriminator D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2 Representation of the proposed framework, illustrating its components and\ntheir main functions: (a)The Vision-MLP module, tasked with extracting\nfeature embeddings from video frames. (b)TheToken-Mixer , an important\nsub-module of Vision-MLP , generates the wave representation for the tokens.\n(c)The Channel-Mixer , a crucial sub-module within Vision-MLP .(d)The\nMLP, a core component of the Channel-Mixer .(e)The fusion procedure\nthat combines RGB and synthetic thermal embeddings, succeeded by the\nTransformer module, which conducts the final pain assessment. . . . . . . . 107\n6.3 Gradual blurring of RGB and synthetic thermal facial images: a series dis-\nplaying varying levels of Gaussian blur applied, with kernel sizes gradually\nincreased from k\u201c0(no blur) to k\u201c191(extensively blurred). . . . . . . 1136.4 Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain)\nclasses in RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191\n(heavily blurred). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1 PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer hav-\ning varying depths, showcasing how token resolution decreases at each stage;\n(b)Composition of the Token-Mixer module, featuring elements like depth-\nwise convolution (DWConv) and batch normalization; (c)Architecture of the\nFeed-Forward Network (FFN) within the Token-Mixer ;(d)The Cascaded\nAttention mechanism implemented across multiple heads, illustrating how\noutputs from preceding heads are incorporated to refine the self-attention\nprocess, culminating in the final output projection; (e)Configuration of the\nproposed multimodal pipeline, employing videos and fNIRS. The embed-\ndings from PainViT\u20131 are represented as waveform diagrams, which are\nmerged into a single diagram that illustrates both modalities before entering\nPainViT\u20132 for final pain evaluation. . . . . . . . . . . . . . . . . . . . . . . 151\n7.2 Waveform illustrations for various data types: (a)original fNIRS signal,\n(b)video embedding derived from PainViT\u20131 , and (c)fNIRS embedding\nobtained from PainViT\u20131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n7.3 Attention maps from the PainViT\u20132 . . . . . . . . . . . . . . . . . . . . . . 152\n7.4 Overview of primary models and their components outlined in this research:\n(a)PainFormer is structured hierarchically into four stages, incorporating\nSpectral andSelf-Attention Layers to extract embeddings from the inputs;\n(b)The Spectral Layer , a key element of PainFormer , uses FFT to ana-\nlyze frequency-specific data along with a learnable filter Kto highlight\ncritical frequencies; (c)The Self-Attention Layer , crucial for PainFormer ,\nenables parallel processing of features and their interconnections; (d)The\nEmbedding-Mixer , employing both cross and self-attention mechanisms, func-\ntions as the component for the final classification of embeddings in pain as-\nsessment; (e)TheVideo-Encoder , designed for compact and efficient encod-\ning, compresses video data into a reduced dimensional form; (f)TheMLP-1\nis part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)TheMLP-3 configuration is integrated into the Embedding-Mixer\nandVideo-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n7.5 Examples of different vision modalities in frame samples: (a)RGB frame,\n(b)synthetic thermal frame, and (c)depth estimation frame. . . . . . . . . 153\n7.6 Examples of different visual representations for biosignals: (a)waveform ,\n(b)spectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD . . 1547.7 An overview of the presented framework. PainFormer , the foundational\nmodel, excels in deriving high-quality embeddings from a diverse array of\nbehavioral and physiological modalities. The evaluation of RGB, thermal,\nand depth videos, alongside various representations of ECG, EMG, GSR,\nand fNIRS such as waveforms and spectrograms, underscores the rich infor-\nmation captured within these embeddings. Leveraging the embeddings from\nPainFormer facilitates the creation of various and diverse unimodal and mul-\ntimodal pipelines designed for the pain assessment task. Each pipeline can\nbe customized to suit the specific modalities involved, dataset characteristics,\nand the demands of the intended application or clinical setting. Our assess-\nments included the development and implementation of several pipelines\nin both unimodal and multimodal contexts, achieving leading-edge results\nacross various modalities and data representations. . . . . . . . . . . . . . 154\n7.8 Attention maps from the PainFormer :(a)(1strow) frames from RGB, ther-\nmal, and depth video modalities; (a)(2ndrow) corresponding attention maps;\n(b)(1strow) attention maps for ECG and EMG; (b)(2ndrow) attention maps\nfor EDA and fNIRS modalities. . . . . . . . . . . . . . . . . . . . . . . . . 155\n1 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n2 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n3 Additional attention maps from the PainViT\u20132 (refer to Section 7.2). . . . . 207List of Tables\n3.1 Most commonly utilized pain databases. . . . . . . . . . . . . . . . . . . . 24\n3.2 Vision-based studies with static analysis. . . . . . . . . . . . . . . . . . . . 32\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 33\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 34\n3.3 Vision-based studies with temporal utilization. . . . . . . . . . . . . . . . . 39\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 40\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 41\n3.4 Touch sensor-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Audio-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6 Multimodal-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.7 Interpretation approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1 Results for the Basic Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Results for the Gender Scheme (1). . . . . . . . . . . . . . . . . . . . . . . 60\n4.3 Results for the Age Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Results for the Gender-Age Scheme (Males) (1). . . . . . . . . . . . . . . . 62\n4.5 Results for the Gender-Age Scheme (Females) (1). . . . . . . . . . . . . . . 63\n4.6 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (1). 63\n4.7 Hyper-parameters used in our approach. . . . . . . . . . . . . . . . . . . . 65\n4.8 Results for the Basic Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . 68\n4.9 Results for the Gender Scheme (2). . . . . . . . . . . . . . . . . . . . . . . 68\n4.10 Results for the Age Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.11 Results for the Gender-Age Scheme (2). . . . . . . . . . . . . . . . . . . . 68\n4.12 Comparison of results adopting the feature augmentation approach. . . . . . 70\n4.13 Comparison of results adopting the MT-NN approach. . . . . . . . . . . . . 71\n4.14 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2). 72\n5.1 Training details for the automatic pain assessment. . . . . . . . . . . . . . 79\n5.2 Results on the pain estimation tasks. . . . . . . . . . . . . . . . . . . . . . 81\n5.3 Results for the pain estimation tasks using various numbers of input frames. 82\n5.4 Comparison of studies utilizing BioVid , RGB videos, and LOSO validation. 84\n5.5 Datasets utilized for the pre-training process of the framework. . . . . . . . 91\nxxiii5.6 Training details for the automatic pain assessment. . . . . . . . . . . . . . 92\n5.7 Results utilizing the video modality. . . . . . . . . . . . . . . . . . . . . . 93\n5.8 Results utilizing the heart rate modality. . . . . . . . . . . . . . . . . . . . 95\n5.9 Results utilizing the video &the heart rate modality. . . . . . . . . . . . . . 95\n5.10 Comparison of studies utilizing BioVid &LOSO validation, reported on ac-\ncuracy %. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.11 Module parameters and computational cost in FLOPS for the proposed frame-\nwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.1 Datasets utilized for the pretraining process of the framework. . . . . . . . 110\n6.2 Training specifications, and number of parameters and FLOPS for each mod-\nule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.3 Results utilizing the RGB video. . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Results utilizing the synthetic thermal video. . . . . . . . . . . . . . . . . . 112\n6.5 Results utilizing the RGB &the synthetic thermal video. . . . . . . . . . . . 113\n6.6 Results utilizing the fusion of RGB &synthetic thermal video. . . . . . . . 115\n6.7 Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.116\n6.8 Comparison with the MIntPAIN dataset. . . . . . . . . . . . . . . . . . . . 116\n7.1 Number of parameters and FLOPS for the components of the proposed Twins-\nPainViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.2 Datasets utilized for the pretraining process of the framework. . . . . . . . 123\n7.3 Training details for the automatic pain assessment. . . . . . . . . . . . . . 124\n7.4 Results utilizing the video modality & Addition method. . . . . . . . . . . . 125\n7.5 Results utilizing the video modality & Concatenation method. . . . . . . . . 125\n7.6 Results utilizing the HbR & Addition method. . . . . . . . . . . . . . . . . 126\n7.7 Results utilizing the HbR & Concatenation method. . . . . . . . . . . . . . 126\n7.8 Results utilizing the HbO & Addition method. . . . . . . . . . . . . . . . . 126\n7.9 Results utilizing the HbO & Concatenation method. . . . . . . . . . . . . . 127\n7.10 Results utilizing the HbR, HbO & Addition method. . . . . . . . . . . . . . 127\n7.11 Results utilizing the videos, HbO & Addition method. . . . . . . . . . . . . 127\n7.12 Results utilizing the videos, HbO & Single Diagram method. . . . . . . . . 128\n7.13 Comparison with the validation baseline provided by the AI4PAIN challenge\norganizers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.14 Number of parameters and FLOPS for the modules of the proposed framework.130\n7.15 Details of the PainFormer\u2019s architecture. . . . . . . . . . . . . . . . . . . . 132\n7.16 Datasets utilized for the multitask learning-based pretraining process of the\nframework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347.17 Training details of the proposed framework. . . . . . . . . . . . . . . . . . 135\n7.18 Results utilizing the video modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.19 Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.20 Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.21 Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.22 Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and\nF1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.23 Results on the validation set of AI4Pain dataset, multilevel classification task,\nreported on accuracy, recall and F1 score. . . . . . . . . . . . . . . . . . . 144\n7.24 Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.25 Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4\ntask and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.26 Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs.\nP4task and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.27 Comparison of studies on the testing set of AI4Pain dataset. . . . . . . . . . 148\n1 Results utilizing the video modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n2 Results utilizing the heart rate modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n3 Results utilizing the video &the heart rate modality reported on precision,\nrecall and F1 score (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . 202\n4 Results utilizing the RGB video modality, reported on recall and F1 score\n(refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n5 Results utilizing the synthetic thermal video modality, reported on recall and\nF1 score (refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . 203\n6 Results utilizing the fusion of RGB &synthetic thermal video modality, re-\nported on recall and F1 score (refer to Section 6.2). . . . . . . . . . . . . . 204\n7 Results of the proposed approaches, reported on macro-averaged precision,\nrecall and F1 score (refer to Section 7.2). . . . . . . . . . . . . . . . . . . . 204Chapter 1\nIntroduction\nContents\n1.1 Context and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Scope and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions \u2013 Peer-review Publications . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.1 Context and Motivation\nPain is a complex and deeply personal experience that is subjective by nature. Traditionally,\nit has been described in terms of its sensory dimension [1]. However, extensive research\nhas highlighted the importance of affective, cognitive and social aspects in shaping this ex-\nperience [2]. Studies have explored physiological, psychological, and socio-environmental\nfactors that contribute to the experience of pain. It is understood as a result of biological evo-\nlution and as influenced by psychological and social factors. As Ridell et al. [3] noted, \u201cPain\nis a synthesis\u2013a sum that is greater than its parts. \u201d The brain\u2019s ability to alter the perception\nof sensory inputs through the interplay of emotion, cognition, and social processes is signifi-\ncant. Although natural systems establish the initial biological framework for pain perception,\nthis structure is highly adaptable, particularly in humans. Throughout a person\u2019s life, both\nbiological developments and personal experiences significantly reshape this framework.\nA key question driving pain research across biological, psychological, and computational\nfields is why this topic of pain is meaningful and important. This question also forms the\nbasis for initiating this thesis, highlighting the broader relevance of studying pain. Williams\nand Kappesser [4] provide a compelling explanation, stating, \u201cWe care because we are wired\nto care: to attend to other people\u2019s expression of pain and to understand its meaning; to feel\ndistress in relation to their distress; and to be motivated to reduce their distress, and ours,\nif we are able to do so. \u201d This highlights the intrinsic human response to empathize and\n12 CHAPTER 1. INTRODUCTION\nalleviate pain, underlining the fundamental importance of this research area. Indeed, from a\nDarwinian perspective, pain serves a crucial role. The manifestation of pain in humans and\nthe reactions it elicits are examined through an evolutionary lens. Pain facilitates recovery by\npromoting responses to harmful stimuli and behaviors that demonstrate the adverse nature\nof painful experiences, common among animals. Specifically, the facial expression of pain,\nwhich communicates discomfort directly to those nearby, is universally recognized across\ndifferent ages, ethnicities, roles, and relationships. Evidence from healed major fractures\n[5, 6] suggests that injured members of hominid groups were not left to fend for themselves\nbut were supported through their recovery, indicating the fundamental importance of pain\nexpression in our evolutionary history.\nPain is a widespread health concern globally, affecting up to 30% of the adult popula-\ntion [7] and between 83% and93% of elderly adults in residential care [8]. The Global Bur-\nden of Disease (GBD) study identifies pain as the primary cause of years lived with disability\n(YLD) [9], with major contributors including chronic back pain, musculoskeletal disorders,\nand neck pain [10]. Pain impacts individuals and poses significant clinical, economic, and\nsocial challenges. In the United States, the economic and healthcare costs related to pain\ndue to reduced work productivity range from $560 to$635 billion annually, surpassing the\ncosts associated with heart disease, cancer, and diabetes combined [11]. In Europe, chronic\npain\u2019s direct healthcare costs and indirect socioeconomic impacts account for 3%to10% of\nthe GDP [12]. In Australia, the average annual cost for individuals among the 15.4%living\nwith chronic pain ranges from AU $22,588to AU $42,979, including non-financial costs [13].\nBeyond direct effects on health, pain contributes to a range of adverse outcomes, such as opi-\noid dependency, drug overuse, addiction, declining social relationships, and psychological\ndisorders [14]. In the last two decades, prescription opioid use has surged in the United\nStates, where overdose deaths have increased more than fourfold from 1999 to2016 [15].\nAdditionally, side effects from these opioids, like lethargy, depression, anxiety, and nausea,\nseverely impact workforce productivity and overall life quality [16].\nAccurate pain assessment is crucial for early diagnosis, disease progression monitoring,\nand treatment effectiveness evaluation, particularly in managing chronic pain [17]. This criti-\ncal role has resulted in pain being recognized as \u201cthe fifth vital sign\u201d in nursing literature [18].\nPain assessment is also fundamental in physiotherapy, where therapists apply external stim-\nuli and need to gauge the patient\u2019s pain levels accurately [19]. Objective evaluation of pain is\nessential to provide appropriate care, especially for vulnerable populations who may not be\nable to communicate their pain effectively, such as infants, young children, individuals with\nmental health issues, and the elderly. Various methods are used for pain assessment, with\nself-reporting\u2013where individuals describe their pain experiences\u2013considered the gold stan-\ndard [20]. Pain evaluation methods in clinical environments include quantifiable measures\nlike the Numeric Pain Rating Scale (NPRS), Visual Analogue Scale (V AS), and quantitative1.1. CONTEXT AND MOTIVATION 3\nsensory testing techniques such as the pressure pain detection threshold (PPDT) [21]. Behav-\nioral indicators are also crucial and include facial expressions ( e.g., grimacing, open mouth,\nlifted eyebrows), vocalizations (like crying, moaning, or screaming), and movements of the\nbody and head [22]. Physiological measures such as electrocardiography (ECG), electromyo-\ngraphy (EMG), galvanic skin responses (GSR), and respiration rates further contribute to\nunderstanding pain\u2019s physiological aspects [17]. Additionally, brain monitoring techniques\nlike near-infrared spectroscopy (fNIRS) have effectively detected changes in hemodynamic\nactivity associated with pain stimuli [23].\nCaregivers and family members often determine the presence or absence of pain in pa-\ntients by observing their behavioral or physiological responses [17]. However, accurately\nassessing pain poses a significant challenge for clinicians [24], especially with nonverbal\npatients such as the elderly, who may have reduced expressive abilities or may be reluctant\nto communicate pain [25]. Extensive research indicates that pain manifestations vary signif-\nicantly across different genders and ages, adding to the complexity of its assessment [26].\nFurther complicating the assessment process are the heightened workload and fatigue ex-\nperienced by nursing staff due to the demands of patient monitoring [27]. Technological\nsolutions are necessary for continuous patient monitoring. Nevertheless, concerns remain\nabout the objectivity and accuracy of these observations, as inadequately trained or biased\nobservers may struggle to assess pain [28] accurately. Even among trained observers, in-\nterpretations of behaviors can vary [22], and social and interpersonal dynamics can signif-\nicantly affect the pain assessment process, influencing both the evaluators\u2019 judgments and\nthe patients\u2019 expressions of pain [29]. Additionally, the presence of an observer can lead pa-\ntients to modify their behavior [30], and expressing pain through scales and measurements\ncan be challenging [31]. While self-reporting is used because pain is inherently subjective,\nrelying solely on a one-dimensional pain score fails to capture this complex phenomenon,\noften leading to inadequate pain management [32].\nGiven the challenges described above, scientific computing (SC) researchers have fo-\ncused on developing models and algorithms to enhance automatic pain recognition systems\nover the last two decades. Their goal is to accurately determine the presence and intensity\nof pain by analyzing physiological and behavioral indicators. Adopting deep learning and\nartificial intelligence (AI) techniques has expanded these automatic methods, designed to\ninterpret the complex and varied nature of pain [17]. Numerous studies have underscored\nthe effectiveness of automated systems that utilize behavioral or physiological modalities\nfor pain assessment [33]. Sario et al. [34] have shown the capability of these systems to\naccurately recognize pain through facial expressions, proving their utility in clinical envi-\nronments. Multimodal sensing has shown particular promise, offering enhanced accuracy\nin pain detection systems [22]. Furthermore, including temporal aspects in these modalities\nhas proven to significantly improve the accuracy of pain assessments [17].4 CHAPTER 1. INTRODUCTION\n1.2 Scope and Challenges\nAlthough considerable research has been conducted on automatic pain assessment, studies\nhave yet to explore factors like demographics and social aspects from a computational angle.\nFurthermore, despite the existence of deep learning-based methods, the approaches we ob-\nserve are often outdated and repeatedly recycled. For these reasons, we aimed to address two\nissues by (i)attempting to evaluate the social or demographic context, which significantly\nimpacts and influences pain sensation and perception, and (ii)introducing innovative deep\nlearning methods inspired by the latest developments in AI and generative AI literature. We\nbelieve these approaches can forge new paths in pain research, enhance the accuracy of rec-\nognizing this complex phenomenon, and, ultimately, be adopted in real-world scenarios to\nassist those in need. Additionally, (iii)recognizing the skepticism towards new technologies\namong clinicians and the general public, especially regarding the limited understanding of\nhow deep learning models function, we have devoted a portion of our research to interpret-\ning these models to offer some level of explanation and help the adoption process of them in\nclinical settings.\nNevertheless, this thesis initially faced challenges related to our objectives and goals as\nthe research progressed. The availability of pain datasets (to be discussed in the next chapter)\nis limited. Only a few datasets are available, and crucially, they are limited in size. This re-\nstriction poses a significant challenge for developing deep learning models, which typically\nrequire a large volume of data. In automatic pain assessment, researchers who develop deep\nlearning methods typically confront a decision: either train their models from scratch, which\ncan introduce performance limitations, or employ pre-trained models. These pre-trained\nmodels are generally trained on broadly available image datasets that include a variety of\nsubjects like animals and objects, or they rely on older architectures that were trained explic-\nitly on facial datasets. In this thesis, we addressed these issues by independently pre-training\nour deep-learning models using diverse datasets related explicitly to human facial images\nand biosignals. This strategy allowed us to design specific architectures to meet our unique\nneeds for each scenario, free from the constraints of relying on models developed and trained\nby others. Furthermore, we explored and evaluated several pre-training techniques to assess\ntheir effectiveness in pain assessment applications.\nRegarding, our objective to explore methods that utilize various modalities individually\nand in combination in a multimodal manner further constrains our dataset options. More-\nover, as previously outlined, our interest in the sociodemographic aspects of pain necessitates\ndatasets that include this information type, intensifying our challenges. For these reasons,\nthis thesis focuses specifically on examining the impact of age and gender on pain. In addi-\ntion, led us to utilize two pain datasets that most closely match the characteristics necessary\nfor our research, particularly in terms of demographic elements and multimodality.1.3. CONTRIBUTIONS \u2013 PEER-REVIEW PUBLICATIONS 5\n1.3 Contributions \u2013 Peer-review Publications\nThis section outlines the publications and projects produced during the Ph.D. research on\nautomatic pain assessment, where I was the first author.\n1.Automatic assessment of pain based on deep learning methods: A systematic re-\nview [17]\nThis systematic literature review (SLR) was conducted at the start of this Ph.D. re-\nsearch. This paper aims to explore the surge in recent years of deep learning algorithms\nadopted by researchers to encode the multidimensional nature of pain into meaning-\nful features. Specifically, this systematic review examines the models, methods, and\ndata types used to establish the foundation for deep learning-based automatic pain\nassessment systems. It identified relevant original studies from digital libraries such\nasScopus ,IEEE Xplore , and ACM Digital Library , following defined inclusion and\nexclusion criteria for studies published until December 2021 . The findings highlight\nthe critical role of multimodal approaches in automatic pain estimation, particularly\nin clinical environments, and emphasize the substantial gains observed with the inclu-\nsion of temporal exploitation of modalities. The review also recommends selecting\nhigh-performing deep learning architectures and methods, encouraging the adoption\nof robust evaluation protocols and interpretability techniques to deliver reliable and\nunderstandable outcomes. Additionally, it underscores the current limitations of exist-\ning pain databases in adequately supporting the development, validation, and practical\napplication of deep learning models as decision-support tools in real-world settings.\nFurthermore, we believe this paper is valuable not only for this Ph.D. project but also\nfor other practitioners and researchers in the field.\n2.Automatic Pain Intensity Estimation based on Electrocardiogram and Demographic\nFactors [35]\nThis study investigated the relationship between gender, age, and pain sensation and\ntheir effects on the automatic pain assessment process. By analyzing physiological\nsignals, particularly electrocardiography (ECG), we estimated pain intensity and ex-\namined the influence of these demographic factors. Utilizing the Pan-Tompkins algo-\nrithm for feature extraction and applying well-established classification methods, we\nexplored the correlation between gender, age, and pain manifestation.\n3.Multi-task Neural Networks for Pain Intensity Estimation Using Electrocardiogram\nand Demographic Factors [36]\nInspired by the previous study, this research further explored the influence of gender\nand age on pain perception. In this work, we analyze electrocardiography signals\nto uncover variations in pain perception across different demographic groups. We6 CHAPTER 1. INTRODUCTION\nleveraged these insights by developing a novel multi-task neural network for automatic\npain estimation, incorporating age and gender data for each individual. The study\ndemonstrated the advantages of this approach compared to other existing methods.\n4.A Full Transformer-based Framework for Automatic Pain Estimation using Videos\n[37]\nThis study introduced an innovative full transformer-based framework featuring a Trans-\nformer in Transformer (TNT) model combined with cross-attention and self-attention\nblocks. We achieved state-of-the-art performance using video data from the BioVid\ndatabase, demonstrating the model\u2019s effectiveness, efficiency, and strong generaliza-\ntion across primary pain estimation tasks.\n5.Multimodal automatic assessment of acute pain through facial videos and heart rate\nsignals utilizing transformer-based architectures [38]\nThis study presented a multimodal automatic acute pain assessment framework, inte-\ngrating video and heart rate signals. The framework consists of four key modules:\ntheSpatial Module , which extracts embeddings from videos; the Heart Rate Encoder ,\nwhich maps heart rate signals into a higher-dimensional space; the AugmNet , which\ngenerates learning-based augmentations in the latent space; and the Temporal Mod-\nule, which leverages the video and heart rate embeddings for the final assessment.\nThe Spatial Module undergoes a two-stage pre-training process: first, it learns uni-\nversal facial features through face recognition, followed by emotion recognition in a\nmultitask learning approach, enabling high-quality embeddings for pain assessment.\nExperiments with facial videos and heart rate data extracted from electrocardiograms\nin the BioVid database, alongside direct comparisons to 29studies, demonstrate state-\nof-the-art performance in unimodal and multimodal settings while maintaining high\nefficiency. In the multimodal setting, the framework achieved 82.74% accuracy for bi-\nnary pain classification and 39.77% for multi-level pain classification, using only 9.62\nmillion parameters across the entire framework.\n6.Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-\nMLP Architecture [39]\nThis paper introduced synthetic thermal videos generated by Generative Adversarial\nNetworks , which are integrated into the pain recognition process to assess their effec-\ntiveness. The framework employs a Vision-MLP andTransformer -based module, lever-\naging RBG and synthetic thermal videos in unimodal and multimodal settings. Exper-\niments conducted using facial videos from the BioVid database highlighted synthetic\nthermal videos\u2019 effectiveness and showcased their potential benefits in pain recognition\ntasks.1.4. THESIS OUTLINE 7\n7.Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for\nMultimodal Automatic Pain Assessment using Facial Videos and fNIRS [40]\nThis study was submitted to the First Multimodal Sensing Grand Challenge for Next-\nGen Pain Assessment (AI4PAIN) . The proposed multimodal framework leverages fa-\ncial videos and fNIRS, offering a modality-agnostic approach that eliminates the need\nfor domain-specific models. Utilizing a dual ViT configuration and waveform repre-\nsentations for both fNIRS and the extracted embeddings from the two modalities, the\nmethod demonstrates its effectiveness, achieving an accuracy of 46.76% in the multi-\nlevel pain assessment task.\n8.PainFormer: a Vision Foundation Model for Automatic Pain Assessment [41]1\nThis study introduces PainFormer , a vision foundation model built on multi-task learn-\ning principles and trained across 14distinct tasks and datasets comprising 10.9million\nsamples. As an embedding extractor for various input modalities, PainFormer provides\nfeature representations to the Embedding-Mixer , a transformer-based module respon-\nsible for conducting the final pain assessment. Extensive experimentation using both\nbehavioral modalities\u2013including RGB, synthetic thermal, and estimated depth videos\u2013\nand physiological modalities like ECG, EMG, GSR, and fNIRS revealed PainFormer \u2019s\nability to extract high-quality embeddings from diverse inputs. Tested on the BioVid\nandAI4Pain datasets and compared to more than 60existing methods, the framework\ndemonstrated state-of-the-art performance in unimodal and multimodal settings, posi-\ntioning itself as a step toward developing general-purpose models for automated pain\nevaluation.\n1.4 Thesis Outline\nThe dissertation is organized into the following chapters:\nChapter 2 introduces the foundational concepts of pain from biological, psychological, and\nclinical perspectives.\nChapter 3 reviews existing literature on automatic pain assessment using deep learning\nmethods and details the pain datasets used.\nChapter 4 outlines and proposes methods for evaluating demographic variables, their uti-\nlization, and their integration into an automatic pain assessment framework.\nChapter 5 discusses methods that utilize video and wearable device data, exploring the\ntrade-offs between efficiency and accuracy. It also proposes efficient, fast, effective models\nsuitable for real-world applications.\nChapter 6 explores synthetic data in pain assessment and introduces synthetic thermal im-\n1Under Review8 CHAPTER 1. INTRODUCTION\nagery techniques to enhance performance in automatic pain recognition.\nChapter 7 discusses general-purpose models, introduces a modality-agnostic framework,\nand presents the first foundation model used in automatic pain assessment.\nChapter 8 concludes the thesis with a final discussion, offering perspectives and ideas for\nfuture research in automatic pain assessment.Chapter 2\nClinical Pain Assessment\nContents\n2.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . . . . 20\n2.1 Chapter Overview\nThis chapter provides an anatomical and physiological overview of pain, focusing on the\nmechanisms responsible for generating, transmitting, processing, and interpreting pain sig-\nnals. It examines the various types of pain and explores the actions and expressions typically\nassociated with pain. Additionally, it reviews current pain assessment methods used in clin-\nical settings for adults, children, and newborns. The chapter also discusses developing and\nvalidating existing clinical pain assessment tools. This foundational knowledge is essen-\ntial for understanding the development and validation of computer-assisted pain assessment\n910 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nmethods discussed in later chapters. Finally, it highlights the challenges faced in clinical\npain assessment and underscores the need for automated pain assessment techniques.\n2.2 Biology of Pain\nPain, according to the International Association for the Study of Pain (IASP) [42], is \u201can\nunpleasant sensory and emotional experience associated with actual or potential tissue dam-\nage, or described in terms of such damage\u201d. Biologically, pain is an undesirable sensation\noriginating from the peripheral nervous system. Its fundamental function is to engage sen-\nsory neurons, notifying the organism of potential harm and playing a vital role in recognizing\nand responding to threats [43].\nThe transmission of a noxious stimulus from the periphery to the central nervous sys-\ntem involves a complex pathway through the spinal cord, resulting in the physical sensation\nof pain and a corresponding emotional response and memory. This process culminates in\nthe perception of pain. The initial stage of pain processing occurs when a stimulus at noci-\nceptive sensory fibers in the periphery is converted into an action potential. A nerve signal\nis generated if the stimulus is strong enough to surpass the action potential threshold [44].\nThis signal travels along the primary afferent fiber toward the central nervous system. As the\nstimulus intensity grows, more nerve fibers and areas of the nervous system are engaged [44].\nDue to their branching nature, primary afferent fibers typically relay information from sev-\neral pain receptors. These fibers and their receptors comprise a sensory unit, which gathers\ndata from a specific receptive field [44]. When receptive fields are larger and overlap with\nnearby fields, it becomes more challenging for the sensory system to locate the source of pain\naccurately. The primary afferent neuron is a pseudounipolar neuron that splits into a periph-\neral and central axon. The cell bodies of these neurons are located in the peripheral nervous\nsystem, within the posterior or cranial root ganglia. The peripheral axon extends to the skin,\nmuscles, tendons, or joints, branching into terminal fibers that connect with somatosensory\nreceptors. In contrast, the central axon leads to the central nervous system [45].\nPeripheral somatosensory fibers are categorized into three main groups. The first group\nincludes A\u00b4\u03b1,A\u00b4\u03b2,A\u00b4\u03b3fibers, large, myelinated fibers that rapidly conduct sig-\nnals [46]. These fibers involve touch and proprioception but are not associated with pain\nperception. The second group consists of A\u00b4\u03b4fibers, which are smaller and slower con-\nducting. Certain A\u00b4\u03b4fibers play a key role in pain sensation, with some responding only\nto intense mechanical stimuli and others reacting to noxious and non-noxious heat. The\nthird group comprises Cfibers, which are small, unmyelinated, and conduct signals very\nslowly. Most Cfibers are polymodal for pain perception, responding to various noxious\nmechanical, thermal, and chemical stimuli. These fibers are mainly linked to burning pain\nsensations [43]. The sensation of pain, known as nociception, is primarily facilitated by2.3. CLASSIFICATION AND CHARACTERISTICS OF PAIN 11\nvarious intracellular and extracellular molecular messengers. When activated by a specific\nstimulus, nociceptors relay information through glutamate, an excitatory neurotransmitter.\nAdditionally, inflammatory mediators are released at the injury site, further stimulating no-\nciceptor activation by releasing chemicals such as neurotransmitters ( e.g., serotonin), lipids\n(e.g., prostaglandins), peptides ( e.g., bradykinin), and neurotrophins ( e.g., nerve growth fac-\ntor) [46]. There are ascending tracts responsible for transmitting sensory information from\nthe periphery to the central nervous system. Fibers that convey two-point discrimination, tac-\ntile information, pressure, vibration, and proprioception ascend via the dorsal column of the\nspinal cord, forming the gracile and cuneate fasciculi. Fibers transmitting pain, temperature,\nand crude touch from somatic and visceral structures travel through the lateral spinothalamic\ntract. The anterior spinothalamic tract also transmits pain, temperature, and touch informa-\ntion to the brainstem and diencephalon (Figure 2.1) [47].\n2.3 Classification and Characteristics of Pain\nAccording to neurobiologist Clifford Woolf [48], pain can be classified into three categories\nbased on its function and characteristics: nociceptive ,inflammatory , and pathological pain.\nThese classes and their respective functions are illustrated in Figure 2.2.\nNociceptive pain (refer to Figure. 2.2(A)), arising from tissue damage, is a high-threshold\npain that activates only in response to intense stimuli [49], serving as a vital warning signal\nto the body. The neurobiological system responsible for nociceptive pain evolved from the\nability of even the most primitive nervous systems to detect impending or actual tissue dam-\nage caused by external stimuli. Its protective role requires immediate attention and action,\nachieved through the withdrawal reflex it initiates, the unpleasant sensation it produces, and\nthe emotional distress it triggers. Nociceptive pain demands avoidance in the present mo-\nment, and when activated, it overrides most other neural processes [48].\nInflammatory pain (refer to Figure. 2.2(B)) is also protective and adaptive, increasing\nsensory sensitivity following tissue damage to aid healing by discouraging movement and\ncontact with the injured area. This heightened sensitivity, or tenderness, helps prevent further\nharm and supports recovery, as seen after surgical wounds or inflamed joints where normally\nnon-painful stimuli now cause pain. It is triggered by immune system activation in response\nto tissue injury or infection. Despite its adaptive role, this pain often needs to be alleviated in\npatients with persistent inflammation, such as in rheumatoid arthritis or severe injuries [48].\nPathological pain (Figure. 2.2(C)) is maladaptive, arising from abnormal nervous sys-\ntem functioning and not serving a protective role. Unlike nociceptive and inflammatory pain,\npathological pain is a disease state of the nervous system itself. It may occur following\nnerve damage (neuropathic pain) or in conditions without apparent damage or inflammation\n(dysfunction l pain). Examples of dysfunctional pain include fibromyalgia, irritable bowel12 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFIGURE\n 2: Spinothalamic tract.\nPain, temperature, and some touch and pressure afferents end in the posterior horn. Second- or\nhigher-order fibers cross the midline, form the spinothalamic tract, and ascend to the ventral\nposterolateral (VPL) nucleus of the thalamus (and also to other thalamic nuclei not shown).\nThalamic cells then project to the somatosensory cortex of the postcentral gyrus, to the insula,\nand to other cortical areas (also not shown). Along their course through the brainstem,\nspinothalamic fibers give off many collaterals to the reticular formation (RF). The inset to the left\nshows the lamination of fibers in the posterior columns and the spinothalamic tract in a leg-\nlower trunk-upper trunk-arm sequence. The inset to the right shows the longitudinal formation\nof the spinothalamic tract. Primary afferents ascend several segments in Lissauer\u02bcs tract before\nall their branches terminate; fibers crossing to join the spinothalamic tract do so with a rostral\ninclination. As a result, a cordotomy incision at any given level would spare most of the\ninformation entering the contralateral side of the spinal cord at that level, and to be effective,\nthe incision must be made several segments rostral to the highest dermatomal level of pain.\n2017 Khalid et al. Cureus 9(10): e1754. DOI 10.7759/cureus.1754\n5\n of \n14\nFigure 2.1: The spinothalamic tract (STT) [43]. Pain, temperature, and some touch afferents\nend in the posterior horn, where second-order fibers cross the midline to form the\nspinothalamic tract, ascending to the thalamus and projecting to various cortical\nareas. Along the way, collaterals connect to the reticular formation. Due to the ros-\ntral inclination of fibers in Lissauer\u2019s tract, cordotomy must be performed several\nsegments above the pain level for effective relief.\nsyndrome, tension headaches, and temporomandibular joint disease, where significant pain\nexists without an apparent noxious stimulus or peripheral pathology. Pathological pain, a\nlow-threshold pain primarily driven by amplified sensory signals in the central nervous sys-\ntem, is the clinical pain syndrome with the greatest unmet need. To analogize, while nocicep-\ntive pain acts as a fire alarm for intense heat, and inflammatory pain reacts to warm tempera-\ntures, pathological pain is a false alarm triggered by a system malfunction. Thus, treatment\nmust specifically target the underlying mechanisms causing each type of pain [48].\nPain from a time-duration perspective can be categorized by duration into acute and2.4. PAIN INDICATORS 13\nchronic , with chronic pain persisting or recurring for more than three months [50]. Acute\npain is typically related to identifiable physiological damage from injury, surgery, illness,\ntrauma, or medical procedures and generally subsides once the underlying cause is resolved.\nHowever, if untreated, it may develop into chronic pain. Acute pain is further classified\nintoprocedural pain, caused by medical interventions such as muscular injections [51], and\npostoperative pain, which occurs after surgery and is a significant concern for both patients\nand healthcare providers. Effective management is crucial to aid recovery and prevent the\ntransition to chronic pain [52]. Chronic pain manifests in various forms, including chronic-\nrecurrent pain, like migraine headaches, and chronic-continuous pain, such as persistent low\nback pain [53].\n2.4 Pain Indicators\nPain can manifest in numerous ways and is often shaped by individual characteristics and\nenvironmental influences. Various human expressions, actions, and bodily responses have\nbeen linked to pain, serving both communicative and coping purposes. These pain indicators\nare generally categorized into three primary groups: (i)behavioral, (ii)physiological, and\n(iii)biochemical. While these indicators are universally present, certain expressions are more\nprominent in specific groups. For instance, crying is a common pain response across all age\ngroups but is more frequently observed in younger infants. This may be due to contextual\nfactors\u2014such as culture, social status, age, and ego\u2014influencing how pain is expressed\nover time. Adults, for example, may suppress crying in favor of other vocalizations, such as\ngroans and moans, as crying could be perceived as inappropriate in certain contexts. These\nmediating factors are often considered when interpreting pain indicators. The following\nsections will delve into each of these three categories [51].\n2.4.1 Behavioral Indicators\nBehavioral indicators such as facial expressions ( e.g., grimacing, open mouth, raised eye-\nbrows), vocalizations ( e.g., crying, moaning, screaming), and various bodily movements\n(e.g., changes in posture, signs of tension) are vital markers used in assessing pain [22].\nFacial expressions and limb movements in response to acute pain are typically rapid and\ninvoluntary. Facial reactions include brow bulging, eye squeezing, nasolabial furrow forma-\ntion [54], grimacing, clenched teeth, jaw-dropping, and tightened lips [55]. Body movements\nassociated with pain include bracing (gripping an object or the affected area during move-\nment), rubbing (massaging the painful area), restlessness (constant shifting of position) [55],\nand knee flexion [56]. Non-verbal vocalizations such as groaning, moaning, sighing, crying,\nand gasping [57] also indicate pain. Verbal expressions like \u201couch\u201d ,\u201cstop\u201d ,\u201cthat hurts\u201d ,14 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFigure 2.2: Pain classification [48]: (A)Nociceptive pain , which results from detecting po-\ntentially harmful stimuli and serves a protective function. (B)Inflammatory pain is\nlinked to tissue damage and immune cell infiltration, increasing pain sensitivity dur-\ning healing. (C)Pathological pain is a disease state caused by either nervous sys-\ntem damage (neuropathic) or abnormal nervous system function (dysfunctional).\n\u201cthat is enough\u201d , and even cursing [55] also serve as pain indicators. Interestingly, swearing\nhas been found to significantly alleviate pain, although its effect diminishes with frequent\nuse over a short period [58, 59].2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 15\n2.4.2 Physiological Indicators\nVital signs can reflect the state of the central nervous system, and since pain is mediated\nthrough this system, trends in vital signs can provide insights into pain levels. Clinical stud-\nies [60, 61] have examined physiological changes in response to pain and established em-\npirical solid evidence linking pain to vital sign alterations. However, as vital signs can also\nchange due to other non-pain-related pathological conditions, it is recommended that they be\nassessed alongside behavioral pain indicators for accuracy. Physiological pain responses are\nconsidered more reliable than behavioral signals, as they cannot be consciously controlled\nor altered. Physiological measurements such as electrocardiography (ECG), electromyogra-\nphy (EMG), galvanic skin responses (GSR), and respiration rate provide critical insights into\nthe body\u2019s reaction to pain [17]. In addition, brain monitoring techniques like near-infrared\nspectroscopy (fNIRS) have demonstrated the ability to detect pain-related hemodynamic\nchanges [23]. At the same time, functional magnetic resonance imaging (fMRI) has been\nexplored for assessing pain in both normal and pathological conditions [62].\n2.4.3 Biochemical Indicators\nCompared to other pain indicators, biochemical changes are the most precise and sensitive\nreactions to pain. However, their routine use in pain assessment is restricted due to the\ninvasive nature of measurement techniques [63]. These biochemical responses are most\nevident during surgical procedures with limited anesthesia, leading to increased levels of\nendorphins, norepinephrine, cortisol, growth hormones, renin, glucagon, aldosterone, and\ncatecholamines, along with a decrease in insulin levels [60].\n2.5 Sociodemographic and Psychological Variables\nIn1965 , Melzack and Wall [64] introduced the \u201cGate Control Theory\u201d , which interprets pain\nfrom two perspectives. The first involves the mechanisms of nociceptive signal transmission\nand modulation, while the second emphasizes pain as a psychophysiological phenomenon\narising from the interaction between physiological and psychological factors [53]. Observa-\ntions, empirical research, and theoretical models increasingly suggest that a comprehensive\nunderstanding of pain requires a biopsychological approach. It is also becoming apparent\nthat, although pain is often regarded as private and subjective, it is also fundamentally a so-\ncial experience [53]. Pain is not solely explained by biomedical components ( e.g., muscle\ndamage) but also involves psychological ( e.g., cognitive, affective) and social factors ( e.g.,\nfriends, family, health professionals), leading to what is known as a biopsychosocial sensa-\ntion [65]. Numerous factors contribute to how painful experiences are expressed and per-\nceived, varying wildly due to social and personal biases. These factors prompted Williams16 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nand Craig [2] to define pain as \u201ca distressing experience associated with actual or potential\ntissue damage with sensory, emotional, cognitive, and social components. \u201d\n2.5.1 Sex and Gender\nSeveral studies have explored the relationship between gender and pain expressiveness, as\nwell as variations in pain reporting. Research indicates that women generally exhibit a lower\npain threshold compared to men. A meta-analysis by Boerner et al. [66] on gender differ-\nences in children and adolescents found that girls over the age of 12reported higher pain\nintensity in response to cold-induced pain than boys. Furthermore, multiple studies suggest\nthat women tend to describe a greater degree of pain compared to men. In addition to bi-\nological differences, psychological aspects linked to gender also play a role. For instance,\nindividuals with a masculine identity may be less inclined to express or report their pain or\nseek assistance [67].\nMoreover, the manifestation of pain is not only influenced by the individual\u2019s gender but\nalso by dyadic interactions between people of different sexes. Levine and Desimone [68]\nconducted one of the initial studies on this phenomenon, showing that male participants in\na cold pressure experiment reported lower pain intensity when a female experimenter was\npresent. Similarly, McClelland and McCubbin [69] found that female participants expressed\nand reported higher pain levels when accompanied by a female friend. This dynamic also\nextends to patient-healthcare provider interactions. In studying health records, Vigil and\nAlcock [70] discovered that when the pain intensity was reported as high, the patients ( i.e.,\nmen and women) were examined by a female doctor or nurse. Additionally, studies exam-\nining gender differences among physicians in pain treatment options revealed that female\npatients were more likely to receive prescriptions for more potent drugs, such as analgesics,\nand female physicians were more likely to prescribe medications. Extensive research has\nalso shown that both lay observers and healthcare professionals tend to estimate higher pain\nlevels for female patients compared to male patients [71]. Hooper et al. [72] further noted\nthat clinicians communicate more effectively with female patients, often displaying greater\nempathy. Gender roles, beliefs, and expectations play a significant role in understanding\nhow social factors influence the differences in pain perception and experience between men\nand women [73].\n2.5.2 Age\nAge plays a crucial role in pain assessment and management. At the same time, there are\nsignificant challenges, limitations, and biases related to the patient\u2019s age group. Two of the\nmost vulnerable groups, albeit for different reasons, are the elderly and infants.\nPain recognition and interpretation among the elderly, particularly by caregivers, often2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 17\npresent unique challenges. Older adults frequently exhibit stoicism and reluctance to ex-\npress their pain, while healthcare providers struggle to accurately assess the patient\u2019s pain,\nleading to inappropriate pain management decisions [74]. McPherson et al. [75] noted that\ndespite caregivers\u2019 accommodating and empathetic relationships with elderly patients, con-\nflicts still arise. Older patients may resist acknowledging their weaknesses and accepting\nhelp, which can cause them to conceal their pain. The situation becomes even more complex\nwhen dealing with dementia, a disorder encompassing a range of conditions ( e.g., Parkin-\nson\u2019s, Alzheimer\u2019s, Vascular dementia), characterized by abnormal brain changes that im-\npair cognitive and linguistic abilities. A person with dementia may find it challenging to\ncommunicate their pain verbally. However, non-verbal pain expressions remain intact even\nin moderate dementia, although such reactions can be exaggerated [76]. However, aggres-\nsive behavior and disturbances in dementia patients, often caused by pain, are frequently\nmisinterpreted as psychiatric symptoms, leading to improper medication that can have life-\nthreatening consequences [77]. Caregivers of dementia patients face additional challenges,\nnot only related to pain management but also in addressing dementia\u2019s impact on language\nand memory. Particularly in the later stages of dementia, patients encounter severe pain\ncommunication difficulties due to cognitive decline, necessitating that caregivers recognize\nbehavioral and contextual indicators of pain [74]. Age is also known to cause changes in\nskin characteristics, such as texture, rigidity, and elasticity, which impact the performance of\nemotional face recognition tasks [78].\nInfants represent another vulnerable age group where pain assessment requires special-\nized attention, particularly when they experience painful events. The first challenge is obvi-\nously their limited reporting ability to express their pain through language. Although crying\nmight appear to signal pain, this is an oversimplified and unreliable method, as crying can in-\ndicate a variety of situations, such as discomfort, hunger, or pain. Accurately discerning the\ntype of cry is only one part of the challenge; assessing pain in infants is far more complex and\ninfluenced by numerous factors, including the interpersonal relationships within their envi-\nronment. Riddell and Racine [79] found that through various distressing experiences, infants\ncan learn that specific signaling behaviors can prompt their caregiver\u2019s proximity. This at-\ntachment dynamic suggests that, to some extent, infants may consciously utilize pain-related\nbehaviors to elicit responses from their caregivers. Similarly, the context affects older chil-\ndren as well; for example, self-reports of pain tend to be significantly lower when a parent is\npresent compared to when the child is alone [80].\n2.5.3 Psychological Factors\nMultiple studies have revealed that several psychological factors are consistently linked with\npain-related behavior, including depression, pain-related fear, and catastrophizing. Research18 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nfocusing on the impact of depression and anxiety on pain-related behavior has been con-\nducted mainly on patient populations. These studies have shown that depressed individ-\nuals exhibit more pronounced protective and communicative behaviors compared to non-\ndepressed patients [81]. Similarly, numerous studies suggest that patients with higher levels\nof anxiety demonstrate more pain-related behaviors than those with lower anxiety levels [82].\nDespite the frequent coexistence of pain with psychological conditions, research indicates\nthat these patients often experience underestimation of their pain. For instance, De Ruddere\net al. [83] found that patients dealing with psychological stressors such as anxiety, depres-\nsion, and daily life challenges are often perceived by physiotherapists as experiencing less\nsevere pain, illustrating the influence of psychosocial factors on the patient\u2019s pain experience.\n2.5.4 Race and Culture\nPain expression is generally understood across ethnicities and cultures, though differences\nexist in how it is conveyed [4]. However, cultural variations and the nuances of facial ex-\npressions related to emotion are complex and necessitate deeper study. Additionally, racial\nand cultural biases significantly influence pain assessment, judgment, and interpretation.\nExtensive research highlights the impact of a patient\u2019s race as a sociodemographic factor\non observer responses. The most examined topic relates to the different responses toward\nCaucasian versus non-Caucasian individuals, particularly African Americans, who are more\nlikely to have their pain underestimated and undertreated by healthcare providers [84].\nEthnocultural factors are crucial in shaping how individuals perceive and express pain.\nFor example, Western cultures often emphasize conservative expressions and self-control,\nleading to restrained responses in personal pain experiences and in perceiving others\u2019 pain\n[3]. Differences also arise in coping mechanisms; African Americans, for instance, are more\nprone to catastrophizing pain events compared to European Americans [85]. Furthermore,\nevidence shows racial biases in pain treatment across various racial groups, with certain\ngroups being more sensitive to pain but receiving lower-quality treatment [86]. For exam-\nple, Cleeland et al. [87] found that minority cancer patients, mainly Black and Hispanic\nindividuals, were more likely to experience inadequate analgesia compared to non-minority\npatients.\n2.5.5 Observer\u2019s Impact on Pain\nThe variability in pain management stems from the interplay of various elements, including\nsociocultural, biomedical, and psychosocial factors, especially in cases of chronic pain [88].\nWhen it comes to the observer responsible for assessing a patient\u2019s pain, several characteris-\ntics directly influence the objectivity of their evaluation. The first and perhaps most critical\nfactor is the observer\u2019s experience level. One would expect that more experience leads to2.6. IMPACT OF INADEQUATE PAIN MANAGEMENT 19\nbetter and more accurate assessments, but studies show that even experienced healthcare\nproviders consistently underestimate pain, much like laypersons [28]. The greater the expe-\nrience, the more pronounced the underestimation tends to be. This may be due to desensitiza-\ntion caused by repeated exposure to pain events, as seen in the differences between internists\nand surgeons in their evaluation of postoperative pain, with surgeons often encountering se-\nvere pain more regularly [89]. Another significant factor is the observer\u2019s knowledge and\nbeliefs about pain. For example, [83] found that laypersons and healthcare professionals\nwithout physical signs of pain might view the patient\u2019s complaints less seriously. Proper\ntraining is also essential for adequate pain assessment, which is why the Department of\nHealth and Human Services (DHHS) initiated a strategic program to improve healthcare\nproviders\u2019 education and knowledge regarding pain management, following evidence of in-\nadequate training in the field [90].\n2.6 Impact of Inadequate Pain Management\nThe experience of pain, particularly persistent pain, can have detrimental effects on the indi-\nvidual and their surrounding environment. Thoughts about severe pain often lead to grief and\nfear, causing individuals to perceive pain as a threat and feel incapable of managing it. This\ncan prompt avoidance behaviors aimed at escaping perceived harm [91]. Studies have shown\nthat children with a catastrophizing mindset about pain struggle with daily activities, while\nadolescents with chronic pain tend to have fewer friends and may miss out on social and\nentertainment opportunities, putting them at greater risk of victimization [92]. These adoles-\ncents often feel isolated and lonely compared to their healthy peers, and they may experience\nanxiety in social interactions [93]. Parental reactions to their children\u2019s pain can further com-\nplicate the situation, as parents with catastrophic tendencies tend to engage in overprotective\nbehaviors that hinder the child\u2019s functioning and psychosocial development [94]. Addition-\nally, the family\u2019s overall dynamic is affected, with the patient\u2019s sadness, sleep disorders, and\nchanges in leisure activities impacting the household [74].\nOn a biological level, pain, particularly when experienced early and severely, can alter\nthe brain and nervous system. These early pain experiences can disrupt neurobiological\ndevelopment and affect how pain is processed later in life [95]. A growing body of research\nlinks chronic pain to changes in the medial prefrontal cortex, a region crucial to emotional\nprocessing. Chronic pain is associated with structural and biochemical alterations in this\nbrain area, suggesting that these changes play a role in the pathophysiology of chronic pain\n[96].20 CHAPTER 2. CLINICAL PAIN ASSESSMENT\n2.7 Pain Measurement Scales and Metrics\nIn clinical settings, self-reporting remains the gold standard for assessing pain, allowing in-\ndividuals to describe their pain\u2019s intensity and location. Various self-report scales have been\ndeveloped for different age groups, such as the visual analog scale (V AS) [97] and the verbal\nrating scale (VRS) [98]. Additionally, observation-based scales, where a third party eval-\nuates the pain\u2019s severity, include tools like the Prkachin and Solomon pain intensity scale\n(PSPI) [99] and the neonatal/infant pain scale (NIPS) [100]. However, some studies suggest\nthat patients may exaggerate their pain severity to prompt more aggressive treatment inter-\nventions [101], raising concerns about the accuracy of self-reported symptoms. Therefore,\nobjective pain measurement remains clinically crucial.Chapter 3\nAutomatic Pain Assessment\u2013A Literature\nReview\nContents\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database . . . . 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 49\n2122 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research Di-\nrections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.1 Chapter Overview\nThis chapter corresponds to the publication [17], a systematic literature review (SLR) con-\nducted at the start of this Ph.D. research. This review facilitated an understanding of au-\ntomatic pain assessment methods, particularly those based on deep learning, and the tech-\nniques and strategies employed. It enabled the identification and proposal of new approaches\nthat could enhance the effectiveness of pain recognition.\nAdditionally, it allowed for identifying gaps in the literature from other reviews con-\nducted on this specific research topic. Every existing systematic review on pain assessment\nwas identified and assessed, revealing several insights. The first review on automatic pain\nassessment, published by Prkachin in 2009 [102], did not cover papers on deep learning,\nas the practical implementations of deep architectures only began around 2012. Zamzmi et\nal.[103] focused their review exclusively on infants, omitting deep learning methods. In\n2018, Chen et al. [104] reviewed automated pain detection methods using the Facial Ac-\ntion Coding System (FACS), noting only three publications that employed deep learning\ntechniques. In 2019, Hassan et al. [105] included only seven papers that used deep learn-\ning methods in their review. Similarly, Werner et al. [106], also in 2019, discussed pain\nassessment without restrictions on modalities or age groups, finding fewer than ten papers\nthat reported on deep learning methods. In 2020, Al-Eidan et al. [107] published the first\nsystematic literature review titled \u201cDeep-Learning-Based Models for Pain Recognition: A\nSystematic Review\u201d, which included fifteen papers but was critiqued for having significant\nlimitations and incorrect information. It was noted that some papers analyzed might not be\nrelevant, and there was confusion between \u201cneural networks\u201d and \u201cdeep learning\u201d. For in-\nstance, while study [105] mentioned using neural network approaches, they did not provide\nevidence of using deep learning methods. Moreover, in the study [104], the authors devel-\noped a neural network with only two layers combined with handcrafted features, which does\nnot qualify as a deep learning method. Additionally, studies [103, 107] focused on detect-\ning protective movement behaviors in chronic pain patients, which deviates from the central\ntopic of automatic pain assessment. Several reviews and SLRs on automatic pain assessment\nhave been published, but none exclusively or adequately focus on deep learning methods.\nThis SLR aims to bridge this gap by thoroughly reviewing deep learning techniques used for\nautomatic pain assessment.3.2. MODALITIES AND HARDWARE FOR AUTOMATIC PAIN ASSESSMENT 23\n3.2 Modalities and Hardware for Automatic Pain Assessment\nCreating an automatic pain assessment system hinges on capturing the necessary input data\nthrough various information channels, referred to as modalities. These modalities are cat-\negorized into behavioral and physiological types. A system utilizing only one modality is\ntermed unimodal, whereas a multimodal system incorporates multiple modalities.\nKey behavioral modalities encompass facial expressions, body movements, gestures, and\nauditory signals. Researchers use a range of optical and light sensors to record images or\nvideo sequences of facial and body movements. Commonly, researchers employ color RGB\ncameras, but depth and thermal sensors are also used to enhance visual data. Motion capture\nsensors are also employed to track movements, and microphones are frequently employed\nto capture sound. On the physiological front, modalities often involve biosignals that detect\nelectrical activities from various tissues and organs. Techniques such as electrocardiogra-\nphy (ECG), electromyography (EMG), electrodermal activity (EDA), photoplethysmogra-\nphy (PPG), blood oxygen saturation (SpO2), near-infrared spectroscopy (NIRS), respiration\nrate, and skin temperature are commonly used to gauge pain. Multiple sensors can mea-\nsure several modalities simultaneously \u2014 for instance, strain sensors and cameras can track\nrespiration rates.\nBesides the sensors that gather input data, the computational hardware is crucial. Deep\nlearning-based systems operate in two phases: training and inference. The training phase is\nparticularly resource-intensive, necessitating a graphics processing unit (GPU). The trained\nmodel makes predictions on new data during inference, typically processed on a central\nprocessing unit (CPU). The choice of hardware depends on various factors, especially in\nreal-time scenarios where low latency is crucial, compared to offline settings where data\nprocessing can be deferred. Additionally, characteristics of the model, such as floating point\noperations per second (FLOPS) and total computational operations, are significant consider-\nations.\n3.3 Pain Databases\nAccess to data is crucial for evaluating methods and algorithms in automatic pain assess-\nment. However, only a few databases have explicitly been developed for automatic pain\nrecognition based on human behavioral and physiological changes. Unlike the extensive\ndata found in most facial expression databases, publicly accessible pain datasets often offer\nlimited samples and suffer from significant class imbalance. This primarily stems from the\nethical concerns associated with collecting pain data. Table 3.1 lists the principal databases\nreviewed in the studies. Figure 3.1 shows how frequently each database was used. Most\nresearch utilized publicly available datasets, with some studies exploring multiple datasets.24 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nFew studies used private datasets, mainly those aimed at detecting pain in neonates. The\nUNBC-McMaster Shoulder Pain Archive Database [108] is the most utilized, followed by\nTheBioVid Heat Pain Database [109]. The former contains 200facial videos of 25individ-\nuals with shoulder pain. At the same time, the latter includes facial videos and biopotentials\nof90healthy participants subjected to experimentally induced heat pain at four intensity\nlevels. The following subsections provide a brief description of some of these datasets.\nTable 3.1: Most commonly utilized pain databases.\nDatabase Modality Population Annotation\nGranularityAnnotation Labels\nUNBC-McMaster\nShoulder PainA[108]RGB video of face 25 adults with shoulder painFrame level\nSequence levelFACS\nV AS, OPI\nBioVidA[109] RGB video of face, EDA, ECG,\nEMG87 healthy adults Sequence level stimulus\n(calibrated per person)\nMIntPAINA[110] RGB-Depth-Thermal video of\nface20 healthy adults Sequence level stimulus\n(calibrated per person),\nV AS\niCOPEA[111] RGB photographs of face 26 healthy neonates Frame level pain, cry, rest, air puff,\nfriction\niCOPEvidA[112] Grayscale video of face 49 neonates Sequence level pain, no pain\nNPAD-IA[113] RGB video of face & body, HR,\nSpO2, BP, NIRS36 healthy neonates & 9 neonates\nwith tissue injured by surgerySequence level NIPS, N-PASS\nAPN-dbA[114] RGB video of face 112 healthy neonates Sequence level NFLAPS, NIPS, NFCS\nEmoPainN[115] video, audio, EMG, MoCap 22 adults with chronic pack pain &\n28 healthy adultsSequence level self-report, naive OPI\nSenseEmotionN\n[116]video of face, audio, EDA, ECG,\nEMG, RSP45 healthy adults Sequence level stimulus\n(calibrated per person)\nX-ITEN[117] RGB-Thermal video of face,\nRGB-Depth video of body, au-\ndio, EDA, ECG, EMG134 healthy adults Sequence level stimulus\n(calibrated per person)\nA: Publicly available by request, complete or part of the dataset N: Not yet available Modality: HR: heart rate SpO2: oxygen saturation rate BP:\nblood pressure NIRS: near-infrared spectroscopy MoCap: motion capture RSP: respiration rate EDA: electrodermal activity ECG: electrocardiogram EMG:\nelectromyogram Annotation Labels: FACS: Facial Action Coding System V AS: visual analogue scale OPI: observer pain intensity NIPS: neonatal infant\nscale N-PASS: neonatal pain, agitation and sedation scale NFLAPS: neonatal face and limb acute pain scale NFCS: neonatal facial coding system\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database\nTheUNBC McMaster Shoulder Pain Database [108] comprises 200video sequences show-\ning the facial expressions of 25subjects undergoing motion tests, including arm abduction\nand external and internal rotations. The data collection utilized both active and passive ap-\nproaches: in the active mode, subjects moved their affected arms to their bearable limit,\nwhile in the passive mode, a physiotherapist moved the subjects\u2019 arms. Each video sequence\ncontains about 60to700frames, totaling 48,398, with 82.71% of frames scoring a pain\nrating of zero, indicating a significant imbalance in the data. All frames are FACS-coded\nfor pain-related action units (AUs)\u2014AU4, AU6, AU7, AU9, AU10, AU12, AU20, AU25,\nAU26, AU27, and AU43\u2014with each AU coded for intensity from A to E, 0, or5, except\nfor AU43 (closed eyes), which is coded as either present or absent. Pain scores are assigned\nusing the PSPI metric based on the intensity of the AUs present. Additionally, the database3.3. PAIN DATABASES 25\n0815233038455360\nUNBC-McMasterBioVidEmoPainSenseEmotionX-ITEMIntPAINiCOPEiCOPEvidNPAD-IAPN-dbother\nTable 1Category AUNBC-McMaster59BioVid21EmoPain7SenseEmotion5X-ITE2MIntPAIN4iCOPE3iCOPEvid1NPAD-I5APN-db1other21\n1\nFigure 3.1: The number of studies utilizing these specific datasets. Note that various studies\nused multiple datasets to conduct their experiments.\nincludes 66facial landmarks per frame, determined by an active appearance model. Pain as-\nsessments also include self-reports using two Likert scales with 15options each and a visual\nanalog scale (V AS) from 1(no pain) to 10(extreme pain). One scale measures the sen-\nsory intensity from \u201cextremely weak\u201d to \u201cextremely intense\u201d , while the other assesses the\naffective-motivation aspect of pain from \u201cbearable\u201d to \u201cextremely excruciating\u201d. Indepen-\ndent observer pain intensity (OPI) ratings use a 6-point scale from 0(no pain) to 5(intense\npain). The UNBC database is currently the most extensively utilized dataset for automatic\npain recognition among publicly available resources.\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database\nTheBioVid dataset [109] is a prominent resource in pain research, comprising facial videos,\nelectrocardiograms, electromyograms, and galvanic skin response data from eighty-seven\npn=87qhealthy participants ( 44males and 43females, aged 20to65). The pain was in-\nduced using a thermode on the participants\u2019 right arm, with pain and tolerance thresholds\nestablished before data collection. These thresholds defined the range of pain from No Pain\n(NP) to Very Severe Pain (P 4), encompassing five levels of pain intensity. The temperatures\nfor the pain inductions ranged from P 1to P 4and did not exceed 50.5\u02ddC. Each participant\nunderwent 20inductions at each of four pain levels, with each induction lasting 4sfollowed\nby a recovery period of 8to12s. In addition, 20baseline measurements were taken at 32\u02ddC\n(NP), totaling 100stimulations per participant, randomly administered. Data processing\nsegmented these into 5.5sdurations starting 1safter the target temperature was reached, re-26 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsulting in 8,700samples across the five pain intensity classes, equally distributed among all\nmodalities for each participant. Video recordings were made at a frame rate of 25FPS, and\nbiosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\n3.3.3 The EmoPain Database\nTheEmoPain [115] dataset encompasses various pain indicators, including body movements,\naudio, biosignals, and postural and facial expressions. It features video and audio recordings\nof22patients ( 7male, 15female) exhibiting natural pain expressions while engaging in\nphysiotherapy-like exercises. These exercises, performed at regular and challenging levels,\ninclude a sitting-standing sequence, balancing on one leg for five minutes, and reaching for-\nward while standing. The video signals are captured in high resolution ( 1024\u02c61024 pixels)\nusing eight cameras positioned at various angles, enhanced by specialized lighting condi-\ntions. Audio is recorded with two microphones: an AKG C-1000S MKIII placed near the\ncameras and an AKG HC 577 L worn by the patients, both operating at a 48kHz sampling\nrate with bit Pulse Code Modulation. Body movements and postures are tracked using a mo-\ntion capture suit with 18sensors distributed across the body. Biosignals are monitored with\nfour sEMG sensors attached to the trapezius and lumbar para-spinal muscles. Additionally,\nthe dataset provides continuous frame-wise pain ratings for facial expressions by eight naive\nannotators and binary frame-wise annotations for protective behaviors by four experts, along\nwith coordinates from 26body nodes. Six annotated protective behaviors include stiffness,\nbracing, hesitation, limping, rubbing, and abrupt actions. Audio and EMG signals from the\neight activities per subject also contribute to multimodal pain recognition. Like the UNBC\ndatabase, EmoPain faces significant challenges due to data sparsity and imbalance\u2014only\n11.4%of frames show facial expressions of pain, and 8.6%show protective behaviors. This\nscarcity complicates pain recognition research, necessitating the development of methods\nthat efficiently utilize limited data to achieve optimal performance.\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database\nTheX-ITE [117] dataset is one of the largest pain datasets but is not publicly available. It\ninvolved 134healthy adults ( 67men and 67women) aged between 18and50. The aver-\nage age was 31.4years (SD = 9.7), with men averaging 33.4years (SD = 9.3) and women\n32.9years (SD = 10.2). Participants had no chronic pain, depression, psychiatric disorders,\nneurological conditions, headache syndromes, or cardiovascular disease, nor had they taken\npain medication or painkillers before the experiment. Pain stimuli were stimulated using the\nMedoc PATHWAY Model ATS for heat pain on the forearm and the Digitimer DS7A for elec-\ntrical pain on the index and middle fingers. Both modalities featured phasic stimuli (short,\n5seconds) and tonic stimuli (long, 60seconds), each in three intensities. After calibration,3.4. UNIMODAL STUDIES 27\nparticipants underwent a 90-minute stimulation phase where phasic stimuli were repeated\n30times in a randomized sequence with 8-12-second pauses. The tonic stimuli were applied\nonce per intensity, totaling six per participant, each followed by a five-minute pause. The\nhighest intensity tonic stimuli for heat and electrical pain were induced at the experiment\u2019s\nending, with the other stimuli randomly interspersed during the phasic period. Simultane-\nous to the pain stimulation, various sensors collected multimodal pain response data: frontal\nand side view RGB videos for facial expression and head pose analysis, audio for paralin-\nguistic response analysis, electrocardiogram (ECG) to monitor heart rate variability, surface\nelectromyography (EMG) to assess muscle activity in the trapezius, corrugator supercilii,\nand zygomaticus major, electrodermal activity (EDA) to measure sweating, video for body\nmovement analysis, and thermal video for facial temperature changes.\n3.3.5 The AI4Pain Database\nThe AI4Pain Grand Challenge 2024 [118] dataset is a recent contribution to the pain re-\nsearch field, tailored for sophisticated pain recognition tasks using fNIRS and facial video\ndata. This dataset involves sixty-five volunteers pn=65q, including 23females, with ages\nranging from 17to52years (mean age of 29.06years and a standard deviation of 8.28years).\nAlthough it captures physiological signals such as photoplethysmography (PPG), electroder-\nmal activity (EDA), and respiration (RESP), these signals are not publicly available yet. The\ndataset is segmented into three parts: training ( 41volunteers), validation ( 12volunteers),\nand testing ( 12volunteers). The experimental setup includes fNIRS data recorded with an\nArtinis device, measuring changes in oxygenated and deoxygenated haemoglobin concentra-\ntions across 24channels targeting the prefrontal cortex. The optodes configuration includes\n10sources and 8detectors spaced 30mm apart, using near-infrared light at 760nm and 840\nnm, sampled at 50Hz. Additionally, facial movements are captured by a Logitech Stream-\nCam at30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low Pain ,\nandHigh Pain . It features 65instances of No Pain (each lasting 60s),780instances of Low\nPain (each lasting 10s), and 780instances of High Pain (each lasting 10s). The No Pain\ninstances, recorded during baseline, serve as control data. The Low Pain instances reflect\nmild pain responses, and the High Pain instances capture significant pain, both derived from\na pain tolerance test and reflected in the corresponding neurological and behavioral data\nrecorded.\n3.4 Unimodal studies\nThis section presents the studies that utilized only one information channel to estimate the\nsubject\u2019s pain condition.28 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.1 Vision-based: Static Analysis\nThe first publicly available pain database that significantly contributed to the development\nof automatic pain assessment methods was the UNBC-McMaster Shoulder Pain Database .\nNumerous studies have employed this dataset. Pedersen [119] implemented the first deep\nlearning approach in 2015 to address the pain assessment problem, utilizing a 4-layer contrac-\ntive autoencoder. He combined the encoded representations with a support vector machine\n(SVM), achieving high performance in frame-level pain detection. A significant advance-\nment in vision-based pain recognition methods was the EmoPain challenge in 2020, which\nbecame the first international competition to compare machine learning methods for chronic\npain assessment. Egede et al. [120] presented the EMOPAIN 2020 Challenge , utilizing a\ndataset composed of features extracted via both handcrafted methods and deep-learned mod-\nels. They utilized facial landmarks, histogram of oriented gradients (HOG), and deep vectors\nfrom VGG-16 [121] and ResNet-50 [122], both pre-trained on the Aff-Wild dataset1. The au-\nthors report that combining hand-engineered features with deep learning cues led to the best\nperformance. Similarly, Yang et al. [123] extracted both low- and high-level features from lo-\ncal descriptors and the pre-trained VGG-16 CNN, combining them through weighted coeffi-\ncients. Semwal and Londhe [124] demonstrated that fusing deep-learned features with facial\nlandmarks is beneficial for multi-class pain estimation. Lakshminarayan et al. [125] com-\nbined deep-learned features with handcrafted ones\u2014namely features from VGG-16 [121]\nandResNet-50 [122], HOG, action unit occurrence and intensity, facial landmarks, and head\npose\u2014through a fully connected network. Their study found that combining VGG-16 with\nhandcrafted features lowered regression error, whereas [126] achieved maximal performance\nusing only VGG-16 features with a fully connected network.\nConversely, Semwal and Londhe [127] noted the limitations of traditional handcrafted\nfeature engineering and the computational expense of deep neural networks. As a solution,\nthey proposed a relatively shallow 4-layer CNN, which reduces computational costs due to\nfewer parameters while achieving performance comparable to deeper models. A different\napproach came from [128], where the authors focused on representing facial expressions\nas compact binary codes for pain intensity classification. Feature extraction was conducted\nusing a pre-trained model [129], with a fully connected network used to generate the binary\ncodes.\nSeveral studies utilized CNN ensemble designs with varying architectures to exploit fea-\nture diversity. Semwal and Londhe [130] combined predictions from three compact CNNs\u2014\nVGG-16 ,M-MobileNet [131], and GoogleNet [132]\u2014using the average ensemble rule, re-\nsulting in improved classification performance. Kharghanian et al. [133] developed a con-\nvolutional deep belief network (CDBN) using unsupervised feature learning. An SVM used\n1https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge3.4. UNIMODAL STUDIES 29\nthe extracted features to differentiate between two states for binary pain classification ( i.e.,\npain vs. no pain). Later, [134] added two layers to the CDBN, though the results were not\ndirectly comparable due to differing evaluation methods.\nSeveral papers suggest that because pain is predominantly expressed in specific facial\nregions, focusing on these areas rather than the whole face could improve model accuracy by\nreducing noise. Huang et al. [135] initially identified the left eye, right eye, nose, and mouth\nas key regions and utilized a multi-stream CNN for feature extraction, assigning learned\nweights to enhance attention on these regions. Xin et al. [136] employed a 9-layer CNN\nwith an attention mechanism to assign different weights to face regions, resulting in more\naccurate attention face maps and boosting prediction accuracy by up to 19%. Cui and Huang\n[137] introduced a multi-scale regional attention network (MSRAN), which uses multiple\ncropping regions from video frames. The framework includes self-attention and relation-\nattention modules to highlight pain-relevant regions and explore interrelationships. Li et\nal.[138] extended this concept by integrating contrastive and multi-task training through an\nautoencoder, building on the work of [139].\nOne challenge in pain intensity estimation is that individual facial features, such as face\nshape, can introduce significant variability in how pain is expressed. This makes it difficult\nto distinguish between adjacent intensity levels. To address this, Peng et al. [140] examined\nfacial shape information and developed a deep multi-task network to account for the rela-\ntionship between pain recognition and shape, which improved pain estimation performance.\nSimilarly, Xin et al. [141] proposed a novel multi-task framework that combines a CNN\nfeature learning module with an autoencoder attention component, also estimating subject\nidentity, as individual differences in pain manifestation are key. Their experiments achieved\nstate-of-the-art results on publicly available datasets.\nMost studies report results obtained from controlled laboratory settings, which typically\nfeature proper lighting, minimal head pose variability, and no occlusions. However, such\nconditions do not represent typical hospital environments. Semwal and Londhe [142] ad-\ndressed this by focusing on pain assessment in uncontrolled settings, developing a shallow\nCNN with three convolutional layers that performed comparably to deeper pre-trained mod-\nels. In a subsequent study [143], they introduced a more complex framework comprising\nthree modules that leveraged high-level spatial descriptors with both local and global geomet-\nric cues, achieving results comparable to models like GoogleNet [144] and VGG [121]. Lee\nand Wang [145] explored pain assessment in intensive care unit (ICU) settings, where par-\ntially occluded faces frequently complicate facial analysis. They developed a 4-layer CNN\ncombined with an extreme learning machine (ELM) for final estimation. Virrey and Cae-\nsarendra [146] used CNNs to classify sections of frames where pain was triggered, peaked,\nand subsided. Nugroho et al. [147] tackled pain detection in smart home-care settings, par-\nticularly for elderly patients, using relatively low-power mobile devices. They modified the30 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nOpenFace2library, based on pre-trained FaceNets [148], and showed that transfer learning\ncould enable real-time binary classification ( pain vs.no pain ), even on low-powered hard-\nware.\nResearchers like Dai et al. [149] and Menchetti et al. [150] have noted that most models,\nwhether deep or shallow, are trained on dataset-specific features rather than actual pain-\nrelated features. Moreover, most studies employ validation methods using the same dataset,\nwhile cross-dataset performance is rarely addressed, limiting real-world applicability. To\ntackle these issues, Dai et al. [149] combined pain and emotion detection datasets to develop\na real-time pain assessment system with better generalization capabilities. They emphasized\nthe importance of cross-corpus evaluation, real-time testing, and the need for well-balanced,\necologically valid pain datasets [151].\nSeveral studies have explored combining pain scales to improve prediction objectivity\nand reliability. Liu et al. [152] developed a two-stage personalized model trained using active\nappearance model (AAM) facial landmarks and multi-task learning, with visual analog scale\n(V AS) and observed pain index (OPI) as ground truth. Xu et al. [153] similarly reduced\nmean square error (MSE) by incorporating various pain scales with the VGG-Face model.\nHowever, Casti et al. [154] pointed out the limitations of original ground truth data due to\nsubjectivity and annotation inconsistencies. To address this, they re-annotated their dataset\nwith judgments from multiple experts, using multidimensional scaling to map frames to\nillumination-invariant 3D space, which they then fed into a pre-trained AlexNet [155].\nCelona and Manoni [156] investigated neonatal facial expressions to detect pain, achiev-\ning the highest accuracy when utilizing two pre-trained models: VGG-Face [157] and mapped\nLBP+CNN (MBPCNN) [158]. Similarly, Lu and Hao [159] found that pre-trained models\nwere crucial for small datasets like neonates, as training from scratch led to overfitting. They\nachieved optimal classification performance by fine-tuning the entire VGG-16 model [122].\nHowever, Zamzmi et al. [160] argue that most face recognition methods are tailored for\nadults and thus less applicable to infants. They developed a lightweight 2D CNN trained\nend-to-end and achieved high pain detection accuracy, but external validation on a different\nneonatal dataset revealed challenges with generalizability. In 2019, Brahnam et al. [112]\nintroduced the iCOPEvid neonatal video dataset, a significant contribution since the only\npublicly available neonatal pain dataset [111] previously contained only static images. Their\nexperiments showed that local descriptors based on the bag-of-features (BoF) approach out-\nperformed deep learning models like VGG-Face andResNet . Combining handcrafted and\ndeep-learned features offered only a marginal improvement in performance. In contrast, Za-\nmzmi et al. [161] found that the most effective approach for binary classification (pain vs.\nno pain) was the fusion of high-level features from VGG [162] and optical flow strains, with\n2http://cmusatyalab.github.io/openface3.4. UNIMODAL STUDIES 31\nnaive Bayes serving as the classifier. Celona and Brahnam [163] applied a Wasserstein gen-\nerative adversarial network with gradient penalty (WGAN-GP) [164], demonstrating that\ntraining set augmentation with synthetic samples improved classification performance. Ta-\nble 3.2 summarizes the vision-based studies focusing exclusively on the spatial dimension.32 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [112]F (RGB) texture\ndescriptors- FF 2D CNN`SVM SL C P O 49 k-fold iCOPEvid 79.80 AUC\n'15 [119]F (RGB) - - - AE SVM SeSL,\nSLC P PS 25 LOSO UNBC 86.10 ACC,\n96.50 AUC\n'20 [120]F (RGB) - - FF 2D CNN`NN SL R IC O 36 hold-out EmoPain 0.91 MAE;\n'18 [123]F (RGB) HOG,\nstatistics- FF 2D CNN`SVR SL R IC PS 25 LOSO UNBC 1.44 MSE;\n'21 [130]F (RGB) - - DF 2D CNN`- SL C ID PS 25 k-fold UNBC 93.87 ACC;\n'16 [133]F (RGB) - - - CDBN SVM UL C P PS 25 LOSO:UNBC 87.20 ACC;\n'21 [134]F (RGB) - - - CDBN SVM SL C P PS 25 LOSO UNBC 93.16 AUC\n'19 [135]F (RGB) - - FF 2D CNN - SL C ID1, IC PS 25 LOSO UNBC 88.191ACC\n'20 [136]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 51.10 ACC;\n'20 [140]F (RGB) - - FF 2D CNN`- SL R ID S 25 ? UNBC 79.94 ACC;\n'21 [142]F (RGB) - - - 2D CNN - SL C ID O 8 k-fold other 97.48 ACC;\n'19 [145]F (RGB) - - - 2D CNN ELM SL R IC PS 25 k-fold UNBC\u201a1.22 MSE;\n'19 [146]F (RGB) - - - 2D CNN - SL C TR, CL, DI PS 25 k-fold UNBC 60.00 ACC\n'19 [150]F (RGB) - - - 2D CNN`- SL C AUs-D PS 25, 43 k-fold UNBC & CK+1,\nWilkie97.701ACC;\n'17 [152]F (RGB) statistics - - NN GPM WSL R IC O, S 25 k-fold UNBC 2.18 MAE\n'20 [153]F (RGB) statistics - FF 2D CNN`NN SL R IC S 25 k-fold UNBC 1.95 MAE;\n'19 [154]F (RGB) LBP, MDS - - 2D CNN`- SL C ID O 25 hold-out UNBC 80.00 ACC\n'18 [159]F (RGB) - - - 2D CNN`- SL C ID O ? hold-out other 78.30 ACC\n`: Pre-trained model -:Not exist &: in Dataset indicates the utilization of cross-database training/validation ?: Not found :: The authors provide additional experiments with other validation methods \u201a: The authors\nutilized occluded facial images ;: The authors provide additional metrics Modality: F: face region Non deep features: LBP: local binary pattern MDS: multidimensional scaling Fusion: M: fusion of modalities E:\nfusion of deep learned features or hand-crafted features Deep models: AE: autoencoder RCNN: recurrent convolutional neural network CDBN: convolutional deep belief network CNN: convolutional neural network\nNN: neural network WGAN-GP: Wasserstein generative adversarial model with gradient penalty Non deep model: SVM: support vector machine GPM: Gaussian process regression model kNN: k-nearest neighbors\nNB: naive Bayes ELM: extreme learning machine Learning Method: SL: supervised learning SeSL: semi-supervised learning UL: unsupervised learning WSL: weakly supervised learning Classific./Regres.: C:\nclassification R: regression Objective: P: presence of pain ID: intensity in discrete scale IC: intensity in continuous scale TR: trigger CL: climax DI: diminishing AUs-D: Action Units detection GT: ground truth\nPS: Prkachin and Solomon S: self-report O: observer rating ST: stimulus Validation Method: LOSO: leave one subject out Metrics: AUC: Area Under the ROC Curve ACC: accuracy PPV: precision MSE: mean\nsquared error MAE: mean absolute error3.4. UNIMODAL STUDIES 33Table 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [124]F (RGB) facial landmarks - FF 2D CNN NN SL C, R ID, IC1P 25 LOSO:UNBC 0.171MSE;\n'20 [125]F (RGB) HOG, head pose,\nAUs intensity/\noccurrence, facial\nlandmarksFF - 2D CNN`NN SL R IC O 36 hold-out EmoPain 5.48 RMSE;\n'20 [126]F (RGB) - - - 2D CNN`NN SL R IC O 36 hold-out EmoPain 1.49 RMSE;\n'18 [127]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 92.00 ACC;\n'18 [128]F (RGB) statistics, distance\nmetrics- FF 2D CNN`- SL C, R ID, IC PS 25 LOSO UNBC 0.81 PCC,\n0.69 MSE\n'21 [137]F (RGB) - - FF 2D CNN`- SL C, R ID, IC P 25 LOSO UNBC 91.13 ACC,\n0.78 PCC,\n0.46 MSE\n'18 [138]F (RGB) - - - AE`- SL R IC PS 25 k-fold UNBC 0.33 MAE;\n'21 [141]F (RGB) - - FF [AE, 2D CNN]Y- SL C, R ID1, IC2,\nP3P, ST 25, 87 LOSO UNBC1,\nBioVid (A)289.1711ACC,\n0.8121PCC,\n85.6532ACC,\n40.4012ACC\n'21 [143]F (RGB) entropy texture\ndescriptors- - 2D CNN`- SL C ID O 8 k-fold other 0.92 PPV;\n'18 [147]F (RGB) - - - 2D CNN`- SL C P PS 14 k-fold UNBC 93.00 ACC\n'19 [149]F (RGB) - - - 2D CNN - SL C P PS 25, 20 k-fold UNBC &\nBioVid (A)\u02db56.75 ACC\n'17 [156]F (RGB) HOG, LBP - FF 2D CNN`SVM SL C P O 26 LOSO iCOPE 73.78 ACC\n'19 [160]F (RGB) - - - 2D CNN - SL C P O 31 LOSO NPAD1,\niCOPE296.981ACC;,\n89.802ACC\n'21 [165]F (RGB) - - - 2D CNN`- FL C P PS 25 LOSO UNBC 76.00 ACC;\n'21 [166]F (RGB) - - - 2D CNN`- SL C P O 25 hold-out UNBC 75.49 ACC\n'21 [167]F (RGB) - - - 2D CNN`SVR SL R IC P 25 LOSO UNBC 0.34 MSE\n'21 [168]F (RGB) - - - 2D R-CNN - SL C P O ? hold-out other 87.80 PPV\nY: The authors combined the deep models into a unified framework \u02db: The authors experimented with additional datasets combinations Non deep features: AUs: actions units HOG: histogram of oriented gradients Non\ndeep model: SVR: support vector regression Learning Method: FL: federated learning Metrics: RMSE: root mean squared error34 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [161]F (RGB) optical flow - FF 2D CNN`SVM,\nkNN, NBSL C P O 31 k-fold other 92.71 ACC,\n94.80 AUR\n'19 [163]F (RGB) - - - WGAN-GP - SL C P O 26 LOSO iCOPE 93.38 ACC\n'17 [169]F (RGB) - - - 2D CNN`- SL R IC PS 25 LOSO UNBC 0.99 MAE;\n'20 [170]F (RGB) - - - 2D CNN - SL C ID ST 87 hold-out BioVid (A) 36.60 ACC\n'20 [171]F (RGB) - - - 2D CNN - SL C P PS 25 hold-out UNBC 97.00 PPV;\n'21 [172]F (RGB) - - - 2D CNN - SL C ID P 28 LOSO:UNBC 90.30 ACC\n'19 [173]F (RGB) - - - 2D CNN - SL C P O 31 hold-out NPAD1,\niCOPE291.001ACC;,\n84.502ACC;\n'21 [174]F (RGB) - - - 2D CNN`- SL C P O 26, 30 hold-out iCOPE &\nUNIFESP89.90 ACC;\n'21 [175]F (RGB) - - - 2D CNN - SL C AUs-D P 10 hold-out Pain-ICU 77.00 ACC;3.4. UNIMODAL STUDIES 35\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach)\nPain assessment is particularly challenging due to its complex and dynamic nature. Rely-\ning on static, individual frames to assess pain fails to capture the phenomenon\u2019s temporal\nprogression and often leads to inaccurate estimations. Additionally, many studies highlight\nthe difficulties of applying deep learning techniques to small datasets, with one proposed\nsolution being the combination of deep learning and traditional feature extraction methods.\nEgede et al. [176] addressed this by extracting deep features from a pre-trained CNN, explic-\nitly targeting the eyes and mouth regions. Using a relevance vector regressor (RVR), they\ndemonstrated that combining deep and hand-crafted features led to optimal performance. De-\nspite the valuable insights the UNBC-McMaster database provides, its imbalanced sample\ndistribution\u2014particularly the limited number of frames showing pain\u2014poses a significant\nchallenge for deep learning models. In response, Egede and Valstar [177] devised a method\nbased on the observation that neighboring pain level classes share many common features.\nThis approach allowed them to avoid extracting all possible features for classes with fewer\nsamples, as certain features had already been utilized from other related classes. The study\nalso showed that combining deep and hand-crafted features improved performance. How-\never, in a later study [178], the authors applied a similar approach, using only deep-learned\nfeatures to address data imbalance, but could not replicate the same high-performance levels.\nTavakolian et al. [179] took a different approach, focusing on the detection of genuine\nversus acted pain through facial expressions, a technique with important applications in both\nmedical and forensic contexts. They developed a residual GAN (R-GAN) to capture subtle\nfacial changes and the dynamic nature of expressions, using a weighted spatio-temporal pool-\ning (WSP) method. In a subsequent study [180], the authors suggested that self-supervised\nlearning could reduce the time and effort needed for data labeling, as it does not require\ncomplete dataset annotation. They introduced a new similarity function for learning general-\nized representations with a Siamese network. They also employed statistical spatio-temporal\ndistillation (SSD) based on the Gaussian scale mixture (GSM) to improve computational effi-\nciency. This technique encodes spatiotemporal variations in facial videos into a single RGB\nimage, simplifying the model while maintaining effectiveness.\nOther studies also aim to capture the dynamic aspects of pain. For instance, [181] com-\nbined a random forest classifier with the pre-trained MobileNetV2 model [182], encoding\nvideos by selecting and merging three frames from different time points into a single image.\nOthman et al. [183] emphasized the importance of using diverse datasets\u2014including vary-\ning age, gender, pose, occlusion, and lighting conditions\u2014to improve model generalization.\nThey used multiple data combinations and a reduced version of MobileNetV2 , showing that\ncross-dataset training is essential for achieving better generalizability.36 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.3 Vision-based: Implicit Temporal Utilization\nSeveral studies have explored the application of 3D CNNs for pain assessment. Tavakolian\nand Hadid [184] developed a 3D CNN to capture dynamic facial representations from videos.\nThey noted that researchers often use fixed temporal kernel depths when employing 3D\nconvolution techniques, which limits the ability to capture short, mid, and long temporal\nranges simultaneously. To address this, they designed a model with parallel 3D convolutional\nlayers featuring variable temporal depths, allowing the capture of temporal dependencies\nfrom 32consecutive frames. Similarly, Wang and Sun [185] applied 3D convolutions based\non the architecture proposed in [186], consisting of 8convolutional layers with 3\u02c63\u02c63\nfilters. While they reported high performance, the authors acknowledged that extracting deep\nfeatures from small datasets posed a challenge for model generalization. In a related study,\nHuang et al. [187] developed a framework that integrated 3D, 2D, and 1D CNNs to extract\nspatio-temporal, spatial, and geometric features. For the 3D CNN component, they modified\nthe architecture from [188] by using discrete kernels of 1\u02c63\u02c63and3\u02c61\u02c61rather than the\ntraditional 3\u02c63\u02c63kernel. Other researchers have also proposed 3D deep CNNs with varying\ntemporal depths to capture short, mid, and long-range facial expression variations [189].\nRecognizing the difficulty and time consumption involved in training a deep 3D CNN from\nscratch, they introduced a cross-architecture knowledge transfer learning technique, utilizing\na pre-trained 2D CNN to assist in the training of the 3D CNN. In studies by Praveen et\nal.[190] and [191], the authors employed weakly-supervised domain adaptation, where the\nsource domain focused on human affective expressions and the target domain was explicitly\nrelated to pain expressions. Their framework featured an inflated 3D-CNN (I3D) [192],\nincorporating 3convolutional layers and 3inception modules [132] to capture both spatial\nand temporal information from video data.\nBargshady et al. [193] opted to use the HSV color space instead of RGB, arguing that it\nbetter reflects human visual perception for tasks such as skin pixel detection and multi-face\ndetection. They employed the pre-trained VGG-Face [157] for feature extraction, followed\nby a temporal convolutional network (TCN) using dilated causal convolutional operations to\nleverage temporal dependencies. Rezaei et al. [194] tackled the challenge of pain detection\nin people with dementia, a difficult task due to insufficient pain-related images or videos\nof elderly subjects in existing datasets. They developed a 10-layer 2D CNN that processed\npairs of pain and no-pain images, analyzing frame-to-frame changes and employing con-\ntrastive training methods [195]. The model demonstrated high performance in both healthy\nindividuals and people with dementia. In another study, Pandit and Schmitt [196] explored\nthe potential of using shallow 1D CNN architectures for real-time pain recognition. They ex-\ntracted facial action units from each frame using the OpenFace 2.03toolkit, with promising\n3https://github.com/TadasBaltrusaitis/OpenFace3.4. UNIMODAL STUDIES 37\nresults for pain detection in real-time settings.\n3.4.4 Vision-based: Explicit Temporal Utilization\nSeveral efforts have focused on addressing the limitations of static frames by developing\ndedicated temporal modules. Zhou et al. [197] tackled this issue using a regression frame-\nwork based on a 4-layer recurrent convolutional neural network (RCNN), each with a se-\nquence length of 3time steps. Rodriguez et al. [198] leveraged dynamic information by\ndesigning an LSTM model fed with feature vectors extracted from VGG-16 [122]. Simi-\nlarly, Bellantonio et al. [199] emphasized that facial expressions evolve, making it essential\nto analyze the spatio-temporal dimension of pain. They improved estimation performance\nusing a fine-tuned 16-layer CNN model [157], an LSTM processing 16frames as a time\nwindow, and super-resolution techniques. In another study, Bargshady et al. [200] com-\nbined the VGG-Face CNN [157] with a 3-layer LSTM to extract spatio-temporal features\nfrom grayscale images, applying zero-phase component analysis (ZCA). In [201], principal\ncomponent analysis (PCA) was used to reduce dimensionality. Mauricio et al. [202] also\nemployed VGG-Face but replaced LSTM with a 2-layer gated recurrent unit (GRU) to cap-\nture temporal dependencies. Thuseethan et al. [203] used a conventional 2D CNN and two\nRCNNs to extract temporal features from previous and subsequent frames, enhancing the\ntime dimension of expression analysis.\nA similar approach was followed by Bargshady et al. [204], who employed ensemble\nlearning with three distinct CNN-biLSTM modules, merging their outputs for the final pre-\ndiction. Salekin et al. [205] used a bilinear CNN (B-CNN) based on the VGG architecture\n[121], pre-trained on VGGFace24andImageNet5datasets, along with an LSTM to capture\ntemporal dependencies in image sequences. Kalischek et al. [206] explored deep domain\nadaptation for facial expression and pain detection, utilizing the self-ensembling approach\n[207] with a long-term recurrent convolutional network (LRCN). While they achieved state-\nof-the-art results for facial expression recognition, performance was lower for pain detection,\nlikely due to the subtle nature of pain-related expressions.\nDespite the availability of additional information in pain datasets, multi-task approaches\nremain limited. Martinez et al. [208] proposed a personalized multi-task learning method\nbased on individual physiological and behavioral pain responses. They extracted AAM fa-\ncial landmarks, processed them through a biLSTM to produce PSPI scores, and predicted the\nfinal V AS score. Erekat et al. [209] combined AlexNet [155] with 2 GRU layers to capture\ntemporal dependencies, using both self and observer-reported pain intensity as ground truth.\nVuet al. [210] developed a multi-task framework to estimate pain levels while reconstruct-\n4https://www.robots.ox.ac.uk/ \u02dcvgg/data/vgg_face\n5https://www.image-net.org38 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\ning heatmaps of action unit locations, improving model generalization with a CNN-LSTM\ncombination to capture micro facial movements.\nHuang et al. [211] noted that specific frames within a video sequence exhibit more pro-\nnounced pain expressions, requiring special handling. They developed a novel framework\nusing attention saliency maps with a VGG-16 model, GRUs and learned weights for each\nframe\u2019s contribution to pain intensity estimation. The study demonstrated that dynamic and\nsalient features can significantly improve performance. Similarly, Yu et al. [212] used VGG-\n11 (configuration A) and an LSTM to create an attention mechanism, predicting pain in-\ntensity from 16consecutive frames. Xu and Liu [213] adopted a ResNet-50 model with an\nattention mechanism to extract spatial features, followed by a transformer encoder to capture\ntemporal sequences, achieving promising results.\nIn other studies, Ragolta et al. [214] used extracted action units to train a 2-layer LSTM\npredicting pain on an 11-point scale, employing curriculum learning. Guo et al. [215] devel-\noped a convolutional LSTM (C-LSTM) to extract both spatial and temporal features from\nvideos, showing that temporal models outperform non-temporal models for pain estimation\naccuracy. Rasipuram et al. [216] utilized in-the-wild video data for pain detection, gener-\nating a 3D morphable model without relying on facial landmarks and combining it with an\nLSTM. Zhi and Wan [217] introduced sparse coding with LSTM (SLTM), using the iterative\nhard thresholding algorithm (ISTA) [218] to capture dynamic facial expressions. Although\nSLTM did not achieve high performance, it offers speed and efficiency for specific applica-\ntions. Finally, Thiam et al. [219] developed a method combining motion history and optical\nflow images with a 10-layer CNN and 2-layer biLSTM, showing that weighted score aggre-\ngation improves performance. Table 3.3 summarizes studies incorporating the modalities\u2019\ntemporal dimensions.3.4. UNIMODAL STUDIES 39Table 3.3: Vision-based studies with temporal utilization.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'17 [176] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVR SL R IC PS 25 LOSO UNBC 0.99 RMSE,\n0.67 PCC\n'17 [177] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVM SL R IC PS 25 LOSO UNBC 1.04 RMSE,\n0.64 PCC\n\u201918 [178] F (RGB) - - - NL 2D CNN - SL R IC PS 25 LOSO UNBC 1.20 RMSE,\n0.47 PCC\n'18 [184] F (RGB) - - - I 3D CNN - SL R IC PS 25 LOSO UNBC 0.53 MSE,\n0.84 PCC;\n'18 [185] F (RGB) HOG,\ngeometric\ndifference- DF I 3D CNN SVR SL R IC PS 25 LOSO UNBC 0.94 RMSE,\n0.67 PCC\n'20 [191] F (RGB) - - - I 3D CNN`- WSL R IC PS 24, ? LOSO UNBC\n& RECOLA0.64 MAE,\n0.82 PCC;\n'16 [197] F (RGB) - - FF E RCNN - SL R IC PS 25 LOSO UNBC 1.54 MSE,\n0.65 PCC\n'17 [198] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C, R P, IC1PS 25 LOSO UNBC 0.741MSE,\n0.781PCC;\n'17 [199] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 61.90 ACC\n'19 [200] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 75.20 ACC\n'20 [201] F (RGB) PCA - DF E [2D CNN`,\n1D CNN, biLSTM]Y- SL C ID PS 25 LOSO:UNBC 85.00 ACC;\n'19 [202] F (RGB) - - - E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 85.40 ACC,\n0.62 MSE;\n'19 [203] F (RGB) - - FF E [2D CNN, RCNN]Y- SL R IC PS 25 LOSO UNBC 1.29 MSE,\n0.73 PCC\n'17 [208] F (RGB) - - FF E biLSTM HCRF,\nFCSL C IC O,\nS25 hold-out UNBC 2.46 MAE;\nNon deep features: PCA: principal component analysis Temporal Exploitation: NL: non-machine learning method I: implicit method E: explicit method Deep models: RCNN: recurrent convolutional neural network\nLSTM: long short memory networks biLSTM: bidirectional neural network GRU: gated recurrent unit Non deep models: SVM: support vector machine RVM: relevance vector machine GPM: Gaussian process regression\nmodel HCRF: hidden conditional random fields FC: fully connected SVR: support vector regression Objective: I2: intensity in binary pairs Metrics: PCC: Pearson correlation coefficient40 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [114]F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN RVR SL R IC O 13 LOSO APN-DB 1.71 MAE;\n'19 [179]F (RGB) - - - NL R-GAN - UL C genuine\nvs posedPS,\nST25,\n34,\n87,\n87? UNBC\n& STOIC\n& BioVid (A)\n& BioVid (D)90.97 ACC\n'20 [180]F (RGB) - - FF NL 2D CNN`- SSL C IC P,\nST25\n87LOSO UNBC1,\nBioVid (A)2\u20180.781PCC;,\n71.022AUC;\n'21 [181]F (RGB) AUs\nintensity- H NL 2D CNN`RF SL C ID ST 127 k-fold X-ITE 25.00 ACC\n'19 [183]F (RGB) - - - NL 2D CNN - SL C P ST 87\n134k-fold BioVid (A)\n& X-ITE\u201867.90 ACC\n'20 [209]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC O,\nS25 k-fold UNBC 2.34 MAE\n'20 [211]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC PS 19 LOSO UNBC 0.21 MSE,\n0.89 PCC\n'19 [212]F (RGB) - - FF E [2D CNN, LSTM]Y- SL R IC PS 24 LOSO UNBC 1.22 MSE;,\n0.40 PCC;\n'20 [214]F (RGB) AUs\nintensity- - E LSTM - SL R IC O 36 hold-out EmoPain 2.12 RMSE,\n1.60 MAE;\n'20 [216]F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C P O ? k-fold UNBC 78.20 ACC;\n'20 [219]F (RGB) - - DF E [2D CNN, biLSTM,\nNN]Y- SL C P ST 87\n40LOSO BioVid (A)1,\nSenseEmotion269.251ACC,\n64.352ACC\n'20 [220]F (RGB) - - FF E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 0.84 ACC,\n0.69 PCC;\n\u2018: The authors provide experiments with cross-dataset settings Fusion: H: hybrid Non deep models: RF: random forest classifier3.4. UNIMODAL STUDIES 41Table 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [187]F (RGB) facial\nlandmarks- DF I [3D CNN`,\n2D CNN`,\n1D CNN, FC]Y- SL R IC PS 25 LOSO UNBC 0.76 MSE,\n0.82 PCC;\n'19 [189]F (RGB) - - - I [2D CNN`,\n3D CNN]Y- UL,\nSLC, R IC1, P2P,\nST25, 87 LOSO UNBC1,\nBioVid (A)20.9211PCC;,\n86.0222AUC\n'20 [190]F (RGB) - - - I 3D CNN`- WSL R IC PS 24,?,\n87, 18LOSO UNBC1\n& RECOLA\n& BioVid (A)2\u20180.741PCC,\n0.342PCC\n'20 [193]F (RGB) PCA - FF I [2D CNN`,\nTCN]Y- SL C ID P,\nST25, 20 LOSO:UNBC1,\nMIntPAIN292.441ACC;,\n89.002ACC;\n'20 [194]F (RGB) - - - I 2D CNN - SL C, R IC, P1P 95, 25 k-fold UofR & UNBC182.0011PCC;\n'20 [196]F (RGB) AUs\noccurrence- FF I 1D CNN - SL R IC P 24, 87 hold-\noutUNBC1,\nBioVid (A)0.801CCC\n'20 [204]F (RGB) PCA - DF E [2D CNN`, 1D\nCNN, biLSTM]Y- SL C ID PS,\nST25, 20 k-fold UNBC1,\nMIntPAIN286.001ACC;\n92.262ACC;\n'20 [205]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- SL R P, IC1O 45 LOSO NPAD 3.991MSE,\n1.552MAE\n'19 [206]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- UL C P ST 40 LOSO SenseEmotion 60.61 ACC\n'21 [210]F (RGB) - - - E [2D CNN`,\nLSTM]Y- SL R IC P 25, 27 LOSO UNBC1,\nDISFA\u20180.60`MSE,\n0.82`PCC;\n'21 [213]F (RGB) - - - E [2D CNN`,\nTransformer]Y- SL R IC P 25 LOSO UNBC 0.40 MSE,\n0.76 PCC;\n'21 [215]F (RGB) - - - E 2D C-LSTM - SL C ID S 29 hold-\noutother 69.58 F1\n'19 [217]F (RGB) - - FF E SLSTM - SL C P1, ID2ST 85 LOSO BioVid (A) 61.701ACC\n29.702ACC\n'21 [221]F (RGB) - - - I 3D CNN`- SL R IC S 25 k-fold UNBC 0.66 ICC;\nFusion: H: hybrid Deep models: TCN: temporal convolutional neural network C-LSTM: convolutional-LSTM SLTM: sparse long short memory network Learning Method: SSL: self-supervised learning Metrics: F1:\nF1 score CCC: concordance correlation coefficient42 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.5 Touch sensor-based\nTouch (contact) sensors provide a viable alternative for pain assessment, often outperforming\nvision-based methods. Table 3.4 highlights studies that utilized contact sensor data to evalu-\nate pain. Yu et al. [222] analyzed three categories of pain-no pain, moderate pain, and severe\npain\u2014using EEG signals. They extracted several bands from the biosignals, including al-\npha, beta, and gamma, and applied a convolutional module. The study found that combining\nthese bands yielded better results than evaluating them independently. Similarly, [223] used\nEEG potentials with an autoencoder to compress the raw data and applied a logistic regressor\nfor classification.\nOther researchers, such as Rojas et al. [224], utilized functional near-infrared spec-\ntroscopy (fNIRS) for pain detection. They developed three models\u2014multilayer perceptron\n(MLP), LSTM, and biLSTM\u2014with biLSTM demonstrating superior accuracy. Addition-\nally, [225] focused on PPG signals, extracting hand-crafted features from the time and fre-\nquency domains, which were then combined with a deep belief network (DBN) to achieve\nover65% accuracy in a 4-class pain assessment task. Hu et al. [226] used kinematic data\nto compare healthy individuals with those suffering from low back pain (LBP). Their ap-\nproach, which employed two stacked LSTM layers, reached over 97% accuracy in binary\nclassification using raw motion data. Lastly, Mamontov et al. [227] were the first to apply\nevolutionary algorithms in the design of an optimized recurrent neural network (RNN) for\npain estimation, achieving 91.94% accuracy using EDA signals.\n3.4.6 Audio-based\nA few studies have explored using audio information for pain detection and intensity esti-\nmation, as outlined in Table 3.5. These methods are especially relevant for neonates, where\nfrequent facial and body occlusions make analyzing cries a more effective approach for pain\ndetection. Chang and Li [228] concentrated on infant cries to differentiate between hunger,\npain, and sleepiness. They transformed the audio signals into 2D spectrograms using a fast\nFourier transform (FFT) and trained a 2D CNN for feature extraction. Similarly, [229] uti-\nlized spectrograms generated from recorded sounds, employing a model identical to that\nused in [160]. Thiam and Schwenker [230] focused on detecting adult pain by analyzing\nbreathing sounds. They leveraged deep-learned features from spectrograms with Mel-scaled\nshort-time Fourier transform, combined with various handcrafted cues. A CNN followed by\na biLSTM was used to capture spatial and temporal dependencies, integrating both low- and\nhigh-level features. In a different approach, Tsai et al. [231] examined pain events during\nemergency triage. They developed an LSTM autoencoder framework to extract temporal\nfeatures from verbal behavior, reporting encouraging results.3.4. UNIMODAL STUDIES 43Table 3.4: Touch sensor-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'20 [222]EEG - - FF I 1D TCN - S C ID S 32 k-fold other 97.30 ACC;\n'20 [223]EEG - - - I AE (TCN) LR UL, S C P S 29 LOSO other 74.60 ACC\n'21 [224]fNIRS - - - E biLSTM - SL C ID S 18 k-fold other 90.60 ACC;\n'19 [225]PPG - - - NL DBN SBM U, SL C P1, ID2S 100 k-fold other 86.791ACC,\n65.572ACC\n'18 [226]kinematatics - - FF E LSTM - SL C P LBP 44 LOSO other 97.20 ACC;\n'19 [227]EDA - - FF E [RNN, LSTM,\nGRU, NN]YSelfCGA,\nselfCGP,\nPSOPBSL C P ST 40 LOSO Sense-\nEmotion81.94 ACC\n'21 [232]EDA - - - I NN - SL C P1, I2 ST 87,\n55LOSO BioVid (A)1,\nPainMonit284.2211ACC;,\n86.5012ACC;\nModality: PPG: photoplethysmogram fNIRS: functional near-infrared spectroscopy EEG: electroencephalography EDA: electrodermal activity Deep models: DBN: Deep belief network RNN: recurrent neural network\nNon deep models: SBM: selective bagging model LR: Logistic Regression SelfCGA: Self-Configuring Genetic Algorithm SelfCGP: Self-Configuring Genetic Programming PSOPB: Particle Swarm Optimisation with\nparasitic behaviour GT: LBP: low back pain vs healthy population\nTable 3.5: Audio-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'16 [228]audio (cry) - - - - 2D CNN - SL C P O ? k-fold other 78.50 ACC\n'19 [229]audio (cry) - - - - 2D CNN - SL C P O 31 LOSO:NPAD 96.77 ACC;\n'19 [230]audio\n(breathing)MFCCs,\nRASTA-\nPLP,\nDTD- FF E [2D CNN,\nLSTM]YRFc SL C P ST 40 LOSO Sense-\nEmotion64.39 ACC\n'17 [231]audio\n(voice)prosodic-\nspectral\nfeatures,\nSF- FF E LSTM`SVM UL,\nSLC P1, ID2S 63 LOSO other 72.301UAR,\n54.202UAR\nNon deep features: MFCCs: Mel Frequency Cepstral Coefficients RASTA-PLT: Relative Spectral Perceptual Linear Predictive DTD: descriptors from temporal domain SF: statistical features44 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.5 Multimodal studies\nSince pain is a multidimensional phenomenon, combining multiple modalities in a multi-\nmodal system offers a promising approach. Heterogeneous information sources can com-\nplement one another, enhancing specificity and sensitivity. As reported in [106], when in-\ndividual modalities demonstrate good predictive performance, their fusion tends to yield\nimproved outcomes. Moreover, integrating cues from various channels may be helpful and\nnecessary, especially in clinical settings where specific modalities may become unavailable\n(for instance, if the patient turns and their face is occluded). The information channels can\noriginate from (1) the same hardware sensor but focus on different regions of interest, such\nas RGB facial images and RGB body images [233], (2) different hardware sensors but the\nsame region of interest, like RGB facial images and thermal facial images [110], or (3)\ndifferent hardware sensors and information sources, such as RGB facial images and ECG\nsignals [234]. Table 3.6 lists the studies utilizing multimodal approaches.\n3.5.1 Static Analysis\nA commonly used biosignal combination is those of EDA, EMG, and ECG, as these channels\nare found in all main pain reference databases. Thiam et al. [235] applied an early fusion\nmethod by merging these signals into a 2D representation and inputting it into a 9-layer 2D\nCNN. Their results showed a strong correlation between EDA and pain intensity, and com-\nbining all three modalities did not outperform using EDA alone. Al-Qerem et al. [236] used\nleast generative adversarial networks (LSGANs) to enhance EMG, EDA, and ECG samples,\nreporting a notable improvement in classification when using an SVM on the augmented\ndataset. Haque et al. [110] introduced the MIntPAIN dataset, which includes RGB, depth,\nand thermal videos for multi-class ( 5levels) pain recognition. They combined these three vi-\nsual modalities into a 5D matrix (RGB+D+T) and used it to train the pre-trained VGG-Face\nmodel [157], leading to better classification performance in their experiments.\n3.5.2 Temporal Utilization\nZhiet al. [237] proposed a multimodal stream-integrated neural network that leverages video\nand biosignal data. They combined raw facial video frames with optical flow images to cap-\nture spatio-temporal dependencies via 3D CNNs, integrating these with biosignal features\nextracted using LSTMs. The entire network was trained end-to-end, achieving superior re-\nsults compared to their unimodal methods. Beyond facial analysis, Salekin et al. [233]\nfocused on assessing neonatal pain through body movements in videos. After identifying\nrelevant body regions, video frames were fed into a pre-trained VGG-16 [121], connected to\nan LSTM to capture temporal dynamics. In a follow-up study, Salekin et al. [238] fused three3.5. MULTIMODAL STUDIES 45\nmodalities\u2014facial expressions, body movements, and crying sounds\u2013demonstrating that this\nmultimodal approach outperformed unimodal techniques. Similarly, Wang et al. [239] ex-\nplored combining EMG, EDA, and ECG biosignals with handcrafted and learned features\nfrom a biLSTM model. They applied the minimum relevance method (MRMR) to reduce\nthe number of features, resulting in notable outcomes.\nIn addition to EDA, EMG, and ECG, other biosignal combinations have been explored.\nZhao et al. [240] integrated PPG, EDA, and temperature signals, using 2D convolutions for\nspatial feature extraction and time windows for capturing temporal information. Yuan et\nal.[241] successfully estimated pain using whole-body MoCap sensors and EMG, utilizing\nLSTM layers with an attention mechanism in an autoencoder, which reduced training time\nby leveraging latent space representations of raw data. Similarly, Li et al. [242] employed\nMoCap and EMG as data sources and tested various LSTM configurations to predict pain\nintensity, achieving the best performance with a 3-layer vanilla LSTM combined with a 3-\nlayer fully connected network.46 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.6: Multimodal-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [110]F (RGB,\nthermal,\ndepth)- RF - - 2D CNN`- SL C ID S 20 k-fold MIntPAIN 36.55 ACC\n'19 [233]F, B (RGB) - FF - E [2D CNN`,\nLSTM]Y- SL C P O 31 LOSO other 92.48 ACC;\n'19 [234]F (RGB),\nECG, EDAbiosignals\u2019\nfeaturesmFF FF - 2D CNN`RFc SL C I2 S 85 k-fold BioVid (A) 74.00 ACC\n'19 [235]EDA,\nEMG,\nECG- RF - - 2D CNN - SL C P1I2,\nID2S 87,\n86LOSO BioVid (A)1\nBioVid (B)84.4011ACC;,\n36.5412ACC;\n'20 [236]EDA,\nEMG,\nECGBoruta\nfeaturesFF - - LSGAN SVM UL,\nSLC I2, ID1S 85 hold-\noutBioVid (A) 82.801ACC\n'21 [237]F (RGB),\nEDA,\nEMG, ECGoptical\nflowFF FF NL,\nE, I[3D CNN,\nLSTM]Y- SL C, R P1, I2,\nID2S 87,\n40k-fold:BioVid (A)1,\nMIntPain68.2011ACC;,\n28.1021ACC\n'21 [238]F, B (RGB),\nsound- DF - E [2D CNN`,\nLSTM]Y- SL C P O 45 LOSO NPAD 78.95 ACC;\n'20 [239]EDA,\nEMG,\nECGMRMR,\nbiosig-\nnals\u2019\nfeaturesRF\nFFE biLSTM NN SL C P1, I2 S 87 LOSO BioVid (A) 83.301ACC\n'20 [243]EDA,\nEMG,\nECG- FF - I [DDCAE,\nNN]Y- UL,\nSLC P1, I2 S 87 LOSO BioVid (A) 83.991ACC;\n'21 [244]EDA,\nEMG,\nECG, RSP- FF - I [DDCAE,\nNN]Y- UL,\nSL,\nSSLC, R P1, ID2,\nICS 87,\n40LOSO BioVid (A)1,\nSense-\nEmotion84.2511ACC;,\n35.4421ACC;\n'21 [245]EDA, ECG - FF - E 1D CNN,\nLSTM- UL C P1, I2 S 67 hold-\noutBioVid (A) 81.711ACC\n'20 [240]PPG, EDA,\ntemperature- RF - I 2D CNN - SL R\u02ddP1, ID2S 21 k-fold other 96.301ACC,\n95.232ACC\n'20 [241]MoCap,\nEMG- RF - E AE, LSTM - UL,\nSLC ID O 23 LOSO:EmoPain 52.60 ACC;\n'20 [242]MoCap,\nEMG- RF - E LSTM, NN - UL C ID O 30 hold-\noutEmoPain 80.00 ACC;\n'21 [246]MoCap,\nEMG- RF - E LSTM, NN - SL C ID O 30 LOSO:EmoPain 54.60 ACC;\nm: Not specifically described \u02dd: Ordinal Modality F: face region B: body region EMG: electromyography Non deep features: MRMR: Minimum Redundancy Maximum Relevance method Deep models: LSGAN:\nLeast Square Generative Adversarial Networks3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 47\n3.6 Summary of Automatic Pain Assessment Methods\nThis section presents an analysis of the reviewed studies, summarizing the main conclusions\non current methods for automatic pain assessment, their advantages, and corresponding lim-\nitations. Additionally, it offers recommendations for future research directions that could\nadvance the field of pain research from a computational perspective.\n3.6.1 Input\nFirst, we observe a clear imbalance between unimodal and multimodal approaches in pain\nassessment studies. More than 86% of the reported research focuses on unimodal methods,\neven though the databases often contain multiple information channels. Notably, contact\nsensor-based and audio-based approaches are underrepresented, with only seven and four\nstudies, respectively, compared to 84studies that utilize a vision-based approach.\nMultimodal approaches are even less explored, with only 15studies falling into this\ncategory, making it difficult to draw strong conclusions about the effectiveness of specific\nmodality combinations. However, there are indications that EDA sensor data is particularly\nvaluable compared to other biopotentials. Researchers have primarily focused on visual data,\nlikely due to the complexity of implementing multimodal frameworks or the impracticality\nof contact sensors in non-laboratory settings. Further exploration of diverse modality com-\nbinations is necessary to evaluate their potential for pain assessment fully\u2014additionally, 28\nstudies employed non-deep features to enhance deep-learned representations.\nFinally, we identified three primary strategies in examining the approaches that utilize\ntemporal information: non-machine learning-based, machine learning-based (implicit), and\nmachine learning-based (explicit). Non-machine learning-based methods, such as motion\nhistory images [219] or temporal distillation [180], rely on traditional computer vision tech-\nniques. These methods tend to be more straightforward but are generally less sophisti-\ncated. In contrast, machine learning-based approaches [190] [217] offer richer temporal\ninformation and the flexibility to adapt to specific requirements, such as emphasizing certain\nvideo frames. Among the studies reviewed, 55% employed temporal features, with explicit\nmethods\u2014most commonly LSTM models\u2014being the predominant choice. Given that many\nstudies report superior performance when temporal information is incorporated, compared\nto non-temporal methods, it is evident that further emphasis on temporal approaches is war-\nranted.\n3.6.2 Processing\nRegarding machine learning approaches, various models and techniques have been employed\nfor pain estimation. CNN models remain the most widely used, with more than 75% of stud-48 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nies utilizing 1D, 2D, or 3D filters, highlighting the central role of convolution operations\nin deep learning. Sequential models, such as RNNs, GRUs, LSTMs, and biLSTMs, follow\nclosely behind in popularity. Almost half of the studies used pre-trained models to achieve\ntheir desired performance. This suggests that existing pain databases may not be adequate\nfor training deep-learning models from scratch. Non-deep learning models have also been\nemployed in 26studies as auxiliary decision components, with SVMs and shallow neural net-\nworks being the most common choices. There seems to be significant potential for adopting\nnewer deep learning architectures, especially transformer-based models, which have demon-\nstrated state-of-the-art results in various AI research fields and are particularly suited for\nexploiting temporal modality information [247].\nThe predominant learning method used across studies is supervised learning. How-\never, 16papers explored or adopted alternative methods such as unsupervised learning [119,\n133, 179, 189, 206, 223, 225, 231, 236, 241, 243], self-supervised [180, 244], self-supervised\nlearning [180, 244], semi-supervised learning [119], weakly supervised learning [190, 191],\nand federated learning [165]. Given the limited availability of pain data resources, self-\nsupervised learning appears to be the most appropriate method for future research and should\nbe further embraced by the community.\nLastly, it is notable that most studies\u2014approximately 70%\u2014treat pain assessment as a\nclassification problem rather than a regression problem. However, we believe that regres-\nsion more closely reflects the continuous nature of pain and is better suited to capturing the\ncomplexity of pain sensation.\n3.6.3 Evaluation\nThe primary objectives of the reviewed studies were (i)to estimate pain intensity on a dis-\ncrete scale (multi-class classification), (ii)to measure pain intensity on a continuous scale,\nand(iii)to determine the presence or absence of pain (binary classification). Notably, 25\nstudies focused on pain detection rather than pain intensity estimation, which, from a clin-\nical standpoint, is less informative as it does not provide sufficient data for effective pain\nmanagement. From an engineering perspective, detecting the presence or absence of pain is\nalso a more straightforward and less demanding task.\nA small subset of studies took a different approach to pain estimation. For instance, one\nstudy [179] sought to differentiate genuine pain from acted pain. Another [231] explored\npain events in emergency triage settings rather than controlled laboratory environments,\nwhile [234] examined the feasibility of real-time pain detection on IoT devices. Addition-\nally, [142] and [143] aimed to address the issue of occluded faces in pain estimation. So-\nciodemographic and psychological factors were also considered, as seen in studies like [245],\nwhich explored gender differences, and [194], which focused on pain assessment in elderly3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 49\npatients with dementia. The limited exploration of pain estimation in real-world settings\nor unconventional contexts suggests that current approaches may not be fully applicable in\npractical environments like clinics and hospitals.\nVarious annotation types are used regarding ground truth, such as self-reported ratings,\nFACS, and observer scales. Temporal features are critical for accurately estimating pain\nintensity, making the temporal granularity of the ground truth equally important. Several\nstudies have questioned the objectivity of PSPI scores, as noted in [248], which highlights\nthat PSPI scores can be zero even when pain is present or that there may be no visible facial\nexpressions in low-intensity pain. Pain expressions not captured by the FACS system, such\nas raising eyebrows or opening the mouth, further challenge the use of PSPI [249]. Addi-\ntionally, PSPI does not account for pain-related head and body movements, which are par-\nticularly valuable in newborn assessments [250]. For these reasons, we recommend moving\naway from PSPI as ground truth in favor of self-reports and observer scales at the video-\nsegment level.\nAround 54% of the studies employed the leave-one-subject-out (LOSO) validation method,\nwhich is widely regarded as more objective and better for assessing the generalizability of\nmodels. However, LOSO can be less practical due to the increased model size and longer\ntraining times. When researchers use other validation methods, such as k-fold or hold-out,\nit is essential to ensure that consecutive, highly correlated frames from the same subject do\nnot skew the training and validation results, leading to flawed estimations. Moreover, when\nresearchers define their own validation or testing sets, comparing results across studies\u2014\nespecially between classification and regression models\u2014becomes nearly impossible. We\nbelieve standardized evaluation protocols should be developed for each publicly available\ndatabase for these reasons.\n3.6.4 Pain Databases for Evaluation\nThe availability of suitable public databases is arguably the most crucial factor in addressing\nthe challenge of automatic pain assessment. Several aspects must be considered in evaluating\nthese datasets, including the number of subjects and their characteristics, such as age, sex,\nhealth status, and race. Moreover, the ground truth must be objective and offer meaningful\ninsights into the subject\u2019s pain experience [154].\nFig. 3.1 illustrates the number of papers corresponding to the pain database utilized in\neach study. It is clear from this figure that the UNBC andBioVid databases were the most\ncommonly used public datasets. However, the UNBC dataset does not record the subjects\u2019\nages, despite age being a known factor in pain expression [35,66]. While the BioVid dataset\ndoes document age, the oldest participants are only 65years old, which is notable since pain\nand its management are critical issues among individuals aged 65and older [251]. Simi-50 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nlar limitations are found in other pain datasets, such as X-ITE [117], EmoPain [115], and\nSenseEmotion [116].\nIt is well known that aging causes skin changes, including texture, rigidity, and elastic-\nity alterations, which can impact facial emotion recognition tasks [78]. Additionally, race-\nrelated factors can lead to inaccurate pain assessments due to variations in how pain is ex-\npressed [252]. Notably, one study by Nerella et al. [175] reported lower performance when\ntheir model was tested on African American patients. Furthermore, only one study [194]\nwas found that specifically addressed pain estimation in elderly individuals with dementia.\nIn summary, developing objective, automated, and generalizable deep learning-based\npain assessment systems will only be possible if balanced and representative datasets are\navailable for training and external validation.\n3.6.5 Interpretation of Results\nRecent advancements in AI have shown state-of-the-art performance across nearly every\nscientific discipline, often surpassing human accuracy in specific diagnostic tasks [253].\nHowever, a significant drawback of AI solutions, particularly deep neural networks, is their\nlack of transparency, commonly called \u201cblack box AI\u201d. This term highlights how these\nmodels learn intricate functions that are opaque and frequently incomprehensible to hu-\nmans [254]. This opacity is a primary reason for the criticism directed toward deep learning\ntechniques [255]. Various techniques, such as visualizations and gradients-backpropagation\nfocusing on specific units, have been developed to offer insights into how these models func-\ntion. For further reading, refer to the comprehensive review on explanatory techniques in\ndeep learning [256].\nTable 3.7 outlines the different approaches used to interpret model decisions. Only a\nsmall fraction of the reviewed studies\u2014 20out of 110\u2014implemented methods to explain\nhow their models work and which features or elements they focus on. It is important to\nnote that interpretable machine learning can be broadly defined as the \u201cextraction of rele-\nvant knowledge from a machine-learning model concerning relationships either contained\nin data or learned by the model\u201d [257]. To summarize: (i)18% of the reviewed studies\nprovided an approach to enhance the interpretability of the model\u2019s decision, (ii)all of these\nmethods were applied to studies using facial images as the input modality, and (iii)around\nhalf of these studies were conducted by just three specific research groups. These findings\nsuggest that the issue of interpretability and explainability within deep learning remains un-\nderexplored, particularly in the context of automatically classifying pain severity levels.3.7. CHALLENGES AND FUTURE DIRECTIONS 51\nTable 3.7: Interpretation approaches.\nPaper Year Modality Method\n[124] 2021 F (RGB) visualization (saliency maps)\n[128] 2018 F (RGB) visualization (heat maps)\n[130] 2021 F (RGB) visualization (saliency map)\n[133] 2016 F (RGB) visualization (learned filters)\n[134] 2021 F (RGB) visualization (learned filters)\n[135] 2019 F (RGB) visualization (heat maps),\nvalues of learned weights\n[138] 2018 F (RGB) visualization (saliency maps)\n[141] 2021 F (RGB) visualization (attention maps)\n[142] 2021 F (RGB) visualization (saliency map)\n[143] 2021 F (RGB) visualization (activation maps)\n[153] 2020 F (RGB) visualization (pixels contributions)\n[177] 2017 F (RGB) visualization (average saliency map)\n[179] 2019 F (RGB) visualization\n(generated intermediate representation)\n[194] 2020 F (RGB) visualization (saliency maps)\n[196] 2020 F (RGB) weights per AU (contribution of AUs)\n[173] 2019 F (RGB) visualization (feature maps)\n[174] 2021 F (RGB) visualization (integrated gradients)\n[210] 2021 F (RGB) visualization (heatmaps)\n[211] 2020 F (RGB) visualization (attention maps),\nvalues of learned weights\n[212] 2019 F (RGB) visualization (attention maps)\n3.7 Challenges and Future Directions\nThis section discusses the existing challenges in automatic pain assessment and proposes\nfuture research directions to further progress in the field.\n3.7.1 Current Challenges in Automatic Pain Assessment & Future Research Direc-\ntions\nSeveral limitations exist in the current pain databases. Important demographic factors such\nas sex, gender, and age are often missing, and there is an apparent lack of racial diversity\namong subjects. For example, facial structures and emotional expressions vary across Cau-\ncasian, Asian, and African populations [258]. Moreover, social interactions, such as the\npresence of a partner during assessments, could influence pain manifestation and should\nbe included in future datasets [69]. Estimating the location of pain, particularly for infants\nor individuals with communication impairments, is another vital aspect of pain assessment52 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsystems, which current databases largely overlook. Future datasets should incorporate stim-\nuli targeting various body locations. Furthermore, the videos in existing visual databases\noften have low to medium resolution and frame rates, which are inadequate for capturing\nfacial micro-expressions. Audio data is also sparsely represented, though it holds potential\nas a valuable modality. From an audio perspective, integrating natural language processing\n(NLP) methods to extract linguistic features and create multimodal systems is a promising\ndirection, as shown in affective computing research [259]. Finally, specific validation proto-\ncols should be provided with present and future datasets to ensure objective and consistent\ncomparisons across studies.\nFrom an engineering perspective, several issues must be addressed to advance automatic\npain assessment. Developing multimodal approaches is essential for creating robust systems\nwith enhanced capabilities. Not only do multimodal methods demonstrate better perfor-\nmance than unimodal ones, but they are also crucial in real-world scenarios where a specific\nmodality may become unavailable. Additionally, it is essential to exploit each modality\u2019s\ntemporal aspects fully. We encourage using machine learning models or other techniques\nthat can accommodate the dynamic nature of pain. More work is needed to improve the accu-\nracy of multi-level and low-intensity pain estimation. Another area of research involves the\nrelationship between pain and other affective states, such as negative emotions, which often\ncoexist during painful events. Detecting these emotions could improve pain assessment. Ad-\ndressing challenges like occlusions or poor lighting conditions in vision-based systems also\nrequires attention. Researchers should explore these scenarios, even if current databases do\nnot account for them. Real-time application of pain assessment systems is another critical\nfactor, so future studies should measure throughput, such as the number of images processed\nper second during inference. Generalization is another crucial concern for AI systems, and\nevaluating trained models across different pain databases could be valuable. Finally, to facil-\nitate the clinical adoption of AI-based pain assessment systems, the models\u2019 decisions need\ngreater explainability. Developing or adopting methods that improve interpretability will\nenhance their clinical viability.Chapter 4\nDemographic Variables: Their Role and\nImpact\nContents\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.1", "Chapter Overview: Introduction & Related Work": "This chapter includes the findings published in [35, 36]. As discussed in Section 2, research\nhas demonstrated that biological and psychological differences can lead to variations in how\npain is perceived between men and women. Regarding age, it is known that infants who\ncannot express themselves directly and older adults with health issues require specific care\ndue to their unique needs. However, a significant question remains unanswered in pain\nresearch, both from clinical and biological perspectives: Does the sensation of pain change\nas individuals age? Specifically, does a person in pain perceive their situation differently\n5354 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nas they grow older than others in different age groups? Is the sensation of pain evolving\nthroughout life? To the best of our knowledge, this question has not yet been definitively\nexplored thoroughly. This chapter investigates the differences among males and females, as\nwell as age groups, using ECG signals. In addition, it proposes a computational framework\nthat utilizes these two demographic elements to improve pain assessment performance. This\nchapter analyzes ECG signals to explore the differences in pain perception between males\nand females and various age groups. Additionally, it introduces a computational framework\nincorporating these two demographic factors to enhance the accuracy of pain assessments.\nFrom a computational standpoint, the literature concerning the use of demographic fac-\ntors in pain assessment is scarce. The study in [260] utilized a range of biosignals, including\nEDA, respiration rate, diastolic blood pressure, and facial action units, to demonstrate dif-\nferences in pain perception between males and females. Similarly, in research [245], the\nauthors utilized a hybrid CNN-LSTM model that processed ECG and EDA data, highlight-\ning gender-based variations in pain response. Following the publications of our research,\nanother study by Ricken et al. [261] was released, which explored the differences in adap-\ntation and habituation between men and women. This study extracted handcrafted features\nfrom various biosignals (including ECG and EDA) and employed random forest classifiers\nto analyze the data.\n4.2 ECG Analysis with Classical Machine Learning\nWe explore a pain estimation process using ECG signals and examine variations across dif-\nferent demographic groups, focusing on gender and age. Specifically, we analyze how pain\nmanifestation differs between males and females, investigate variations in pain perception\nacross different age groups, and consider the combined effects of age and gender on pain\nperception.\n4.2.1 Methodology\nThis section will describe the electrocardiography processing algorithm and the methods\nused for feature extraction and classification algorithms.\nECG signal Processing and Analysis\nAn ECG signal captures the heart\u2019s electrical activity over time. Typically, a normal ECG\ndisplays a sequence of waves, identified as P, Q, R, S, T, and occasionally U. These waves\nand their intervals provide crucial insights into heart function. The P wave indicates atrial\ndepolarization, the QRS complex signifies ventricular depolarization and contraction, and\nthe T wave corresponds to the repolarization of the ventricles. Each heartbeat is depicted4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 55\nQR\nST\nPQRS  \nComplex\nPR\nInterval\nQT Interval\nFigure 4.1: The PQRST waveform.\nLow Pass Filter ECG  High Pass Filter Differentiation\nAdaptive ThresholdsMoving W indow\nIntegrationSquaringQRS\nComplexBand-Pass Filter\nLow Pass Filter ECG  High Pass Filter\nDifferentiation\nAdaptive\nThresholdsMoving W indow\nIntegrationSquaring\nQRSBand-Pass Filter\nFigure 4.2: The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure.\nthrough the PQRST complex (refer to Figure 1). Accurately detecting the R wave within the\nQRS complex is especially critical as it is the most pronounced peak in the complex. Precise\ndetection of the R wave allows for the calculation of heart rate (HR) and heart rate variability\n(HRV), the latter of which measures the time intervals between successive R waves, known\nas the R-R or Interbeat interval. The Pan-Tompkins algorithm, developed in 1985, is one of\nthe most extensively used real-time QRS detection algorithms [262]. Over the years, both the\noriginal algorithm and its variations have been rigorously tested, consistently proving their\neffectiveness even with noisy and low-quality data [263,264]. The Pan-Tompkins Algorithm\nis frequently cited as a benchmark in the field due to its robust performance, making it a stan-\ndard against which new QRS detection methods are compared [265]. Our research employed\nthe original Pan-Tompkins Algorithm to identify the QRS complex. We integrated the algo-\nrithm in two primary phases: preprocessing and decision-making. The preprocessing stage\nis crucial for conditioning the ECG by eliminating noise and artifacts, smoothing the signal,\nand enhancing the QRS slope. The preprocessing steps of the Pan-Tompkins algorithm are\ndepicted in the flow diagram shown in Figure 4.2.56 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nFeature Extraction\nThe subsequent phase involves extracting specific features based on the inter-beat intervals\n(IBIs). In our study, we calculated several metrics, including the mean of IBIs, the root mean\nsquare of successive differences (RMSSD), the standard deviation of IBIs (SDNN), the slope\nof the linear regression of IBIs, the ratio of SDNN to RMSSD, and the heart rate, as outlined\nbelow:\n1. Mean of IBIs\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (4.1)\nwhere RRrepresents consecutive Rpeaks.\n2. Root mean square of successive differences\nRMSSD\u201cgffe1\nn\u00b41n\u00b41\u00ff\ni\u201c1pRRi`1\u00b4RRiq2 (4.2)\n3. Standard deviation of IBIs\nSDNN\u201cd\n1\nn\u00b41n\u00ff\ni\u201c1pRRi\u00b4\u00b5q2 (4.3)\n4. Slope of the linear regression of IBIs\nATAx\u201cATb, (4.4)\nwhere is calculated using the least-square approximation, where bis the vector of RR\npeak intervals and Ais the corresponding time series.\n5. Ratio of SDNN to RMSSD\nSR\u201cSDNN\nRMSSD(4.5)\n6. Heartbeat rate\nHR\u201c60\u00a8FS\n\u00b5, (4.6)\nwhere FSis the sampling frequency of the ECG recording, typically 512Hz. Figure\n4.3 illustrates the raw ECG signal and the algorithm\u2019s stages.4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 57\n0 500 1000 1500 2000 2500\nTime (ms)Raw Signal\n0 500 1000 1500 2000 2500\nTime (ms)Band Pass Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Derivative Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Squared\n500 1000 1500 2000 2500\nTime (ms)Moving Window Averaged\nSignal\nQRS\nNoise Level\nSignal Level\nAdaptive Threshold\nFigure 4.3: The signal preprocessing using the Pan-Tompkins algorithm.\nClassification Methods\nFor the classification phase, three widely recognized classifiers were utilized: Linear Dis-\ncriminant Analysis (LDA), Support Vector Machine (SVM) with a linear kernel, and SVM\nwith a Radial Basis Function (RBF) kernel.\n1. Linear Discriminant Analysis\nPpX|y\u201ckq\u201cexp\u00b4\n\u00b41\n2pX\u00b4\u00b5kqt\u03a3\u00b41\nkpX\u00b4\u00b5kqt\u00af\np2\u03c0qd{2|\u03a3k|1{2, (4.7)\nwhere Pdenotes the probability density function for the feature set X, conditional on\nthe target class y\u201ck.\n2. SVM with linear kernel\nKpx1, x2q\u201cxT\n1x2, (4.8)\nwhere x1andx2represent feature vectors from two separate classes.58 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n3. SVM with Radial Basis Function (RBF) kernel\nKpx1, x2q\u201cexp\u02dc\n\u00b4||x1\u00b4x2||2\n2\u03c32\u00b8\n, (4.9)\nwhere \u03c3is the parameter defining the width of the RBF kernel.\nDataset Details\nIn this study, we utilized the publicly available \u201cBioVid Heat Pain Database\u201d [109], which\ncontains facial videos and biosignals (ECG, EMG, EDA) from 87participants ( 44males and\n43females, aged 20\u00b465). This dataset is unique because it is the only publicly accessible\nresource that includes the subjects\u2019 age and gender. The data collection involved applying\na heat stimulus to the right arm of each participant using a thermode. Prior to recording,\nthe pain threshold (the temperature at which the participant first perceives heat as pain) and\npain tolerance (the temperature at which the pain becomes intolerable) were established for\neach participant. The study defined specific thresholds as the temperatures for the lowest\nand highest pain levels. Also, it included two intermediate levels, resulting in five pain\nconditions: No pain (NP), mild pain (P 1), moderate pain (P 2), severe pain (P 3), and very\nsevere pain (P 4). Each participant was exposed to 20stimulations for each intensity level,\ngenerating 100samples across the four modalities.\n4.2.2 Experiments\nIn the following experiments, we specifically used Part A of the BioVid , which includes\npre-processed ECG samples filtered through a Butterworth band-pass filter, totaling 8700\nsamples ( 87\u02c6100\u201c8700 ). All experiments were conducted in triplicate under identical\nconditions, using a distinct classifier for each iteration to compare their effectiveness. This\nwas based on the leave-one-subject-out (LOSO) cross-validation method, utilizing all avail-\nable subjects and ECG samples. The performance of each classifier was evaluated based on\naccuracy.\nUsing the previously mentioned classification algorithms, we conducted experiments to\nrecognize pain and its relationship with demographic factors. The classification tasks were\nstructured around the pain conditions in multi-class and binary classification formats. Specif-\nically, five distinct experiments were executed: (i)multi-class pain classification, (ii)NP vs.\nP1,(iii)NP vs. P 2,(iv)NP vs. P 3,(v)NP vs. P 4. In experiment (i), the goal was to cate-\ngorize an ECG signal into one of the five pain conditions, while experiments (ii)-(v) aimed\nto classify signals into one of two pain conditions, either no pain or the specified pain level.\nFurthermore, considering the gender and age of the subjects, we developed four different4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 59\nTable 4.1: Results for the Basic Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)AllMC 23.72 23.79 22.77\nNP vs P 1 50.97 52.38 49.97\nNP vs P 2 52.55 52.78 52.70\nNP vs P 3 55.20 55.37 53.87\nNP vs P 4 58.62 58.39 57.41\nMC: multi-classification NP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain\nP4: very severe pain LDA: Linear Discriminant Analysis LN:Linear RBF: Radial Basis\nFunction\nexperimental schemes: (i)theBasic Scheme , utilizing the entire dataset, (ii)theGender\nScheme , where data were segmented by the gender of the subjects into males and females,\n(iii) the Age Scheme , which grouped subjects into three age categories: \u201820-35\u2019 ,\u201836-50\u2019 ,\n\u201851-65\u2019 , and (iv) the Gender-Age Scheme , which combined both demographic factors, result-\ning in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\n\u2018Males 51-65\u2019 ,\u2018Females 51-65\u2019 . The most successful classification results are displayed\nin Figures 4.4-4.5 for each task and classification method, while Tables 4.1-4.5 detail the\noutcomes of each individual experiment.\n4.2.3 Results\nTable 4.1 shows the results from the entire dataset, where the multi-class pain classification\nachieved a 23.79% accuracy, and performance scores generally increased with pain intensity,\npeaking at 58.62% for NP vs. P 4. This progression highlights the difficulty in detecting\nlower levels of pain severity. Regarding the classification algorithms, SVM (linear) was\nmore effective, except for the highest pain level task, where SVM (RBF) was less successful.\nIn the Gender Scheme (see Table 4.2), notable differences were observed between males\nand females. Overall, females showed a 1.12% higher accuracy variation than males, with\nfemales achieving 60.69% in NP vs. P 4over males\u2019 56.07%. This 4.62% increase suggests\nthat females are more sensitive to higher levels of pain than males. Interestingly, in NP vs. P 1\nand NP vs. P 2, males outperformed females by 1.16% and1.78%, respectively. Consistent\nwith the first scheme, SVM (linear) yielded better results in most tasks. Figure 4.4 illustrates\nthe gender differences in classification accuracy.\nIn the Age-Scheme (refer to Table 4.3), the \u201820-35\u2019 age group achieved 25.06% accuracy\nin multi-level classification, compared to 23.27% and22.35% for the \u201836-50\u2019 and\u201851-65\u2019\ngroups, respectively, indicating that age significantly affects pain perception. The nearly 9%60 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.2: Results for the Gender Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)MalesMC 22.13 22.25 20.70\nNP vs P 1 51.53 52.61 47.72\nNP vs P 2 53.12 53.69 52.15\nNP vs P 3 54.94 54.71 51.36\nNP vs P 4 55.28 56.07 51.36FemalesMC 25.11 24.41 23.41\nNP vs P 1 50.23 51.45 49.06\nNP vs P 2 51.62 51.86 51.91\nNP vs P 3 55.98 55.87 55.29\nNP vs P 4 60.17 60.69 59.82\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.4: Results for the Gender Scheme .\ndifference in the NP vs P 4task between the youngest and oldest groups was particularly no-\ntable. Similar to the gender-based results, minor differences in low pain intensities among\nthe age groups became more pronounced as pain intensity increased. Specifically, the vari-\nance ( \u03c32) between the groups in NP vs. P 1was1.38%. At the same time, in the other tasks,\nit increased to 2.44%,6.35%, and 20.42%, respectively, showing that high pain intensities\nare necessary to discern perceptual differences among age groups. Regarding classification\naccuracy, the \u201820-35\u2019 group showed the highest sensitivity, followed by \u201836-50\u2019 and\u201851-\n65\u2019. Regarding classification methods, the SVM (RBF) performed best in the \u201851-65\u2019 group\nacross almost all tasks, while it underperformed in the \u201820-35\u2019 group, suggesting it is better\nsuited for more challenging, separable classes. Figure 4.5 displays the results from the age\nscheme.\nIn the final analysis, we examined the subjects more closely to gain deeper insights into\nthe relationship between pain and the demographic factors of gender and age. As shown4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 61\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.5: Results for the Age Scheme .\nin Tables 4.4-4.5, the \u2018Females 20-35\u2019 group achieved the highest accuracy in multi-class\npain classification at 24.80%, while \u2018Females 51-65\u2019 led in NP vs. P 1with 55.38%, again\nindicating higher pain sensitivity in females. Moreover, \u2018Females 51-65\u2019 followed by \u2018Males\n51-65\u2019 topped the performance in NP vs. P 2, and in NP vs. P 3,\u2018Females 36-50\u2019 surpassed\nthe next best group, \u2018Males 20-35\u2019 , by3.5%. In the final NP vs. P 4task, \u2018Females 20-\n35\u2019excelled with 67% accuracy, whereas \u2018Males 51-65\u2019 had the lowest at 54.50%, marking\nthem as the most and least pain-sensitive groups, respectively. It is noted that sometimes\nclassification accuracy decreases despite increased pain levels ( e.g.,\u2018Females 36-50\u2019 ). This\nmight be attributed to the subjects becoming accustomed to the stimulus over time during\nthe biosignal recording.\nFigure 4.6 illustrates the classification performance of the six groups in the Gender-Age\nScheme . Additionally, Table 7 compares our results with other studies that used ECG signals\nfrom the BioVid database and followed the same evaluation protocol, ensuring an objective\ncomparison. Our study achieved the best classification performance in both the multi-class\nsetting and the NP vs. P 1and NP vs. P 2tasks, with acceptable results in the remaining binary\nclassification tasks.\n4.2.4 Discussion\nWe analyzed ECG biosignals using the Pan-Tompkins algorithm to detect QRS complexes\nand extracted features about inter-beat intervals. We also evaluated three machine learning\ntechniques, assessing their performance in multi-class and binary pain classification across\nvarious pain intensities. We also examined the influence of gender and age on pain percep-\ntion, discovering significant differences: males generally showed lower sensitivity to high\npain levels. Regarding the age factor, significant variations suggest that pain sensitivity tends\nto diminish with age, potentially increasing the risk of further injury. In certain demographic\ngroups, the difference in pain perception exceeded 12%, underscoring the variability of pain\nsensation among individuals.62 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.3: Results for the Age Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)20-35MC 25.06 24.73 21.96\nNP vs P 1 52.83 52.83 49.90\nNP vs P 2 54.33 53.75 52.75\nNP vs P 3 55.58 56.16 54.66\nNP vs P 4 63.83 63.41 60.7536-50MC 23.27 22.06 23.03\nNP vs P 1 50.34 48.36 50.68\nNP vs P 2 49.13 51.20 50.17\nNP vs P 3 58.10 58.70 58.27\nNP vs P 4 58.10 57.75 55.9451-65MC 21.89 22.07 22.35\nNP vs P 1 52.23 51.87 52.58\nNP vs P 2 52.14 51.69 52.76\nNP vs P 3 53.66 53.39 54.10\nNP vs P 4 54.46 54.19 54.91\nTable 4.4: Results for the Gender-Age Scheme (Males) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Males 20-35MC 23.13 23.20 18.73\nNP vs P 1 52.50 52.83 45.83\nNP vs P 2 54.00 53.50 53.16\nNP vs P 3 56.33 56.50 54.83\nNP vs P 4 60.00 59.00 53.66Males 36-50MC 23.21 22.21 20.92\nNP vs P 1 50.53 50.53 46.42\nNP vs P 2 50.00 51.78 47.50\nNP vs P 3 54.64 56.25 47.32\nNP vs P 4 55.53 56.25 51.96Males 51-65MC 20.06 21.60 19.60\nNP vs P 1 52.66 51.66 50.66\nNP vs P 2 54.00 54.66 51.50\nNP vs P 3 53.00 54.66 51.50\nNP vs P 4 53.33 54.50 49.834.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 63\nTable 4.5: Results for the Gender-Age Scheme (Females) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Females 20-35MC 24.73 24.80 23.26\nNP vs P 1 49.83 51.50 52.00\nNP vs P 2 54.50 53.66 46.50\nNP vs P 3 53.50 52.83 49.00\nNP vs P 4 65.83 67.00 62.16Females 36-50MC 23.06 22.73 21.93\nNP vs P 1 48.16 49.33 48.33\nNP vs P 2 48.66 49.83 47.83\nNP vs P 3 57.50 60.00 55.00\nNP vs P 4 59.00 58.83 56.16Females 51-65MC 21.23 21.84 23.92\nNP vs P 1 48.84 49.80 55.38\nNP vs P 2 51.15 48.65 55.96\nNP vs P 3 53.07 53.07 50.96\nNP vs P 4 52.69 55.00 56.34\nTable 4.6:\nComparison of studies utilizing BioVid , ECG signals\nand LOSO validation (1).\nMethod Task Results\nMartinez and Picard [266] NP vs P 4 57.69\nWerner et al. [267]NP vs P 1 48.70\nNP vs P 2 51.60\nNP vs P 3 56.50\nNP vs P 4 62.00\nThiam et al. [235]MC 23.23\nNP vs P 1 49.71\nNP vs P 2 50.72\nNP vs P 3 52.87\nNP vs P 4 57.04\nOursMC 23.79\nNP vs P 1 52.38\nNP vs P 2 52.78\nNP vs P 3 55.37\nNP vs P 4 58.6264 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65MC232523232224BL vs P1535251495355BL vs P2545552505556BL vs P3575456605553BL vs P4606756595456018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65\n1\nFigure 4.6: Results for the Gender-Age Scheme .\n4.3 ECG Analysis with Multitask Neural Networks\nIn this section, we build on previous analysis 4.2 that explored variations in pain manifesta-\ntion across different demographic groups using ECG signals. It expands this investigation\nby implementing neural networks as the primary machine learning model and introduces a\nnovel multi-task learning (MTL) neural network. This network leverages demographic infor-\nmation to estimate age and gender in addition to pain levels, aiming to enhance the automatic\npain estimation system.\n4.3.1 Methodology\nThe following method we developed is a neural network-based approach. The feature extrac-\ntion process remains the same as previously described in 4.2.1, utilizing the Pan-Tompkins\nalgorithm for ECG signal processing.\nNeural Network\nThe proposed neural network was designed and trained using two distinct approaches: single-\ntask learning (STL) and multi-task learning (MTL). In the multi-task learning framework, the\nnetwork is simultaneous training for pain estimation and predicting age and/or gender.\nSingle-Task Neural Network: The proposed neural network comprises two components:\nthe encoder, which maps the original feature vectors into a higher dimensional space, and\nthe task-specific classifier. In our design, both the encoder and the classifier utilize fully-4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 65\nTable 4.7: Hyper-parameters used in our approach.\nEpochs Optimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsLabel\nsmoothEMA\n300 AdamW 1e-3 cosine 0.1 50 0.1 \u2713\nconnected (FC) layers, which are defined as follows:\nzipsq\u201cbi`nin\u00ff\nj\u201c1Wijsj for i\u201c1, .., n out, (4.10)\nwhere zirepresents the result of linearly combining the incoming inputs sj, with each input\nbeing weighted by Wijand adjusted by a bias bi. Each layer in the encoder is followed by a\nnonlinear activation function, specifically the rectified linear unit (ReLU), which is defined\nas:\n\u03c3pzq\u201c#\n1, z\u011b0\n0, z\u01030(4.11)\nThe classifier\u2019s layers are connected without nonlinearity, the encoder comprises four fully\nconnected (FC) layers with 256,512,1024 , and 1024 neurons respectively. The classifier\nincludes 2layers with 1024 andnneurons, where nrepresents the number of distinct pain\nclasses being classified. The hyperparameters of the network are detailed in Table 4.7.\nMulti-task neural network: This proposed machine learning method is based on the princi-\nple of sharing representations across related tasks, which helps the model better generalize\nto the primary task of pain estimation in this case. We kept the same encoder and pain\nclassifier in this configuration but introduced two additional auxiliary networks for age and\ngender estimation. The architecture of the proposed multi-task learning (MTL) neural net-\nwork is illustrated in Fig. 4.7. The objective of this network is to simultaneously minimize\nthree different losses. We adopt and expand upon the framework suggested by [268] for the\nmulti-task learning loss, where learned weights are applied to each loss function based on\nthe homoscedastic uncertainty of each task:\nLtotal\u201crew1LPain`w1sc1`rew2LAge`w2sc2`rew3LGender`w3sc3. (4.12)\nHere, Lrepresents the corresponding loss, wdenotes the weights, and care the coefficients\nthat modulate the losses LAgeandLGender to prioritize learning in the pain estimation task.\nIt should be noted that all tasks are treated as classification problems, utilizing cross-entropy66 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nL1\nL2\nL3 L4Feature\nvector\nPainAge\nGender6 x 1256 x 1512 x 11024 x 1 1024 x 1\n1024 x 1n x 1512 x 1256 x 1\nL1L2L1L2L336 x 1\n512 x 1256 x 1\nL1L2L32 x 1\nEncoderMTL LossPain\nEstimation\nFigure 4.7: The proposed MTL network: The sizes of the extracted vectors for the network are\nas follows: for the Pain classifier, n\u02c61, where nis the number of pain estimation\ntasks ( e.g.,2for binary classification, 5for multi-class classification); for the Age\nclassifier, 36\u02c61, where 36represents the possible age values of the subjects; for\nthe Gender classifier, 2\u02c61, corresponding to the two possible gender categories\n(i.e., males and females).\nloss with label smoothing:\nLD\u201c\u00b4\u00ff\n\u03b4PDnout\u00ff\ni\u201c1ppi|x\u03b4qlogrqpi|x\u03b4qs. (4.13)\nHere, Ddenotes the pain database, ppi|x\u03b4q \u201c1\u00b4\u03f5represents the probability of the true\nclass igiven the input x\u03b4, and ppi\u2030i\u03b4|x\u03b4q \u201c\u03f5{pnout\u00b41qis the probability distribution\nacross the other classes. This formulation spreads a small portion \u03f5of the probability across\nclasses other than the true class to implement label smoothing. Furthermore, qpi|x\u03b4qis the\nprobability distribution over the classes ias predicted by the network\u2019s output.\n4.3.2 Experiments\nSimilar to 4.2.2, we utilized the BioVid database, specifically focusing on its ECG signals.\nEmploying a single-task neural network (ST-NN), we conducted an initial series of experi-\nments to assess the impact of demographic factors. Building on the concept we proposed\nin the previous section, we devised five experimental schemes: (i)theBasic Scheme , which\nincluded all subjects from the database; (ii)theGender Scheme , which segregated subjects4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 67\ninto male and female groups; (iii)theAge Scheme , which categorized subjects into three age\ngroups\u2014 \u201820-35\u2019 ,\u201836-50\u2019 , and \u201851-65\u2019 ; and (iv)the\u2018Gender-Age Scheme\u2019 , which combined\nboth demographic factors, resulting in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\n\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\u2018Males 51-65\u2019 , and \u2018Females 51-65\u2019 . All experiments were\nconducted in both binary and multi-class classification formats. Specifically, the binary clas-\nsification tasks were (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3, and (4) NP vs. P 4, and the\nmulti-class task utilized all available pain classifications from the database.\n4.3.3 Results\nDemographic Groups\nTable 4.8 presents the classification results of the Basic Scheme , which utilized all subjects\nin the database. For the multi-class pain classification, we achieved an accuracy of 29.43%,\nwith NP vs. P 1scoring 61.15% and NP vs. P 4reaching 68.82%. These results indicate that\nas pain intensity increases, so performs, highlighting the difficulty in recognizing less severe\npain. According to the Gender Scheme (refer to Table 4.9), notable differences emerge\nbetween males and females, particularly at higher pain intensities. Specifically, in NP vs.\nP4, females achieved an accuracy of 69.48% compared to 66.48% for males, with an overall\nvariance of 1.63% between genders, suggesting that females exhibit higher pain sensitivity.\nFigure 4.8a illustrates these gender-based classification disparities. In the Age Scheme (see\nTable 4.10), the \u201820-35\u2019 age group outperformed the \u201836-50\u2019 and\u201851-65\u2019 groups in NP vs. P 4,\nwith accuracies of 72.58%,66.29%, and 64.91%, respectively. While the differences are less\npronounced at lower pain intensities, this scheme still shows that age significantly impacts\npain perception, particularly among the older population. Figure 4.8b shows the results from\nthe age scheme.\nIn the final scheme, by dividing subjects into more specific groups, we can analyze them\nmore precisely and gain better insights into the relationship between gender, age, and pain\nperception. Table 4.11 reveals that in the NP vs. P 4task, the group \u2018Females 20-35\u2019 reached\nthe highest accuracy of 71.67%, significantly outperforming the \u2018Males 51-65\u2019 group, which\nscored the lowest at 60.67%, marking them as the least sensitive group. This pattern is\nconsistent across the multi-class classification and other pain tasks, with \u2018Females 20-35\u2019\nand\u2018Males 51-65\u2019 exhibiting the highest and lowest accuracies, respectively. This supports\nthat females generally experience more pronounced pain responses, while older males have\na reduced pain sensation. It is noted that in some instances, such as with \u2018Males 20-35\u2019\nand\u2018Males 36-50\u2019 , higher pain levels do not necessarily correlate with higher classification\naccuracy, a phenomenon also noted in our previous experiments, in Section 4.2.3. A possible\nexplanation could be the subjects\u2019 habituation to pain stimuli, especially at lower intensities.\nFigure 4.8c visualizes the performance outcomes of the Gender-Age Scheme .68 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.8: Results for the Basic Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN 61.15 62.87 65.14 68.82 29.43\nST-NN: single-task neural network NP: no pain P1: mild pain P2: moderate pain P3: severe pain P4: very severe\npain MC: multi-classification\nTable 4.9: Results for the Gender Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales ST-NN 60.40 63.24 63.18 66.48 28.61\nFemales ST-NN 60.87 62.15 66.98 69.48 30.59\nTable 4.10: Results for the Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n20-35 ST-NN 61.58 64.08 66.08 72.58 31.07\n36-50 ST-NN 60.52 61.38 64.05 66.29 29.59\n51-65 ST-NN 61.70 60.80 62.50 64.91 27.82\nTable 4.11: Results for the Gender-Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales 20-35 ST-NN 62.83 62.33 65.50 71.33 29.73\nMales 36-50 ST-NN 61.79 60.00 59.64 64.11 27.14\nMales 51-65 ST-NN 59.50 58.67 57.33 60.67 26.07\nFemales 20-35 ST-NN 63.17 63.17 66.83 71.67 31.53\nFemales 36-50 ST-NN 59.50 61.00 65.83 67.00 29.13\nFemales 51-65 ST-NN 60.96 60.96 59.23 63.27 27.69\nAugmentation of Feature Vectors\nBuilding on the findings from the previous experiments about the impact of demographic\nfactors on pain perception, we explored the practical use of subjects\u2019 demographic data.\nExperiments were conducted using the Single-Task Neural Network (ST-NN) and feature4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 69\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(a) Gender\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(b) Age\nAge\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-Age\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\nBL vs P1 63 63 62 60 60 61\nBL vs P2 62 63 60 61 59 61\nBL vs P3 66 67 60 66 57 59\nBL vs P4 71 72 64 67 61 63\nMC 30 32 27 29 26 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales 20-35\n Females 20-35\n Males 36-50\n Females 36-50\n Males 51-65\n Females 51-65\nGender\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales\n Females\nAge-1\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-1\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales\n Females\n020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\n20-35 36-50 51-65018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales Females020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\n1\n(c) Gender-Age\nFigure 4.8: Results for the proposed Schemes.\nvectors enhanced with demographic attributes. Initially, the feature vectors, which originally\nconsisted of six features (see 4.3.1), were augmented by adding either one additional fea-\nture ( i.e., the subject\u2019s gender or age) or two additional features (both the subject\u2019s gender\nand age). We conducted the same pain estimation tasks using this enhanced set of features.\nAs shown in Table 4.12, the results demonstrate improved performance with the augmented\nfeature vectors. Specifically, the most effective augmentation involved combining gender70 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.12: Comparison of results adopting the feature augmentation approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAux: Auxiliary information -: original feature vectors F(G): feature vectors with the additional feature of gender F(A): feature\nvectors with the additional feature of age F(GA): feature vectors with the additional features of gender and age\nand age features, which increased the average pain estimation performance by 0.55%. Us-\ning these demographic features individually also improved classification accuracy, albeit\npartially.\nMulti-Task Neural Network\nThe final experiments utilized a multi-task learning framework with the proposed Multi-Task\nNeural Network (MT-NN) outlined in 4.3.1. The classification results for MT-NN, incorpo-\nrating additional tasks of (1) gender estimation, (2) age estimation, and (3) simultaneous\ngender and age estimation, are detailed in Table 4.13. For comparison, the results from ear-\nlier experiments using the Single-Task Neural Network (ST-NN) method are also included\nin Table 4.13. We noted that the task of gender estimation alone performed less effectively\nthan the other tasks. In contrast, the combined gender and age estimation delivered the high-\nest performance across four tasks. Specifically, in the multi-class classification, it achieved\n30.24%, and in NP vs. P 1, it reached 62.8%, marking the best results of any method pre-\nsented in this study. In NP vs. P 3and NP vs. P 4, the combined tasks outperformed the\nindividual gender and age tasks but were slightly less effective than the ST-NN approaches\nusing augmented features. Interestingly, in NP vs P 2, the age estimation task alone excelled,\nachieving 63.97%, the highest result recorded in this study.\nComparing the overall performances of MT-NN with the ST-NN approaches (using both\noriginal and augmented feature vectors), there is a noticeable improvement of 0.71% and\n0.39%, respectively, in average pain estimation accuracy across all tasks. Figure 4.9 visually\ncompare each neural network approach used in this study, encompassing multi-class and\nbinary classification tasks.4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 71\nTable 4.13: Comparison of results adopting the MT-NN approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAll MT-NN T(G) 61.72 63.39 65.95 68.99 30.00\nAll MT-NN T(A) 62.72 63.97 65.40 69.28 29.79\nAll MT-NN T(GA) 62.82 63.68 66.12 69.40 30.24\nT(G): MT-NN with the additional task of gender estimation T(A): MT-NN with the additional task of age estimation T(GA):\nMT-NN with the additional task of gender and age estimation\nComparison with Existing Approaches\nIn this section, we benchmark the results achieved using the Multi-Task Neural Network\n(MT-NN), which incorporated additional tasks of gender and age estimation against relevant\nstudies. These comparative studies also utilized electrocardiography signals from Part A of\ntheBioVid database with all 87participants. To ensure a fair comparison, they followed the\nsame evaluation protocol, specifically the leave-one-subject-out (LOSO) cross-validation.\nThe comparative results are detailed in Table 4.14 and include research that employed hand-\ncrafted features with traditional machine learning algorithms [35] [267], end-to-end deep\nlearning models [269] [235], and finally, hybrid approaches combine hand-crafted features\nwith deep learning classifiers [266]. Our approach, which leverages hand-crafted engineered\nECG features and a high-dimensional mapping from the encoder in combination with multi-\ntask learning neural networks, demonstrated superior performance across all pain estimation\ntasks, whether in binary or multi-class classification settings.\n4.3.4 Discussion\nWe explored multi-task learning neural networks for automatic pain estimation from electro-\ncardiography signals. By implementing the Pan-Tompkins algorithm to identify QRS com-\nplexes, we extracted features associated with inter-beat intervals (IBIs). Numerous experi-\nments were conducted to explore how gender and age influence pain perception, highlighting\ntheir significant impact. Additionally, we introduced two approaches to enhance pain esti-\nmation results by leveraging demographic information. Firstly, we augmented the original\nfeature vectors by incorporating the subjects\u2019 demographic data, improving classification\naccuracy. Secondly, we employed a multi-task learning neural network that combined the72 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(a) Binary classification\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(b) Multi-class classification\nFigure 4.9: Comparison of performances utilizing various neural networks approaches.\nTable 4.14: Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2).\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nGkikas et al. [35]:52.38 52.78 55.37 58.62 23.79\nHuang et al. [269]\u2039d- - - 65.00 28.50\nMartinez and Picard [270]\u00b8- - - 57.69 -\nThiam et al. [235]\u203949.71 50.72 52.87 57.04 23.23\nWernel et al. [267]:48.70 51.60 56.50 62.00 -\nThis study\u00b862.82 63.68 66.12 69.40 30.24\n::hand-crafted features and classic machine learning \u2039: end-to-end deep learning \u00b8: hand-crafted features with deep\nlearning classification algorithms d: pseudo heart rate gain extracted from visual modality\ntasks of pain, gender, and age estimation. This approach yielded superior results compared\nto methods previously discussed in this chapter and other related research. These findings\nindicate that domain-specific features can achieve excellent outcomes when combined with\nwell-designed deep-learning architectures and demographic factors.4.4. SUMMARY 73\n4.4 Summary\nIn this chapter, we examine the impact of age and gender on pain perception using ECG\nsignals to extract relevant features. Our study involved a series of experiments where subjects\nwere categorized into different groups based on gender (males and females) and age (20-35,\n36-50, and 51-65 years). Additionally, we created combined groups that segregated age\ngroups within each gender. Our findings from both approaches provided strong evidence\nof significant differences in pain perception among these groups. Notably, we observed\na12.5%disparity in pain sensitivity between young females and older males. Generally,\nour results confirm that females exhibit higher pain sensitivity than males, aligning with\nfindings from other studies in pain research. A critical discovery from our study is that pain\nsensitivity appears to decrease with age, which may increase the risk of unnoticed injuries.\nWe presented two methods of incorporating demographic information into our models from a\ncomputational perspective. First, we augmented the feature vectors derived from ECGs with\ndemographic data. Second, we utilized a multi-task neural network approach to estimate\npain, gender, and age simultaneously. Both methods demonstrated improved performance\ncompared to the standard approach, indicating that integrating demographic information can\nenhance the accuracy of automatic pain assessment systems.\nWe recommend that clinical pain assessment tools be designed for specific demographic\ngroups to account for the distinct ways pain manifests across different populations. Ad-\nditionally, we emphasize to researchers developing new pain databases the importance of\nincluding demographic factors and information on the social context and psychological con-\nditions of subjects to enhance the quality and applicability of the data collected. Our study\nfocused on analyzing pain sensation through biosignals, specifically ECGs. We propose that\nfurther research should explore pain expressivity through visual mediums such as video. As\npreviously discussed in Section 2, the expression of pain is a crucial and complex issue. Peo-\nple vary in expressiveness; for various reasons, they might exaggerate or even feign pain,\nmaking accurate assessment challenging.74Chapter 5\nOptimization: Balancing Efficiency and Per-\nformance\nContents\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the studies published in [37, 38]. As outlined in\n3.6.3, research in automatic pain assessment has rarely considered real-world situations. For\nexample, [231] implemented their study in an emergency triage setting, while [234] tested\ntheir approach within IoT devices. Additionally, we highlighted that the scarcity of stud-\nies exploring pain estimation in real-world settings or unconventional contexts suggests that\ncurrent methodologies might not be entirely suitable for practical environments like clinics\nor hospitals due to issues with generalization or operational factors such as efficiency and\ninference time. For these reasons, this chapter\u2019s objective is to explore approaches that (i)\n7576 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nutilize modalities readily available and applicable in the market and (ii)examine the impact\nof model size and computational cost on performance. In this context, our methodologies\nand experiments exclusively utilize RGB videos, a universally available modality, particu-\nlarly on mobile devices. Additionally, we incorporate heart rate data, which is commonly\naccessible from various types of wearable technology. It is important to note that although\nwe employ established pain datasets in our experiments, the videos and heart rate data ex-\ntracted from ECGs are akin to those that could be obtained from smartphones and wearables,\nserving as a proof of concept for application in real-world environments. Furthermore, with\nrespect to efficiency and the speed of inference, our goal is to develop the most compact pain\nassessment frameworks possible, ensuring they maintain adequate performance levels.\nNumerous studies highlight the capabilities of automated systems that utilize behavioral\nor physiological modalities for pain assessment [271]. Sario et al. [34] demonstrate the fea-\nsibility of accurately detecting and quantifying pain through facial expressions, establishing\ntheir value in clinical settings. The use of multimodal sensing appears especially promising,\noffering increased accuracy in pain monitoring systems [22]. An important aspect of pain\nmonitoring involves wearable devices that record biopotentials to estimate pain levels. Few\nstudies have investigated the use of mainstream, wearable technology for this purpose, possi-\nbly due to a research preference for more costly, highly precise medical equipment. Leroux\net al. [32] state, \u201cThe challenge is not whether wearable devices will provide useful clinical\ninformation but rather when we will start to use them in practice to enhance the field of pain. \u201d\nAdditionally, Claret et al. [272] explore the potential of using cardiac signals from wearable\nsensors for automatic emotion recognition, confirming the effectiveness of such methods.\nIn this chapter, our deep learning approaches are founded on transformer-based archi-\ntectures. Convolutional Neural Networks (CNNs) have been the cornerstone of mainstream\nneural architectures in computer vision (CV), especially in the field of automatic pain as-\nsessment using images and videos, as we discussed in Section 3. Inspired by the success\nof transformer architecture in natural language processing (NLP), where the self-attention\nmechanism is a fundamental element [273], researchers have developed similar models for\nvisual tasks. The introduction of Vision Transformers (ViT) [274] has established a new\nparadigm in the computer vision domain. This has led to a plethora of new approaches based\non ViT, such as the Transformer in Transformer (TNT) [275], which enhances local feature\nrepresentation by subdividing image patches into smaller sub-patches. While transformer-\nbased models have shown impressive results and offer great flexibility, they tend to scale\npoorly with input size and incur higher computational costs due to the self-attention layers\nthat compute interactions between all input pairs. Efforts to mitigate these challenges in-\nclude replacing self-attention with cross-attention [276] or combining both techniques [277]\nto improve the efficiency and reduce the complexity of these architectures.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 77\nFigure 5.1: The application of face alignment illustrates landmarks in 2D (left) and 3D (right) space.\n5.2 Video Analysis with Vision Transformers\nWe introduce a framework incorporating a vision transformer as a module extracting spatial\nfeatures for individual video frames, combined with a transformer-based model equipped\nwith cross and self-attention blocks, extracting temporal features from the video feature\nsequences. This configuration enables the effective utilization of the temporal dimensions of\nvideo data to deliver more accurate and reliable estimation of the continuous nature of pain.\n5.2.1 Methodology\nThis section outlines the preprocessing methods employed, the design of our framework, the\nimplementation details concerning the training procedure, and the database used.\nPre-processing\nBefore processing videos for pain estimation, applying face detection and alignment was\ncrucial to enhance performance and computational efficiency. We utilized the well-known\nface detector MTCNN [278] in combination with the Face Alignment Network (FAN) [279],\nwhich leverages 3D landmarks. This 3D approach is critical for addressing our specific chal-\nlenges, as head movements tend to increase, particularly during instances of high-intensity\npain, which can lead to inaccurate alignments with 2D methods. Additionally, it should be\nnoted that all experiments were carried out using video frames with a resolution of 224\u02c6224\npixels. Figure 5.1 illustrates the facial alignment process applied to a video frame.\nTransformer-based Framework\nOur framework is composed of two primary components: the \u201cspatial feature extraction\nmodule\u201d , specifically a TNT (Transformer in Transformer) model, and the \u201ctemporal fea-78 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nture extraction module\u201d , which is a transformer with both cross and self-attention blocks.\nThis framework, illustrated in Figure 5.2, includes approximately 24million parameters and\nperforms operations at 4.2GFLOPS.\nSpatial feature extraction module :Each frame is initially divided into npatches represented\nasFk\u201crF1\nk, F2\nk, . . . , Fn\nksPRn\u02c6p\u02c6p\u02c63, where p\u02c6pdenotes the resolution of each patch ( i.e.,\n16\u02c616) and 3represents the number of color channels. These patches are then subdivided\nintomsub-patches to facilitate the model\u2019s learning of global and local features. Each frame\nis thus transformed into a sequence of patches and sub-patches:\nFk\u00d1rFk,n,1, Fk,n,2, . . . , F k,n,ms, (5.1)\nwhere Fk,n,mPRs\u02c6s\u02c63is the m-th sub-patch of the n-th patch of the k-th frame, with each\nsub-patch having a resolution of s\u02c6s(i.e.,4\u02c64). Following this, the patches and sub-\npatches undergo a linear projection and are transformed into embeddings ZandY. Position\nembeddings are then added to retain spatial information:\nZ0\u00d0Z0`Epatch, (5.2)\nwhere Epatchare the position encodings for the patches. Correspondingly, for each sub-patch\nwithin a patch, a position encoding is also added:\nY0\ni\u00d0Y0\ni`Esub-patch , (5.3)\nwhere Esub-patch are the sub-patch position encodings and i\u201c1,2, . . . , m denotes the index of\na sub-patch. These sub-patches are then processed through an \u201cInner Transformer Encoder\u201d ,\nwhich consists of two multi-head self-attention blocks, crucial for dot product attention. The\nattention mechanism is defined as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n, (5.4)\nwhere QPRM\u02c6D, KPRM\u02c6C,andVPRM\u02c6C(Mis the input dimension, CandDare\nchannel dimensions) are projections of the corresponding input and represent the Query, Key,\nand Value matrices. They defined as Q\u201cXW Q,K\u201cXW K, and V\u201cXW V, where W\nare the learnable weight matrices and Xis the input. The output embedding from the \u201cInner\nTransformer Encoder\u201d is then added to the patch embedding and forwarded to the \u201cOuter\nTransformer Encoder\u201d . This encoder comprises three multi-head self-attention blocks, and\nits output is a feature vector d\u201c192. The \u201cspatial feature extraction module\u201d as a whole\nencompasses a depth of 12blocks.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 79\nTable 5.1: Training details for the automatic pain assessment.\nEpochs Optimizer Learning\nRateLR decay Weight\ndecayWarmup\nepochs\n200 AdamW 1e-4 cosine 0.1 5\nLabel\nSmoothingDropPath Attention\nDropOutLoss\nfunctionAugmentation methods\n0.1 0.1 0.1 Cross\nEntropyAugMix [281] &\nTrivialAugment [282]\nDropPath applied to the \u201cspatial feature extraction module\u201d , Attention DropOut applied to the \u201ctemporal\nfeature extraction module\u201d\nTemporal feature extraction module :The extracted embeddings of each input video frame\nare concatenated into a unified vector D, representing the entire video as V\u00f1D\u201c\npd1\"d2\", . . . , dkq. This vector is then processed through the temporal module, a transformer\narchitecture consisting of 1cross-attention and 2self-attention mechanisms, each followed\nby a fully connected neural network (FCN). The introduction of cross-attention, which em-\nploys asymmetry in the attention mechanism, helps reduce computational complexity and\nincrease the model\u2019s efficiency. Specifically, rather than projecting the input with dimen-\nsions M\u02c6D, theQin cross-attention is a learned matrix with dimensions N\u02c6D, where\nN\u0103M. This module\u2019s self-attention components function as detailed in Equation 7.3, with\nthe cross and self-attention units comprising 1and8heads, respectively. In addition, we\nincorporate Fourier feature position encoding [277].\nTraining Details: Before starting the automatic pain estimation training process, we pre-\ntrained the \u201cspatial feature extraction module\u201d using the VGGFace2 dataset [280], incorpo-\nrating over three million facial images from more than nine thousand individuals. Table 7.3\ndetails the hyperparameters of our method and the applied augmentation techniques.\nDatabase Details: For this approach, we used the publicly available BioVid dataset [109],\nas described in the previous chapters.\n5.2.2 Experiments\nIn this section, we detail the experiments conducted for pain estimation. Our experiments\nwere carried out in both binary and multi-level classification formats. Specifically, we con-\nducted binary classification tasks: (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3,(iv)NP vs. P 4,\nand(v)a multi-level pain classification utilizing all available pain classes from the database.\nWe employed the leave-one-subject-out (LOSO) cross-validation method as our evaluation80 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nprotocol. Additionally, the classification metrics used in this study include micro-average ac-\ncuracy, macro-average precision, macro-average recall (sensitivity), and macro-average F1\nscore.\n5.2.3 Results\nPain Estimation\nIn evaluating pain estimation tasks, we noted the following results: For the NP vs. P 1task,\naccuracy reached 65.95%, with precision almost identical at 65.90%. The F1 score was\nFigure 5.2: An overview of our proposed transformer-based framework for automatic pain as-\nsessment.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 81\nTable 5.2: Results on the pain estimation tasks.\nMetricTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAccuracy 65.95 66.87 69.22 73.28 31.52\nPrecision 65.90 66.89 69.18 73.31 31.48\nRecall 67.85 68.34 70.84 74.75 29.94\nF1 65.04 66.19 68.54 72.75 27.82\nNP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain P 4: very severe pain MC: multi-level\nclassification\nslightly lower at 65.04%, and the recall stood out at 67.85%. In the NP vs. P 2task, accuracy\nincreased to 66.87%, and all related metrics improved, with the F1 score climbing by over\n1.15%, highlighting enhanced detection of true positives. The results were notably better\nfor the NP vs. P 3task, with an accuracy of 69.22% and a sensitivity of 70.84%. This is\nexpected as the pain at this level is considered severe, eliciting more pronounced responses\nfrom subjects. In the highest pain task, NP vs. P 4, the recall was particularly high at 74.75%,\nwith an accuracy of 73.28%, demonstrating that the detection of very severe pain is relatively\nmore straightforward due to the pain reaching tolerance thresholds, making it more visibly\nevident through subjects\u2019 facial expressions. However, in the multi-level classification task,\nperformance metrics were lower, illustrating the complexity of estimating all pain levels\nconcurrently; accuracy was only 31.52%, with a recall of 29.94%, pointing to significant\nchallenges in accurately identifying true positives across multiple pain levels.\nIt should be noted that our framework, encompassing both the architectural and procedu-\nral aspects of training, was consistent across all binary and multi-level classification tasks.\nThis was done to evaluate the generalization potential of our approach across all possible\nscenarios provided by the database, akin to real-world clinical settings. The detailed classifi-\ncation outcomes are presented in Table 5.2.\nVideo Sampling\nIn this section, we explore the impact of video frame sampling on automatic pain estimation.\nExperiments detailed in Section 5.2.3 utilized all available frames ( 138) from each video.\nSubsequent experiments employed frame sampling with strides of 2,3, and 4. Starting with\nall138frames, the video feature representation Dhas dimensions 138\u02c6192, totaling 26,496.\nA stride of 2reduces this to 69frames, with Dhaving dimensions 69\u02c6192and totaling\n13,248. With strides 3and4, the frame counts reduce to 46and35, resulting in Dsizes82 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.3: Results for the pain estimation tasks using various numbers of input frames.\nNumber of\nFramesTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n138 65.95 66.87 69.22 73.28 31.52\n69 65.76 66.74 69.15 73.25 31.29\n46 65.66 66.70 68.50 71.78 31.20\n35 65.40 66.12 68.32 72.01 30.80\nFigure 5.3: The impact of the number of input frames on accuracy (left) and on runtime in\nmilliseconds (right). Runtime calculated during inference on a NVIDIA RTX-3090 .\nof8,832and6,720, respectively. Table 5.3 displays the classification accuracies achieved\nwith these varying frame counts for each pain estimation task. Concurrently, Figure 5.3\ndemonstrates how the number of frames affects mean accuracy across the five tasks and\nmean runtime during inference. We noted a performance increase of approximately 1.38%\nwhen using 138frames compared to 35frames. Additionally, the runtime increased by a\nfactor of 3. Despite the longer runtime, each sampling rate allows for real-time automatic\npain estimation when necessary.\nInterpretation\nResearch in deep learning, particularly relevant to healthcare, increasingly focuses on model\ninterpretability to explain decision-making processes. This is crucial for enhancing the trans-\nparency of models, a key factor for their acceptance and integration into clinical settings. In\nour study, we implemented the technique described in [283] to generate relevant maps illus-\ntrating which facial areas our model\u2014the \u201cspatial feature extraction module\u201d \u2014focuses on.\nAs shown in Figure 5.4, the model\u2019s attention is distributed over \u201carbitrary\u201d areas at the on-\nset of a facial expression sequence. However, as the expression of pain intensifies, the focus\nsharpens on specific regions indicative of pain. It is important to note from our relevance5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 83\nFigure 5.4: Relevance Maps.\nmaps that no universal facial expressions are unique to pain. However, there is a noticeable\nconcentration on areas like the mouth and eyes.\nComparison with existing methods\nIn this section, we present a comparison of our results achieved using a transformer-based\nframework that utilizes all available video frames against other studies that also employed\nPart A of the BioVid database with all 87subjects, following the same leave-one-subject-out\n(LOSO) cross-validation protocol. This ensures objective and accurate comparisons, with\nresults detailed in Table 5.4. The studies compared fall into three main categories: i) those\nfocusing exclusively on pain detection (NP vs. P 4),ii) those examining both pain detection\nand multi-level pain estimation, and iii) those that cover all major pain-related tasks.\nOur method, tested across all tasks, recorded the highest performance metrics in bi-\nnary and multi-level pain estimations. Studies limited to specific aspects of pain detec-\ntion or multi-level pain estimation often yielded comparable or superior results, as indicated\nin [219], [180], and [284]. This highlights that while focused studies often show high perfor-\nmance, the broader impact lies in developing systems that perform well across all potential\nscenarios.\n5.2.4 Discussion\nThis research examined the application of transformer-based architectures for automatic pain\nestimation through video analysis. Our framework employed exclusively transformer mod-\nels, leveraging the spatial and temporal aspects of the video frames. The experiments demon-\nstrated the effectiveness of our approach in assessing pain, showing strong generalization\nacross various pain estimation tasks with notable results, particularly with low-intensity pain\nwhere facial expressions are less apparent. Additionally, the framework demonstrated high\nefficiency, suitable for real-time applications. A significant contribution of our work includes\ndeveloping relevance maps highlighting facial areas the model focuses on. We advocate for\ncontinued efforts within the affective computing field to enhance the interpretability of these\ndeep-learning methods.84 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.4: Comparison of studies utilizing BioVid , RGB videos, and LOSO validation.\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nThiam et al. [219] - 69.25 -\nTavakolian et al. [180] - - - 71.02 -\nPatania et al. [284] - - - 73.20 -\nHuang et al. [269] - - - 77.50 34.30\nXinet al. [141] - - - 86.65 40.40\nZhi and Wan [217] 56.50 57.10 59.60 61.70 29.70\nWerner et al. [248] 53.30 56.00 64.00 72.40 30.80\nOur approach 65.95 66.87 69.22 73.28 31.52\n5.3 Video & Heart Rate Analysis with Transformer Architectures\nWe introduce a proof of concept for an automatic pain assessment framework that integrates\nfacial video data captured by an RGB camera with heart rate signals. We build and extend\nour previous analysis in 5.2. Our main objectives include (1) evaluating the effectiveness and\nlimitations of video and heart rate data as standalone modalities in an unimodal setting, (2)\nexploring the efficacy of combining behavioral (video) and physiological (heart rate) markers\nto overcome challenges associated with their reliance on different sensing technologies and\ninformation representations, and (3) analyzing the performance and efficiency of recently\nintroduced transformer-based architectures.\n5.3.1 Methodology\nThis section details the preprocessing methods for video and ECG, the design of the proposed\nframework, the augmentation techniques developed, and the specifics of implementing the\npretraining process.\nPre-processing\nPreparatory processing was essential before feeding data into the pain assessment framework,\nparticularly for the raw ECG data used to compute heart rate. We focused on exploring heart\nrate as the primary feature due to its benefits: It\u2019s readily obtainable from wearable devices,\ncost-effective, and easily accessible. These advantages position heart rate as a potentially\nvaluable feature for automated pain assessment.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 85\nVideo Preprocessing: Video preprocessing included face detection to isolate the facial re-\ngion using the MTCNN face detector [278], which employs multitask cascaded convolu-\ntional networks to predict facial and landmark locations. Predicting landmarks facilitates\nface alignment, which is crucial for accurate facial analysis. However, we observed that\nface alignment reduced the expressiveness linked to head movements, a common behavioral\nindicator of pain. Consequently, face alignment was omitted from our proposed pipeline.\nAdditionally, the resolution of frames post-face detection was standardized at 448\u02c6448\npixels.\nECG preprocessing & analysis: Similar to Section 4, we utilize the Pan-Tompkins [262] al-\ngorithm to detect the QRS complex, the most prominent wave complex in an ECG signal.\nThis algorithm operates in two phases: preprocessing and decision-making. The preprocess-\ning phase focuses on noise removal, artifact elimination, signal smoothing, and enhancing\nthe QRS slope. The decision-making phase involves initial QRS detection using adaptive\nthresholds, a retrospective search to identify any missed QRS complexes, and a method for\ndistinguishing T waves. Following the accurate detection of R waves, the estimation of inter-\nbeat intervals (IBIs) was conducted, leading to the extraction of key features. Specifically,\nwe calculated the mean of the IBIs as follows:\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (5.5)\nwhere nis the total number of IBIs, and RRidenotes the consecutive R\u00b4Rintervals.\nSubsequently, the heart rate was calculated using the following formula:\nHR\u201c60\u00a8FS\n\u00b5, (5.6)\nwhere FSdenotes the sampling frequency of the ECG recording.\nFramework architecture\nThe proposed framework, as illustrated in Figure 5.5, consists of four key components: the\nSpatial-Module that extracts embeddings from video data, the Heart Rate Encoder which\nmaps heart rate signals into a higher dimensional space, the AugmNet that generates aug-\nmentations in the latent space, and the Temporal-Module performs with the final assessment\nof pain.\nSpatial-Module: The architecture for this module draws inspiration from the Transformer\nin Transformer approach as detailed by [275]. The process begins with the initial video86 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(a) Video analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(b) ECG analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(c) Fusion analysis pipeline.\nFigure 5.5: Outline of the proposed framework.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 87\nframe at a resolution of 448\u02c6448pixels, segmented into 4quadrants, each at 224\u02c6224\npixels. This tiling method, which maximizes the utilization of the frame\u2019s resolution, is\ninfluenced by approaches seen in satellite imaging analysis. Our framework incorporates the\n4tiles and the original frame\u2014resized to 224\u02c6224pixels\u2014into our analysis pipeline. Thus,\neach video frame transforms into 5distinct images, denoted as:\nFk\u201crFk,1, Fk,2, . . . , Fk,ts, (5.7)\nwhere kstands for the frame number, and tencompasses the tile count, including the resized\nfull frame. Each tile is initially split into npatches, expressed as:\nFk,t\u201crFk,t,1, Fk,t,2, . . . , Fk,t,nsPRn\u02c6p\u02c6p\u02c63, (5.8)\nwhere p\u02c6pspecifies the resolution of each patch ( 16\u02c616), and 3denotes the RGB channels.\nThese patches are further segmented into msub-patches, allowing the model to capture the\nimage\u2019s global and localized features. Each tile from a frame thus transitions into a sequence\nof patches and sub-patches, represented as\nFk,t\u201crFk,t,n, 1, Fk,t,n, 2, . . . , Fk,t,n,ms.\nConsequently, each video frame is characterized by:\nFk\u00d1\u201c\nFk,t,n,m|tPr1,5s, nPr1,196s, mPr1,16s\u2030\n, (5.9)\nwhere Fk,t,n,mPRs\u02c6s\u02c63defines the m-th sub-patch within the n-th patch of the t-th tile\nfor the k-th frame, with each sub-patch having a resolution of s\u02c6s(4\u02c64). Each frame\nconsists of 5image representations, encompassing 196patches, and each patch contains 16\nsub-patches. The patches and sub-patches are then linearly projected into embeddings Zand\nY. Positional embedding is applied to maintain spatial information, employing 1D learnable\nposition encodings:\nZ0\u00d0Z0`Epatch, (5.10)\nwhere Epatch indicates the position encoding. Each sub-patch also receives its specific posi-\ntional encoding:\nYi\n0\u00d0Yi\n0`Esub\u00b4patch, (5.11)\nwhere Esub\u00b4patch denotes the positional encodings for sub-patches, and irepresents the index\nof a sub-patch within a patch. The sub-patches are processed in the Inner Encoder , which88 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nconsists of four self-attention heads [285], utilizing dot product attention:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n. (5.12)\nThe output from the Inner Encoder integrates into the patch embedding, advancing to the\nOuter Encoder , which mimics the Inner Encoder with ten self-attention heads. The Spatial-\nModule consists of twelve parallel blocks, generating embeddings of dimensionality d\u201c100.\nFor each input video frame, 5distinct output embeddings of dimensionality 100are produced\nand combined to form a comprehensive frame representation:\nD\u201cdFullFrame`pdTile1`dTile2`dTile3`dTile4q\u00a8c, DPR100, (5.13)\nwhere cadjusts the contribution from the tile embeddings, subsequently, the embedding for\neach frame, D, is concatenated with those of other frames to construct a complete video\nrepresentation:\nVD\u201crD1}D2}. . .}Dfs, VDPRN, (5.14)\nwhere frepresents the total number of frames in the video, and Ndenotes the dimensionality\nof the final video embedding.\nHeart Rate Encoder: As mentioned in Section 5.3.1, heart rate is computed from the origi-\nnal ECG every second, producing a heart rate vector of length h\u201c\u03b8for recordings lasting\n\u03b8seconds. It should be noted that when beats per minute (BPM) fall below 60within any\n1-second segment of the ECG, making direct heart rate calculation impractical, the method\naverages heart rates from the data points immediately before and after to maintain a uniform\nset of \u03b8data points. The Heart Rate Encoder , which is part of a transformer-based archi-\ntecture similar to the Inner andOuter Encoders , utilizes one cross-attention head instead\nof self-attention followed by a fully connected neural network (FCN). This use of cross-\nattention introduces an asymmetry that reduces computational load and increases efficiency.\nUnlike traditional input projections that are M\u02c6Din dimension, as detailed in Section\n5.3.1, the Qmatrix in cross-attention is a learnable matrix sized N\u02c6Dwhere N\u0103M. The\nencoder\u2019s internal embeddings are set to a dimensionality of 512and contain only a single\nblock depth. Fourier feature position encodings [277] are also implemented to handle posi-\ntional information. The main goal of this encoder is to transform the initial heart rate vector\nhinto a more complex and richer feature space,\nhPR\u03b8\u00d1EhPR2048,5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 89\nwhere Ehrepresents the enhanced output embedding of this encoder. In the next step, the\noutput from the heart rate encoder is expanded dimensionally via a bicubic interpolation\nmodule. This process enhances the original heart rate\u2019s feature representation, allowing it to\nintegrate smoothly with the video\u2019s embedding representation through addition. The need\nfor identical dimensions in both embedding vectors is critical and is adeptly addressed by this\ninterpolation module. This non-learning-based approach proves to be efficient and effective\nfor encoding purposes. Additionally, interpolation provides the flexibility to dynamically set\nthe dimensionality of the final output embedding, unlike the fixed dimensions typically seen\nin neural network-based methods. Specifically:\nBh\u201c3\u00ff\ni\u201c03\u00ff\nj\u201c0aijpEhq\u00a8px\u00b4x0pEhqqi\u00a8py\u00b4y0pEhqqj, (5.15)\nwhere aijare the coefficients used for interpolation, and Bhis the resulting vector from the\nbicubic interpolation process. The dimension of BhisN, which is the same as that of VD.\nAugmNet: Inspired by recent developments in the augmentation literature [286], employs\na learning-based technique to identify augmentation patterns within the latent space. Un-\nlike conventional methods that perform image augmentations ( e.g., rotation, cropping) in the\npixel space, AugmNet universally applies transformations to the embeddings. This method\neliminates the necessity for specific transformations tailored individually to each modality,\ne.g., image, signal, and text. Incorporating this module within the automatic pain assessment\nframework helps to regularize the learning process and address overfitting issues. Moreover,\ncorrupting the input embeddings compels the following model, especially the Temporal-\nModel , to derive more precise and representative features, thereby improving performance\nin the pain assessment task. The modality-agnostic method effectively applies to embedding\nrepresentations from any original modality, including video and heart rate signals. AugmNet\nadopts an encoder-decoder architecture, where both the encoder and decoder consist of only\n2fully connected layers with the ELU nonlinear activation function applied after each layer.\nFor a session lasting \u03b8seconds, it produces \u03b8\u02c6frames per second frames and \u03b8\u02c6FS\ndata points for video and ECG, respectively. In the video analysis pipeline, the Spatial-\nModule constructs an embedding representation, VD(5.14), from the original video, dimen-\nsioned at d\u02c6FPS\u201cN. In the ECG analysis pipeline, a feature vector with dimension \u03b8\nis created after extracting the heart rate, one data point per second. The Heart Rate Encoder\nand bicubic interpolation then produce an embedding, Bh(5.15), with dimension N. The fu-\nsion of video and heart rate embeddings at the session level is performed, where VDandBh\nare merged by addition, integrating the data from the initial input modalities. The resulting90 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncombined embedding is then processed by AugmNet :\nAD\u00d0AugmNetpVD`Bhq (5.16)\nP\u00d0AD`pVD`Bhq, (5.17)\nwhere Prepresents the transformed embedding vector, serving as input for the final module,\ntheTemporal-Module .AugmNet is active only during training as a standard augmentation\nmethod and remains inactive during inference.\nTemporal-Module: Like the Heart Rate Encoder , this component operates on a transformer-\nbased architecture. It employs a combination of multi-head cross-attention and multi-head\nself-attention mechanisms. The architecture consists of one multi-head cross-attention block\nwith a single attention head and three subsequent multi-head self-attention blocks, each fea-\nturing eight attention heads. An FCN follows each attention block. The internal embed-\ndings in this module have a dimensionality of 128and encompass a single block in depth.\nThe position encoding method used here mirrors that of the Heart Rate Encoder , utilizing\nFourier feature position encoding. This module processes the input embedding Por the sum\npVD`BhqifAugmNet is inactive, to derive the final classification outcome. The learning\nerror is computed during this phase, and the framework undergoes further training.\nSupplementary augmentation methods\nWe also introduced additional augmentation strategies alongside the AugmNet module, which\napplies learned transformations to embeddings. The initial technique, dubbed Basic , com-\nbines polarity inversion and noise addition to manipulate the original inputs by reversing\ntheir polarity and injecting noise. Another technique, named Masking , involves nullifying el-\nements within the embeddings using randomly sized and placed masks that zero out 10\u00b420%\nof the embedding elements. These methods function within the latent space, similar to Augm-\nNet.\nPre-training\nBefore starting the training for automatic pain assessment, we individually pretrained all\nmodules except AugmNet . The Spatial-Module underwent a dual-phase pretraining process.\nInitially, it was pre-trained on VGGFace2 [280], a dataset designed for facial recognition\nto learn basic facial features. This was followed by an advanced training phase involving\nemotion recognition datasets in a multi-task learning framework. These datasets include the\nwidely used AffectNet [287], Compound Facial Expressions of Emotions Database [288],\nRAF Face Database basic [289], and RAF Face Database compound [289], enabling the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 91\nTable 5.5: Datasets utilized for the pre-training process of the framework.\nDataset # samples # classes Task\nVGGFace2 [280] 3.31M 9,131 Face\nAffectNet [287] 0.40M 8 Emotion\nCompound FEE-DB [288] 6,000 26 Emotion\nRAF-DB basic [289] 15,000 7 Emotion\nRAF-DB compound [289] 4,000 11 Emotion\nECG HBC Dataset [291] 0.45M 5 Arrhythmia\nTask: all tasks involve classification\nmodule to adapt to specific emotional expressions related to pain manifestations. During\nthis phase, the model is trained across these datasets simultaneously, employing a multi-task\nlearning loss approach as suggested by [290], where learned weights scale the individual\nlosses to account for the homoscedastic uncertainty of each task:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s`rew3LS3`w3s`rew4LS4`w4s,\nwhere LSdenotes the loss for each dataset and ware the weights adjusting the learning focus\nto optimize the overall loss Ltotal. The Temporal-Module is trained solely on the VGGFace2\ndataset. For this module, images are converted into 1D vectors prior to processing. The\nHeart Rate Encoder is pre-trained using the ECG Heartbeat Categorization Dataset [291],\nwhich includes heartbeat signals from the MIT-BIH Arrhythmia Dataset [292] and the PTB\nDiagnostic ECG Database [293] [294]. Table 5.5 provides a detailed list of the datasets used\nin our training process.\nDataset Details\nIn this study, to assess the developed framework, we utilized the BioVid Heat Pain Database\n[109], which includes facial videos, electrocardiograms, electromyograms, and skin conduc-\ntance levels from 87healthy individuals ( 44males and 43females, aged 20\u00b465). The\ndataset employs a thermode to induce varying pain levels in the participants\u2019 right arm. Ini-\ntially, each participant\u2019s pain threshold (the transition from heat sensation to pain) and pain\ntolerance (the point at which pain becomes unbearable) were determined. These thresholds\ndelineated the minimum and maximum pain levels, along with two intermediary levels, form-\ning five distinct pain intensities: No Pain (NP), Mild Pain (P 1), Moderate Pain (P 2), Severe\nPain (P 3), and Very Severe Pain (P 4). Pain stimuli temperatures ranged from P 1to P 4, capped\nat50.5\u00b0C. Each subject experienced 20stimulations at each of the four intensities (P 1to P 4).\nEach stimulation lasted 4seconds, interspersed with recovery intervals of 8to12seconds.92 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.6: Training details for the automatic pain assessment.\nOptimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsBatch\nsize\nAdamW 1e-4 cosine 0.1 50 32\nThis protocol, along with 20baseline measurements at NP ( 32\u00b0C), culminated in randomly\nordered 100stimulations per participant. The data was preprocessed to capture 5.5-second\nwindows starting 1-second after reaching the target temperature for each stimulation. This\nprocess produced 8,700samples, each 5.5seconds in duration, distributed evenly across the\nfive classes and modalities for all 87subjects.\n5.3.2 Experiments\nThe study leveraged videos and electrocardiograms from Part A of the BioVid dataset, using\nall available samples from the 87participants. The videos were recorded at a rate of 25\nframes per second (FPS), and the electrocardiogram (ECG) signals were sampled at 512\nHertz (Hz). Each recording session lasted 5.5seconds, generating 138video frames and\nECG vectors containing 2,816elements each, then converted into heart rate vectors of 5\ndata points. The complete set of frames and data points from both videos and cardiac signals\nwas utilized in the experiments. The experimental approach included iterative refinement\nof techniques, with the most effective combinations selected for extended training periods\nranging from 500to800epochs to improve feature extraction and performance outcomes.\nTable 5.6 details the training configurations for the automatic pain assessment tasks.\nPain assessment experiments were structured around binary and multi-level classification\nsetups, testing each modality individually and in combination. The binary classification task\ndifferentiated between No Pain (NP) and Very Severe Pain (P 4), whereas the multi-level\nclassification (MC) involved categorizing all pain intensities available in the dataset. The\nevaluation strategy adopted was the leave-one-subject-out (LOSO) cross-validation, and the\nassessment metrics included accuracy, precision, recall (sensitivity), and F1 score. Notably,\na consistent training regimen was applied across both the binary (NP vs. P 4) and multi-level\n(MC) classification tasks without varying the training schedule or optimization strategies.\n5.3.3 Results\nVideo modality\nExperiments related to the video modality explored the effects of pretraining on the Spatial-\nModule , the video analysis pipeline\u2019s performance, particularly the impact of tiling, and the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 93\nTable 5.7: Results utilizing the video modality.\nEpochsPretraining stage Pipeline Augmentations Task\n1st2ndFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n500 \u2713 - \u2713 - \u2713 - - 72.56 31.22\n500 - \u2713 \u2713 - \u2713 - - 74.25 33.34\n500 - \u2713 - \u2713 \u2713 - - 68.07 31.49\n500 - \u2713 \u2713 \u2713 \u2713 - - 65.11 27.84\n500 - \u2713 \u2713 \u2713c\u2713 - - 74.86 33.86\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 - 73.05 32.14\n500 - \u2713 \u2713 \u2713c\u2713 - \u2713 74.83 33.73\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 73.16 32.87\n800 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 77.10 35.39\nStage: referring to pretraining process for Spatial-Module Mask: Masking c: constant-coefficient applied exclusively to the tiles NP:\nNo Pain P 4: Very Severe Pain MC: multiclass pain level\nimplementation of new augmentation techniques. These experiments are detailed in Table\n5.7. Performance enhancements are evident when comparing the first and second pretraining\nstages of the Spatial-Module . For instance, in the NP vs. P 4task, initial pretraining alone\nachieved 72.56% accuracy, while including the second emotion-focused pretraining stage\nincreased accuracy to 74.25%. This trend is also notable in the multi-level classification,\nwhere the second stage added 1.12% to the performance, totaling 33.34%. Further experi-\nments assessed the effect of using tiles in the video representation. Initially, employing four\ntiles led to a performance decrease of over 6%in the binary classification task and 1.85% in\nthe multi-level task. This reduction likely results from the localized information in each tile,\nwhich may capture irrelevant details like non-expressive facial areas or background elements.\nIncluding the resized full-frame ( 224\u02c6224pixels) alongside tiles further decreased accuracy\nto65.11% and27.84% for binary and multi-level tasks, respectively. However, introducing\na coefficient ( c\u201c0.1) to adjust the tile embeddings restored some performance, achieving\n74.86% and33.86% in respective tasks.\nThe integration of two augmentation techniques, Masking andAugmNet , along with the\nBasic method, was then tested. Masking reduced performance by 1.81% and1.72%, and\nAugmNet showed smaller declines of 0.03% and0.13%. Using both techniques together\nresulted in better outcomes than Masking alone but did not independently surpass the per-\nformance of AugmNet . Despite these initial results, combining all augmentation methods\nproved advantageous for extended training periods. This approach addresses the risk of\noverfitting through a robust regularization strategy. Ultimately, this comprehensive strategy\nled to final accuracy rates of 77.10% and35.39% for binary and multi-level classifications,\ndemonstrating its effectiveness in an unimodal, vision-based pain assessment framework.94 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nHeart rate modality\nThe experiments concerning the heart rate modality explore the use of the encoder and vari-\nous augmentation methods. Table 5.8 details all experiments involving the heart rate modal-\nity. Initially, using the original heart rate vectors with a dimensionality of h\u201c5, we achieved\nclassification scores of 61.70% for NP vs P 4and27.60% for the multi-level task. After apply-\ning the Heart Rate Encoder to map these vectors to a higher-dimensional space of h\u201c2048 ,\nwe noted a slight improvement: a 0.23% increase for the binary task and 0.08% for the\nmulti-level task. Despite the considerable increase in embedding size, this modest enhance-\nment suggests that the intrinsic information within the limited heart rate data points does not\nsignificantly enhance the feature representation. Nonetheless, the encoder\u2019s use is vital for\nproducing larger embeddings, especially for our multimodal approach integrating video and\nheart rate data, which will be discussed in subsequent sections.\nWe also tested augmentation methods on the heart rate data. Applying Masking yielded\na slight improvement of 0.02% for the binary task and 0.05% for the multi-level task. Imple-\nmenting AugmNet further enhanced performance to 62.09% and28.11% for binary and multi-\nlevel tasks, respectively. However, combining all augmentations decreased performance to\n61.87% and27.96%. During an extended 800-epoch training period, we achieved 64.87%\naccuracy for the binary task and 29.81% accuracy for the multi-level task using all augmen-\ntations. Despite these gains, we found that augmentations pose more challenges for accurate\nheart rate classification than video. Therefore, we repeated the extended training without\nBasic andMasking , keeping only AugmNet , which improved binary task performance to\n67.04% and multi-level to 31.22%. This reduction in heart rate embedding corruption sig-\nnificantly enhanced performance. The differing effects of augmentations between heart rate\nand video modalities highlight the challenges of using a single, isolated feature in a machine\nlearning system. We infer that heart rate embeddings with limited informational content\nare more vulnerable to significant performance degradation from augmentations than richer\nvideo embeddings.\nMultimodality\nThe results of integrating video and heart rate modalities are detailed in Table 5.9. Based on\nthe insights gained from separate experiments with each modality, we extended the training\nduration to 800epochs. For this integrated approach, we utilized the tiles with a coefficient\nc\u201c0.1and applied AugmNet as the sole augmentation method. This fusion strategy re-\nsulted in a classification accuracy of 82.74% for NP vs. P 4and39.77% for the multi-level\nclassification task. These results mark a substantial enhancement, with improvements of\n5.64% and15.70% over the individual performances of the video and heart rate modalities,\nrespectively, for the binary classification. Similarly, for the multi-level classification, the in-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 95\nTable 5.8: Results utilizing the heart rate modality.\nEpochsHR\nEncoderAugmentations Task\nBasic Mask AugmNet NP vs P 4 MC\n500 - \u2713 - - 61.70 27.60\n500 \u2713 \u2713 - - 61.93 27.68\n500 \u2713 \u2713 \u2713 - 61.95 27.73\n500 \u2713 \u2713 - \u2713 62.09 28.11\n500 \u2713 \u2713 \u2713 \u2713 61.87 27.96\n800 \u2713 \u2713 \u2713 \u2713 64.84 29.81\n800 \u2713 - - \u2713 67.04 31.22\nTable 5.9: Results utilizing the video & the heart rate modality.\nEpochsHR\nEncoderPipeline Augmentations Task\nFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n800 \u2713 \u2713 \u2713c- - \u2713 82.74 39.77\ntegrated approach shows a 4.38% and8.55% increase compared to the standalone modalities.\nThe combination of these two pivotal modalities significantly boosts the efficacy of the pain\nassessment process, outperforming the results obtained by each modality on its own.\nComparison with existing methods\nIn this section, a comparative analysis is performed to evaluate the performance of our\nmethod against other existing approaches documented in the literature. This evaluation\nis based on Part A of the BioVid dataset, including all 87participants. The same evalua-\ntion protocol\u2014leave-one-subject-out (LOSO) cross-validation\u2014is adhered to for all com-\nparisons to ensure fairness and accuracy. Our method is contrasted with both unimodal and\nmultimodal approaches, divided into (1) video-based, (2) ECG-based, and (3) multimodal\nstudies regardless of the modalities involved. The outcomes are summarized in Table 5.10.\nFor video-based studies, our approach, achieving 77.10% in binary and 35.39% in multi-\nlevel classification tasks, is recognized as one of the highest-performing methods. It exceeds\nthe average results of comparative studies by about 4.7%for binary and 3.4%for multi-level\ntasks. Regarding ECG-based studies, our method shows superior performance, exceeding\nthe average by 8.5%and18.1%for binary and multi-level classifications, respectively. Re-\nmarkably, it records the highest classification accuracy in the multi-level task at 31.22%.96 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nThese findings underscore the effectiveness of using heart rate data extracted from ECG\nas a standalone feature, establishing our method\u2019s capability to assess pain accurately and\nachieve state-of-the-art results. In multimodal studies, our approach records an impressive\n82.74% accuracy in the NP vs. P 4task, making it one of the top-performing methods. It\nis slightly outperformed by studies [269] and [243], which achieved 88.10% and83.99%,\nrespectively. For multi-level tasks, comparisons are scarce; however, study [269] reached\n42.20%, and [235] reported 36.54%, positioning our method favorably within this context.\nInference time\nWe explored several methodologies in this study, including a video-based approach, video\nincorporating tiles, a heart rate-based approach, heart rate analysis with an encoder, and a\ncombined multimodal strategy. Figure 5.6 illustrates each method\u2019s inference time in sec-\nonds and the corresponding average accuracy performances across binary and multi-level\ntasks. Table 5.11 details the number of parameters and the computational cost of floating-\npoint operations (FLOPS) for each component. Inference tests were conducted on an Intel\nCore i7-8750H CPU, including the time for face detection in each frame but excluding the\nextraction of heart rate from electrocardiography, focusing on the potential use of automati-\ncally provided cardiac features from wearables.\nThe inference time for the video modality employing the standard pipeline is approxi-\nmately 26seconds. Utilizing the tile pipeline increases inference time significantly, soaring\nto about 130seconds due to processing five image representations per frame\u2014one full frame\nand four tiles\u2014compared to a single image representation in the non-tiled approach. In the\ncontext of heart rate signals, completing a pain assessment requires only 1.2seconds. With\nthe integration of the Heart Rate Encoder , the processing time remains virtually unchanged,\nshowing a negligible increase of less than half a second, highlighting this specific encoder\u2019s\nefficiency. Lastly, the comprehensive multimodal framework incorporating the tiles and the\nHeart Rate Encoder demands about 131seconds, illustrating the increased complexity and\ncomputational requirements when combining multiple modalities.\nInterpretation\nImproving model interpretability is essential for their acceptance and integration into clin-\nical settings. This study generates attention maps from both the Spatial-Module and the\nTemporal-Module , as illustrated in Figure 5.7. For the Spatial-Module , attention maps are de-\nrived from the last fully connected layer\u2019s weight contributions, which are then interpolated\nonto the images to highlight the model\u2019s focal areas. Figure 5.7a displays an original frame\nsequence along with three attention map variations: (1) post-initial pretraining, (2) after the\nsecond pretraining phase, and (3) post-training on the BioVid . Based on face recognition5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 97\nTable 5.10: Comparison of studies utilizing BioVid & LOSO validation, reported on accuracy %.\nStudy ModalityMethod Task\nFeatures Machine Learning Params (M) FLOPS (G) NP vs P 4 MC\n[295] Video optical flow RF - - 70.20 -\n[217] Video raw SLSTM - - 61.70 29.70\n[269] Video raw 3D CNN, - - 77.50 34.30\n[219] Video raw 2D CNN, biLSTM - - 69.25 -\n[180] Video raw 2D CNN 25.00\u00154.00 71.00 -\n[296] Video facial action\ndescriptorsDeep RF - - 72.40 30.80\n[296] Video facial 3D distances Deep RF - - 72.10 30.30\n[284] Video fiducial points GNN - - 73.20 -\n[135]:Video raw 2D CNN - - 71.30 37.60\n[211]:Video raw 2D CNN, GRU 150.00\u0015- 73.90 39.10\n[37] Video raw Transformer 24.00 4.20 73.28 31.52\n[267] Video facial landmarks,\n3D distancesRF 71.60 -\nOur Video raw Transformer 4.20\u00101.62 77.10 35.39\n[235] ECG raw 1D CNN 1.80\u0015- 57.04 23.23\n[270] ECG domain-specific\u00b8LR - - 57.69 -\n[35] ECG domain-specific\u00b8SVM - - 58.39 23.79\n[269] ECG heart rate\u20393D CNN - - 65.00 28.50\n[267] ECG domain-specific RF - - 62.00 -\n[36] ECG domain-specific FCN 4.09\u00120.40 69.40 30.24\n[297] ECG domain-specific\u00b8SVM - - 63.50 -\nOur ECG heart rate Transformer 6.03\u00101.25 67.04 31.22\n[235] ECG, EMG, GSR raw 2D CNN 10.00\u0015- 76.72 36.54\n[270] ECG, GSR domain-specific\u00b8SVM - - 72.20 -\n[269] Video1, ECG2raw1, heart rate2\u20393D CNN - - 88.10 42.20\n[267] ECG1, EMG1,\nGSR1domain-specific1\u00b8RF - - 74.10 -\n[267] Video1,ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 77.80 -\n[297] Video1, ECG2,\nGSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 78.90 -\n[297] Video1, ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8SVM - - 76.60 -\n[243] ECG, EMG, GSR raw DDCAE 4.00\u0015- 83.99 -\nOur Video1,ECG2raw1, heart rate2Transformer 8.60\u00102.44 82.74 39.77\nM: Millions G: Giga :: reimplemented for pain intensity estimation on BioVid by [269]\u2039: pseudo heart rate gain \u00b8: numerous\nfeatures \u0015: parameter count estimated from provided paper details \u0010:AugmNet excluded from parameter count, not used in inference\n\u0012: parameter count not mentioned in study, provided directly by authors -: missing value RF: Random Forest AE-ATT: Autoencoder\nAttention SVM: Support Vector Machines LR: Logistic Regression98 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nVideo Video\n(Tiles)Heart rate Heart rate\n(Encoder)Multimodal0102030405060Accuracy (%)Average Accuracy\n020406080100120\nInference Time (s)\nInference Time\nFigure 5.6: Comparison of mean accuracy and inference period for unimodal and multimodal\nstrategies across NP versus P 4and MC tasks. The diagram adopts a dual-y-axis\nconfiguration\u2014accuracy measurements on the left and time metrics on the right\u2014\nto outline the balance between performance efficacy and computational load, cate-\ngorizing the methodologies along the x-axis.\nTable 5.11: Module parameters and computational cost in FLOPS for the proposed framework.\nModule Params (M) FLOPS (G)\nSpatial-Module 2.57 1.19\nHeart Rate Encoder 4.40 0.82\nAugmNet 1.02 0.02\nTemporal-Module 1.63 0.43\nTotal 9.62 2.46\ntasks, the Spatial-Module produces maps focusing broadly on the facial area, particularly\nthe zygomatic, buccal, oral, mental, and nasal regions. The second stage, oriented towards\nmulti-task emotion recognition, refines the focus, sharpening attention on specific facial ar-\neas, highlighted in the first stage but with greater clarity and emphasis. After training on the\nBioVid for pain assessment, the attention maps show further refined focus on specific facial\nareas, with reduced attention to less relevant regions, ensuring concentrated focus on key\nareas. These maps consistently demonstrate the model\u2019s capability to adjust its focus based\non pain-related facial expressions.\nAttention maps from the Temporal-Module , based on input embeddings, illustrate the\nweight contributions in the module\u2019s final layer, forming easy-to-visualize rectangular pat-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 99\nterns. Figure 5.7b shows examples for three scenarios: (1) video embedding, (2) heart rate\nembedding, and (3) a combined embedding of video and heart rate. The attention maps ex-\nhibit a grid-like pattern, possibly due to the Fourier position encoding used, akin to those\nseen in perceiver-like architectures. The video embedding map shows intense attention\nacross the input. In contrast, the heart rate embedding map focuses attention more sparsely,\nwith a notable concentration in specific areas indicated by intense red coloration. The com-\nbined embedding map displays moderate intensity, consistent with the blended nature of the\ninput. These maps tend to emphasize the latter part of the session, aligning with the timing\nof pain manifestation towards the session\u2019s end, indicating the model\u2019s responsiveness to\nreal-time pain expressions.\n5.3.4 Discussion\nThis research introduced a multimodal framework that integrates video and heart rate sig-\nnals to assess pain automatically. Our innovative approach includes four main modules,\neach characterized by effectiveness and efficiency. The Spatial Module , particularly no-\ntable for its compact size of only 2.57million parameters, ranks as one of the most efficient\nvision-based models in automatic pain assessment literature. Despite limited comparative\nstudies, our model has shown it can match or exceed the performance of larger models. Its\nhigh efficiency and robust performance are primarily due to a thorough pretraining regime\non datasets related to affective responses, crucial for enhancing model capabilities in pain\nestimation tasks. The Heart Rate Encoder , with 4.40million parameters, excels at transform-\ning heart rate data into complex, high-dimensional embeddings, which integrate seamlessly\nwith video data during inference, all within under half a second. This quick processing\nunderscores the encoder\u2019s efficiency, supported by bicubic interpolation that modifies input\ndynamically to achieve variable output dimensions without predefined constraints. AugmNet ,\na novel augmentation module, learns to modify latent space representations directly, prevent-\ning the need for specific augmentation techniques designed for each data type. However, this\nmodule requires careful application to avoid overfitting and other training challenges. The\nTemporal-Module , consisting of 1.63million parameters, is crucial for final pain level as-\nsessments. It leverages a mix of cross- and self-attention mechanisms to enhance efficiency\nand accuracy, demonstrating the potential of transformers in streamlined settings contrary to\ntheir typical use in large-scale applications.\nOur experiments demonstrate that videos are invaluable for discerning individual pain ex-\nperiences by capturing diverse behavioral indicators like facial expressions, eye movements,\nand even slight color changes under stress. Utilizing video data, our methodology reached\nan accuracy of 77.10% in binary classification, effectively distinguishing between no pain\nand very severe pain scenarios. Moreover, it achieved 35.39% accuracy in a multi-level clas-100 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n(a) Attention maps from the Spatial-Module .\n(b) Attention maps from the Temporal-Module .\nFigure 5.7: Regions highlighted in yellow and red denote areas of significant attention. (a)\n(1strow) Sequence of original frames. (2ndrow) Derived from the Spatial-Module\nafter initial stage pretraining. (3rdrow) Derived from the Spatial-Module post sec-\nond stage pretraining. (4throw) Derived from the Spatial-Module trained on the\nBioVid dataset. (b) (1strow) Derived from the Temporal-Module incorporating\nvideo embeddings. (2ndrow) Derived from the Temporal-Module with heart rate\nembeddings. (3rdrow) Derived from the Temporal-Module using a combined em-\nbedding of video and heart rate.5.4. SUMMARY 101\nsification spanning five distinct pain intensities. The heart rate signal, tested as a standalone\nfeature from electrocardiography, showed that remarkable outcomes are possible with this\nsingle data feature, which is pivotal for validating the feasibility of heart rate as a viable\npain indicator. This is crucial as heart rate data is readily accessible from most wearable\ntechnologies, reducing the need for specialized algorithms to handle cardiac signals or raw\nbiosignals, thereby conserving both time and computational resources. Solely using heart\nrate, our model excelled, registering accuracies of 67.04% and31.22% for binary and multi-\nlevel classifications, respectively, among the highest reported. Incorporating video and heart\nrate data, our multimodal method yielded superior results\u2014 82.74% and39.77% for binary\nand multi-level classifications, respectively. These figures significantly enhance video-only\nresults by roughly 9%and heart rate-only outcomes by about 24%. Furthermore, with a total\nparameter count of just 9.62million, our approach stands out for its efficiency and effec-\ntiveness. Overall, this study showcases the synergy achievable by merging video and heart\nrate data, leading to a system outperforming its unimodal counterparts and emphasizing the\npotential of integrated multimodal pain assessment tools.\nUsing attention maps generated by the Spatial-Module , our framework analysis iden-\ntified crucial facial areas like the zygomatic and oral regions as significant for automatic\npain assessment. These maps demonstrated that different pretraining stages refine the focus,\nshowing more targeted attention with specialized training. Similarly, attention maps from\ntheTemporal-Module focused on the latter part of the input image, corresponding to where\npain manifestations are typically observed in the particular dataset.\n5.4 Summary\nThis chapter explores the interplay between model efficiency and performance in automatic\npain assessment tasks. We also aimed to mirror real-world conditions by leveraging read-\nily accessible and applicable modalities without relying on costly precision medical devices.\nConsequently, we utilized RGB videos with a resolution of approximately 1080x1080 and\nheart rate data. The videos utilized are of medium quality, comparable to those captured with\nmobile phone cameras, and the heart rate data simulates readings from wearable devices. It\nis important to note that wearables across various price ranges automatically provide heart\nrate information. Consequently, exploring the potential of using this readily available modal-\nity for pain assessment is crucial. This proof of concept is significant as it could enable\ncost-effective and accessible pain assessment solutions without dependence on specialized\nmedical equipment. Additionally, our study focuses on developing compact and efficient\nmodels that maintain robust performance.\nThe experiments detailed in this chapter reveal that reducing the number of frames used\nin a video-based pipeline by a factor of four minimizes the accuracy loss, under 1.5%, while102 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncutting inference time threefold, facilitating near real-time pain assessment. Furthermore,\nour initially proposed framework, which incorporates 24million parameters and operates at\n4.3GFLOPS, demonstrated exceptionally high performance. In the subsequent experiments,\nwe showcased that heart rate data alone can be effectively used for pain assessment, achiev-\ning impressive results. This finding underscores the practical utility of data available from\nwearables. Additionally, combining video data with heart rate information yielded the high-\nest accuracy in our tests, illustrating that an integrated approach using both behavioral and\nphysiological modalities can significantly enhance performance. Additionally, we demon-\nstrated that creating one of the smallest models documented, with only 9.62million param-\neters and 2.46GFLOPS, allowed us to achieve top-tier results. This highlights that large\nmodels, commonly favored in the current era of AI, are not always necessary for effective\nperformance. However, it is important to note that extensive multi-stage pretraining across\nvarious datasets greatly aided this framework\u2019s success, which was critical in achieving such\nhigh efficiency and effectiveness.\nIn this chapter, we aimed to explore the effectiveness of compact models in achieving\nhigh performance with rapid inference times. We exclusively used heart rate data, mimicking\nthe information typically available from wearable devices, though our experiments did not\nextend to real-world conditions. Instead, they relied on the sole publicly accessible dataset\nexplicitly designed for pain assessment, including facial videos and cardiac signals collected\nin highly controlled laboratory settings. Participants were positioned facing forward un-\nder optimal lighting conditions, with physiological sensors precisely affixed. Recognizing\nthe potential challenges of applying these findings to clinical settings is essential. Issues\nlike inconsistent lighting, unforeseen facial movements, occlusions, or difficulties with sen-\nsor placement must be meticulously addressed to tailor these systems for real-world use.\nMoreover, depending solely on heart rate as a cardiac feature could be restrictive in more\ndemanding scenarios, highlighting the necessity to integrate multiple extracted features or\nuutilizeraw biosignals for comprehensive assessments.Chapter 6\nSynthetic Data: The Role of Thermal Imag-\ning\nContents\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n6.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the study published in [39]. In recent years, syn-\nthetic data generation has gained traction as a viable approach to addressing data scarcity\nand privacy issues while also meeting the requirements for training AI algorithms on unbi-\nased data with adequate sample size and statistical robustness. Additionally, synthetic data\ncan increase the availability and diversity of real data, particularly in rare modalities, which\ncan be essential for training AI-driven diagnostic and predictive models. This enhance-\nment supports healthcare research and improves patient outcomes. [298]. In the literature\non automatic pain assessment, no studies have been reported concerning creating synthetic\nmodalities. This chapter introduces the generation process of synthetic thermal videos using\nGenerative Adversarial Networks (GANs).\nRegarding thermal modality, in recent years, the field of affective computing research\nhas increasingly adopted thermal imaging techniques [299]. This shift was motivated by\n103104 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nstudies showing that stress and cognitive load significantly affect skin temperature [300], at-\ntributable to the autonomic nervous system\u2019s (ANS) regulation of physiological signals such\nas heart rate, respiration rate, blood perfusion, and body temperature. These signals are vital\nindicators of human emotions and affect [299]. Moreover, muscle contractions can alter fa-\ncial temperature by transferring heat to the facial skin [301]. Consequently, thermal imaging\nhas emerged as a viable method for recording transient facial temperatures [302]. Research\nby the authors in [303] on thermal imaging and facial action units to evaluate emotions, such\nas frustration, boredom, and enjoyment, indicated that a multimodal approach yielded the\nmost accurate results. Within pain research, thermal imaging has been explored in limited\nstudies. For instance, [304] reported increased facial temperature in response to painful stim-\nuli, suggesting thermal cameras as effective tools for monitoring pain. Another study [305]\nintroduced a pain dataset consisting of RGB, thermal, and depth videos, finding that while\nthe RGB modality slightly outperformed the others, integrating all modalities provided the\nbest results. This prompted us to explore the specific modality of thermal imagery through\nthe prism of synthesis using generative deep learning.\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks\nWe present a process of creating synthetic thermal videos using GANs in unimodal and\nmultimodal settings alongside RGB video modalities. Integrating a Vision Multilayer Per-\nceptron (MLP) model with a transformer-based module is at the core of our automatic pain\nassessment framework. Key contributions of this research include (1) generating synthetic\nthermal videos to enhance pain assessment as an additional vision modality, (2) assessing\nthe effectiveness of RGB and synthetic thermal videos as standalone modalities, (3) exam-\nining the utility of thermal-related information for pain assessment, and (4) evaluating the\nperformance and implications of the newly developed Vision-MLP architectures.\n6.2.1 Methodology\nThis section outlines the generation of synthetic thermal videos, which will be utilized sub-\nsequently and incorporated into an automatic assessment pipeline.\nSynthetic Thermal Videos\nAn image-to-image translation (I2I) approach has been utilized to create synthetic thermal\nvideos. I2I generative models are designed to bridge different image domains by learning the\ndata distributions inherent to each domain. Here, the source domain comprises RGB images;\nthe target domain is thermal images. In this research, conditional generative adversarial\nnetworks (cGANs) [306] were employed and trained in a supervised manner using paired6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 105\nimages. Fig. 6.1 provides a high-level overview of the method. The generator Gproduces\nimages that appear realistic, whereas the discriminator Dworks to differentiate between\ngenuine and synthetic images through the following minimax game:\nmin\nGmax\nDLcGANpG, Dq, (6.1)\nwhere the objective function LcGANpG, Dqis defined as:\nEx,yrlogDpx, yqs`Ex,zrlogp1\u00b4Dpx, Gpx, zqqqs, (6.2)\nwithxdenoting the actual data, yindicating the target data, and zrepresenting the random\nnoise vector. Here, Gseeks to minimize the objective function, whereas Doperates in\nopposition, striving to maximize it. Additionally, we incorporated the Wasserstein gradient\npenalty (WGAN-GP) [307] to enhance the stability of training. The overall objective is\narticulated as:\nLcGANpG, Dq`\u03bbE\u02c6x,yrp}\u2207\u02c6xDp\u02c6x, yq}2\u00b41q2s, (6.3)\nwhere \u03bbis the penalty coefficient. In the architectural design of our proposed method, which\ndraws inspiration from [308], the generator Gis divided into three distinct modules: an en-\ncoder, which includes two convolutional layers that downsample the input; a middle ResNet\nmodule, featuring nine residual blocks, each equipped with two convolutional layers; and a\ndecoder that upsamples the feature maps back to the final resolution ( i.e.,256\u02c6256) for the\nsynthetic sample. The discriminator D, based on the approach outlined in [309], employs a\npixel-level PatchGAN strategy using 1\u02c61kernels and consists of two convolutional layers.\n6.3 Combination of RGB and Synthetic Thermal Videos\n6.3.1 Methodology\nThis section presents the structure of the proposed automatic pain assessment framework,\nthe augmentation techniques developed, the pre-processing methods employed, and the pre-\ntraining strategy for the modules.\nFramework Architecture\nThe proposed framework consists of two primary modules: a Vision-MLP model that acts\nas a spatial embedding extractor for individual video frames and a transformer-based model\nthat functions as a temporal module, using the embedded representations of the videos for\ntemporal analysis and final pain assessment. Fig. 6.2 displays the modules and their main\ncomponents.106 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nDecoder\n\u2026Encoder\n\u2026Residual blocks\nDiscriminator\nGenerator\nAuthentic/Synthetic?\nFigure 6.1: Illustration of the procedure for creating thermal images, featuring the architecture\nof the Generator G(Encoder, mid-stage ResNet, Decoder), and the Discriminator\nD.\nVision-MLP: MLP-like models have recently emerged as a novel class of vision models,\nproviding an alternative to traditional Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViT). These models are characterized by their straightforward architectures,\nwhich consist of fully connected layers combined with activation functions. They possess\na lower level of inductive bias and rely on basic matrix multiplication operations. Our\nmethodology is grounded in the principles outlined in [310] that introduced the Vision-\nMLP, and [311] that incorporates a wave representation for the patches (also known as\ntokens). Each video frame is initially partitioned into nnon-overlapping tokens Fm\u201c\nrfm,1, fm,2, . . . , f m,nsPRn\u02c6p\u02c6p\u02c63, where pspecifies the resolution of each token, i.e.,16\u02c616\npixels, and 3represents the number of color channels. Each token is subsequently linearly\nprojected into a dimension d\u201c768prior to entering the Vision-MLP (refer to Fig. 2a). The\nfirst principal sub-module, the Channel-Mixer (Fig. 2c), operates independently on each\ntoken fj, enabling interactions among different channels, and is formulated as:\nChannel-Mixer pfj, Wcq\u201cWcfj (6.4)\nwhere Wcdenotes the weight matrix with learnable parameters, and j\u201c1,2, . . . , n . Fol-\nlowing this, the next significant sub-module, the Token-Mixer (Fig. 2b), facilitates commu-\nnication among various tokens, aiding in the extraction of features from different spatial6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 107\nVision-MLPLinear ProjectionToken MixerChannel Mixer\nNormNorm2x\n3xStage-1Token MixerChannel Mixer\nNormNorm4xStage-2Token MixerChannel Mixer\nNormNorm18xStage-3Token MixerChannel Mixer\nNormNorm 3xStage-4\nInput Imagea\nChannel-MixerMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLPChannelsTokensSkip-connectionscMLP\nFully-connectedGELUFully-connectedd\nChannel-MixerWave-BlockWave-Block\nToken-MixerTokensChannels\n......FCNb\nTransformer\n\u2026\nVision-MLP\nWeighted Fusion\nPain Assessment\nCross-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN1x\n8x\n8x4x\n8xe\nFigure 6.2: Representation of the proposed framework, illustrating its components and their\nmain functions: (a)The Vision-MLP module, tasked with extracting feature em-\nbeddings from video frames. (b)The Token-Mixer , an important sub-module of\nVision-MLP , generates the wave representation for the tokens. (c)The Channel-\nMixer , a crucial sub-module within Vision-MLP .(d)The MLP, a core component\nof the Channel-Mixer .(e)The fusion procedure that combines RGB and synthetic\nthermal embeddings, succeeded by the Transformer module, which conducts the\nfinal pain assessment.\nlocations. Typically, in MLP-based models, the token mixers are defined as:\nToken-MixerpF, Wtqj\u201c\u00ff\nkWt\njkdfk, (6.5)\nwhere Wtis the corresponding weight matrix for the tokens, and drepresents element-\nwise multiplication. Our proposed approach modifies tokens into wave-like representations\nto dynamically adjust the interactions between tokens and weights based on their semantic\ncontent. To depict a token fjas a wave \u02dcfjvia a wave function, both amplitude and phase\ninformation is necessary:\n\u02dcfj\u201c|fj|dei\u03b8j. (6.6)\nHere, iis the imaginary unit satisfying i2\u201c \u00b41. The term|fj|indicates the amplitude of\nthe signal, while ei\u03b8jis a periodic function, with \u03b8jrepresenting the phase of the signal. The108 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nabsolute value operation is typically omitted and substituted with 6.4 for simplicity. Each to-\nken\u2019s phase \u03b8jreflects its position within the wave cycle and can, therefore, be characterized\nusing fixed parameters, which are adjustable during the training phase. As such, 6.4 is also\napplied for phase estimation. Given that 6.6 characterizes a wave within the complex domain,\nthe Euler formula is employed to embed tokens within the neural network architecture:\n\u02dcfj\u201c|fj|dcos\u03b8j`i|fj|dsin\u03b8j. (6.7)\nCombining 6.5 and 6.7, a token is represented as:\nfj\u201c\u00ff\nWt\njkfkdcos\u03b8k`Wi\njkfkdsin\u03b8k (6.8)\n\u00f9\u00f1\u00ff\nWt\njkfkdcospWcfkq`Wi\njkfkdsinpWcfkq (6.9)\nwhere Wt,Wc, and Wiare the learnable weight matrices. The described process focuses\non wave-like representations and is conducted within the Token-Mixer , particularly in the\nWave-Block . The Token-Mixer architecture consists of three blocks: two Wave-Blocks and\noneChannel-Mixer operating in parallel. The Vision-MLP module is organized into four\nstages, each featuring a sequence of a Token-Mixer and a Channel-Mixer block, preceded\nby a normalization layer. The depth of parallel blocks in each stage is 3,4,18, and 3, re-\nspectively, allowing for hierarchical embeddings extraction with corresponding dimensions\nacross stages 64,128,320, and 100.\nFusion: For each input frame, the Vision-MLP extracts an embedding of dimensionality d\u201c\n100. The embeddings from the individual frames of a specific video are then concatenated\nto form a comprehensive embedding representation of the video:\nVD\u201crd1}d2}\u00a8\u00a8\u00a8} dms,VDPRN, (6.10)\nwhere mrepresents the number of frames in the video, and Nis the dimensionality of the\nultimate embedding. Following this, the embeddings from the RGB and synthetic thermal\nvideos are combined through a weighted fusion process:\nVFused\u201cw1\u00a8VRGB`w2\u00a8VThermal ,VFusedPRN. (6.11)\nThis fusion strategy integrates the respective embeddings using learned weights w1andw2,\nwhich adjust the influence of the RGB and thermal embeddings, respectively. This weighted\nsummation achieves an effective integration, capturing the relevance of each modality in the\nresultant fused representation VFused .6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 109\nTransformer: The fused embeddings are then input into a transformer-based module that\nincludes self-attention and cross-attention blocks (Fig. 2e). The self-attention mechanism is\nexpressed as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dk\u02d9\nV. (6.12)\nIn this formulation, QPRM\u02c6C,KPRM\u02c6C, and VPRM\u02c6Care the Query, Key, and Value\nmatrices respectively, where Mindicates the input dimension, and Crepresents the channel\ndimension. The cross-attention mechanism also utilizes a dot product operation; however,\nQfor cross-attention is dimensioned N\u02c6Crather than M\u02c6C, with N\u0103Mproviding\na reduction in computational cost. Each self-attention and cross-attention block features 1\nand8attention heads, respectively, and the entire Transformer module consists of 4parallel\nblocks. The output embeddings, with a dimensionality of 340, are used to perform the final\npain assessment via a fully connected neural network.\nAugmentation Methods: Two augmentation techniques have been implemented within the\nframework. Firstly, the method known as Basic is utilized, which combines polarity in-\nversion with the addition of noise. This approach modifies the original input embedding by\ninverting the polarity of data elements and adding random noise from a Gaussian distribution,\nthus introducing variability and perturbations. Secondly, the Masking technique involves ap-\nplying zero-valued masks to the embeddings, effectively eliminating sections of the vectors.\nThe dimensions of these masks are randomly determined, ranging from 10% to 50% of the\nembedding\u2019s total dimensions, and are randomly positioned within the embeddings.\nPre-processing: The pre-processing involves face detection to isolate the facial region. The\nMTCNN face detector [278] is used, which employs multitask cascaded convolutional neural\nnetworks to identify faces and landmarks. It is essential to highlight that the face detector\nwas applied exclusively to the individual RGB frames, and the coordinates of the detected\nface were then applied to the corresponding synthetic thermal frames. The resolution of all\nframes was standardized at 224\u02c6224pixels.\nPre-training: For the I2I approach, the SpeakingFaces dataset [312] was employed to train\nthe proposed GAN model for translating RGB to synthetic thermal videos. Additionally, be-\nfore commencing the automatic pain assessment training, the Vision-MLP andTransformer\nmodules underwent pre-training. The Vision-MLP was subject to a three-stage pre-training\nstrategy: initially, it was trained on DigiFace-1M [313] to acquire basic facial features. It\nwas then trained on AffectNet [287] and RAF Face Database basic [289] to learn features\nrelated to basic emotions through multi-task learning. Lastly, the Compound Facial Expres-\nsions of Emotions Database [288] and the RAF Face Database compound [289] were used110 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.1: Datasets utilized for the pretraining process of the framework.\nDataset # samples # classes Task\nSpeakingFaces [312] 4.58M 142 Face\u0015\nDigiFace-1M [313] 1.00M 10,000 Face\u0010\nAffectNet [287] 0.40M 8 Emotion\u0010\nCompound FEE-DB [288] 6,000 26 Emotion\u0010\nRAF-DB basic [289] 15,000 7 Emotion\u0010\nRAF-DB compound [289] 4,000 11 Emotion\u0010\n\u0015: includes face image pairs for the I2I task \u0010: includes images for face or emotion\nrecognition tasks\nto learn features of compound emotions in a similar multi-task framework. The multi-task\nlearning process is represented as:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s, (6.13)\nwhere LSis the loss for the specific task associated with different datasets, and ware the\nlearned weights that guide the learning process in minimizing the collective loss Ltotal, in-\ntegrating all the individual losses. The Transformer was pre-trained solely on the DigiFace-\n1M[313], adapting the input images into 1D vectors to suit its architectural needs. Table 6.1\noutlines the datasets utilized in the pre-training phase.\n6.3.2 Experiments\nThe proposed framework was assessed using the BioVid Heat Pain Database [109], which\ncomprises facial videos, electrocardiograms, electromyograms, and skin conductance levels\nfrom 87healthy subjects. Participants underwent heat-induced pain via a thermode on their\nright arm at five different intensities: no pain (NP), mild pain (P 1), moderate pain (P 2), se-\nvere pain (P 3), and very severe pain (P 4). Each level was applied 20times to each subject,\nresulting in 100samples per modality and a total of 1740 samples per class. Experiments\nwere structured around binary and multi-level classification schemes to assess pain, analyz-\ning each modality individually and collectively. Binary classification aimed to distinguish\nbetween no pain (NP) and very severe pain (P 4), while multi-level classification (MC) was\ntasked with categorizing all levels of pain intensity present in the dataset. The leave-one-\nsubject-out (LOSO) method was utilized for validation, and accuracy served as the metric\nfor performance evaluation. Table 6.2 outlines the training details for the automatic pain\nassessment, including parameter number and the computational cost measured in floating-\npoint operations (FLOPS) for each module.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 111\nTable 6.2: Training specifications, and number of parameters and FLOPS for each module.\nTraining Details Vision-MLP Transformer\nOptimizer: AdamW Params: 7.35 M Params: 7.96 M\nLearning rate: 2e-5 FLOPS: 30.95 G FLOPS: 30.90 G\nLR decay: cosine\nWeight decay: 0.1\nWarmup epochs: 5\nBatch size: 32\nTotal Params: 15.31 Millions FLOPS: 61.85 Giga\nTable 6.3: Results utilizing the RGB video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.37 30.23\n200 \u2713 20-50 0.7 70.26 28.50\n300 \u2713 30-50 0.9 70.05 30.02\nMasking: indicates the percentage of the input embedding to which zero-value masking is applied\nP(Aug): represents the probability of applying the augmentation methods of Basic & Masking NP: No\nPain P 4: Very Severe Pain MC: multiclass pain level\n6.3.3 Results\nRGB Videos\nWithin the context of the RGB video modality, we recorded an accuracy of 69.37% for the\nbinary classification task (NP vs. P 4) and 30.23% for the multi-class classification (MC).\nThe use of the Masking augmentation method, which obscured 20\u00b450% of the input em-\nbeddings, yielded a slight increase in binary classification accuracy by 0.89%. However, it\nled to a reduction in multi-class classification accuracy. By extending the training period to\n300epochs, modifying the Masking method to cover 30\u00b450% of the embeddings, and ap-\nplying a 90% probability to both augmentation methods, the accuracies improved to 70.05%\nand30.02% for the binary and multi-class tasks, respectively. This represents an average\naccuracy gain of just under 0.5%. The classification outcomes are detailed in Table 6.3.\nSynthetic Thermal Videos\nIn the synthetic thermal modality experiments conducted under identical conditions, the ini-\ntial accuracies were 69.97% for the binary classification and 30.04% for the multi-class clas-\nsification. Enhancing the intensity of the masking method yielded modest gains in accuracy112 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.4: Results utilizing the synthetic thermal video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.97 30.04\n200 \u2713 20-50 0.7 70.20 30.50\n300 \u2713 30-50 0.9 70.69 29.60\nof0.23% for the binary classification and 0.46% for the multi-class classification. Subse-\nquent final accuracies were 70.69% for the binary classification and 29.60% for the multi-\nclass classification, reflecting an average increment of 0.28%. This discrepancy likely arises\nfrom the challenge of detecting nuanced facial changes linked to low-level pain, exacerbated\nby more aggressive augmentation that potentially reduces performance. The summarized\nresults are presented in Table 6.4.\nAdditional Analysis on RGB & Synthetic Thermal Videos\nThe results from the previous section showed a surprising similarity in performance between\nthe RGB and synthetic thermal modalities. Specifically, the RGB modality achieved maxi-\nmum accuracies of 70.26% for the NP vs. P4 task and 30.23% for the MC task. Similarly,\nthe synthetic thermal modality reached top accuracies of 70.69% and30.50% for the same\ntasks, respectively. On average, the thermal video performances were about 1%higher than\nthose for the RGB modality. This was unexpected, considering the synthetic modality was\ninitially considered inferior to the original. This prompted further investigation into why\nsynthetic modalities might perform comparably to or better than the original RGB modality.\nA key question involved the relevance and efficacy of the thermal-related data featured in the\nsynthetic videos. The theory proposed that minimizing facial expressions in thermal videos\ncould enhance the clarity of thermal data assessment.\nGaussian blurring was incrementally applied to RGB and synthetic thermal videos, as\nshown in Fig. 6.3, with kernel sizes increasing from 0to191. According to Table 6.5, at\na kernel size of k\u201c0, a performance differential of 0.47%, favoring the thermal modality,\nconfirms prior findings. This gap slightly expands to 0.49% atk\u201c41. Remarkably, at\nk\u201c91, the disparity enlarges to 2.13% and increases to 5.90% atk\u201c191, where blurring\nis most intense. The results reveal that as facial expressions become less visible through\nblurring, synthetic thermal videos consistently outperform RGB videos, with respective ac-\ncuracies of 66.24% versus 60.34%. As blurring intensifies from k\u201c0tok\u201c191, accuracy\nrates for synthetic thermal and RGB modalities decrease by 1.81% and7.13%, respectively.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 113\nFigure 6.3: Gradual blurring of RGB and synthetic thermal facial images: a series displaying\nvarying levels of Gaussian blur applied, with kernel sizes gradually increased from\nk\u201c0(no blur) to k\u201c191(extensively blurred).\nTable 6.5: Results utilizing the RGB & the synthetic thermal video.\nEpochs Modality BlurAugmentations Task\nBasic Masking P(Aug) NP vs P 4\n100 RGB 0 \u2713 10-20 0.7 67.47\n100 Thermal 0 \u2713 10-20 0.7 68.05\n100 RGB 41 \u2713 10-20 0.7 66.61\n100 Thermal 41 \u2713 10-20 0.7 67.10\n100 RGB 91 \u2713 10-20 0.7 64.80\n100 Thermal 91 \u2713 10-20 0.7 66.93\n100 RGB 191 \u2713 10-20 0.7 60.34\n100 Thermal 191 \u2713 10-20 0.7 66.24\nBlur: Gaussian blurring with kernel sizes k\nThis suggests that critical information, such as visually represented facial temperature in\nthe synthetic modality, is unaffected or slightly impacted. Fig. 6.4 displays the embedding\ndistribution for the RGB and synthetic thermal modalities at k\u201c0andk\u201c191, highlight-\ning a distinct variation in distribution patterns, albeit with ambiguous data point separation.\nFork\u201c191, while RGB embeddings tend to cluster and potentially overlap, many points\nconspicuously stray from the central mass without any distinct arrangement. Conversely, the\ndata points for the synthetic modality spread more uniformly, possibly indicating better class\ndifferentiation.114 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nRGB-0\nThermal-0 Thermal-191RGB-191\nNP\nP4\nFigure 6.4: Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain) classes\nin RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191(heavily\nblurred).\nFusion\nThree fusion strategies were assessed for multimodal analysis involving RGB and synthetic\nthermal video data. Initially, the fusion strategy referenced in 6.11 utilized learned weights\nw1andw2to scale the contributions of each modality. A second strategy incorporated an\nadditional weight, w3, modifying the formula to w3\u00a8pw1\u00a8VRGB`w2\u00a8VThermalq. The\nthird method bypassed learned weights altogether, directly combining the embedding vectors\nfrom the modalities. The results, detailed in Table 6.6, indicate that omitting learned weights\nachieved accuracies of 64.92% and26.40% for the binary and multi-class tasks, respectively.\nThe introduction of w3reduced 0.5%in accuracy for both tasks. The strategy using only\nweights w1andw2yielded the best outcomes, with accuracies of 65.08% and26.50% for\nthe binary and multi-class tasks, respectively. By maintaining the use of weights w1and\nw2and increasing the training duration from 100to300epochs, while consistent with the\naugmentation settings, accuracies improved to 69.50% and29.80% for the binary and multi-\nclass tasks, respectively. Further prolonging the training to 500epochs, with no evidence\nof overfitting, led to further improved performances, with final accuracies of 71.03% and\n30.70% for the respective tasks.6.4. SUMMARY 115\nTable 6.6: Results utilizing the fusion of RGB & synthetic thermal video.\nEpochsFusion\nweightsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n100 \u2013 \u2713 10-20 0.7 64.92 26.40\n100 W2 \u2713 10-20 0.7 65.08 26.50\n100 W3 \u2713 10-20 0.7 64.42 25.90\n300 W2 \u2713 10-20 0.7 69.50 29.80\n500 W2 \u2713 10-20 0.7 71.03 30.70\nW2: utilization of [w 1,w2] W3: utilization of [w 1,w2,w3]\nComparison with Existing Methods\nThis section compares the proposed method with other methodologies reported in the liter-\nature. We utilize Part A of the BioVid dataset, which includes all 87subjects, employing\nthe LOSO cross-validation protocol for validation. The results are displayed in Table 6.7.\nOur vision-based approach, which leverages RGB and synthetic thermal modalities, shows\nperformances comparable to or surpass those of prior studies. Relative to results in the lit-\nerature from [180, 217, 219, 295], our method achieved higher accuracy in both binary and\nmulti-level tasks. Notably, the research in [296] recorded accuracies of 72.40% and30.80%,\nmarking an improvement of 1.37% and0.10% over our method, respectively. The highest\nreported results are from [37], which utilized a transformer-based architecture.\nAdditionally, Table 6.8 compares our results with those from [305], where the authors\nintroduced the MIntPAIN dataset comprising both RGB and thermal videos for automatic\npain assessment across five intensity levels. Our analysis revealed that the accuracies of\nthe RGB and thermal modalities were closely matched at 18.55% and18.33%, respectively,\nwhich parallels our observations of similar performance between RGB and synthetic ther-\nmal modalities. By integrating these modalities, the authors in [305] reported a significant\nperformance increase of 30.77%, surpassing our modest gains. It should be emphasized that\nin [305], the performance levels of the individual modalities were below the random guess\nprediction threshold of 20%. It was only through their combination that performance was\nelevated above this threshold.\n6.4 Summary\nThis chapter investigated the creation of synthetic thermal imagery via GAN models to as-\nsess its utility in automatic pain evaluation. Additionally, a novel framework incorporating\naVision-MLP and supported by a Transformer module as the core of the assessment sys-\ntem was introduced. The experiments demonstrated the effectiveness of the synthetic ther-116 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.7: Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.\nStudy MethodTask\nNP vs P 4 MC\nWerner et al. [296] Deep RF 72.40 30.80\nWerner et al. [295] RF 70.20 \u2013\nZhiet al. [217] SLSTM 61.70 29.70\nThiam et al. [219] 2D CNN, biLSTM 69.25 \u2013\nTavakolian et al. [180] 2D CNN 71.00 \u2013\nGkikas et al. [37] Vision-Transformer 73.28 31.52\nOur Vision-MLP 71.03 30.70\nTable 6.8: Comparison with the MIntPAIN dataset.\nStudy Dataset ModalityTask\nMC\nHaque et al. [305] MIntPAINRGB 18.55\nThermal\u02dd18.33\nFusion 30.77\nOur BioVidRGB 30.02\nThermal\u203929.69\nFusion 30.70\n\u02dd:real\u2039: synthetic\nmal modality, which achieved performances comparable to or better than the original RGB\nmodality. This research also delved into the factors contributing to this effectiveness, par-\nticularly the role of temperature color representations in the analysis. Furthermore, various\nfusion techniques were used to evaluate the combination of the two vision modalities, high-\nlighting the potential for performance improvements over single-modality approaches. It\nis important to note that further enhancements and experimental work, especially with the\nmultimodal approach, could improve outcomes. The generation and integration of synthetic\nmodalities, such as thermal imagery, within an automatic pain assessment framework exhibit\nconsiderable promise, warranting additional exploration and research.Chapter 7\nGeneral-Purpose Models\nContents\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 117\n7.2", "ECG Analysis with Classical Machine Learning": ". . . . . . . . . . . . . . . 54\n4.2.1", "Methodology": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5 Optimization: Balancing Efficiency and Performance 75\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n6 Synthetic Data: The Role of Thermal Imaging 103\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7 General-Purpose Models 117\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n8 Conclusions, Perspectives and Future Work 157\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nBibliography 163\nAppendix 201\nAcronyms 209List of Figures\n2.1 The spinothalamic tract (STT) [43]. Pain, temperature, and some touch affer-\nents end in the posterior horn, where second-order fibers cross the midline\nto form the spinothalamic tract, ascending to the thalamus and projecting to\nvarious cortical areas. Along the way, collaterals connect to the reticular for-\nmation. Due to the rostral inclination of fibers in Lissauer\u2019s tract, cordotomy\nmust be performed several segments above the pain level for effective relief. 12\n2.2 Pain classification [48]: (A)Nociceptive pain , which results from detecting\npotentially harmful stimuli and serves a protective function. (B)Inflamma-\ntory pain is linked to tissue damage and immune cell infiltration, increas-\ning pain sensitivity during healing. (C)Pathological pain is a disease state\ncaused by either nervous system damage (neuropathic) or abnormal nervous\nsystem function (dysfunctional). . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 The number of studies utilizing these specific datasets. Note that various\nstudies used multiple datasets to conduct their experiments. . . . . . . . . . 25\n4.1 The PQRST waveform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure. . 55\n4.3 The signal preprocessing using the Pan-Tompkins algorithm. . . . . . . . . 57\n4.4 Results for the Gender Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Results for the Age Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.6 Results for the Gender-Age Scheme . . . . . . . . . . . . . . . . . . . . . . 64\n4.7 The proposed MTL network: The sizes of the extracted vectors for the net-\nwork are as follows: for the Pain classifier, n\u02c61, where nis the number of\npain estimation tasks ( e.g.,2for binary classification, 5for multi-class clas-\nsification); for the Age classifier, 36\u02c61, where 36represents the possible\nage values of the subjects; for the Gender classifier, 2\u02c61, corresponding to\nthe two possible gender categories ( i.e., males and females). . . . . . . . . 66\n4.8 Results for the proposed Schemes. . . . . . . . . . . . . . . . . . . . . . . 69\n4.9 Comparison of performances utilizing various neural networks approaches. 72\nxix5.1 The application of face alignment illustrates landmarks in 2D (left) and 3D\n(right) space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 An overview of our proposed transformer-based framework for automatic\npain assessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 The impact of the number of input frames on accuracy (left) and on runtime\nin milliseconds (right). Runtime calculated during inference on a NVIDIA\nRTX-3090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Relevance Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5 Outline of the proposed framework. . . . . . . . . . . . . . . . . . . . . . 86\n5.6 Comparison of mean accuracy and inference period for unimodal and multi-\nmodal strategies across NP versus P 4and MC tasks. The diagram adopts a\ndual-y-axis configuration\u2014accuracy measurements on the left and time met-\nrics on the right\u2014to outline the balance between performance efficacy and\ncomputational load, categorizing the methodologies along the x-axis. . . . . 98\n5.7 Regions highlighted in yellow and red denote areas of significant attention.\n(a) (1strow) Sequence of original frames. (2ndrow) Derived from the\nSpatial-Module after initial stage pretraining. (3rdrow) Derived from the\nSpatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module trained on the BioVid dataset. (b) (1strow) Derived from\ntheTemporal-Module incorporating video embeddings. (2ndrow) Derived\nfrom the Temporal-Module with heart rate embeddings. (3rdrow) Derived\nfrom the Temporal-Module using a combined embedding of video and heart\nrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.1 Illustration of the procedure for creating thermal images, featuring the archi-\ntecture of the Generator G(Encoder, mid-stage ResNet, Decoder), and the\nDiscriminator D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2 Representation of the proposed framework, illustrating its components and\ntheir main functions: (a)The Vision-MLP module, tasked with extracting\nfeature embeddings from video frames. (b)TheToken-Mixer , an important\nsub-module of Vision-MLP , generates the wave representation for the tokens.\n(c)The Channel-Mixer , a crucial sub-module within Vision-MLP .(d)The\nMLP, a core component of the Channel-Mixer .(e)The fusion procedure\nthat combines RGB and synthetic thermal embeddings, succeeded by the\nTransformer module, which conducts the final pain assessment. . . . . . . . 107\n6.3 Gradual blurring of RGB and synthetic thermal facial images: a series dis-\nplaying varying levels of Gaussian blur applied, with kernel sizes gradually\nincreased from k\u201c0(no blur) to k\u201c191(extensively blurred). . . . . . . 1136.4 Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain)\nclasses in RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191\n(heavily blurred). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1 PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer hav-\ning varying depths, showcasing how token resolution decreases at each stage;\n(b)Composition of the Token-Mixer module, featuring elements like depth-\nwise convolution (DWConv) and batch normalization; (c)Architecture of the\nFeed-Forward Network (FFN) within the Token-Mixer ;(d)The Cascaded\nAttention mechanism implemented across multiple heads, illustrating how\noutputs from preceding heads are incorporated to refine the self-attention\nprocess, culminating in the final output projection; (e)Configuration of the\nproposed multimodal pipeline, employing videos and fNIRS. The embed-\ndings from PainViT\u20131 are represented as waveform diagrams, which are\nmerged into a single diagram that illustrates both modalities before entering\nPainViT\u20132 for final pain evaluation. . . . . . . . . . . . . . . . . . . . . . . 151\n7.2 Waveform illustrations for various data types: (a)original fNIRS signal,\n(b)video embedding derived from PainViT\u20131 , and (c)fNIRS embedding\nobtained from PainViT\u20131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n7.3 Attention maps from the PainViT\u20132 . . . . . . . . . . . . . . . . . . . . . . 152\n7.4 Overview of primary models and their components outlined in this research:\n(a)PainFormer is structured hierarchically into four stages, incorporating\nSpectral andSelf-Attention Layers to extract embeddings from the inputs;\n(b)The Spectral Layer , a key element of PainFormer , uses FFT to ana-\nlyze frequency-specific data along with a learnable filter Kto highlight\ncritical frequencies; (c)The Self-Attention Layer , crucial for PainFormer ,\nenables parallel processing of features and their interconnections; (d)The\nEmbedding-Mixer , employing both cross and self-attention mechanisms, func-\ntions as the component for the final classification of embeddings in pain as-\nsessment; (e)TheVideo-Encoder , designed for compact and efficient encod-\ning, compresses video data into a reduced dimensional form; (f)TheMLP-1\nis part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)TheMLP-3 configuration is integrated into the Embedding-Mixer\nandVideo-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n7.5 Examples of different vision modalities in frame samples: (a)RGB frame,\n(b)synthetic thermal frame, and (c)depth estimation frame. . . . . . . . . 153\n7.6 Examples of different visual representations for biosignals: (a)waveform ,\n(b)spectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD . . 1547.7 An overview of the presented framework. PainFormer , the foundational\nmodel, excels in deriving high-quality embeddings from a diverse array of\nbehavioral and physiological modalities. The evaluation of RGB, thermal,\nand depth videos, alongside various representations of ECG, EMG, GSR,\nand fNIRS such as waveforms and spectrograms, underscores the rich infor-\nmation captured within these embeddings. Leveraging the embeddings from\nPainFormer facilitates the creation of various and diverse unimodal and mul-\ntimodal pipelines designed for the pain assessment task. Each pipeline can\nbe customized to suit the specific modalities involved, dataset characteristics,\nand the demands of the intended application or clinical setting. Our assess-\nments included the development and implementation of several pipelines\nin both unimodal and multimodal contexts, achieving leading-edge results\nacross various modalities and data representations. . . . . . . . . . . . . . 154\n7.8 Attention maps from the PainFormer :(a)(1strow) frames from RGB, ther-\nmal, and depth video modalities; (a)(2ndrow) corresponding attention maps;\n(b)(1strow) attention maps for ECG and EMG; (b)(2ndrow) attention maps\nfor EDA and fNIRS modalities. . . . . . . . . . . . . . . . . . . . . . . . . 155\n1 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n2 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n3 Additional attention maps from the PainViT\u20132 (refer to Section 7.2). . . . . 207List of Tables\n3.1 Most commonly utilized pain databases. . . . . . . . . . . . . . . . . . . . 24\n3.2 Vision-based studies with static analysis. . . . . . . . . . . . . . . . . . . . 32\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 33\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 34\n3.3 Vision-based studies with temporal utilization. . . . . . . . . . . . . . . . . 39\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 40\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 41\n3.4 Touch sensor-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Audio-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6 Multimodal-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.7 Interpretation approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1 Results for the Basic Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Results for the Gender Scheme (1). . . . . . . . . . . . . . . . . . . . . . . 60\n4.3 Results for the Age Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Results for the Gender-Age Scheme (Males) (1). . . . . . . . . . . . . . . . 62\n4.5 Results for the Gender-Age Scheme (Females) (1). . . . . . . . . . . . . . . 63\n4.6 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (1). 63\n4.7 Hyper-parameters used in our approach. . . . . . . . . . . . . . . . . . . . 65\n4.8 Results for the Basic Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . 68\n4.9 Results for the Gender Scheme (2). . . . . . . . . . . . . . . . . . . . . . . 68\n4.10 Results for the Age Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.11 Results for the Gender-Age Scheme (2). . . . . . . . . . . . . . . . . . . . 68\n4.12 Comparison of results adopting the feature augmentation approach. . . . . . 70\n4.13 Comparison of results adopting the MT-NN approach. . . . . . . . . . . . . 71\n4.14 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2). 72\n5.1 Training details for the automatic pain assessment. . . . . . . . . . . . . . 79\n5.2 Results on the pain estimation tasks. . . . . . . . . . . . . . . . . . . . . . 81\n5.3 Results for the pain estimation tasks using various numbers of input frames. 82\n5.4 Comparison of studies utilizing BioVid , RGB videos, and LOSO validation. 84\n5.5 Datasets utilized for the pre-training process of the framework. . . . . . . . 91\nxxiii5.6 Training details for the automatic pain assessment. . . . . . . . . . . . . . 92\n5.7 Results utilizing the video modality. . . . . . . . . . . . . . . . . . . . . . 93\n5.8 Results utilizing the heart rate modality. . . . . . . . . . . . . . . . . . . . 95\n5.9 Results utilizing the video &the heart rate modality. . . . . . . . . . . . . . 95\n5.10 Comparison of studies utilizing BioVid &LOSO validation, reported on ac-\ncuracy %. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.11 Module parameters and computational cost in FLOPS for the proposed frame-\nwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.1 Datasets utilized for the pretraining process of the framework. . . . . . . . 110\n6.2 Training specifications, and number of parameters and FLOPS for each mod-\nule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.3 Results utilizing the RGB video. . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Results utilizing the synthetic thermal video. . . . . . . . . . . . . . . . . . 112\n6.5 Results utilizing the RGB &the synthetic thermal video. . . . . . . . . . . . 113\n6.6 Results utilizing the fusion of RGB &synthetic thermal video. . . . . . . . 115\n6.7 Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.116\n6.8 Comparison with the MIntPAIN dataset. . . . . . . . . . . . . . . . . . . . 116\n7.1 Number of parameters and FLOPS for the components of the proposed Twins-\nPainViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.2 Datasets utilized for the pretraining process of the framework. . . . . . . . 123\n7.3 Training details for the automatic pain assessment. . . . . . . . . . . . . . 124\n7.4 Results utilizing the video modality & Addition method. . . . . . . . . . . . 125\n7.5 Results utilizing the video modality & Concatenation method. . . . . . . . . 125\n7.6 Results utilizing the HbR & Addition method. . . . . . . . . . . . . . . . . 126\n7.7 Results utilizing the HbR & Concatenation method. . . . . . . . . . . . . . 126\n7.8 Results utilizing the HbO & Addition method. . . . . . . . . . . . . . . . . 126\n7.9 Results utilizing the HbO & Concatenation method. . . . . . . . . . . . . . 127\n7.10 Results utilizing the HbR, HbO & Addition method. . . . . . . . . . . . . . 127\n7.11 Results utilizing the videos, HbO & Addition method. . . . . . . . . . . . . 127\n7.12 Results utilizing the videos, HbO & Single Diagram method. . . . . . . . . 128\n7.13 Comparison with the validation baseline provided by the AI4PAIN challenge\norganizers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.14 Number of parameters and FLOPS for the modules of the proposed framework.130\n7.15 Details of the PainFormer\u2019s architecture. . . . . . . . . . . . . . . . . . . . 132\n7.16 Datasets utilized for the multitask learning-based pretraining process of the\nframework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347.17 Training details of the proposed framework. . . . . . . . . . . . . . . . . . 135\n7.18 Results utilizing the video modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.19 Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.20 Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.21 Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.22 Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and\nF1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.23 Results on the validation set of AI4Pain dataset, multilevel classification task,\nreported on accuracy, recall and F1 score. . . . . . . . . . . . . . . . . . . 144\n7.24 Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.25 Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4\ntask and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.26 Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs.\nP4task and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.27 Comparison of studies on the testing set of AI4Pain dataset. . . . . . . . . . 148\n1 Results utilizing the video modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n2 Results utilizing the heart rate modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n3 Results utilizing the video &the heart rate modality reported on precision,\nrecall and F1 score (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . 202\n4 Results utilizing the RGB video modality, reported on recall and F1 score\n(refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n5 Results utilizing the synthetic thermal video modality, reported on recall and\nF1 score (refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . 203\n6 Results utilizing the fusion of RGB &synthetic thermal video modality, re-\nported on recall and F1 score (refer to Section 6.2). . . . . . . . . . . . . . 204\n7 Results of the proposed approaches, reported on macro-averaged precision,\nrecall and F1 score (refer to Section 7.2). . . . . . . . . . . . . . . . . . . . 204Chapter 1\nIntroduction\nContents\n1.1 Context and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Scope and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions \u2013 Peer-review Publications . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.1 Context and Motivation\nPain is a complex and deeply personal experience that is subjective by nature. Traditionally,\nit has been described in terms of its sensory dimension [1]. However, extensive research\nhas highlighted the importance of affective, cognitive and social aspects in shaping this ex-\nperience [2]. Studies have explored physiological, psychological, and socio-environmental\nfactors that contribute to the experience of pain. It is understood as a result of biological evo-\nlution and as influenced by psychological and social factors. As Ridell et al. [3] noted, \u201cPain\nis a synthesis\u2013a sum that is greater than its parts. \u201d The brain\u2019s ability to alter the perception\nof sensory inputs through the interplay of emotion, cognition, and social processes is signifi-\ncant. Although natural systems establish the initial biological framework for pain perception,\nthis structure is highly adaptable, particularly in humans. Throughout a person\u2019s life, both\nbiological developments and personal experiences significantly reshape this framework.\nA key question driving pain research across biological, psychological, and computational\nfields is why this topic of pain is meaningful and important. This question also forms the\nbasis for initiating this thesis, highlighting the broader relevance of studying pain. Williams\nand Kappesser [4] provide a compelling explanation, stating, \u201cWe care because we are wired\nto care: to attend to other people\u2019s expression of pain and to understand its meaning; to feel\ndistress in relation to their distress; and to be motivated to reduce their distress, and ours,\nif we are able to do so. \u201d This highlights the intrinsic human response to empathize and\n12 CHAPTER 1. INTRODUCTION\nalleviate pain, underlining the fundamental importance of this research area. Indeed, from a\nDarwinian perspective, pain serves a crucial role. The manifestation of pain in humans and\nthe reactions it elicits are examined through an evolutionary lens. Pain facilitates recovery by\npromoting responses to harmful stimuli and behaviors that demonstrate the adverse nature\nof painful experiences, common among animals. Specifically, the facial expression of pain,\nwhich communicates discomfort directly to those nearby, is universally recognized across\ndifferent ages, ethnicities, roles, and relationships. Evidence from healed major fractures\n[5, 6] suggests that injured members of hominid groups were not left to fend for themselves\nbut were supported through their recovery, indicating the fundamental importance of pain\nexpression in our evolutionary history.\nPain is a widespread health concern globally, affecting up to 30% of the adult popula-\ntion [7] and between 83% and93% of elderly adults in residential care [8]. The Global Bur-\nden of Disease (GBD) study identifies pain as the primary cause of years lived with disability\n(YLD) [9], with major contributors including chronic back pain, musculoskeletal disorders,\nand neck pain [10]. Pain impacts individuals and poses significant clinical, economic, and\nsocial challenges. In the United States, the economic and healthcare costs related to pain\ndue to reduced work productivity range from $560 to$635 billion annually, surpassing the\ncosts associated with heart disease, cancer, and diabetes combined [11]. In Europe, chronic\npain\u2019s direct healthcare costs and indirect socioeconomic impacts account for 3%to10% of\nthe GDP [12]. In Australia, the average annual cost for individuals among the 15.4%living\nwith chronic pain ranges from AU $22,588to AU $42,979, including non-financial costs [13].\nBeyond direct effects on health, pain contributes to a range of adverse outcomes, such as opi-\noid dependency, drug overuse, addiction, declining social relationships, and psychological\ndisorders [14]. In the last two decades, prescription opioid use has surged in the United\nStates, where overdose deaths have increased more than fourfold from 1999 to2016 [15].\nAdditionally, side effects from these opioids, like lethargy, depression, anxiety, and nausea,\nseverely impact workforce productivity and overall life quality [16].\nAccurate pain assessment is crucial for early diagnosis, disease progression monitoring,\nand treatment effectiveness evaluation, particularly in managing chronic pain [17]. This criti-\ncal role has resulted in pain being recognized as \u201cthe fifth vital sign\u201d in nursing literature [18].\nPain assessment is also fundamental in physiotherapy, where therapists apply external stim-\nuli and need to gauge the patient\u2019s pain levels accurately [19]. Objective evaluation of pain is\nessential to provide appropriate care, especially for vulnerable populations who may not be\nable to communicate their pain effectively, such as infants, young children, individuals with\nmental health issues, and the elderly. Various methods are used for pain assessment, with\nself-reporting\u2013where individuals describe their pain experiences\u2013considered the gold stan-\ndard [20]. Pain evaluation methods in clinical environments include quantifiable measures\nlike the Numeric Pain Rating Scale (NPRS), Visual Analogue Scale (V AS), and quantitative1.1. CONTEXT AND MOTIVATION 3\nsensory testing techniques such as the pressure pain detection threshold (PPDT) [21]. Behav-\nioral indicators are also crucial and include facial expressions ( e.g., grimacing, open mouth,\nlifted eyebrows), vocalizations (like crying, moaning, or screaming), and movements of the\nbody and head [22]. Physiological measures such as electrocardiography (ECG), electromyo-\ngraphy (EMG), galvanic skin responses (GSR), and respiration rates further contribute to\nunderstanding pain\u2019s physiological aspects [17]. Additionally, brain monitoring techniques\nlike near-infrared spectroscopy (fNIRS) have effectively detected changes in hemodynamic\nactivity associated with pain stimuli [23].\nCaregivers and family members often determine the presence or absence of pain in pa-\ntients by observing their behavioral or physiological responses [17]. However, accurately\nassessing pain poses a significant challenge for clinicians [24], especially with nonverbal\npatients such as the elderly, who may have reduced expressive abilities or may be reluctant\nto communicate pain [25]. Extensive research indicates that pain manifestations vary signif-\nicantly across different genders and ages, adding to the complexity of its assessment [26].\nFurther complicating the assessment process are the heightened workload and fatigue ex-\nperienced by nursing staff due to the demands of patient monitoring [27]. Technological\nsolutions are necessary for continuous patient monitoring. Nevertheless, concerns remain\nabout the objectivity and accuracy of these observations, as inadequately trained or biased\nobservers may struggle to assess pain [28] accurately. Even among trained observers, in-\nterpretations of behaviors can vary [22], and social and interpersonal dynamics can signif-\nicantly affect the pain assessment process, influencing both the evaluators\u2019 judgments and\nthe patients\u2019 expressions of pain [29]. Additionally, the presence of an observer can lead pa-\ntients to modify their behavior [30], and expressing pain through scales and measurements\ncan be challenging [31]. While self-reporting is used because pain is inherently subjective,\nrelying solely on a one-dimensional pain score fails to capture this complex phenomenon,\noften leading to inadequate pain management [32].\nGiven the challenges described above, scientific computing (SC) researchers have fo-\ncused on developing models and algorithms to enhance automatic pain recognition systems\nover the last two decades. Their goal is to accurately determine the presence and intensity\nof pain by analyzing physiological and behavioral indicators. Adopting deep learning and\nartificial intelligence (AI) techniques has expanded these automatic methods, designed to\ninterpret the complex and varied nature of pain [17]. Numerous studies have underscored\nthe effectiveness of automated systems that utilize behavioral or physiological modalities\nfor pain assessment [33]. Sario et al. [34] have shown the capability of these systems to\naccurately recognize pain through facial expressions, proving their utility in clinical envi-\nronments. Multimodal sensing has shown particular promise, offering enhanced accuracy\nin pain detection systems [22]. Furthermore, including temporal aspects in these modalities\nhas proven to significantly improve the accuracy of pain assessments [17].4 CHAPTER 1. INTRODUCTION\n1.2 Scope and Challenges\nAlthough considerable research has been conducted on automatic pain assessment, studies\nhave yet to explore factors like demographics and social aspects from a computational angle.\nFurthermore, despite the existence of deep learning-based methods, the approaches we ob-\nserve are often outdated and repeatedly recycled. For these reasons, we aimed to address two\nissues by (i)attempting to evaluate the social or demographic context, which significantly\nimpacts and influences pain sensation and perception, and (ii)introducing innovative deep\nlearning methods inspired by the latest developments in AI and generative AI literature. We\nbelieve these approaches can forge new paths in pain research, enhance the accuracy of rec-\nognizing this complex phenomenon, and, ultimately, be adopted in real-world scenarios to\nassist those in need. Additionally, (iii)recognizing the skepticism towards new technologies\namong clinicians and the general public, especially regarding the limited understanding of\nhow deep learning models function, we have devoted a portion of our research to interpret-\ning these models to offer some level of explanation and help the adoption process of them in\nclinical settings.\nNevertheless, this thesis initially faced challenges related to our objectives and goals as\nthe research progressed. The availability of pain datasets (to be discussed in the next chapter)\nis limited. Only a few datasets are available, and crucially, they are limited in size. This re-\nstriction poses a significant challenge for developing deep learning models, which typically\nrequire a large volume of data. In automatic pain assessment, researchers who develop deep\nlearning methods typically confront a decision: either train their models from scratch, which\ncan introduce performance limitations, or employ pre-trained models. These pre-trained\nmodels are generally trained on broadly available image datasets that include a variety of\nsubjects like animals and objects, or they rely on older architectures that were trained explic-\nitly on facial datasets. In this thesis, we addressed these issues by independently pre-training\nour deep-learning models using diverse datasets related explicitly to human facial images\nand biosignals. This strategy allowed us to design specific architectures to meet our unique\nneeds for each scenario, free from the constraints of relying on models developed and trained\nby others. Furthermore, we explored and evaluated several pre-training techniques to assess\ntheir effectiveness in pain assessment applications.\nRegarding, our objective to explore methods that utilize various modalities individually\nand in combination in a multimodal manner further constrains our dataset options. More-\nover, as previously outlined, our interest in the sociodemographic aspects of pain necessitates\ndatasets that include this information type, intensifying our challenges. For these reasons,\nthis thesis focuses specifically on examining the impact of age and gender on pain. In addi-\ntion, led us to utilize two pain datasets that most closely match the characteristics necessary\nfor our research, particularly in terms of demographic elements and multimodality.1.3. CONTRIBUTIONS \u2013 PEER-REVIEW PUBLICATIONS 5\n1.3 Contributions \u2013 Peer-review Publications\nThis section outlines the publications and projects produced during the Ph.D. research on\nautomatic pain assessment, where I was the first author.\n1.Automatic assessment of pain based on deep learning methods: A systematic re-\nview [17]\nThis systematic literature review (SLR) was conducted at the start of this Ph.D. re-\nsearch. This paper aims to explore the surge in recent years of deep learning algorithms\nadopted by researchers to encode the multidimensional nature of pain into meaning-\nful features. Specifically, this systematic review examines the models, methods, and\ndata types used to establish the foundation for deep learning-based automatic pain\nassessment systems. It identified relevant original studies from digital libraries such\nasScopus ,IEEE Xplore , and ACM Digital Library , following defined inclusion and\nexclusion criteria for studies published until December 2021 . The findings highlight\nthe critical role of multimodal approaches in automatic pain estimation, particularly\nin clinical environments, and emphasize the substantial gains observed with the inclu-\nsion of temporal exploitation of modalities. The review also recommends selecting\nhigh-performing deep learning architectures and methods, encouraging the adoption\nof robust evaluation protocols and interpretability techniques to deliver reliable and\nunderstandable outcomes. Additionally, it underscores the current limitations of exist-\ning pain databases in adequately supporting the development, validation, and practical\napplication of deep learning models as decision-support tools in real-world settings.\nFurthermore, we believe this paper is valuable not only for this Ph.D. project but also\nfor other practitioners and researchers in the field.\n2.Automatic Pain Intensity Estimation based on Electrocardiogram and Demographic\nFactors [35]\nThis study investigated the relationship between gender, age, and pain sensation and\ntheir effects on the automatic pain assessment process. By analyzing physiological\nsignals, particularly electrocardiography (ECG), we estimated pain intensity and ex-\namined the influence of these demographic factors. Utilizing the Pan-Tompkins algo-\nrithm for feature extraction and applying well-established classification methods, we\nexplored the correlation between gender, age, and pain manifestation.\n3.Multi-task Neural Networks for Pain Intensity Estimation Using Electrocardiogram\nand Demographic Factors [36]\nInspired by the previous study, this research further explored the influence of gender\nand age on pain perception. In this work, we analyze electrocardiography signals\nto uncover variations in pain perception across different demographic groups. We6 CHAPTER 1. INTRODUCTION\nleveraged these insights by developing a novel multi-task neural network for automatic\npain estimation, incorporating age and gender data for each individual. The study\ndemonstrated the advantages of this approach compared to other existing methods.\n4.A Full Transformer-based Framework for Automatic Pain Estimation using Videos\n[37]\nThis study introduced an innovative full transformer-based framework featuring a Trans-\nformer in Transformer (TNT) model combined with cross-attention and self-attention\nblocks. We achieved state-of-the-art performance using video data from the BioVid\ndatabase, demonstrating the model\u2019s effectiveness, efficiency, and strong generaliza-\ntion across primary pain estimation tasks.\n5.Multimodal automatic assessment of acute pain through facial videos and heart rate\nsignals utilizing transformer-based architectures [38]\nThis study presented a multimodal automatic acute pain assessment framework, inte-\ngrating video and heart rate signals. The framework consists of four key modules:\ntheSpatial Module , which extracts embeddings from videos; the Heart Rate Encoder ,\nwhich maps heart rate signals into a higher-dimensional space; the AugmNet , which\ngenerates learning-based augmentations in the latent space; and the Temporal Mod-\nule, which leverages the video and heart rate embeddings for the final assessment.\nThe Spatial Module undergoes a two-stage pre-training process: first, it learns uni-\nversal facial features through face recognition, followed by emotion recognition in a\nmultitask learning approach, enabling high-quality embeddings for pain assessment.\nExperiments with facial videos and heart rate data extracted from electrocardiograms\nin the BioVid database, alongside direct comparisons to 29studies, demonstrate state-\nof-the-art performance in unimodal and multimodal settings while maintaining high\nefficiency. In the multimodal setting, the framework achieved 82.74% accuracy for bi-\nnary pain classification and 39.77% for multi-level pain classification, using only 9.62\nmillion parameters across the entire framework.\n6.Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-\nMLP Architecture [39]\nThis paper introduced synthetic thermal videos generated by Generative Adversarial\nNetworks , which are integrated into the pain recognition process to assess their effec-\ntiveness. The framework employs a Vision-MLP andTransformer -based module, lever-\naging RBG and synthetic thermal videos in unimodal and multimodal settings. Exper-\niments conducted using facial videos from the BioVid database highlighted synthetic\nthermal videos\u2019 effectiveness and showcased their potential benefits in pain recognition\ntasks.1.4. THESIS OUTLINE 7\n7.Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for\nMultimodal Automatic Pain Assessment using Facial Videos and fNIRS [40]\nThis study was submitted to the First Multimodal Sensing Grand Challenge for Next-\nGen Pain Assessment (AI4PAIN) . The proposed multimodal framework leverages fa-\ncial videos and fNIRS, offering a modality-agnostic approach that eliminates the need\nfor domain-specific models. Utilizing a dual ViT configuration and waveform repre-\nsentations for both fNIRS and the extracted embeddings from the two modalities, the\nmethod demonstrates its effectiveness, achieving an accuracy of 46.76% in the multi-\nlevel pain assessment task.\n8.PainFormer: a Vision Foundation Model for Automatic Pain Assessment [41]1\nThis study introduces PainFormer , a vision foundation model built on multi-task learn-\ning principles and trained across 14distinct tasks and datasets comprising 10.9million\nsamples. As an embedding extractor for various input modalities, PainFormer provides\nfeature representations to the Embedding-Mixer , a transformer-based module respon-\nsible for conducting the final pain assessment. Extensive experimentation using both\nbehavioral modalities\u2013including RGB, synthetic thermal, and estimated depth videos\u2013\nand physiological modalities like ECG, EMG, GSR, and fNIRS revealed PainFormer \u2019s\nability to extract high-quality embeddings from diverse inputs. Tested on the BioVid\nandAI4Pain datasets and compared to more than 60existing methods, the framework\ndemonstrated state-of-the-art performance in unimodal and multimodal settings, posi-\ntioning itself as a step toward developing general-purpose models for automated pain\nevaluation.\n1.4 Thesis Outline\nThe dissertation is organized into the following chapters:\nChapter 2 introduces the foundational concepts of pain from biological, psychological, and\nclinical perspectives.\nChapter 3 reviews existing literature on automatic pain assessment using deep learning\nmethods and details the pain datasets used.\nChapter 4 outlines and proposes methods for evaluating demographic variables, their uti-\nlization, and their integration into an automatic pain assessment framework.\nChapter 5 discusses methods that utilize video and wearable device data, exploring the\ntrade-offs between efficiency and accuracy. It also proposes efficient, fast, effective models\nsuitable for real-world applications.\nChapter 6 explores synthetic data in pain assessment and introduces synthetic thermal im-\n1Under Review8 CHAPTER 1. INTRODUCTION\nagery techniques to enhance performance in automatic pain recognition.\nChapter 7 discusses general-purpose models, introduces a modality-agnostic framework,\nand presents the first foundation model used in automatic pain assessment.\nChapter 8 concludes the thesis with a final discussion, offering perspectives and ideas for\nfuture research in automatic pain assessment.Chapter 2\nClinical Pain Assessment\nContents\n2.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . . . . 20\n2.1 Chapter Overview\nThis chapter provides an anatomical and physiological overview of pain, focusing on the\nmechanisms responsible for generating, transmitting, processing, and interpreting pain sig-\nnals. It examines the various types of pain and explores the actions and expressions typically\nassociated with pain. Additionally, it reviews current pain assessment methods used in clin-\nical settings for adults, children, and newborns. The chapter also discusses developing and\nvalidating existing clinical pain assessment tools. This foundational knowledge is essen-\ntial for understanding the development and validation of computer-assisted pain assessment\n910 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nmethods discussed in later chapters. Finally, it highlights the challenges faced in clinical\npain assessment and underscores the need for automated pain assessment techniques.\n2.2 Biology of Pain\nPain, according to the International Association for the Study of Pain (IASP) [42], is \u201can\nunpleasant sensory and emotional experience associated with actual or potential tissue dam-\nage, or described in terms of such damage\u201d. Biologically, pain is an undesirable sensation\noriginating from the peripheral nervous system. Its fundamental function is to engage sen-\nsory neurons, notifying the organism of potential harm and playing a vital role in recognizing\nand responding to threats [43].\nThe transmission of a noxious stimulus from the periphery to the central nervous sys-\ntem involves a complex pathway through the spinal cord, resulting in the physical sensation\nof pain and a corresponding emotional response and memory. This process culminates in\nthe perception of pain. The initial stage of pain processing occurs when a stimulus at noci-\nceptive sensory fibers in the periphery is converted into an action potential. A nerve signal\nis generated if the stimulus is strong enough to surpass the action potential threshold [44].\nThis signal travels along the primary afferent fiber toward the central nervous system. As the\nstimulus intensity grows, more nerve fibers and areas of the nervous system are engaged [44].\nDue to their branching nature, primary afferent fibers typically relay information from sev-\neral pain receptors. These fibers and their receptors comprise a sensory unit, which gathers\ndata from a specific receptive field [44]. When receptive fields are larger and overlap with\nnearby fields, it becomes more challenging for the sensory system to locate the source of pain\naccurately. The primary afferent neuron is a pseudounipolar neuron that splits into a periph-\neral and central axon. The cell bodies of these neurons are located in the peripheral nervous\nsystem, within the posterior or cranial root ganglia. The peripheral axon extends to the skin,\nmuscles, tendons, or joints, branching into terminal fibers that connect with somatosensory\nreceptors. In contrast, the central axon leads to the central nervous system [45].\nPeripheral somatosensory fibers are categorized into three main groups. The first group\nincludes A\u00b4\u03b1,A\u00b4\u03b2,A\u00b4\u03b3fibers, large, myelinated fibers that rapidly conduct sig-\nnals [46]. These fibers involve touch and proprioception but are not associated with pain\nperception. The second group consists of A\u00b4\u03b4fibers, which are smaller and slower con-\nducting. Certain A\u00b4\u03b4fibers play a key role in pain sensation, with some responding only\nto intense mechanical stimuli and others reacting to noxious and non-noxious heat. The\nthird group comprises Cfibers, which are small, unmyelinated, and conduct signals very\nslowly. Most Cfibers are polymodal for pain perception, responding to various noxious\nmechanical, thermal, and chemical stimuli. These fibers are mainly linked to burning pain\nsensations [43]. The sensation of pain, known as nociception, is primarily facilitated by2.3. CLASSIFICATION AND CHARACTERISTICS OF PAIN 11\nvarious intracellular and extracellular molecular messengers. When activated by a specific\nstimulus, nociceptors relay information through glutamate, an excitatory neurotransmitter.\nAdditionally, inflammatory mediators are released at the injury site, further stimulating no-\nciceptor activation by releasing chemicals such as neurotransmitters ( e.g., serotonin), lipids\n(e.g., prostaglandins), peptides ( e.g., bradykinin), and neurotrophins ( e.g., nerve growth fac-\ntor) [46]. There are ascending tracts responsible for transmitting sensory information from\nthe periphery to the central nervous system. Fibers that convey two-point discrimination, tac-\ntile information, pressure, vibration, and proprioception ascend via the dorsal column of the\nspinal cord, forming the gracile and cuneate fasciculi. Fibers transmitting pain, temperature,\nand crude touch from somatic and visceral structures travel through the lateral spinothalamic\ntract. The anterior spinothalamic tract also transmits pain, temperature, and touch informa-\ntion to the brainstem and diencephalon (Figure 2.1) [47].\n2.3 Classification and Characteristics of Pain\nAccording to neurobiologist Clifford Woolf [48], pain can be classified into three categories\nbased on its function and characteristics: nociceptive ,inflammatory , and pathological pain.\nThese classes and their respective functions are illustrated in Figure 2.2.\nNociceptive pain (refer to Figure. 2.2(A)), arising from tissue damage, is a high-threshold\npain that activates only in response to intense stimuli [49], serving as a vital warning signal\nto the body. The neurobiological system responsible for nociceptive pain evolved from the\nability of even the most primitive nervous systems to detect impending or actual tissue dam-\nage caused by external stimuli. Its protective role requires immediate attention and action,\nachieved through the withdrawal reflex it initiates, the unpleasant sensation it produces, and\nthe emotional distress it triggers. Nociceptive pain demands avoidance in the present mo-\nment, and when activated, it overrides most other neural processes [48].\nInflammatory pain (refer to Figure. 2.2(B)) is also protective and adaptive, increasing\nsensory sensitivity following tissue damage to aid healing by discouraging movement and\ncontact with the injured area. This heightened sensitivity, or tenderness, helps prevent further\nharm and supports recovery, as seen after surgical wounds or inflamed joints where normally\nnon-painful stimuli now cause pain. It is triggered by immune system activation in response\nto tissue injury or infection. Despite its adaptive role, this pain often needs to be alleviated in\npatients with persistent inflammation, such as in rheumatoid arthritis or severe injuries [48].\nPathological pain (Figure. 2.2(C)) is maladaptive, arising from abnormal nervous sys-\ntem functioning and not serving a protective role. Unlike nociceptive and inflammatory pain,\npathological pain is a disease state of the nervous system itself. It may occur following\nnerve damage (neuropathic pain) or in conditions without apparent damage or inflammation\n(dysfunction l pain). Examples of dysfunctional pain include fibromyalgia, irritable bowel12 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFIGURE\n 2: Spinothalamic tract.\nPain, temperature, and some touch and pressure afferents end in the posterior horn. Second- or\nhigher-order fibers cross the midline, form the spinothalamic tract, and ascend to the ventral\nposterolateral (VPL) nucleus of the thalamus (and also to other thalamic nuclei not shown).\nThalamic cells then project to the somatosensory cortex of the postcentral gyrus, to the insula,\nand to other cortical areas (also not shown). Along their course through the brainstem,\nspinothalamic fibers give off many collaterals to the reticular formation (RF). The inset to the left\nshows the lamination of fibers in the posterior columns and the spinothalamic tract in a leg-\nlower trunk-upper trunk-arm sequence. The inset to the right shows the longitudinal formation\nof the spinothalamic tract. Primary afferents ascend several segments in Lissauer\u02bcs tract before\nall their branches terminate; fibers crossing to join the spinothalamic tract do so with a rostral\ninclination. As a result, a cordotomy incision at any given level would spare most of the\ninformation entering the contralateral side of the spinal cord at that level, and to be effective,\nthe incision must be made several segments rostral to the highest dermatomal level of pain.\n2017 Khalid et al. Cureus 9(10): e1754. DOI 10.7759/cureus.1754\n5\n of \n14\nFigure 2.1: The spinothalamic tract (STT) [43]. Pain, temperature, and some touch afferents\nend in the posterior horn, where second-order fibers cross the midline to form the\nspinothalamic tract, ascending to the thalamus and projecting to various cortical\nareas. Along the way, collaterals connect to the reticular formation. Due to the ros-\ntral inclination of fibers in Lissauer\u2019s tract, cordotomy must be performed several\nsegments above the pain level for effective relief.\nsyndrome, tension headaches, and temporomandibular joint disease, where significant pain\nexists without an apparent noxious stimulus or peripheral pathology. Pathological pain, a\nlow-threshold pain primarily driven by amplified sensory signals in the central nervous sys-\ntem, is the clinical pain syndrome with the greatest unmet need. To analogize, while nocicep-\ntive pain acts as a fire alarm for intense heat, and inflammatory pain reacts to warm tempera-\ntures, pathological pain is a false alarm triggered by a system malfunction. Thus, treatment\nmust specifically target the underlying mechanisms causing each type of pain [48].\nPain from a time-duration perspective can be categorized by duration into acute and2.4. PAIN INDICATORS 13\nchronic , with chronic pain persisting or recurring for more than three months [50]. Acute\npain is typically related to identifiable physiological damage from injury, surgery, illness,\ntrauma, or medical procedures and generally subsides once the underlying cause is resolved.\nHowever, if untreated, it may develop into chronic pain. Acute pain is further classified\nintoprocedural pain, caused by medical interventions such as muscular injections [51], and\npostoperative pain, which occurs after surgery and is a significant concern for both patients\nand healthcare providers. Effective management is crucial to aid recovery and prevent the\ntransition to chronic pain [52]. Chronic pain manifests in various forms, including chronic-\nrecurrent pain, like migraine headaches, and chronic-continuous pain, such as persistent low\nback pain [53].\n2.4 Pain Indicators\nPain can manifest in numerous ways and is often shaped by individual characteristics and\nenvironmental influences. Various human expressions, actions, and bodily responses have\nbeen linked to pain, serving both communicative and coping purposes. These pain indicators\nare generally categorized into three primary groups: (i)behavioral, (ii)physiological, and\n(iii)biochemical. While these indicators are universally present, certain expressions are more\nprominent in specific groups. For instance, crying is a common pain response across all age\ngroups but is more frequently observed in younger infants. This may be due to contextual\nfactors\u2014such as culture, social status, age, and ego\u2014influencing how pain is expressed\nover time. Adults, for example, may suppress crying in favor of other vocalizations, such as\ngroans and moans, as crying could be perceived as inappropriate in certain contexts. These\nmediating factors are often considered when interpreting pain indicators. The following\nsections will delve into each of these three categories [51].\n2.4.1 Behavioral Indicators\nBehavioral indicators such as facial expressions ( e.g., grimacing, open mouth, raised eye-\nbrows), vocalizations ( e.g., crying, moaning, screaming), and various bodily movements\n(e.g., changes in posture, signs of tension) are vital markers used in assessing pain [22].\nFacial expressions and limb movements in response to acute pain are typically rapid and\ninvoluntary. Facial reactions include brow bulging, eye squeezing, nasolabial furrow forma-\ntion [54], grimacing, clenched teeth, jaw-dropping, and tightened lips [55]. Body movements\nassociated with pain include bracing (gripping an object or the affected area during move-\nment), rubbing (massaging the painful area), restlessness (constant shifting of position) [55],\nand knee flexion [56]. Non-verbal vocalizations such as groaning, moaning, sighing, crying,\nand gasping [57] also indicate pain. Verbal expressions like \u201couch\u201d ,\u201cstop\u201d ,\u201cthat hurts\u201d ,14 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFigure 2.2: Pain classification [48]: (A)Nociceptive pain , which results from detecting po-\ntentially harmful stimuli and serves a protective function. (B)Inflammatory pain is\nlinked to tissue damage and immune cell infiltration, increasing pain sensitivity dur-\ning healing. (C)Pathological pain is a disease state caused by either nervous sys-\ntem damage (neuropathic) or abnormal nervous system function (dysfunctional).\n\u201cthat is enough\u201d , and even cursing [55] also serve as pain indicators. Interestingly, swearing\nhas been found to significantly alleviate pain, although its effect diminishes with frequent\nuse over a short period [58, 59].2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 15\n2.4.2 Physiological Indicators\nVital signs can reflect the state of the central nervous system, and since pain is mediated\nthrough this system, trends in vital signs can provide insights into pain levels. Clinical stud-\nies [60, 61] have examined physiological changes in response to pain and established em-\npirical solid evidence linking pain to vital sign alterations. However, as vital signs can also\nchange due to other non-pain-related pathological conditions, it is recommended that they be\nassessed alongside behavioral pain indicators for accuracy. Physiological pain responses are\nconsidered more reliable than behavioral signals, as they cannot be consciously controlled\nor altered. Physiological measurements such as electrocardiography (ECG), electromyogra-\nphy (EMG), galvanic skin responses (GSR), and respiration rate provide critical insights into\nthe body\u2019s reaction to pain [17]. In addition, brain monitoring techniques like near-infrared\nspectroscopy (fNIRS) have demonstrated the ability to detect pain-related hemodynamic\nchanges [23]. At the same time, functional magnetic resonance imaging (fMRI) has been\nexplored for assessing pain in both normal and pathological conditions [62].\n2.4.3 Biochemical Indicators\nCompared to other pain indicators, biochemical changes are the most precise and sensitive\nreactions to pain. However, their routine use in pain assessment is restricted due to the\ninvasive nature of measurement techniques [63]. These biochemical responses are most\nevident during surgical procedures with limited anesthesia, leading to increased levels of\nendorphins, norepinephrine, cortisol, growth hormones, renin, glucagon, aldosterone, and\ncatecholamines, along with a decrease in insulin levels [60].\n2.5 Sociodemographic and Psychological Variables\nIn1965 , Melzack and Wall [64] introduced the \u201cGate Control Theory\u201d , which interprets pain\nfrom two perspectives. The first involves the mechanisms of nociceptive signal transmission\nand modulation, while the second emphasizes pain as a psychophysiological phenomenon\narising from the interaction between physiological and psychological factors [53]. Observa-\ntions, empirical research, and theoretical models increasingly suggest that a comprehensive\nunderstanding of pain requires a biopsychological approach. It is also becoming apparent\nthat, although pain is often regarded as private and subjective, it is also fundamentally a so-\ncial experience [53]. Pain is not solely explained by biomedical components ( e.g., muscle\ndamage) but also involves psychological ( e.g., cognitive, affective) and social factors ( e.g.,\nfriends, family, health professionals), leading to what is known as a biopsychosocial sensa-\ntion [65]. Numerous factors contribute to how painful experiences are expressed and per-\nceived, varying wildly due to social and personal biases. These factors prompted Williams16 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nand Craig [2] to define pain as \u201ca distressing experience associated with actual or potential\ntissue damage with sensory, emotional, cognitive, and social components. \u201d\n2.5.1 Sex and Gender\nSeveral studies have explored the relationship between gender and pain expressiveness, as\nwell as variations in pain reporting. Research indicates that women generally exhibit a lower\npain threshold compared to men. A meta-analysis by Boerner et al. [66] on gender differ-\nences in children and adolescents found that girls over the age of 12reported higher pain\nintensity in response to cold-induced pain than boys. Furthermore, multiple studies suggest\nthat women tend to describe a greater degree of pain compared to men. In addition to bi-\nological differences, psychological aspects linked to gender also play a role. For instance,\nindividuals with a masculine identity may be less inclined to express or report their pain or\nseek assistance [67].\nMoreover, the manifestation of pain is not only influenced by the individual\u2019s gender but\nalso by dyadic interactions between people of different sexes. Levine and Desimone [68]\nconducted one of the initial studies on this phenomenon, showing that male participants in\na cold pressure experiment reported lower pain intensity when a female experimenter was\npresent. Similarly, McClelland and McCubbin [69] found that female participants expressed\nand reported higher pain levels when accompanied by a female friend. This dynamic also\nextends to patient-healthcare provider interactions. In studying health records, Vigil and\nAlcock [70] discovered that when the pain intensity was reported as high, the patients ( i.e.,\nmen and women) were examined by a female doctor or nurse. Additionally, studies exam-\nining gender differences among physicians in pain treatment options revealed that female\npatients were more likely to receive prescriptions for more potent drugs, such as analgesics,\nand female physicians were more likely to prescribe medications. Extensive research has\nalso shown that both lay observers and healthcare professionals tend to estimate higher pain\nlevels for female patients compared to male patients [71]. Hooper et al. [72] further noted\nthat clinicians communicate more effectively with female patients, often displaying greater\nempathy. Gender roles, beliefs, and expectations play a significant role in understanding\nhow social factors influence the differences in pain perception and experience between men\nand women [73].\n2.5.2 Age\nAge plays a crucial role in pain assessment and management. At the same time, there are\nsignificant challenges, limitations, and biases related to the patient\u2019s age group. Two of the\nmost vulnerable groups, albeit for different reasons, are the elderly and infants.\nPain recognition and interpretation among the elderly, particularly by caregivers, often2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 17\npresent unique challenges. Older adults frequently exhibit stoicism and reluctance to ex-\npress their pain, while healthcare providers struggle to accurately assess the patient\u2019s pain,\nleading to inappropriate pain management decisions [74]. McPherson et al. [75] noted that\ndespite caregivers\u2019 accommodating and empathetic relationships with elderly patients, con-\nflicts still arise. Older patients may resist acknowledging their weaknesses and accepting\nhelp, which can cause them to conceal their pain. The situation becomes even more complex\nwhen dealing with dementia, a disorder encompassing a range of conditions ( e.g., Parkin-\nson\u2019s, Alzheimer\u2019s, Vascular dementia), characterized by abnormal brain changes that im-\npair cognitive and linguistic abilities. A person with dementia may find it challenging to\ncommunicate their pain verbally. However, non-verbal pain expressions remain intact even\nin moderate dementia, although such reactions can be exaggerated [76]. However, aggres-\nsive behavior and disturbances in dementia patients, often caused by pain, are frequently\nmisinterpreted as psychiatric symptoms, leading to improper medication that can have life-\nthreatening consequences [77]. Caregivers of dementia patients face additional challenges,\nnot only related to pain management but also in addressing dementia\u2019s impact on language\nand memory. Particularly in the later stages of dementia, patients encounter severe pain\ncommunication difficulties due to cognitive decline, necessitating that caregivers recognize\nbehavioral and contextual indicators of pain [74]. Age is also known to cause changes in\nskin characteristics, such as texture, rigidity, and elasticity, which impact the performance of\nemotional face recognition tasks [78].\nInfants represent another vulnerable age group where pain assessment requires special-\nized attention, particularly when they experience painful events. The first challenge is obvi-\nously their limited reporting ability to express their pain through language. Although crying\nmight appear to signal pain, this is an oversimplified and unreliable method, as crying can in-\ndicate a variety of situations, such as discomfort, hunger, or pain. Accurately discerning the\ntype of cry is only one part of the challenge; assessing pain in infants is far more complex and\ninfluenced by numerous factors, including the interpersonal relationships within their envi-\nronment. Riddell and Racine [79] found that through various distressing experiences, infants\ncan learn that specific signaling behaviors can prompt their caregiver\u2019s proximity. This at-\ntachment dynamic suggests that, to some extent, infants may consciously utilize pain-related\nbehaviors to elicit responses from their caregivers. Similarly, the context affects older chil-\ndren as well; for example, self-reports of pain tend to be significantly lower when a parent is\npresent compared to when the child is alone [80].\n2.5.3 Psychological Factors\nMultiple studies have revealed that several psychological factors are consistently linked with\npain-related behavior, including depression, pain-related fear, and catastrophizing. Research18 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nfocusing on the impact of depression and anxiety on pain-related behavior has been con-\nducted mainly on patient populations. These studies have shown that depressed individ-\nuals exhibit more pronounced protective and communicative behaviors compared to non-\ndepressed patients [81]. Similarly, numerous studies suggest that patients with higher levels\nof anxiety demonstrate more pain-related behaviors than those with lower anxiety levels [82].\nDespite the frequent coexistence of pain with psychological conditions, research indicates\nthat these patients often experience underestimation of their pain. For instance, De Ruddere\net al. [83] found that patients dealing with psychological stressors such as anxiety, depres-\nsion, and daily life challenges are often perceived by physiotherapists as experiencing less\nsevere pain, illustrating the influence of psychosocial factors on the patient\u2019s pain experience.\n2.5.4 Race and Culture\nPain expression is generally understood across ethnicities and cultures, though differences\nexist in how it is conveyed [4]. However, cultural variations and the nuances of facial ex-\npressions related to emotion are complex and necessitate deeper study. Additionally, racial\nand cultural biases significantly influence pain assessment, judgment, and interpretation.\nExtensive research highlights the impact of a patient\u2019s race as a sociodemographic factor\non observer responses. The most examined topic relates to the different responses toward\nCaucasian versus non-Caucasian individuals, particularly African Americans, who are more\nlikely to have their pain underestimated and undertreated by healthcare providers [84].\nEthnocultural factors are crucial in shaping how individuals perceive and express pain.\nFor example, Western cultures often emphasize conservative expressions and self-control,\nleading to restrained responses in personal pain experiences and in perceiving others\u2019 pain\n[3]. Differences also arise in coping mechanisms; African Americans, for instance, are more\nprone to catastrophizing pain events compared to European Americans [85]. Furthermore,\nevidence shows racial biases in pain treatment across various racial groups, with certain\ngroups being more sensitive to pain but receiving lower-quality treatment [86]. For exam-\nple, Cleeland et al. [87] found that minority cancer patients, mainly Black and Hispanic\nindividuals, were more likely to experience inadequate analgesia compared to non-minority\npatients.\n2.5.5 Observer\u2019s Impact on Pain\nThe variability in pain management stems from the interplay of various elements, including\nsociocultural, biomedical, and psychosocial factors, especially in cases of chronic pain [88].\nWhen it comes to the observer responsible for assessing a patient\u2019s pain, several characteris-\ntics directly influence the objectivity of their evaluation. The first and perhaps most critical\nfactor is the observer\u2019s experience level. One would expect that more experience leads to2.6. IMPACT OF INADEQUATE PAIN MANAGEMENT 19\nbetter and more accurate assessments, but studies show that even experienced healthcare\nproviders consistently underestimate pain, much like laypersons [28]. The greater the expe-\nrience, the more pronounced the underestimation tends to be. This may be due to desensitiza-\ntion caused by repeated exposure to pain events, as seen in the differences between internists\nand surgeons in their evaluation of postoperative pain, with surgeons often encountering se-\nvere pain more regularly [89]. Another significant factor is the observer\u2019s knowledge and\nbeliefs about pain. For example, [83] found that laypersons and healthcare professionals\nwithout physical signs of pain might view the patient\u2019s complaints less seriously. Proper\ntraining is also essential for adequate pain assessment, which is why the Department of\nHealth and Human Services (DHHS) initiated a strategic program to improve healthcare\nproviders\u2019 education and knowledge regarding pain management, following evidence of in-\nadequate training in the field [90].\n2.6 Impact of Inadequate Pain Management\nThe experience of pain, particularly persistent pain, can have detrimental effects on the indi-\nvidual and their surrounding environment. Thoughts about severe pain often lead to grief and\nfear, causing individuals to perceive pain as a threat and feel incapable of managing it. This\ncan prompt avoidance behaviors aimed at escaping perceived harm [91]. Studies have shown\nthat children with a catastrophizing mindset about pain struggle with daily activities, while\nadolescents with chronic pain tend to have fewer friends and may miss out on social and\nentertainment opportunities, putting them at greater risk of victimization [92]. These adoles-\ncents often feel isolated and lonely compared to their healthy peers, and they may experience\nanxiety in social interactions [93]. Parental reactions to their children\u2019s pain can further com-\nplicate the situation, as parents with catastrophic tendencies tend to engage in overprotective\nbehaviors that hinder the child\u2019s functioning and psychosocial development [94]. Addition-\nally, the family\u2019s overall dynamic is affected, with the patient\u2019s sadness, sleep disorders, and\nchanges in leisure activities impacting the household [74].\nOn a biological level, pain, particularly when experienced early and severely, can alter\nthe brain and nervous system. These early pain experiences can disrupt neurobiological\ndevelopment and affect how pain is processed later in life [95]. A growing body of research\nlinks chronic pain to changes in the medial prefrontal cortex, a region crucial to emotional\nprocessing. Chronic pain is associated with structural and biochemical alterations in this\nbrain area, suggesting that these changes play a role in the pathophysiology of chronic pain\n[96].20 CHAPTER 2. CLINICAL PAIN ASSESSMENT\n2.7 Pain Measurement Scales and Metrics\nIn clinical settings, self-reporting remains the gold standard for assessing pain, allowing in-\ndividuals to describe their pain\u2019s intensity and location. Various self-report scales have been\ndeveloped for different age groups, such as the visual analog scale (V AS) [97] and the verbal\nrating scale (VRS) [98]. Additionally, observation-based scales, where a third party eval-\nuates the pain\u2019s severity, include tools like the Prkachin and Solomon pain intensity scale\n(PSPI) [99] and the neonatal/infant pain scale (NIPS) [100]. However, some studies suggest\nthat patients may exaggerate their pain severity to prompt more aggressive treatment inter-\nventions [101], raising concerns about the accuracy of self-reported symptoms. Therefore,\nobjective pain measurement remains clinically crucial.Chapter 3\nAutomatic Pain Assessment\u2013A Literature\nReview\nContents\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database . . . . 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 49\n2122 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research Di-\nrections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.1 Chapter Overview\nThis chapter corresponds to the publication [17], a systematic literature review (SLR) con-\nducted at the start of this Ph.D. research. This review facilitated an understanding of au-\ntomatic pain assessment methods, particularly those based on deep learning, and the tech-\nniques and strategies employed. It enabled the identification and proposal of new approaches\nthat could enhance the effectiveness of pain recognition.\nAdditionally, it allowed for identifying gaps in the literature from other reviews con-\nducted on this specific research topic. Every existing systematic review on pain assessment\nwas identified and assessed, revealing several insights. The first review on automatic pain\nassessment, published by Prkachin in 2009 [102], did not cover papers on deep learning,\nas the practical implementations of deep architectures only began around 2012. Zamzmi et\nal.[103] focused their review exclusively on infants, omitting deep learning methods. In\n2018, Chen et al. [104] reviewed automated pain detection methods using the Facial Ac-\ntion Coding System (FACS), noting only three publications that employed deep learning\ntechniques. In 2019, Hassan et al. [105] included only seven papers that used deep learn-\ning methods in their review. Similarly, Werner et al. [106], also in 2019, discussed pain\nassessment without restrictions on modalities or age groups, finding fewer than ten papers\nthat reported on deep learning methods. In 2020, Al-Eidan et al. [107] published the first\nsystematic literature review titled \u201cDeep-Learning-Based Models for Pain Recognition: A\nSystematic Review\u201d, which included fifteen papers but was critiqued for having significant\nlimitations and incorrect information. It was noted that some papers analyzed might not be\nrelevant, and there was confusion between \u201cneural networks\u201d and \u201cdeep learning\u201d. For in-\nstance, while study [105] mentioned using neural network approaches, they did not provide\nevidence of using deep learning methods. Moreover, in the study [104], the authors devel-\noped a neural network with only two layers combined with handcrafted features, which does\nnot qualify as a deep learning method. Additionally, studies [103, 107] focused on detect-\ning protective movement behaviors in chronic pain patients, which deviates from the central\ntopic of automatic pain assessment. Several reviews and SLRs on automatic pain assessment\nhave been published, but none exclusively or adequately focus on deep learning methods.\nThis SLR aims to bridge this gap by thoroughly reviewing deep learning techniques used for\nautomatic pain assessment.3.2. MODALITIES AND HARDWARE FOR AUTOMATIC PAIN ASSESSMENT 23\n3.2 Modalities and Hardware for Automatic Pain Assessment\nCreating an automatic pain assessment system hinges on capturing the necessary input data\nthrough various information channels, referred to as modalities. These modalities are cat-\negorized into behavioral and physiological types. A system utilizing only one modality is\ntermed unimodal, whereas a multimodal system incorporates multiple modalities.\nKey behavioral modalities encompass facial expressions, body movements, gestures, and\nauditory signals. Researchers use a range of optical and light sensors to record images or\nvideo sequences of facial and body movements. Commonly, researchers employ color RGB\ncameras, but depth and thermal sensors are also used to enhance visual data. Motion capture\nsensors are also employed to track movements, and microphones are frequently employed\nto capture sound. On the physiological front, modalities often involve biosignals that detect\nelectrical activities from various tissues and organs. Techniques such as electrocardiogra-\nphy (ECG), electromyography (EMG), electrodermal activity (EDA), photoplethysmogra-\nphy (PPG), blood oxygen saturation (SpO2), near-infrared spectroscopy (NIRS), respiration\nrate, and skin temperature are commonly used to gauge pain. Multiple sensors can mea-\nsure several modalities simultaneously \u2014 for instance, strain sensors and cameras can track\nrespiration rates.\nBesides the sensors that gather input data, the computational hardware is crucial. Deep\nlearning-based systems operate in two phases: training and inference. The training phase is\nparticularly resource-intensive, necessitating a graphics processing unit (GPU). The trained\nmodel makes predictions on new data during inference, typically processed on a central\nprocessing unit (CPU). The choice of hardware depends on various factors, especially in\nreal-time scenarios where low latency is crucial, compared to offline settings where data\nprocessing can be deferred. Additionally, characteristics of the model, such as floating point\noperations per second (FLOPS) and total computational operations, are significant consider-\nations.\n3.3 Pain Databases\nAccess to data is crucial for evaluating methods and algorithms in automatic pain assess-\nment. However, only a few databases have explicitly been developed for automatic pain\nrecognition based on human behavioral and physiological changes. Unlike the extensive\ndata found in most facial expression databases, publicly accessible pain datasets often offer\nlimited samples and suffer from significant class imbalance. This primarily stems from the\nethical concerns associated with collecting pain data. Table 3.1 lists the principal databases\nreviewed in the studies. Figure 3.1 shows how frequently each database was used. Most\nresearch utilized publicly available datasets, with some studies exploring multiple datasets.24 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nFew studies used private datasets, mainly those aimed at detecting pain in neonates. The\nUNBC-McMaster Shoulder Pain Archive Database [108] is the most utilized, followed by\nTheBioVid Heat Pain Database [109]. The former contains 200facial videos of 25individ-\nuals with shoulder pain. At the same time, the latter includes facial videos and biopotentials\nof90healthy participants subjected to experimentally induced heat pain at four intensity\nlevels. The following subsections provide a brief description of some of these datasets.\nTable 3.1: Most commonly utilized pain databases.\nDatabase Modality Population Annotation\nGranularityAnnotation Labels\nUNBC-McMaster\nShoulder PainA[108]RGB video of face 25 adults with shoulder painFrame level\nSequence levelFACS\nV AS, OPI\nBioVidA[109] RGB video of face, EDA, ECG,\nEMG87 healthy adults Sequence level stimulus\n(calibrated per person)\nMIntPAINA[110] RGB-Depth-Thermal video of\nface20 healthy adults Sequence level stimulus\n(calibrated per person),\nV AS\niCOPEA[111] RGB photographs of face 26 healthy neonates Frame level pain, cry, rest, air puff,\nfriction\niCOPEvidA[112] Grayscale video of face 49 neonates Sequence level pain, no pain\nNPAD-IA[113] RGB video of face & body, HR,\nSpO2, BP, NIRS36 healthy neonates & 9 neonates\nwith tissue injured by surgerySequence level NIPS, N-PASS\nAPN-dbA[114] RGB video of face 112 healthy neonates Sequence level NFLAPS, NIPS, NFCS\nEmoPainN[115] video, audio, EMG, MoCap 22 adults with chronic pack pain &\n28 healthy adultsSequence level self-report, naive OPI\nSenseEmotionN\n[116]video of face, audio, EDA, ECG,\nEMG, RSP45 healthy adults Sequence level stimulus\n(calibrated per person)\nX-ITEN[117] RGB-Thermal video of face,\nRGB-Depth video of body, au-\ndio, EDA, ECG, EMG134 healthy adults Sequence level stimulus\n(calibrated per person)\nA: Publicly available by request, complete or part of the dataset N: Not yet available Modality: HR: heart rate SpO2: oxygen saturation rate BP:\nblood pressure NIRS: near-infrared spectroscopy MoCap: motion capture RSP: respiration rate EDA: electrodermal activity ECG: electrocardiogram EMG:\nelectromyogram Annotation Labels: FACS: Facial Action Coding System V AS: visual analogue scale OPI: observer pain intensity NIPS: neonatal infant\nscale N-PASS: neonatal pain, agitation and sedation scale NFLAPS: neonatal face and limb acute pain scale NFCS: neonatal facial coding system\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database\nTheUNBC McMaster Shoulder Pain Database [108] comprises 200video sequences show-\ning the facial expressions of 25subjects undergoing motion tests, including arm abduction\nand external and internal rotations. The data collection utilized both active and passive ap-\nproaches: in the active mode, subjects moved their affected arms to their bearable limit,\nwhile in the passive mode, a physiotherapist moved the subjects\u2019 arms. Each video sequence\ncontains about 60to700frames, totaling 48,398, with 82.71% of frames scoring a pain\nrating of zero, indicating a significant imbalance in the data. All frames are FACS-coded\nfor pain-related action units (AUs)\u2014AU4, AU6, AU7, AU9, AU10, AU12, AU20, AU25,\nAU26, AU27, and AU43\u2014with each AU coded for intensity from A to E, 0, or5, except\nfor AU43 (closed eyes), which is coded as either present or absent. Pain scores are assigned\nusing the PSPI metric based on the intensity of the AUs present. Additionally, the database3.3. PAIN DATABASES 25\n0815233038455360\nUNBC-McMasterBioVidEmoPainSenseEmotionX-ITEMIntPAINiCOPEiCOPEvidNPAD-IAPN-dbother\nTable 1Category AUNBC-McMaster59BioVid21EmoPain7SenseEmotion5X-ITE2MIntPAIN4iCOPE3iCOPEvid1NPAD-I5APN-db1other21\n1\nFigure 3.1: The number of studies utilizing these specific datasets. Note that various studies\nused multiple datasets to conduct their experiments.\nincludes 66facial landmarks per frame, determined by an active appearance model. Pain as-\nsessments also include self-reports using two Likert scales with 15options each and a visual\nanalog scale (V AS) from 1(no pain) to 10(extreme pain). One scale measures the sen-\nsory intensity from \u201cextremely weak\u201d to \u201cextremely intense\u201d , while the other assesses the\naffective-motivation aspect of pain from \u201cbearable\u201d to \u201cextremely excruciating\u201d. Indepen-\ndent observer pain intensity (OPI) ratings use a 6-point scale from 0(no pain) to 5(intense\npain). The UNBC database is currently the most extensively utilized dataset for automatic\npain recognition among publicly available resources.\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database\nTheBioVid dataset [109] is a prominent resource in pain research, comprising facial videos,\nelectrocardiograms, electromyograms, and galvanic skin response data from eighty-seven\npn=87qhealthy participants ( 44males and 43females, aged 20to65). The pain was in-\nduced using a thermode on the participants\u2019 right arm, with pain and tolerance thresholds\nestablished before data collection. These thresholds defined the range of pain from No Pain\n(NP) to Very Severe Pain (P 4), encompassing five levels of pain intensity. The temperatures\nfor the pain inductions ranged from P 1to P 4and did not exceed 50.5\u02ddC. Each participant\nunderwent 20inductions at each of four pain levels, with each induction lasting 4sfollowed\nby a recovery period of 8to12s. In addition, 20baseline measurements were taken at 32\u02ddC\n(NP), totaling 100stimulations per participant, randomly administered. Data processing\nsegmented these into 5.5sdurations starting 1safter the target temperature was reached, re-26 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsulting in 8,700samples across the five pain intensity classes, equally distributed among all\nmodalities for each participant. Video recordings were made at a frame rate of 25FPS, and\nbiosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\n3.3.3 The EmoPain Database\nTheEmoPain [115] dataset encompasses various pain indicators, including body movements,\naudio, biosignals, and postural and facial expressions. It features video and audio recordings\nof22patients ( 7male, 15female) exhibiting natural pain expressions while engaging in\nphysiotherapy-like exercises. These exercises, performed at regular and challenging levels,\ninclude a sitting-standing sequence, balancing on one leg for five minutes, and reaching for-\nward while standing. The video signals are captured in high resolution ( 1024\u02c61024 pixels)\nusing eight cameras positioned at various angles, enhanced by specialized lighting condi-\ntions. Audio is recorded with two microphones: an AKG C-1000S MKIII placed near the\ncameras and an AKG HC 577 L worn by the patients, both operating at a 48kHz sampling\nrate with bit Pulse Code Modulation. Body movements and postures are tracked using a mo-\ntion capture suit with 18sensors distributed across the body. Biosignals are monitored with\nfour sEMG sensors attached to the trapezius and lumbar para-spinal muscles. Additionally,\nthe dataset provides continuous frame-wise pain ratings for facial expressions by eight naive\nannotators and binary frame-wise annotations for protective behaviors by four experts, along\nwith coordinates from 26body nodes. Six annotated protective behaviors include stiffness,\nbracing, hesitation, limping, rubbing, and abrupt actions. Audio and EMG signals from the\neight activities per subject also contribute to multimodal pain recognition. Like the UNBC\ndatabase, EmoPain faces significant challenges due to data sparsity and imbalance\u2014only\n11.4%of frames show facial expressions of pain, and 8.6%show protective behaviors. This\nscarcity complicates pain recognition research, necessitating the development of methods\nthat efficiently utilize limited data to achieve optimal performance.\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database\nTheX-ITE [117] dataset is one of the largest pain datasets but is not publicly available. It\ninvolved 134healthy adults ( 67men and 67women) aged between 18and50. The aver-\nage age was 31.4years (SD = 9.7), with men averaging 33.4years (SD = 9.3) and women\n32.9years (SD = 10.2). Participants had no chronic pain, depression, psychiatric disorders,\nneurological conditions, headache syndromes, or cardiovascular disease, nor had they taken\npain medication or painkillers before the experiment. Pain stimuli were stimulated using the\nMedoc PATHWAY Model ATS for heat pain on the forearm and the Digitimer DS7A for elec-\ntrical pain on the index and middle fingers. Both modalities featured phasic stimuli (short,\n5seconds) and tonic stimuli (long, 60seconds), each in three intensities. After calibration,3.4. UNIMODAL STUDIES 27\nparticipants underwent a 90-minute stimulation phase where phasic stimuli were repeated\n30times in a randomized sequence with 8-12-second pauses. The tonic stimuli were applied\nonce per intensity, totaling six per participant, each followed by a five-minute pause. The\nhighest intensity tonic stimuli for heat and electrical pain were induced at the experiment\u2019s\nending, with the other stimuli randomly interspersed during the phasic period. Simultane-\nous to the pain stimulation, various sensors collected multimodal pain response data: frontal\nand side view RGB videos for facial expression and head pose analysis, audio for paralin-\nguistic response analysis, electrocardiogram (ECG) to monitor heart rate variability, surface\nelectromyography (EMG) to assess muscle activity in the trapezius, corrugator supercilii,\nand zygomaticus major, electrodermal activity (EDA) to measure sweating, video for body\nmovement analysis, and thermal video for facial temperature changes.\n3.3.5 The AI4Pain Database\nThe AI4Pain Grand Challenge 2024 [118] dataset is a recent contribution to the pain re-\nsearch field, tailored for sophisticated pain recognition tasks using fNIRS and facial video\ndata. This dataset involves sixty-five volunteers pn=65q, including 23females, with ages\nranging from 17to52years (mean age of 29.06years and a standard deviation of 8.28years).\nAlthough it captures physiological signals such as photoplethysmography (PPG), electroder-\nmal activity (EDA), and respiration (RESP), these signals are not publicly available yet. The\ndataset is segmented into three parts: training ( 41volunteers), validation ( 12volunteers),\nand testing ( 12volunteers). The experimental setup includes fNIRS data recorded with an\nArtinis device, measuring changes in oxygenated and deoxygenated haemoglobin concentra-\ntions across 24channels targeting the prefrontal cortex. The optodes configuration includes\n10sources and 8detectors spaced 30mm apart, using near-infrared light at 760nm and 840\nnm, sampled at 50Hz. Additionally, facial movements are captured by a Logitech Stream-\nCam at30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low Pain ,\nandHigh Pain . It features 65instances of No Pain (each lasting 60s),780instances of Low\nPain (each lasting 10s), and 780instances of High Pain (each lasting 10s). The No Pain\ninstances, recorded during baseline, serve as control data. The Low Pain instances reflect\nmild pain responses, and the High Pain instances capture significant pain, both derived from\na pain tolerance test and reflected in the corresponding neurological and behavioral data\nrecorded.\n3.4 Unimodal studies\nThis section presents the studies that utilized only one information channel to estimate the\nsubject\u2019s pain condition.28 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.1 Vision-based: Static Analysis\nThe first publicly available pain database that significantly contributed to the development\nof automatic pain assessment methods was the UNBC-McMaster Shoulder Pain Database .\nNumerous studies have employed this dataset. Pedersen [119] implemented the first deep\nlearning approach in 2015 to address the pain assessment problem, utilizing a 4-layer contrac-\ntive autoencoder. He combined the encoded representations with a support vector machine\n(SVM), achieving high performance in frame-level pain detection. A significant advance-\nment in vision-based pain recognition methods was the EmoPain challenge in 2020, which\nbecame the first international competition to compare machine learning methods for chronic\npain assessment. Egede et al. [120] presented the EMOPAIN 2020 Challenge , utilizing a\ndataset composed of features extracted via both handcrafted methods and deep-learned mod-\nels. They utilized facial landmarks, histogram of oriented gradients (HOG), and deep vectors\nfrom VGG-16 [121] and ResNet-50 [122], both pre-trained on the Aff-Wild dataset1. The au-\nthors report that combining hand-engineered features with deep learning cues led to the best\nperformance. Similarly, Yang et al. [123] extracted both low- and high-level features from lo-\ncal descriptors and the pre-trained VGG-16 CNN, combining them through weighted coeffi-\ncients. Semwal and Londhe [124] demonstrated that fusing deep-learned features with facial\nlandmarks is beneficial for multi-class pain estimation. Lakshminarayan et al. [125] com-\nbined deep-learned features with handcrafted ones\u2014namely features from VGG-16 [121]\nandResNet-50 [122], HOG, action unit occurrence and intensity, facial landmarks, and head\npose\u2014through a fully connected network. Their study found that combining VGG-16 with\nhandcrafted features lowered regression error, whereas [126] achieved maximal performance\nusing only VGG-16 features with a fully connected network.\nConversely, Semwal and Londhe [127] noted the limitations of traditional handcrafted\nfeature engineering and the computational expense of deep neural networks. As a solution,\nthey proposed a relatively shallow 4-layer CNN, which reduces computational costs due to\nfewer parameters while achieving performance comparable to deeper models. A different\napproach came from [128], where the authors focused on representing facial expressions\nas compact binary codes for pain intensity classification. Feature extraction was conducted\nusing a pre-trained model [129], with a fully connected network used to generate the binary\ncodes.\nSeveral studies utilized CNN ensemble designs with varying architectures to exploit fea-\nture diversity. Semwal and Londhe [130] combined predictions from three compact CNNs\u2014\nVGG-16 ,M-MobileNet [131], and GoogleNet [132]\u2014using the average ensemble rule, re-\nsulting in improved classification performance. Kharghanian et al. [133] developed a con-\nvolutional deep belief network (CDBN) using unsupervised feature learning. An SVM used\n1https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge3.4. UNIMODAL STUDIES 29\nthe extracted features to differentiate between two states for binary pain classification ( i.e.,\npain vs. no pain). Later, [134] added two layers to the CDBN, though the results were not\ndirectly comparable due to differing evaluation methods.\nSeveral papers suggest that because pain is predominantly expressed in specific facial\nregions, focusing on these areas rather than the whole face could improve model accuracy by\nreducing noise. Huang et al. [135] initially identified the left eye, right eye, nose, and mouth\nas key regions and utilized a multi-stream CNN for feature extraction, assigning learned\nweights to enhance attention on these regions. Xin et al. [136] employed a 9-layer CNN\nwith an attention mechanism to assign different weights to face regions, resulting in more\naccurate attention face maps and boosting prediction accuracy by up to 19%. Cui and Huang\n[137] introduced a multi-scale regional attention network (MSRAN), which uses multiple\ncropping regions from video frames. The framework includes self-attention and relation-\nattention modules to highlight pain-relevant regions and explore interrelationships. Li et\nal.[138] extended this concept by integrating contrastive and multi-task training through an\nautoencoder, building on the work of [139].\nOne challenge in pain intensity estimation is that individual facial features, such as face\nshape, can introduce significant variability in how pain is expressed. This makes it difficult\nto distinguish between adjacent intensity levels. To address this, Peng et al. [140] examined\nfacial shape information and developed a deep multi-task network to account for the rela-\ntionship between pain recognition and shape, which improved pain estimation performance.\nSimilarly, Xin et al. [141] proposed a novel multi-task framework that combines a CNN\nfeature learning module with an autoencoder attention component, also estimating subject\nidentity, as individual differences in pain manifestation are key. Their experiments achieved\nstate-of-the-art results on publicly available datasets.\nMost studies report results obtained from controlled laboratory settings, which typically\nfeature proper lighting, minimal head pose variability, and no occlusions. However, such\nconditions do not represent typical hospital environments. Semwal and Londhe [142] ad-\ndressed this by focusing on pain assessment in uncontrolled settings, developing a shallow\nCNN with three convolutional layers that performed comparably to deeper pre-trained mod-\nels. In a subsequent study [143], they introduced a more complex framework comprising\nthree modules that leveraged high-level spatial descriptors with both local and global geomet-\nric cues, achieving results comparable to models like GoogleNet [144] and VGG [121]. Lee\nand Wang [145] explored pain assessment in intensive care unit (ICU) settings, where par-\ntially occluded faces frequently complicate facial analysis. They developed a 4-layer CNN\ncombined with an extreme learning machine (ELM) for final estimation. Virrey and Cae-\nsarendra [146] used CNNs to classify sections of frames where pain was triggered, peaked,\nand subsided. Nugroho et al. [147] tackled pain detection in smart home-care settings, par-\nticularly for elderly patients, using relatively low-power mobile devices. They modified the30 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nOpenFace2library, based on pre-trained FaceNets [148], and showed that transfer learning\ncould enable real-time binary classification ( pain vs.no pain ), even on low-powered hard-\nware.\nResearchers like Dai et al. [149] and Menchetti et al. [150] have noted that most models,\nwhether deep or shallow, are trained on dataset-specific features rather than actual pain-\nrelated features. Moreover, most studies employ validation methods using the same dataset,\nwhile cross-dataset performance is rarely addressed, limiting real-world applicability. To\ntackle these issues, Dai et al. [149] combined pain and emotion detection datasets to develop\na real-time pain assessment system with better generalization capabilities. They emphasized\nthe importance of cross-corpus evaluation, real-time testing, and the need for well-balanced,\necologically valid pain datasets [151].\nSeveral studies have explored combining pain scales to improve prediction objectivity\nand reliability. Liu et al. [152] developed a two-stage personalized model trained using active\nappearance model (AAM) facial landmarks and multi-task learning, with visual analog scale\n(V AS) and observed pain index (OPI) as ground truth. Xu et al. [153] similarly reduced\nmean square error (MSE) by incorporating various pain scales with the VGG-Face model.\nHowever, Casti et al. [154] pointed out the limitations of original ground truth data due to\nsubjectivity and annotation inconsistencies. To address this, they re-annotated their dataset\nwith judgments from multiple experts, using multidimensional scaling to map frames to\nillumination-invariant 3D space, which they then fed into a pre-trained AlexNet [155].\nCelona and Manoni [156] investigated neonatal facial expressions to detect pain, achiev-\ning the highest accuracy when utilizing two pre-trained models: VGG-Face [157] and mapped\nLBP+CNN (MBPCNN) [158]. Similarly, Lu and Hao [159] found that pre-trained models\nwere crucial for small datasets like neonates, as training from scratch led to overfitting. They\nachieved optimal classification performance by fine-tuning the entire VGG-16 model [122].\nHowever, Zamzmi et al. [160] argue that most face recognition methods are tailored for\nadults and thus less applicable to infants. They developed a lightweight 2D CNN trained\nend-to-end and achieved high pain detection accuracy, but external validation on a different\nneonatal dataset revealed challenges with generalizability. In 2019, Brahnam et al. [112]\nintroduced the iCOPEvid neonatal video dataset, a significant contribution since the only\npublicly available neonatal pain dataset [111] previously contained only static images. Their\nexperiments showed that local descriptors based on the bag-of-features (BoF) approach out-\nperformed deep learning models like VGG-Face andResNet . Combining handcrafted and\ndeep-learned features offered only a marginal improvement in performance. In contrast, Za-\nmzmi et al. [161] found that the most effective approach for binary classification (pain vs.\nno pain) was the fusion of high-level features from VGG [162] and optical flow strains, with\n2http://cmusatyalab.github.io/openface3.4. UNIMODAL STUDIES 31\nnaive Bayes serving as the classifier. Celona and Brahnam [163] applied a Wasserstein gen-\nerative adversarial network with gradient penalty (WGAN-GP) [164], demonstrating that\ntraining set augmentation with synthetic samples improved classification performance. Ta-\nble 3.2 summarizes the vision-based studies focusing exclusively on the spatial dimension.32 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [112]F (RGB) texture\ndescriptors- FF 2D CNN`SVM SL C P O 49 k-fold iCOPEvid 79.80 AUC\n'15 [119]F (RGB) - - - AE SVM SeSL,\nSLC P PS 25 LOSO UNBC 86.10 ACC,\n96.50 AUC\n'20 [120]F (RGB) - - FF 2D CNN`NN SL R IC O 36 hold-out EmoPain 0.91 MAE;\n'18 [123]F (RGB) HOG,\nstatistics- FF 2D CNN`SVR SL R IC PS 25 LOSO UNBC 1.44 MSE;\n'21 [130]F (RGB) - - DF 2D CNN`- SL C ID PS 25 k-fold UNBC 93.87 ACC;\n'16 [133]F (RGB) - - - CDBN SVM UL C P PS 25 LOSO:UNBC 87.20 ACC;\n'21 [134]F (RGB) - - - CDBN SVM SL C P PS 25 LOSO UNBC 93.16 AUC\n'19 [135]F (RGB) - - FF 2D CNN - SL C ID1, IC PS 25 LOSO UNBC 88.191ACC\n'20 [136]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 51.10 ACC;\n'20 [140]F (RGB) - - FF 2D CNN`- SL R ID S 25 ? UNBC 79.94 ACC;\n'21 [142]F (RGB) - - - 2D CNN - SL C ID O 8 k-fold other 97.48 ACC;\n'19 [145]F (RGB) - - - 2D CNN ELM SL R IC PS 25 k-fold UNBC\u201a1.22 MSE;\n'19 [146]F (RGB) - - - 2D CNN - SL C TR, CL, DI PS 25 k-fold UNBC 60.00 ACC\n'19 [150]F (RGB) - - - 2D CNN`- SL C AUs-D PS 25, 43 k-fold UNBC & CK+1,\nWilkie97.701ACC;\n'17 [152]F (RGB) statistics - - NN GPM WSL R IC O, S 25 k-fold UNBC 2.18 MAE\n'20 [153]F (RGB) statistics - FF 2D CNN`NN SL R IC S 25 k-fold UNBC 1.95 MAE;\n'19 [154]F (RGB) LBP, MDS - - 2D CNN`- SL C ID O 25 hold-out UNBC 80.00 ACC\n'18 [159]F (RGB) - - - 2D CNN`- SL C ID O ? hold-out other 78.30 ACC\n`: Pre-trained model -:Not exist &: in Dataset indicates the utilization of cross-database training/validation ?: Not found :: The authors provide additional experiments with other validation methods \u201a: The authors\nutilized occluded facial images ;: The authors provide additional metrics Modality: F: face region Non deep features: LBP: local binary pattern MDS: multidimensional scaling Fusion: M: fusion of modalities E:\nfusion of deep learned features or hand-crafted features Deep models: AE: autoencoder RCNN: recurrent convolutional neural network CDBN: convolutional deep belief network CNN: convolutional neural network\nNN: neural network WGAN-GP: Wasserstein generative adversarial model with gradient penalty Non deep model: SVM: support vector machine GPM: Gaussian process regression model kNN: k-nearest neighbors\nNB: naive Bayes ELM: extreme learning machine Learning Method: SL: supervised learning SeSL: semi-supervised learning UL: unsupervised learning WSL: weakly supervised learning Classific./Regres.: C:\nclassification R: regression Objective: P: presence of pain ID: intensity in discrete scale IC: intensity in continuous scale TR: trigger CL: climax DI: diminishing AUs-D: Action Units detection GT: ground truth\nPS: Prkachin and Solomon S: self-report O: observer rating ST: stimulus Validation Method: LOSO: leave one subject out Metrics: AUC: Area Under the ROC Curve ACC: accuracy PPV: precision MSE: mean\nsquared error MAE: mean absolute error3.4. UNIMODAL STUDIES 33Table 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [124]F (RGB) facial landmarks - FF 2D CNN NN SL C, R ID, IC1P 25 LOSO:UNBC 0.171MSE;\n'20 [125]F (RGB) HOG, head pose,\nAUs intensity/\noccurrence, facial\nlandmarksFF - 2D CNN`NN SL R IC O 36 hold-out EmoPain 5.48 RMSE;\n'20 [126]F (RGB) - - - 2D CNN`NN SL R IC O 36 hold-out EmoPain 1.49 RMSE;\n'18 [127]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 92.00 ACC;\n'18 [128]F (RGB) statistics, distance\nmetrics- FF 2D CNN`- SL C, R ID, IC PS 25 LOSO UNBC 0.81 PCC,\n0.69 MSE\n'21 [137]F (RGB) - - FF 2D CNN`- SL C, R ID, IC P 25 LOSO UNBC 91.13 ACC,\n0.78 PCC,\n0.46 MSE\n'18 [138]F (RGB) - - - AE`- SL R IC PS 25 k-fold UNBC 0.33 MAE;\n'21 [141]F (RGB) - - FF [AE, 2D CNN]Y- SL C, R ID1, IC2,\nP3P, ST 25, 87 LOSO UNBC1,\nBioVid (A)289.1711ACC,\n0.8121PCC,\n85.6532ACC,\n40.4012ACC\n'21 [143]F (RGB) entropy texture\ndescriptors- - 2D CNN`- SL C ID O 8 k-fold other 0.92 PPV;\n'18 [147]F (RGB) - - - 2D CNN`- SL C P PS 14 k-fold UNBC 93.00 ACC\n'19 [149]F (RGB) - - - 2D CNN - SL C P PS 25, 20 k-fold UNBC &\nBioVid (A)\u02db56.75 ACC\n'17 [156]F (RGB) HOG, LBP - FF 2D CNN`SVM SL C P O 26 LOSO iCOPE 73.78 ACC\n'19 [160]F (RGB) - - - 2D CNN - SL C P O 31 LOSO NPAD1,\niCOPE296.981ACC;,\n89.802ACC\n'21 [165]F (RGB) - - - 2D CNN`- FL C P PS 25 LOSO UNBC 76.00 ACC;\n'21 [166]F (RGB) - - - 2D CNN`- SL C P O 25 hold-out UNBC 75.49 ACC\n'21 [167]F (RGB) - - - 2D CNN`SVR SL R IC P 25 LOSO UNBC 0.34 MSE\n'21 [168]F (RGB) - - - 2D R-CNN - SL C P O ? hold-out other 87.80 PPV\nY: The authors combined the deep models into a unified framework \u02db: The authors experimented with additional datasets combinations Non deep features: AUs: actions units HOG: histogram of oriented gradients Non\ndeep model: SVR: support vector regression Learning Method: FL: federated learning Metrics: RMSE: root mean squared error34 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [161]F (RGB) optical flow - FF 2D CNN`SVM,\nkNN, NBSL C P O 31 k-fold other 92.71 ACC,\n94.80 AUR\n'19 [163]F (RGB) - - - WGAN-GP - SL C P O 26 LOSO iCOPE 93.38 ACC\n'17 [169]F (RGB) - - - 2D CNN`- SL R IC PS 25 LOSO UNBC 0.99 MAE;\n'20 [170]F (RGB) - - - 2D CNN - SL C ID ST 87 hold-out BioVid (A) 36.60 ACC\n'20 [171]F (RGB) - - - 2D CNN - SL C P PS 25 hold-out UNBC 97.00 PPV;\n'21 [172]F (RGB) - - - 2D CNN - SL C ID P 28 LOSO:UNBC 90.30 ACC\n'19 [173]F (RGB) - - - 2D CNN - SL C P O 31 hold-out NPAD1,\niCOPE291.001ACC;,\n84.502ACC;\n'21 [174]F (RGB) - - - 2D CNN`- SL C P O 26, 30 hold-out iCOPE &\nUNIFESP89.90 ACC;\n'21 [175]F (RGB) - - - 2D CNN - SL C AUs-D P 10 hold-out Pain-ICU 77.00 ACC;3.4. UNIMODAL STUDIES 35\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach)\nPain assessment is particularly challenging due to its complex and dynamic nature. Rely-\ning on static, individual frames to assess pain fails to capture the phenomenon\u2019s temporal\nprogression and often leads to inaccurate estimations. Additionally, many studies highlight\nthe difficulties of applying deep learning techniques to small datasets, with one proposed\nsolution being the combination of deep learning and traditional feature extraction methods.\nEgede et al. [176] addressed this by extracting deep features from a pre-trained CNN, explic-\nitly targeting the eyes and mouth regions. Using a relevance vector regressor (RVR), they\ndemonstrated that combining deep and hand-crafted features led to optimal performance. De-\nspite the valuable insights the UNBC-McMaster database provides, its imbalanced sample\ndistribution\u2014particularly the limited number of frames showing pain\u2014poses a significant\nchallenge for deep learning models. In response, Egede and Valstar [177] devised a method\nbased on the observation that neighboring pain level classes share many common features.\nThis approach allowed them to avoid extracting all possible features for classes with fewer\nsamples, as certain features had already been utilized from other related classes. The study\nalso showed that combining deep and hand-crafted features improved performance. How-\never, in a later study [178], the authors applied a similar approach, using only deep-learned\nfeatures to address data imbalance, but could not replicate the same high-performance levels.\nTavakolian et al. [179] took a different approach, focusing on the detection of genuine\nversus acted pain through facial expressions, a technique with important applications in both\nmedical and forensic contexts. They developed a residual GAN (R-GAN) to capture subtle\nfacial changes and the dynamic nature of expressions, using a weighted spatio-temporal pool-\ning (WSP) method. In a subsequent study [180], the authors suggested that self-supervised\nlearning could reduce the time and effort needed for data labeling, as it does not require\ncomplete dataset annotation. They introduced a new similarity function for learning general-\nized representations with a Siamese network. They also employed statistical spatio-temporal\ndistillation (SSD) based on the Gaussian scale mixture (GSM) to improve computational effi-\nciency. This technique encodes spatiotemporal variations in facial videos into a single RGB\nimage, simplifying the model while maintaining effectiveness.\nOther studies also aim to capture the dynamic aspects of pain. For instance, [181] com-\nbined a random forest classifier with the pre-trained MobileNetV2 model [182], encoding\nvideos by selecting and merging three frames from different time points into a single image.\nOthman et al. [183] emphasized the importance of using diverse datasets\u2014including vary-\ning age, gender, pose, occlusion, and lighting conditions\u2014to improve model generalization.\nThey used multiple data combinations and a reduced version of MobileNetV2 , showing that\ncross-dataset training is essential for achieving better generalizability.36 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.3 Vision-based: Implicit Temporal Utilization\nSeveral studies have explored the application of 3D CNNs for pain assessment. Tavakolian\nand Hadid [184] developed a 3D CNN to capture dynamic facial representations from videos.\nThey noted that researchers often use fixed temporal kernel depths when employing 3D\nconvolution techniques, which limits the ability to capture short, mid, and long temporal\nranges simultaneously. To address this, they designed a model with parallel 3D convolutional\nlayers featuring variable temporal depths, allowing the capture of temporal dependencies\nfrom 32consecutive frames. Similarly, Wang and Sun [185] applied 3D convolutions based\non the architecture proposed in [186], consisting of 8convolutional layers with 3\u02c63\u02c63\nfilters. While they reported high performance, the authors acknowledged that extracting deep\nfeatures from small datasets posed a challenge for model generalization. In a related study,\nHuang et al. [187] developed a framework that integrated 3D, 2D, and 1D CNNs to extract\nspatio-temporal, spatial, and geometric features. For the 3D CNN component, they modified\nthe architecture from [188] by using discrete kernels of 1\u02c63\u02c63and3\u02c61\u02c61rather than the\ntraditional 3\u02c63\u02c63kernel. Other researchers have also proposed 3D deep CNNs with varying\ntemporal depths to capture short, mid, and long-range facial expression variations [189].\nRecognizing the difficulty and time consumption involved in training a deep 3D CNN from\nscratch, they introduced a cross-architecture knowledge transfer learning technique, utilizing\na pre-trained 2D CNN to assist in the training of the 3D CNN. In studies by Praveen et\nal.[190] and [191], the authors employed weakly-supervised domain adaptation, where the\nsource domain focused on human affective expressions and the target domain was explicitly\nrelated to pain expressions. Their framework featured an inflated 3D-CNN (I3D) [192],\nincorporating 3convolutional layers and 3inception modules [132] to capture both spatial\nand temporal information from video data.\nBargshady et al. [193] opted to use the HSV color space instead of RGB, arguing that it\nbetter reflects human visual perception for tasks such as skin pixel detection and multi-face\ndetection. They employed the pre-trained VGG-Face [157] for feature extraction, followed\nby a temporal convolutional network (TCN) using dilated causal convolutional operations to\nleverage temporal dependencies. Rezaei et al. [194] tackled the challenge of pain detection\nin people with dementia, a difficult task due to insufficient pain-related images or videos\nof elderly subjects in existing datasets. They developed a 10-layer 2D CNN that processed\npairs of pain and no-pain images, analyzing frame-to-frame changes and employing con-\ntrastive training methods [195]. The model demonstrated high performance in both healthy\nindividuals and people with dementia. In another study, Pandit and Schmitt [196] explored\nthe potential of using shallow 1D CNN architectures for real-time pain recognition. They ex-\ntracted facial action units from each frame using the OpenFace 2.03toolkit, with promising\n3https://github.com/TadasBaltrusaitis/OpenFace3.4. UNIMODAL STUDIES 37\nresults for pain detection in real-time settings.\n3.4.4 Vision-based: Explicit Temporal Utilization\nSeveral efforts have focused on addressing the limitations of static frames by developing\ndedicated temporal modules. Zhou et al. [197] tackled this issue using a regression frame-\nwork based on a 4-layer recurrent convolutional neural network (RCNN), each with a se-\nquence length of 3time steps. Rodriguez et al. [198] leveraged dynamic information by\ndesigning an LSTM model fed with feature vectors extracted from VGG-16 [122]. Simi-\nlarly, Bellantonio et al. [199] emphasized that facial expressions evolve, making it essential\nto analyze the spatio-temporal dimension of pain. They improved estimation performance\nusing a fine-tuned 16-layer CNN model [157], an LSTM processing 16frames as a time\nwindow, and super-resolution techniques. In another study, Bargshady et al. [200] com-\nbined the VGG-Face CNN [157] with a 3-layer LSTM to extract spatio-temporal features\nfrom grayscale images, applying zero-phase component analysis (ZCA). In [201], principal\ncomponent analysis (PCA) was used to reduce dimensionality. Mauricio et al. [202] also\nemployed VGG-Face but replaced LSTM with a 2-layer gated recurrent unit (GRU) to cap-\nture temporal dependencies. Thuseethan et al. [203] used a conventional 2D CNN and two\nRCNNs to extract temporal features from previous and subsequent frames, enhancing the\ntime dimension of expression analysis.\nA similar approach was followed by Bargshady et al. [204], who employed ensemble\nlearning with three distinct CNN-biLSTM modules, merging their outputs for the final pre-\ndiction. Salekin et al. [205] used a bilinear CNN (B-CNN) based on the VGG architecture\n[121], pre-trained on VGGFace24andImageNet5datasets, along with an LSTM to capture\ntemporal dependencies in image sequences. Kalischek et al. [206] explored deep domain\nadaptation for facial expression and pain detection, utilizing the self-ensembling approach\n[207] with a long-term recurrent convolutional network (LRCN). While they achieved state-\nof-the-art results for facial expression recognition, performance was lower for pain detection,\nlikely due to the subtle nature of pain-related expressions.\nDespite the availability of additional information in pain datasets, multi-task approaches\nremain limited. Martinez et al. [208] proposed a personalized multi-task learning method\nbased on individual physiological and behavioral pain responses. They extracted AAM fa-\ncial landmarks, processed them through a biLSTM to produce PSPI scores, and predicted the\nfinal V AS score. Erekat et al. [209] combined AlexNet [155] with 2 GRU layers to capture\ntemporal dependencies, using both self and observer-reported pain intensity as ground truth.\nVuet al. [210] developed a multi-task framework to estimate pain levels while reconstruct-\n4https://www.robots.ox.ac.uk/ \u02dcvgg/data/vgg_face\n5https://www.image-net.org38 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\ning heatmaps of action unit locations, improving model generalization with a CNN-LSTM\ncombination to capture micro facial movements.\nHuang et al. [211] noted that specific frames within a video sequence exhibit more pro-\nnounced pain expressions, requiring special handling. They developed a novel framework\nusing attention saliency maps with a VGG-16 model, GRUs and learned weights for each\nframe\u2019s contribution to pain intensity estimation. The study demonstrated that dynamic and\nsalient features can significantly improve performance. Similarly, Yu et al. [212] used VGG-\n11 (configuration A) and an LSTM to create an attention mechanism, predicting pain in-\ntensity from 16consecutive frames. Xu and Liu [213] adopted a ResNet-50 model with an\nattention mechanism to extract spatial features, followed by a transformer encoder to capture\ntemporal sequences, achieving promising results.\nIn other studies, Ragolta et al. [214] used extracted action units to train a 2-layer LSTM\npredicting pain on an 11-point scale, employing curriculum learning. Guo et al. [215] devel-\noped a convolutional LSTM (C-LSTM) to extract both spatial and temporal features from\nvideos, showing that temporal models outperform non-temporal models for pain estimation\naccuracy. Rasipuram et al. [216] utilized in-the-wild video data for pain detection, gener-\nating a 3D morphable model without relying on facial landmarks and combining it with an\nLSTM. Zhi and Wan [217] introduced sparse coding with LSTM (SLTM), using the iterative\nhard thresholding algorithm (ISTA) [218] to capture dynamic facial expressions. Although\nSLTM did not achieve high performance, it offers speed and efficiency for specific applica-\ntions. Finally, Thiam et al. [219] developed a method combining motion history and optical\nflow images with a 10-layer CNN and 2-layer biLSTM, showing that weighted score aggre-\ngation improves performance. Table 3.3 summarizes studies incorporating the modalities\u2019\ntemporal dimensions.3.4. UNIMODAL STUDIES 39Table 3.3: Vision-based studies with temporal utilization.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'17 [176] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVR SL R IC PS 25 LOSO UNBC 0.99 RMSE,\n0.67 PCC\n'17 [177] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVM SL R IC PS 25 LOSO UNBC 1.04 RMSE,\n0.64 PCC\n\u201918 [178] F (RGB) - - - NL 2D CNN - SL R IC PS 25 LOSO UNBC 1.20 RMSE,\n0.47 PCC\n'18 [184] F (RGB) - - - I 3D CNN - SL R IC PS 25 LOSO UNBC 0.53 MSE,\n0.84 PCC;\n'18 [185] F (RGB) HOG,\ngeometric\ndifference- DF I 3D CNN SVR SL R IC PS 25 LOSO UNBC 0.94 RMSE,\n0.67 PCC\n'20 [191] F (RGB) - - - I 3D CNN`- WSL R IC PS 24, ? LOSO UNBC\n& RECOLA0.64 MAE,\n0.82 PCC;\n'16 [197] F (RGB) - - FF E RCNN - SL R IC PS 25 LOSO UNBC 1.54 MSE,\n0.65 PCC\n'17 [198] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C, R P, IC1PS 25 LOSO UNBC 0.741MSE,\n0.781PCC;\n'17 [199] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 61.90 ACC\n'19 [200] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 75.20 ACC\n'20 [201] F (RGB) PCA - DF E [2D CNN`,\n1D CNN, biLSTM]Y- SL C ID PS 25 LOSO:UNBC 85.00 ACC;\n'19 [202] F (RGB) - - - E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 85.40 ACC,\n0.62 MSE;\n'19 [203] F (RGB) - - FF E [2D CNN, RCNN]Y- SL R IC PS 25 LOSO UNBC 1.29 MSE,\n0.73 PCC\n'17 [208] F (RGB) - - FF E biLSTM HCRF,\nFCSL C IC O,\nS25 hold-out UNBC 2.46 MAE;\nNon deep features: PCA: principal component analysis Temporal Exploitation: NL: non-machine learning method I: implicit method E: explicit method Deep models: RCNN: recurrent convolutional neural network\nLSTM: long short memory networks biLSTM: bidirectional neural network GRU: gated recurrent unit Non deep models: SVM: support vector machine RVM: relevance vector machine GPM: Gaussian process regression\nmodel HCRF: hidden conditional random fields FC: fully connected SVR: support vector regression Objective: I2: intensity in binary pairs Metrics: PCC: Pearson correlation coefficient40 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [114]F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN RVR SL R IC O 13 LOSO APN-DB 1.71 MAE;\n'19 [179]F (RGB) - - - NL R-GAN - UL C genuine\nvs posedPS,\nST25,\n34,\n87,\n87? UNBC\n& STOIC\n& BioVid (A)\n& BioVid (D)90.97 ACC\n'20 [180]F (RGB) - - FF NL 2D CNN`- SSL C IC P,\nST25\n87LOSO UNBC1,\nBioVid (A)2\u20180.781PCC;,\n71.022AUC;\n'21 [181]F (RGB) AUs\nintensity- H NL 2D CNN`RF SL C ID ST 127 k-fold X-ITE 25.00 ACC\n'19 [183]F (RGB) - - - NL 2D CNN - SL C P ST 87\n134k-fold BioVid (A)\n& X-ITE\u201867.90 ACC\n'20 [209]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC O,\nS25 k-fold UNBC 2.34 MAE\n'20 [211]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC PS 19 LOSO UNBC 0.21 MSE,\n0.89 PCC\n'19 [212]F (RGB) - - FF E [2D CNN, LSTM]Y- SL R IC PS 24 LOSO UNBC 1.22 MSE;,\n0.40 PCC;\n'20 [214]F (RGB) AUs\nintensity- - E LSTM - SL R IC O 36 hold-out EmoPain 2.12 RMSE,\n1.60 MAE;\n'20 [216]F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C P O ? k-fold UNBC 78.20 ACC;\n'20 [219]F (RGB) - - DF E [2D CNN, biLSTM,\nNN]Y- SL C P ST 87\n40LOSO BioVid (A)1,\nSenseEmotion269.251ACC,\n64.352ACC\n'20 [220]F (RGB) - - FF E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 0.84 ACC,\n0.69 PCC;\n\u2018: The authors provide experiments with cross-dataset settings Fusion: H: hybrid Non deep models: RF: random forest classifier3.4. UNIMODAL STUDIES 41Table 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [187]F (RGB) facial\nlandmarks- DF I [3D CNN`,\n2D CNN`,\n1D CNN, FC]Y- SL R IC PS 25 LOSO UNBC 0.76 MSE,\n0.82 PCC;\n'19 [189]F (RGB) - - - I [2D CNN`,\n3D CNN]Y- UL,\nSLC, R IC1, P2P,\nST25, 87 LOSO UNBC1,\nBioVid (A)20.9211PCC;,\n86.0222AUC\n'20 [190]F (RGB) - - - I 3D CNN`- WSL R IC PS 24,?,\n87, 18LOSO UNBC1\n& RECOLA\n& BioVid (A)2\u20180.741PCC,\n0.342PCC\n'20 [193]F (RGB) PCA - FF I [2D CNN`,\nTCN]Y- SL C ID P,\nST25, 20 LOSO:UNBC1,\nMIntPAIN292.441ACC;,\n89.002ACC;\n'20 [194]F (RGB) - - - I 2D CNN - SL C, R IC, P1P 95, 25 k-fold UofR & UNBC182.0011PCC;\n'20 [196]F (RGB) AUs\noccurrence- FF I 1D CNN - SL R IC P 24, 87 hold-\noutUNBC1,\nBioVid (A)0.801CCC\n'20 [204]F (RGB) PCA - DF E [2D CNN`, 1D\nCNN, biLSTM]Y- SL C ID PS,\nST25, 20 k-fold UNBC1,\nMIntPAIN286.001ACC;\n92.262ACC;\n'20 [205]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- SL R P, IC1O 45 LOSO NPAD 3.991MSE,\n1.552MAE\n'19 [206]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- UL C P ST 40 LOSO SenseEmotion 60.61 ACC\n'21 [210]F (RGB) - - - E [2D CNN`,\nLSTM]Y- SL R IC P 25, 27 LOSO UNBC1,\nDISFA\u20180.60`MSE,\n0.82`PCC;\n'21 [213]F (RGB) - - - E [2D CNN`,\nTransformer]Y- SL R IC P 25 LOSO UNBC 0.40 MSE,\n0.76 PCC;\n'21 [215]F (RGB) - - - E 2D C-LSTM - SL C ID S 29 hold-\noutother 69.58 F1\n'19 [217]F (RGB) - - FF E SLSTM - SL C P1, ID2ST 85 LOSO BioVid (A) 61.701ACC\n29.702ACC\n'21 [221]F (RGB) - - - I 3D CNN`- SL R IC S 25 k-fold UNBC 0.66 ICC;\nFusion: H: hybrid Deep models: TCN: temporal convolutional neural network C-LSTM: convolutional-LSTM SLTM: sparse long short memory network Learning Method: SSL: self-supervised learning Metrics: F1:\nF1 score CCC: concordance correlation coefficient42 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.5 Touch sensor-based\nTouch (contact) sensors provide a viable alternative for pain assessment, often outperforming\nvision-based methods. Table 3.4 highlights studies that utilized contact sensor data to evalu-\nate pain. Yu et al. [222] analyzed three categories of pain-no pain, moderate pain, and severe\npain\u2014using EEG signals. They extracted several bands from the biosignals, including al-\npha, beta, and gamma, and applied a convolutional module. The study found that combining\nthese bands yielded better results than evaluating them independently. Similarly, [223] used\nEEG potentials with an autoencoder to compress the raw data and applied a logistic regressor\nfor classification.\nOther researchers, such as Rojas et al. [224], utilized functional near-infrared spec-\ntroscopy (fNIRS) for pain detection. They developed three models\u2014multilayer perceptron\n(MLP), LSTM, and biLSTM\u2014with biLSTM demonstrating superior accuracy. Addition-\nally, [225] focused on PPG signals, extracting hand-crafted features from the time and fre-\nquency domains, which were then combined with a deep belief network (DBN) to achieve\nover65% accuracy in a 4-class pain assessment task. Hu et al. [226] used kinematic data\nto compare healthy individuals with those suffering from low back pain (LBP). Their ap-\nproach, which employed two stacked LSTM layers, reached over 97% accuracy in binary\nclassification using raw motion data. Lastly, Mamontov et al. [227] were the first to apply\nevolutionary algorithms in the design of an optimized recurrent neural network (RNN) for\npain estimation, achieving 91.94% accuracy using EDA signals.\n3.4.6 Audio-based\nA few studies have explored using audio information for pain detection and intensity esti-\nmation, as outlined in Table 3.5. These methods are especially relevant for neonates, where\nfrequent facial and body occlusions make analyzing cries a more effective approach for pain\ndetection. Chang and Li [228] concentrated on infant cries to differentiate between hunger,\npain, and sleepiness. They transformed the audio signals into 2D spectrograms using a fast\nFourier transform (FFT) and trained a 2D CNN for feature extraction. Similarly, [229] uti-\nlized spectrograms generated from recorded sounds, employing a model identical to that\nused in [160]. Thiam and Schwenker [230] focused on detecting adult pain by analyzing\nbreathing sounds. They leveraged deep-learned features from spectrograms with Mel-scaled\nshort-time Fourier transform, combined with various handcrafted cues. A CNN followed by\na biLSTM was used to capture spatial and temporal dependencies, integrating both low- and\nhigh-level features. In a different approach, Tsai et al. [231] examined pain events during\nemergency triage. They developed an LSTM autoencoder framework to extract temporal\nfeatures from verbal behavior, reporting encouraging results.3.4. UNIMODAL STUDIES 43Table 3.4: Touch sensor-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'20 [222]EEG - - FF I 1D TCN - S C ID S 32 k-fold other 97.30 ACC;\n'20 [223]EEG - - - I AE (TCN) LR UL, S C P S 29 LOSO other 74.60 ACC\n'21 [224]fNIRS - - - E biLSTM - SL C ID S 18 k-fold other 90.60 ACC;\n'19 [225]PPG - - - NL DBN SBM U, SL C P1, ID2S 100 k-fold other 86.791ACC,\n65.572ACC\n'18 [226]kinematatics - - FF E LSTM - SL C P LBP 44 LOSO other 97.20 ACC;\n'19 [227]EDA - - FF E [RNN, LSTM,\nGRU, NN]YSelfCGA,\nselfCGP,\nPSOPBSL C P ST 40 LOSO Sense-\nEmotion81.94 ACC\n'21 [232]EDA - - - I NN - SL C P1, I2 ST 87,\n55LOSO BioVid (A)1,\nPainMonit284.2211ACC;,\n86.5012ACC;\nModality: PPG: photoplethysmogram fNIRS: functional near-infrared spectroscopy EEG: electroencephalography EDA: electrodermal activity Deep models: DBN: Deep belief network RNN: recurrent neural network\nNon deep models: SBM: selective bagging model LR: Logistic Regression SelfCGA: Self-Configuring Genetic Algorithm SelfCGP: Self-Configuring Genetic Programming PSOPB: Particle Swarm Optimisation with\nparasitic behaviour GT: LBP: low back pain vs healthy population\nTable 3.5: Audio-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'16 [228]audio (cry) - - - - 2D CNN - SL C P O ? k-fold other 78.50 ACC\n'19 [229]audio (cry) - - - - 2D CNN - SL C P O 31 LOSO:NPAD 96.77 ACC;\n'19 [230]audio\n(breathing)MFCCs,\nRASTA-\nPLP,\nDTD- FF E [2D CNN,\nLSTM]YRFc SL C P ST 40 LOSO Sense-\nEmotion64.39 ACC\n'17 [231]audio\n(voice)prosodic-\nspectral\nfeatures,\nSF- FF E LSTM`SVM UL,\nSLC P1, ID2S 63 LOSO other 72.301UAR,\n54.202UAR\nNon deep features: MFCCs: Mel Frequency Cepstral Coefficients RASTA-PLT: Relative Spectral Perceptual Linear Predictive DTD: descriptors from temporal domain SF: statistical features44 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.5 Multimodal studies\nSince pain is a multidimensional phenomenon, combining multiple modalities in a multi-\nmodal system offers a promising approach. Heterogeneous information sources can com-\nplement one another, enhancing specificity and sensitivity. As reported in [106], when in-\ndividual modalities demonstrate good predictive performance, their fusion tends to yield\nimproved outcomes. Moreover, integrating cues from various channels may be helpful and\nnecessary, especially in clinical settings where specific modalities may become unavailable\n(for instance, if the patient turns and their face is occluded). The information channels can\noriginate from (1) the same hardware sensor but focus on different regions of interest, such\nas RGB facial images and RGB body images [233], (2) different hardware sensors but the\nsame region of interest, like RGB facial images and thermal facial images [110], or (3)\ndifferent hardware sensors and information sources, such as RGB facial images and ECG\nsignals [234]. Table 3.6 lists the studies utilizing multimodal approaches.\n3.5.1 Static Analysis\nA commonly used biosignal combination is those of EDA, EMG, and ECG, as these channels\nare found in all main pain reference databases. Thiam et al. [235] applied an early fusion\nmethod by merging these signals into a 2D representation and inputting it into a 9-layer 2D\nCNN. Their results showed a strong correlation between EDA and pain intensity, and com-\nbining all three modalities did not outperform using EDA alone. Al-Qerem et al. [236] used\nleast generative adversarial networks (LSGANs) to enhance EMG, EDA, and ECG samples,\nreporting a notable improvement in classification when using an SVM on the augmented\ndataset. Haque et al. [110] introduced the MIntPAIN dataset, which includes RGB, depth,\nand thermal videos for multi-class ( 5levels) pain recognition. They combined these three vi-\nsual modalities into a 5D matrix (RGB+D+T) and used it to train the pre-trained VGG-Face\nmodel [157], leading to better classification performance in their experiments.\n3.5.2 Temporal Utilization\nZhiet al. [237] proposed a multimodal stream-integrated neural network that leverages video\nand biosignal data. They combined raw facial video frames with optical flow images to cap-\nture spatio-temporal dependencies via 3D CNNs, integrating these with biosignal features\nextracted using LSTMs. The entire network was trained end-to-end, achieving superior re-\nsults compared to their unimodal methods. Beyond facial analysis, Salekin et al. [233]\nfocused on assessing neonatal pain through body movements in videos. After identifying\nrelevant body regions, video frames were fed into a pre-trained VGG-16 [121], connected to\nan LSTM to capture temporal dynamics. In a follow-up study, Salekin et al. [238] fused three3.5. MULTIMODAL STUDIES 45\nmodalities\u2014facial expressions, body movements, and crying sounds\u2013demonstrating that this\nmultimodal approach outperformed unimodal techniques. Similarly, Wang et al. [239] ex-\nplored combining EMG, EDA, and ECG biosignals with handcrafted and learned features\nfrom a biLSTM model. They applied the minimum relevance method (MRMR) to reduce\nthe number of features, resulting in notable outcomes.\nIn addition to EDA, EMG, and ECG, other biosignal combinations have been explored.\nZhao et al. [240] integrated PPG, EDA, and temperature signals, using 2D convolutions for\nspatial feature extraction and time windows for capturing temporal information. Yuan et\nal.[241] successfully estimated pain using whole-body MoCap sensors and EMG, utilizing\nLSTM layers with an attention mechanism in an autoencoder, which reduced training time\nby leveraging latent space representations of raw data. Similarly, Li et al. [242] employed\nMoCap and EMG as data sources and tested various LSTM configurations to predict pain\nintensity, achieving the best performance with a 3-layer vanilla LSTM combined with a 3-\nlayer fully connected network.46 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.6: Multimodal-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [110]F (RGB,\nthermal,\ndepth)- RF - - 2D CNN`- SL C ID S 20 k-fold MIntPAIN 36.55 ACC\n'19 [233]F, B (RGB) - FF - E [2D CNN`,\nLSTM]Y- SL C P O 31 LOSO other 92.48 ACC;\n'19 [234]F (RGB),\nECG, EDAbiosignals\u2019\nfeaturesmFF FF - 2D CNN`RFc SL C I2 S 85 k-fold BioVid (A) 74.00 ACC\n'19 [235]EDA,\nEMG,\nECG- RF - - 2D CNN - SL C P1I2,\nID2S 87,\n86LOSO BioVid (A)1\nBioVid (B)84.4011ACC;,\n36.5412ACC;\n'20 [236]EDA,\nEMG,\nECGBoruta\nfeaturesFF - - LSGAN SVM UL,\nSLC I2, ID1S 85 hold-\noutBioVid (A) 82.801ACC\n'21 [237]F (RGB),\nEDA,\nEMG, ECGoptical\nflowFF FF NL,\nE, I[3D CNN,\nLSTM]Y- SL C, R P1, I2,\nID2S 87,\n40k-fold:BioVid (A)1,\nMIntPain68.2011ACC;,\n28.1021ACC\n'21 [238]F, B (RGB),\nsound- DF - E [2D CNN`,\nLSTM]Y- SL C P O 45 LOSO NPAD 78.95 ACC;\n'20 [239]EDA,\nEMG,\nECGMRMR,\nbiosig-\nnals\u2019\nfeaturesRF\nFFE biLSTM NN SL C P1, I2 S 87 LOSO BioVid (A) 83.301ACC\n'20 [243]EDA,\nEMG,\nECG- FF - I [DDCAE,\nNN]Y- UL,\nSLC P1, I2 S 87 LOSO BioVid (A) 83.991ACC;\n'21 [244]EDA,\nEMG,\nECG, RSP- FF - I [DDCAE,\nNN]Y- UL,\nSL,\nSSLC, R P1, ID2,\nICS 87,\n40LOSO BioVid (A)1,\nSense-\nEmotion84.2511ACC;,\n35.4421ACC;\n'21 [245]EDA, ECG - FF - E 1D CNN,\nLSTM- UL C P1, I2 S 67 hold-\noutBioVid (A) 81.711ACC\n'20 [240]PPG, EDA,\ntemperature- RF - I 2D CNN - SL R\u02ddP1, ID2S 21 k-fold other 96.301ACC,\n95.232ACC\n'20 [241]MoCap,\nEMG- RF - E AE, LSTM - UL,\nSLC ID O 23 LOSO:EmoPain 52.60 ACC;\n'20 [242]MoCap,\nEMG- RF - E LSTM, NN - UL C ID O 30 hold-\noutEmoPain 80.00 ACC;\n'21 [246]MoCap,\nEMG- RF - E LSTM, NN - SL C ID O 30 LOSO:EmoPain 54.60 ACC;\nm: Not specifically described \u02dd: Ordinal Modality F: face region B: body region EMG: electromyography Non deep features: MRMR: Minimum Redundancy Maximum Relevance method Deep models: LSGAN:\nLeast Square Generative Adversarial Networks3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 47\n3.6 Summary of Automatic Pain Assessment Methods\nThis section presents an analysis of the reviewed studies, summarizing the main conclusions\non current methods for automatic pain assessment, their advantages, and corresponding lim-\nitations. Additionally, it offers recommendations for future research directions that could\nadvance the field of pain research from a computational perspective.\n3.6.1 Input\nFirst, we observe a clear imbalance between unimodal and multimodal approaches in pain\nassessment studies. More than 86% of the reported research focuses on unimodal methods,\neven though the databases often contain multiple information channels. Notably, contact\nsensor-based and audio-based approaches are underrepresented, with only seven and four\nstudies, respectively, compared to 84studies that utilize a vision-based approach.\nMultimodal approaches are even less explored, with only 15studies falling into this\ncategory, making it difficult to draw strong conclusions about the effectiveness of specific\nmodality combinations. However, there are indications that EDA sensor data is particularly\nvaluable compared to other biopotentials. Researchers have primarily focused on visual data,\nlikely due to the complexity of implementing multimodal frameworks or the impracticality\nof contact sensors in non-laboratory settings. Further exploration of diverse modality com-\nbinations is necessary to evaluate their potential for pain assessment fully\u2014additionally, 28\nstudies employed non-deep features to enhance deep-learned representations.\nFinally, we identified three primary strategies in examining the approaches that utilize\ntemporal information: non-machine learning-based, machine learning-based (implicit), and\nmachine learning-based (explicit). Non-machine learning-based methods, such as motion\nhistory images [219] or temporal distillation [180], rely on traditional computer vision tech-\nniques. These methods tend to be more straightforward but are generally less sophisti-\ncated. In contrast, machine learning-based approaches [190] [217] offer richer temporal\ninformation and the flexibility to adapt to specific requirements, such as emphasizing certain\nvideo frames. Among the studies reviewed, 55% employed temporal features, with explicit\nmethods\u2014most commonly LSTM models\u2014being the predominant choice. Given that many\nstudies report superior performance when temporal information is incorporated, compared\nto non-temporal methods, it is evident that further emphasis on temporal approaches is war-\nranted.\n3.6.2 Processing\nRegarding machine learning approaches, various models and techniques have been employed\nfor pain estimation. CNN models remain the most widely used, with more than 75% of stud-48 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nies utilizing 1D, 2D, or 3D filters, highlighting the central role of convolution operations\nin deep learning. Sequential models, such as RNNs, GRUs, LSTMs, and biLSTMs, follow\nclosely behind in popularity. Almost half of the studies used pre-trained models to achieve\ntheir desired performance. This suggests that existing pain databases may not be adequate\nfor training deep-learning models from scratch. Non-deep learning models have also been\nemployed in 26studies as auxiliary decision components, with SVMs and shallow neural net-\nworks being the most common choices. There seems to be significant potential for adopting\nnewer deep learning architectures, especially transformer-based models, which have demon-\nstrated state-of-the-art results in various AI research fields and are particularly suited for\nexploiting temporal modality information [247].\nThe predominant learning method used across studies is supervised learning. How-\never, 16papers explored or adopted alternative methods such as unsupervised learning [119,\n133, 179, 189, 206, 223, 225, 231, 236, 241, 243], self-supervised [180, 244], self-supervised\nlearning [180, 244], semi-supervised learning [119], weakly supervised learning [190, 191],\nand federated learning [165]. Given the limited availability of pain data resources, self-\nsupervised learning appears to be the most appropriate method for future research and should\nbe further embraced by the community.\nLastly, it is notable that most studies\u2014approximately 70%\u2014treat pain assessment as a\nclassification problem rather than a regression problem. However, we believe that regres-\nsion more closely reflects the continuous nature of pain and is better suited to capturing the\ncomplexity of pain sensation.\n3.6.3 Evaluation\nThe primary objectives of the reviewed studies were (i)to estimate pain intensity on a dis-\ncrete scale (multi-class classification), (ii)to measure pain intensity on a continuous scale,\nand(iii)to determine the presence or absence of pain (binary classification). Notably, 25\nstudies focused on pain detection rather than pain intensity estimation, which, from a clin-\nical standpoint, is less informative as it does not provide sufficient data for effective pain\nmanagement. From an engineering perspective, detecting the presence or absence of pain is\nalso a more straightforward and less demanding task.\nA small subset of studies took a different approach to pain estimation. For instance, one\nstudy [179] sought to differentiate genuine pain from acted pain. Another [231] explored\npain events in emergency triage settings rather than controlled laboratory environments,\nwhile [234] examined the feasibility of real-time pain detection on IoT devices. Addition-\nally, [142] and [143] aimed to address the issue of occluded faces in pain estimation. So-\nciodemographic and psychological factors were also considered, as seen in studies like [245],\nwhich explored gender differences, and [194], which focused on pain assessment in elderly3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 49\npatients with dementia. The limited exploration of pain estimation in real-world settings\nor unconventional contexts suggests that current approaches may not be fully applicable in\npractical environments like clinics and hospitals.\nVarious annotation types are used regarding ground truth, such as self-reported ratings,\nFACS, and observer scales. Temporal features are critical for accurately estimating pain\nintensity, making the temporal granularity of the ground truth equally important. Several\nstudies have questioned the objectivity of PSPI scores, as noted in [248], which highlights\nthat PSPI scores can be zero even when pain is present or that there may be no visible facial\nexpressions in low-intensity pain. Pain expressions not captured by the FACS system, such\nas raising eyebrows or opening the mouth, further challenge the use of PSPI [249]. Addi-\ntionally, PSPI does not account for pain-related head and body movements, which are par-\nticularly valuable in newborn assessments [250]. For these reasons, we recommend moving\naway from PSPI as ground truth in favor of self-reports and observer scales at the video-\nsegment level.\nAround 54% of the studies employed the leave-one-subject-out (LOSO) validation method,\nwhich is widely regarded as more objective and better for assessing the generalizability of\nmodels. However, LOSO can be less practical due to the increased model size and longer\ntraining times. When researchers use other validation methods, such as k-fold or hold-out,\nit is essential to ensure that consecutive, highly correlated frames from the same subject do\nnot skew the training and validation results, leading to flawed estimations. Moreover, when\nresearchers define their own validation or testing sets, comparing results across studies\u2014\nespecially between classification and regression models\u2014becomes nearly impossible. We\nbelieve standardized evaluation protocols should be developed for each publicly available\ndatabase for these reasons.\n3.6.4 Pain Databases for Evaluation\nThe availability of suitable public databases is arguably the most crucial factor in addressing\nthe challenge of automatic pain assessment. Several aspects must be considered in evaluating\nthese datasets, including the number of subjects and their characteristics, such as age, sex,\nhealth status, and race. Moreover, the ground truth must be objective and offer meaningful\ninsights into the subject\u2019s pain experience [154].\nFig. 3.1 illustrates the number of papers corresponding to the pain database utilized in\neach study. It is clear from this figure that the UNBC andBioVid databases were the most\ncommonly used public datasets. However, the UNBC dataset does not record the subjects\u2019\nages, despite age being a known factor in pain expression [35,66]. While the BioVid dataset\ndoes document age, the oldest participants are only 65years old, which is notable since pain\nand its management are critical issues among individuals aged 65and older [251]. Simi-50 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nlar limitations are found in other pain datasets, such as X-ITE [117], EmoPain [115], and\nSenseEmotion [116].\nIt is well known that aging causes skin changes, including texture, rigidity, and elastic-\nity alterations, which can impact facial emotion recognition tasks [78]. Additionally, race-\nrelated factors can lead to inaccurate pain assessments due to variations in how pain is ex-\npressed [252]. Notably, one study by Nerella et al. [175] reported lower performance when\ntheir model was tested on African American patients. Furthermore, only one study [194]\nwas found that specifically addressed pain estimation in elderly individuals with dementia.\nIn summary, developing objective, automated, and generalizable deep learning-based\npain assessment systems will only be possible if balanced and representative datasets are\navailable for training and external validation.\n3.6.5 Interpretation of Results\nRecent advancements in AI have shown state-of-the-art performance across nearly every\nscientific discipline, often surpassing human accuracy in specific diagnostic tasks [253].\nHowever, a significant drawback of AI solutions, particularly deep neural networks, is their\nlack of transparency, commonly called \u201cblack box AI\u201d. This term highlights how these\nmodels learn intricate functions that are opaque and frequently incomprehensible to hu-\nmans [254]. This opacity is a primary reason for the criticism directed toward deep learning\ntechniques [255]. Various techniques, such as visualizations and gradients-backpropagation\nfocusing on specific units, have been developed to offer insights into how these models func-\ntion. For further reading, refer to the comprehensive review on explanatory techniques in\ndeep learning [256].\nTable 3.7 outlines the different approaches used to interpret model decisions. Only a\nsmall fraction of the reviewed studies\u2014 20out of 110\u2014implemented methods to explain\nhow their models work and which features or elements they focus on. It is important to\nnote that interpretable machine learning can be broadly defined as the \u201cextraction of rele-\nvant knowledge from a machine-learning model concerning relationships either contained\nin data or learned by the model\u201d [257]. To summarize: (i)18% of the reviewed studies\nprovided an approach to enhance the interpretability of the model\u2019s decision, (ii)all of these\nmethods were applied to studies using facial images as the input modality, and (iii)around\nhalf of these studies were conducted by just three specific research groups. These findings\nsuggest that the issue of interpretability and explainability within deep learning remains un-\nderexplored, particularly in the context of automatically classifying pain severity levels.3.7. CHALLENGES AND FUTURE DIRECTIONS 51\nTable 3.7: Interpretation approaches.\nPaper Year Modality Method\n[124] 2021 F (RGB) visualization (saliency maps)\n[128] 2018 F (RGB) visualization (heat maps)\n[130] 2021 F (RGB) visualization (saliency map)\n[133] 2016 F (RGB) visualization (learned filters)\n[134] 2021 F (RGB) visualization (learned filters)\n[135] 2019 F (RGB) visualization (heat maps),\nvalues of learned weights\n[138] 2018 F (RGB) visualization (saliency maps)\n[141] 2021 F (RGB) visualization (attention maps)\n[142] 2021 F (RGB) visualization (saliency map)\n[143] 2021 F (RGB) visualization (activation maps)\n[153] 2020 F (RGB) visualization (pixels contributions)\n[177] 2017 F (RGB) visualization (average saliency map)\n[179] 2019 F (RGB) visualization\n(generated intermediate representation)\n[194] 2020 F (RGB) visualization (saliency maps)\n[196] 2020 F (RGB) weights per AU (contribution of AUs)\n[173] 2019 F (RGB) visualization (feature maps)\n[174] 2021 F (RGB) visualization (integrated gradients)\n[210] 2021 F (RGB) visualization (heatmaps)\n[211] 2020 F (RGB) visualization (attention maps),\nvalues of learned weights\n[212] 2019 F (RGB) visualization (attention maps)\n3.7 Challenges and Future Directions\nThis section discusses the existing challenges in automatic pain assessment and proposes\nfuture research directions to further progress in the field.\n3.7.1 Current Challenges in Automatic Pain Assessment & Future Research Direc-\ntions\nSeveral limitations exist in the current pain databases. Important demographic factors such\nas sex, gender, and age are often missing, and there is an apparent lack of racial diversity\namong subjects. For example, facial structures and emotional expressions vary across Cau-\ncasian, Asian, and African populations [258]. Moreover, social interactions, such as the\npresence of a partner during assessments, could influence pain manifestation and should\nbe included in future datasets [69]. Estimating the location of pain, particularly for infants\nor individuals with communication impairments, is another vital aspect of pain assessment52 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsystems, which current databases largely overlook. Future datasets should incorporate stim-\nuli targeting various body locations. Furthermore, the videos in existing visual databases\noften have low to medium resolution and frame rates, which are inadequate for capturing\nfacial micro-expressions. Audio data is also sparsely represented, though it holds potential\nas a valuable modality. From an audio perspective, integrating natural language processing\n(NLP) methods to extract linguistic features and create multimodal systems is a promising\ndirection, as shown in affective computing research [259]. Finally, specific validation proto-\ncols should be provided with present and future datasets to ensure objective and consistent\ncomparisons across studies.\nFrom an engineering perspective, several issues must be addressed to advance automatic\npain assessment. Developing multimodal approaches is essential for creating robust systems\nwith enhanced capabilities. Not only do multimodal methods demonstrate better perfor-\nmance than unimodal ones, but they are also crucial in real-world scenarios where a specific\nmodality may become unavailable. Additionally, it is essential to exploit each modality\u2019s\ntemporal aspects fully. We encourage using machine learning models or other techniques\nthat can accommodate the dynamic nature of pain. More work is needed to improve the accu-\nracy of multi-level and low-intensity pain estimation. Another area of research involves the\nrelationship between pain and other affective states, such as negative emotions, which often\ncoexist during painful events. Detecting these emotions could improve pain assessment. Ad-\ndressing challenges like occlusions or poor lighting conditions in vision-based systems also\nrequires attention. Researchers should explore these scenarios, even if current databases do\nnot account for them. Real-time application of pain assessment systems is another critical\nfactor, so future studies should measure throughput, such as the number of images processed\nper second during inference. Generalization is another crucial concern for AI systems, and\nevaluating trained models across different pain databases could be valuable. Finally, to facil-\nitate the clinical adoption of AI-based pain assessment systems, the models\u2019 decisions need\ngreater explainability. Developing or adopting methods that improve interpretability will\nenhance their clinical viability.Chapter 4\nDemographic Variables: Their Role and\nImpact\nContents\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.1 Chapter Overview: Introduction & Related Work\nThis chapter includes the findings published in [35, 36]. As discussed in Section 2, research\nhas demonstrated that biological and psychological differences can lead to variations in how\npain is perceived between men and women. Regarding age, it is known that infants who\ncannot express themselves directly and older adults with health issues require specific care\ndue to their unique needs. However, a significant question remains unanswered in pain\nresearch, both from clinical and biological perspectives: Does the sensation of pain change\nas individuals age? Specifically, does a person in pain perceive their situation differently\n5354 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nas they grow older than others in different age groups? Is the sensation of pain evolving\nthroughout life? To the best of our knowledge, this question has not yet been definitively\nexplored thoroughly. This chapter investigates the differences among males and females, as\nwell as age groups, using ECG signals. In addition, it proposes a computational framework\nthat utilizes these two demographic elements to improve pain assessment performance. This\nchapter analyzes ECG signals to explore the differences in pain perception between males\nand females and various age groups. Additionally, it introduces a computational framework\nincorporating these two demographic factors to enhance the accuracy of pain assessments.\nFrom a computational standpoint, the literature concerning the use of demographic fac-\ntors in pain assessment is scarce. The study in [260] utilized a range of biosignals, including\nEDA, respiration rate, diastolic blood pressure, and facial action units, to demonstrate dif-\nferences in pain perception between males and females. Similarly, in research [245], the\nauthors utilized a hybrid CNN-LSTM model that processed ECG and EDA data, highlight-\ning gender-based variations in pain response. Following the publications of our research,\nanother study by Ricken et al. [261] was released, which explored the differences in adap-\ntation and habituation between men and women. This study extracted handcrafted features\nfrom various biosignals (including ECG and EDA) and employed random forest classifiers\nto analyze the data.\n4.2 ECG Analysis with Classical Machine Learning\nWe explore a pain estimation process using ECG signals and examine variations across dif-\nferent demographic groups, focusing on gender and age. Specifically, we analyze how pain\nmanifestation differs between males and females, investigate variations in pain perception\nacross different age groups, and consider the combined effects of age and gender on pain\nperception.\n4.2.1 Methodology\nThis section will describe the electrocardiography processing algorithm and the methods\nused for feature extraction and classification algorithms.\nECG signal Processing and Analysis\nAn ECG signal captures the heart\u2019s electrical activity over time. Typically, a normal ECG\ndisplays a sequence of waves, identified as P, Q, R, S, T, and occasionally U. These waves\nand their intervals provide crucial insights into heart function. The P wave indicates atrial\ndepolarization, the QRS complex signifies ventricular depolarization and contraction, and\nthe T wave corresponds to the repolarization of the ventricles. Each heartbeat is depicted4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 55\nQR\nST\nPQRS  \nComplex\nPR\nInterval\nQT Interval\nFigure 4.1: The PQRST waveform.\nLow Pass Filter ECG  High Pass Filter Differentiation\nAdaptive ThresholdsMoving W indow\nIntegrationSquaringQRS\nComplexBand-Pass Filter\nLow Pass Filter ECG  High Pass Filter\nDifferentiation\nAdaptive\nThresholdsMoving W indow\nIntegrationSquaring\nQRSBand-Pass Filter\nFigure 4.2: The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure.\nthrough the PQRST complex (refer to Figure 1). Accurately detecting the R wave within the\nQRS complex is especially critical as it is the most pronounced peak in the complex. Precise\ndetection of the R wave allows for the calculation of heart rate (HR) and heart rate variability\n(HRV), the latter of which measures the time intervals between successive R waves, known\nas the R-R or Interbeat interval. The Pan-Tompkins algorithm, developed in 1985, is one of\nthe most extensively used real-time QRS detection algorithms [262]. Over the years, both the\noriginal algorithm and its variations have been rigorously tested, consistently proving their\neffectiveness even with noisy and low-quality data [263,264]. The Pan-Tompkins Algorithm\nis frequently cited as a benchmark in the field due to its robust performance, making it a stan-\ndard against which new QRS detection methods are compared [265]. Our research employed\nthe original Pan-Tompkins Algorithm to identify the QRS complex. We integrated the algo-\nrithm in two primary phases: preprocessing and decision-making. The preprocessing stage\nis crucial for conditioning the ECG by eliminating noise and artifacts, smoothing the signal,\nand enhancing the QRS slope. The preprocessing steps of the Pan-Tompkins algorithm are\ndepicted in the flow diagram shown in Figure 4.2.56 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nFeature Extraction\nThe subsequent phase involves extracting specific features based on the inter-beat intervals\n(IBIs). In our study, we calculated several metrics, including the mean of IBIs, the root mean\nsquare of successive differences (RMSSD), the standard deviation of IBIs (SDNN), the slope\nof the linear regression of IBIs, the ratio of SDNN to RMSSD, and the heart rate, as outlined\nbelow:\n1. Mean of IBIs\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (4.1)\nwhere RRrepresents consecutive Rpeaks.\n2. Root mean square of successive differences\nRMSSD\u201cgffe1\nn\u00b41n\u00b41\u00ff\ni\u201c1pRRi`1\u00b4RRiq2 (4.2)\n3. Standard deviation of IBIs\nSDNN\u201cd\n1\nn\u00b41n\u00ff\ni\u201c1pRRi\u00b4\u00b5q2 (4.3)\n4. Slope of the linear regression of IBIs\nATAx\u201cATb, (4.4)\nwhere is calculated using the least-square approximation, where bis the vector of RR\npeak intervals and Ais the corresponding time series.\n5. Ratio of SDNN to RMSSD\nSR\u201cSDNN\nRMSSD(4.5)\n6. Heartbeat rate\nHR\u201c60\u00a8FS\n\u00b5, (4.6)\nwhere FSis the sampling frequency of the ECG recording, typically 512Hz. Figure\n4.3 illustrates the raw ECG signal and the algorithm\u2019s stages.4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 57\n0 500 1000 1500 2000 2500\nTime (ms)Raw Signal\n0 500 1000 1500 2000 2500\nTime (ms)Band Pass Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Derivative Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Squared\n500 1000 1500 2000 2500\nTime (ms)Moving Window Averaged\nSignal\nQRS\nNoise Level\nSignal Level\nAdaptive Threshold\nFigure 4.3: The signal preprocessing using the Pan-Tompkins algorithm.\nClassification Methods\nFor the classification phase, three widely recognized classifiers were utilized: Linear Dis-\ncriminant Analysis (LDA), Support Vector Machine (SVM) with a linear kernel, and SVM\nwith a Radial Basis Function (RBF) kernel.\n1. Linear Discriminant Analysis\nPpX|y\u201ckq\u201cexp\u00b4\n\u00b41\n2pX\u00b4\u00b5kqt\u03a3\u00b41\nkpX\u00b4\u00b5kqt\u00af\np2\u03c0qd{2|\u03a3k|1{2, (4.7)\nwhere Pdenotes the probability density function for the feature set X, conditional on\nthe target class y\u201ck.\n2. SVM with linear kernel\nKpx1, x2q\u201cxT\n1x2, (4.8)\nwhere x1andx2represent feature vectors from two separate classes.58 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n3. SVM with Radial Basis Function (RBF) kernel\nKpx1, x2q\u201cexp\u02dc\n\u00b4||x1\u00b4x2||2\n2\u03c32\u00b8\n, (4.9)\nwhere \u03c3is the parameter defining the width of the RBF kernel.\nDataset Details\nIn this study, we utilized the publicly available \u201cBioVid Heat Pain Database\u201d [109], which\ncontains facial videos and biosignals (ECG, EMG, EDA) from 87participants ( 44males and\n43females, aged 20\u00b465). This dataset is unique because it is the only publicly accessible\nresource that includes the subjects\u2019 age and gender. The data collection involved applying\na heat stimulus to the right arm of each participant using a thermode. Prior to recording,\nthe pain threshold (the temperature at which the participant first perceives heat as pain) and\npain tolerance (the temperature at which the pain becomes intolerable) were established for\neach participant. The study defined specific thresholds as the temperatures for the lowest\nand highest pain levels. Also, it included two intermediate levels, resulting in five pain\nconditions: No pain (NP), mild pain (P 1), moderate pain (P 2), severe pain (P 3), and very\nsevere pain (P 4). Each participant was exposed to 20stimulations for each intensity level,\ngenerating 100samples across the four modalities.\n4.2.2 Experiments\nIn the following experiments, we specifically used Part A of the BioVid , which includes\npre-processed ECG samples filtered through a Butterworth band-pass filter, totaling 8700\nsamples ( 87\u02c6100\u201c8700 ). All experiments were conducted in triplicate under identical\nconditions, using a distinct classifier for each iteration to compare their effectiveness. This\nwas based on the leave-one-subject-out (LOSO) cross-validation method, utilizing all avail-\nable subjects and ECG samples. The performance of each classifier was evaluated based on\naccuracy.\nUsing the previously mentioned classification algorithms, we conducted experiments to\nrecognize pain and its relationship with demographic factors. The classification tasks were\nstructured around the pain conditions in multi-class and binary classification formats. Specif-\nically, five distinct experiments were executed: (i)multi-class pain classification, (ii)NP vs.\nP1,(iii)NP vs. P 2,(iv)NP vs. P 3,(v)NP vs. P 4. In experiment (i), the goal was to cate-\ngorize an ECG signal into one of the five pain conditions, while experiments (ii)-(v) aimed\nto classify signals into one of two pain conditions, either no pain or the specified pain level.\nFurthermore, considering the gender and age of the subjects, we developed four different4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 59\nTable 4.1: Results for the Basic Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)AllMC 23.72 23.79 22.77\nNP vs P 1 50.97 52.38 49.97\nNP vs P 2 52.55 52.78 52.70\nNP vs P 3 55.20 55.37 53.87\nNP vs P 4 58.62 58.39 57.41\nMC: multi-classification NP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain\nP4: very severe pain LDA: Linear Discriminant Analysis LN:Linear RBF: Radial Basis\nFunction\nexperimental schemes: (i)theBasic Scheme , utilizing the entire dataset, (ii)theGender\nScheme , where data were segmented by the gender of the subjects into males and females,\n(iii) the Age Scheme , which grouped subjects into three age categories: \u201820-35\u2019 ,\u201836-50\u2019 ,\n\u201851-65\u2019 , and (iv) the Gender-Age Scheme , which combined both demographic factors, result-\ning in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\n\u2018Males 51-65\u2019 ,\u2018Females 51-65\u2019 . The most successful classification results are displayed\nin Figures 4.4-4.5 for each task and classification method, while Tables 4.1-4.5 detail the\noutcomes of each individual experiment.\n4.2.3 Results\nTable 4.1 shows the results from the entire dataset, where the multi-class pain classification\nachieved a 23.79% accuracy, and performance scores generally increased with pain intensity,\npeaking at 58.62% for NP vs. P 4. This progression highlights the difficulty in detecting\nlower levels of pain severity. Regarding the classification algorithms, SVM (linear) was\nmore effective, except for the highest pain level task, where SVM (RBF) was less successful.\nIn the Gender Scheme (see Table 4.2), notable differences were observed between males\nand females. Overall, females showed a 1.12% higher accuracy variation than males, with\nfemales achieving 60.69% in NP vs. P 4over males\u2019 56.07%. This 4.62% increase suggests\nthat females are more sensitive to higher levels of pain than males. Interestingly, in NP vs. P 1\nand NP vs. P 2, males outperformed females by 1.16% and1.78%, respectively. Consistent\nwith the first scheme, SVM (linear) yielded better results in most tasks. Figure 4.4 illustrates\nthe gender differences in classification accuracy.\nIn the Age-Scheme (refer to Table 4.3), the \u201820-35\u2019 age group achieved 25.06% accuracy\nin multi-level classification, compared to 23.27% and22.35% for the \u201836-50\u2019 and\u201851-65\u2019\ngroups, respectively, indicating that age significantly affects pain perception. The nearly 9%60 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.2: Results for the Gender Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)MalesMC 22.13 22.25 20.70\nNP vs P 1 51.53 52.61 47.72\nNP vs P 2 53.12 53.69 52.15\nNP vs P 3 54.94 54.71 51.36\nNP vs P 4 55.28 56.07 51.36FemalesMC 25.11 24.41 23.41\nNP vs P 1 50.23 51.45 49.06\nNP vs P 2 51.62 51.86 51.91\nNP vs P 3 55.98 55.87 55.29\nNP vs P 4 60.17 60.69 59.82\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.4: Results for the Gender Scheme .\ndifference in the NP vs P 4task between the youngest and oldest groups was particularly no-\ntable. Similar to the gender-based results, minor differences in low pain intensities among\nthe age groups became more pronounced as pain intensity increased. Specifically, the vari-\nance ( \u03c32) between the groups in NP vs. P 1was1.38%. At the same time, in the other tasks,\nit increased to 2.44%,6.35%, and 20.42%, respectively, showing that high pain intensities\nare necessary to discern perceptual differences among age groups. Regarding classification\naccuracy, the \u201820-35\u2019 group showed the highest sensitivity, followed by \u201836-50\u2019 and\u201851-\n65\u2019. Regarding classification methods, the SVM (RBF) performed best in the \u201851-65\u2019 group\nacross almost all tasks, while it underperformed in the \u201820-35\u2019 group, suggesting it is better\nsuited for more challenging, separable classes. Figure 4.5 displays the results from the age\nscheme.\nIn the final analysis, we examined the subjects more closely to gain deeper insights into\nthe relationship between pain and the demographic factors of gender and age. As shown4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 61\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.5: Results for the Age Scheme .\nin Tables 4.4-4.5, the \u2018Females 20-35\u2019 group achieved the highest accuracy in multi-class\npain classification at 24.80%, while \u2018Females 51-65\u2019 led in NP vs. P 1with 55.38%, again\nindicating higher pain sensitivity in females. Moreover, \u2018Females 51-65\u2019 followed by \u2018Males\n51-65\u2019 topped the performance in NP vs. P 2, and in NP vs. P 3,\u2018Females 36-50\u2019 surpassed\nthe next best group, \u2018Males 20-35\u2019 , by3.5%. In the final NP vs. P 4task, \u2018Females 20-\n35\u2019excelled with 67% accuracy, whereas \u2018Males 51-65\u2019 had the lowest at 54.50%, marking\nthem as the most and least pain-sensitive groups, respectively. It is noted that sometimes\nclassification accuracy decreases despite increased pain levels ( e.g.,\u2018Females 36-50\u2019 ). This\nmight be attributed to the subjects becoming accustomed to the stimulus over time during\nthe biosignal recording.\nFigure 4.6 illustrates the classification performance of the six groups in the Gender-Age\nScheme . Additionally, Table 7 compares our results with other studies that used ECG signals\nfrom the BioVid database and followed the same evaluation protocol, ensuring an objective\ncomparison. Our study achieved the best classification performance in both the multi-class\nsetting and the NP vs. P 1and NP vs. P 2tasks, with acceptable results in the remaining binary\nclassification tasks.\n4.2.4 Discussion\nWe analyzed ECG biosignals using the Pan-Tompkins algorithm to detect QRS complexes\nand extracted features about inter-beat intervals. We also evaluated three machine learning\ntechniques, assessing their performance in multi-class and binary pain classification across\nvarious pain intensities. We also examined the influence of gender and age on pain percep-\ntion, discovering significant differences: males generally showed lower sensitivity to high\npain levels. Regarding the age factor, significant variations suggest that pain sensitivity tends\nto diminish with age, potentially increasing the risk of further injury. In certain demographic\ngroups, the difference in pain perception exceeded 12%, underscoring the variability of pain\nsensation among individuals.62 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.3: Results for the Age Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)20-35MC 25.06 24.73 21.96\nNP vs P 1 52.83 52.83 49.90\nNP vs P 2 54.33 53.75 52.75\nNP vs P 3 55.58 56.16 54.66\nNP vs P 4 63.83 63.41 60.7536-50MC 23.27 22.06 23.03\nNP vs P 1 50.34 48.36 50.68\nNP vs P 2 49.13 51.20 50.17\nNP vs P 3 58.10 58.70 58.27\nNP vs P 4 58.10 57.75 55.9451-65MC 21.89 22.07 22.35\nNP vs P 1 52.23 51.87 52.58\nNP vs P 2 52.14 51.69 52.76\nNP vs P 3 53.66 53.39 54.10\nNP vs P 4 54.46 54.19 54.91\nTable 4.4: Results for the Gender-Age Scheme (Males) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Males 20-35MC 23.13 23.20 18.73\nNP vs P 1 52.50 52.83 45.83\nNP vs P 2 54.00 53.50 53.16\nNP vs P 3 56.33 56.50 54.83\nNP vs P 4 60.00 59.00 53.66Males 36-50MC 23.21 22.21 20.92\nNP vs P 1 50.53 50.53 46.42\nNP vs P 2 50.00 51.78 47.50\nNP vs P 3 54.64 56.25 47.32\nNP vs P 4 55.53 56.25 51.96Males 51-65MC 20.06 21.60 19.60\nNP vs P 1 52.66 51.66 50.66\nNP vs P 2 54.00 54.66 51.50\nNP vs P 3 53.00 54.66 51.50\nNP vs P 4 53.33 54.50 49.834.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 63\nTable 4.5: Results for the Gender-Age Scheme (Females) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Females 20-35MC 24.73 24.80 23.26\nNP vs P 1 49.83 51.50 52.00\nNP vs P 2 54.50 53.66 46.50\nNP vs P 3 53.50 52.83 49.00\nNP vs P 4 65.83 67.00 62.16Females 36-50MC 23.06 22.73 21.93\nNP vs P 1 48.16 49.33 48.33\nNP vs P 2 48.66 49.83 47.83\nNP vs P 3 57.50 60.00 55.00\nNP vs P 4 59.00 58.83 56.16Females 51-65MC 21.23 21.84 23.92\nNP vs P 1 48.84 49.80 55.38\nNP vs P 2 51.15 48.65 55.96\nNP vs P 3 53.07 53.07 50.96\nNP vs P 4 52.69 55.00 56.34\nTable 4.6:\nComparison of studies utilizing BioVid , ECG signals\nand LOSO validation (1).\nMethod Task Results\nMartinez and Picard [266] NP vs P 4 57.69\nWerner et al. [267]NP vs P 1 48.70\nNP vs P 2 51.60\nNP vs P 3 56.50\nNP vs P 4 62.00\nThiam et al. [235]MC 23.23\nNP vs P 1 49.71\nNP vs P 2 50.72\nNP vs P 3 52.87\nNP vs P 4 57.04\nOursMC 23.79\nNP vs P 1 52.38\nNP vs P 2 52.78\nNP vs P 3 55.37\nNP vs P 4 58.6264 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65MC232523232224BL vs P1535251495355BL vs P2545552505556BL vs P3575456605553BL vs P4606756595456018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65\n1\nFigure 4.6: Results for the Gender-Age Scheme .\n4.3 ECG Analysis with Multitask Neural Networks\nIn this section, we build on previous analysis 4.2 that explored variations in pain manifesta-\ntion across different demographic groups using ECG signals. It expands this investigation\nby implementing neural networks as the primary machine learning model and introduces a\nnovel multi-task learning (MTL) neural network. This network leverages demographic infor-\nmation to estimate age and gender in addition to pain levels, aiming to enhance the automatic\npain estimation system.\n4.3.1 Methodology\nThe following method we developed is a neural network-based approach. The feature extrac-\ntion process remains the same as previously described in 4.2.1, utilizing the Pan-Tompkins\nalgorithm for ECG signal processing.\nNeural Network\nThe proposed neural network was designed and trained using two distinct approaches: single-\ntask learning (STL) and multi-task learning (MTL). In the multi-task learning framework, the\nnetwork is simultaneous training for pain estimation and predicting age and/or gender.\nSingle-Task Neural Network: The proposed neural network comprises two components:\nthe encoder, which maps the original feature vectors into a higher dimensional space, and\nthe task-specific classifier. In our design, both the encoder and the classifier utilize fully-4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 65\nTable 4.7: Hyper-parameters used in our approach.\nEpochs Optimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsLabel\nsmoothEMA\n300 AdamW 1e-3 cosine 0.1 50 0.1 \u2713\nconnected (FC) layers, which are defined as follows:\nzipsq\u201cbi`nin\u00ff\nj\u201c1Wijsj for i\u201c1, .., n out, (4.10)\nwhere zirepresents the result of linearly combining the incoming inputs sj, with each input\nbeing weighted by Wijand adjusted by a bias bi. Each layer in the encoder is followed by a\nnonlinear activation function, specifically the rectified linear unit (ReLU), which is defined\nas:\n\u03c3pzq\u201c#\n1, z\u011b0\n0, z\u01030(4.11)\nThe classifier\u2019s layers are connected without nonlinearity, the encoder comprises four fully\nconnected (FC) layers with 256,512,1024 , and 1024 neurons respectively. The classifier\nincludes 2layers with 1024 andnneurons, where nrepresents the number of distinct pain\nclasses being classified. The hyperparameters of the network are detailed in Table 4.7.\nMulti-task neural network: This proposed machine learning method is based on the princi-\nple of sharing representations across related tasks, which helps the model better generalize\nto the primary task of pain estimation in this case. We kept the same encoder and pain\nclassifier in this configuration but introduced two additional auxiliary networks for age and\ngender estimation. The architecture of the proposed multi-task learning (MTL) neural net-\nwork is illustrated in Fig. 4.7. The objective of this network is to simultaneously minimize\nthree different losses. We adopt and expand upon the framework suggested by [268] for the\nmulti-task learning loss, where learned weights are applied to each loss function based on\nthe homoscedastic uncertainty of each task:\nLtotal\u201crew1LPain`w1sc1`rew2LAge`w2sc2`rew3LGender`w3sc3. (4.12)\nHere, Lrepresents the corresponding loss, wdenotes the weights, and care the coefficients\nthat modulate the losses LAgeandLGender to prioritize learning in the pain estimation task.\nIt should be noted that all tasks are treated as classification problems, utilizing cross-entropy66 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nL1\nL2\nL3 L4Feature\nvector\nPainAge\nGender6 x 1256 x 1512 x 11024 x 1 1024 x 1\n1024 x 1n x 1512 x 1256 x 1\nL1L2L1L2L336 x 1\n512 x 1256 x 1\nL1L2L32 x 1\nEncoderMTL LossPain\nEstimation\nFigure 4.7: The proposed MTL network: The sizes of the extracted vectors for the network are\nas follows: for the Pain classifier, n\u02c61, where nis the number of pain estimation\ntasks ( e.g.,2for binary classification, 5for multi-class classification); for the Age\nclassifier, 36\u02c61, where 36represents the possible age values of the subjects; for\nthe Gender classifier, 2\u02c61, corresponding to the two possible gender categories\n(i.e., males and females).\nloss with label smoothing:\nLD\u201c\u00b4\u00ff\n\u03b4PDnout\u00ff\ni\u201c1ppi|x\u03b4qlogrqpi|x\u03b4qs. (4.13)\nHere, Ddenotes the pain database, ppi|x\u03b4q \u201c1\u00b4\u03f5represents the probability of the true\nclass igiven the input x\u03b4, and ppi\u2030i\u03b4|x\u03b4q \u201c\u03f5{pnout\u00b41qis the probability distribution\nacross the other classes. This formulation spreads a small portion \u03f5of the probability across\nclasses other than the true class to implement label smoothing. Furthermore, qpi|x\u03b4qis the\nprobability distribution over the classes ias predicted by the network\u2019s output.\n4.3.2 Experiments\nSimilar to 4.2.2, we utilized the BioVid database, specifically focusing on its ECG signals.\nEmploying a single-task neural network (ST-NN), we conducted an initial series of experi-\nments to assess the impact of demographic factors. Building on the concept we proposed\nin the previous section, we devised five experimental schemes: (i)theBasic Scheme , which\nincluded all subjects from the database; (ii)theGender Scheme , which segregated subjects4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 67\ninto male and female groups; (iii)theAge Scheme , which categorized subjects into three age\ngroups\u2014 \u201820-35\u2019 ,\u201836-50\u2019 , and \u201851-65\u2019 ; and (iv)the\u2018Gender-Age Scheme\u2019 , which combined\nboth demographic factors, resulting in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\n\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\u2018Males 51-65\u2019 , and \u2018Females 51-65\u2019 . All experiments were\nconducted in both binary and multi-class classification formats. Specifically, the binary clas-\nsification tasks were (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3, and (4) NP vs. P 4, and the\nmulti-class task utilized all available pain classifications from the database.\n4.3.3 Results\nDemographic Groups\nTable 4.8 presents the classification results of the Basic Scheme , which utilized all subjects\nin the database. For the multi-class pain classification, we achieved an accuracy of 29.43%,\nwith NP vs. P 1scoring 61.15% and NP vs. P 4reaching 68.82%. These results indicate that\nas pain intensity increases, so performs, highlighting the difficulty in recognizing less severe\npain. According to the Gender Scheme (refer to Table 4.9), notable differences emerge\nbetween males and females, particularly at higher pain intensities. Specifically, in NP vs.\nP4, females achieved an accuracy of 69.48% compared to 66.48% for males, with an overall\nvariance of 1.63% between genders, suggesting that females exhibit higher pain sensitivity.\nFigure 4.8a illustrates these gender-based classification disparities. In the Age Scheme (see\nTable 4.10), the \u201820-35\u2019 age group outperformed the \u201836-50\u2019 and\u201851-65\u2019 groups in NP vs. P 4,\nwith accuracies of 72.58%,66.29%, and 64.91%, respectively. While the differences are less\npronounced at lower pain intensities, this scheme still shows that age significantly impacts\npain perception, particularly among the older population. Figure 4.8b shows the results from\nthe age scheme.\nIn the final scheme, by dividing subjects into more specific groups, we can analyze them\nmore precisely and gain better insights into the relationship between gender, age, and pain\nperception. Table 4.11 reveals that in the NP vs. P 4task, the group \u2018Females 20-35\u2019 reached\nthe highest accuracy of 71.67%, significantly outperforming the \u2018Males 51-65\u2019 group, which\nscored the lowest at 60.67%, marking them as the least sensitive group. This pattern is\nconsistent across the multi-class classification and other pain tasks, with \u2018Females 20-35\u2019\nand\u2018Males 51-65\u2019 exhibiting the highest and lowest accuracies, respectively. This supports\nthat females generally experience more pronounced pain responses, while older males have\na reduced pain sensation. It is noted that in some instances, such as with \u2018Males 20-35\u2019\nand\u2018Males 36-50\u2019 , higher pain levels do not necessarily correlate with higher classification\naccuracy, a phenomenon also noted in our previous experiments, in Section 4.2.3. A possible\nexplanation could be the subjects\u2019 habituation to pain stimuli, especially at lower intensities.\nFigure 4.8c visualizes the performance outcomes of the Gender-Age Scheme .68 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.8: Results for the Basic Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN 61.15 62.87 65.14 68.82 29.43\nST-NN: single-task neural network NP: no pain P1: mild pain P2: moderate pain P3: severe pain P4: very severe\npain MC: multi-classification\nTable 4.9: Results for the Gender Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales ST-NN 60.40 63.24 63.18 66.48 28.61\nFemales ST-NN 60.87 62.15 66.98 69.48 30.59\nTable 4.10: Results for the Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n20-35 ST-NN 61.58 64.08 66.08 72.58 31.07\n36-50 ST-NN 60.52 61.38 64.05 66.29 29.59\n51-65 ST-NN 61.70 60.80 62.50 64.91 27.82\nTable 4.11: Results for the Gender-Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales 20-35 ST-NN 62.83 62.33 65.50 71.33 29.73\nMales 36-50 ST-NN 61.79 60.00 59.64 64.11 27.14\nMales 51-65 ST-NN 59.50 58.67 57.33 60.67 26.07\nFemales 20-35 ST-NN 63.17 63.17 66.83 71.67 31.53\nFemales 36-50 ST-NN 59.50 61.00 65.83 67.00 29.13\nFemales 51-65 ST-NN 60.96 60.96 59.23 63.27 27.69\nAugmentation of Feature Vectors\nBuilding on the findings from the previous experiments about the impact of demographic\nfactors on pain perception, we explored the practical use of subjects\u2019 demographic data.\nExperiments were conducted using the Single-Task Neural Network (ST-NN) and feature4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 69\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(a) Gender\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(b) Age\nAge\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-Age\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\nBL vs P1 63 63 62 60 60 61\nBL vs P2 62 63 60 61 59 61\nBL vs P3 66 67 60 66 57 59\nBL vs P4 71 72 64 67 61 63\nMC 30 32 27 29 26 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales 20-35\n Females 20-35\n Males 36-50\n Females 36-50\n Males 51-65\n Females 51-65\nGender\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales\n Females\nAge-1\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-1\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales\n Females\n020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\n20-35 36-50 51-65018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales Females020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\n1\n(c) Gender-Age\nFigure 4.8: Results for the proposed Schemes.\nvectors enhanced with demographic attributes. Initially, the feature vectors, which originally\nconsisted of six features (see 4.3.1), were augmented by adding either one additional fea-\nture ( i.e., the subject\u2019s gender or age) or two additional features (both the subject\u2019s gender\nand age). We conducted the same pain estimation tasks using this enhanced set of features.\nAs shown in Table 4.12, the results demonstrate improved performance with the augmented\nfeature vectors. Specifically, the most effective augmentation involved combining gender70 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.12: Comparison of results adopting the feature augmentation approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAux: Auxiliary information -: original feature vectors F(G): feature vectors with the additional feature of gender F(A): feature\nvectors with the additional feature of age F(GA): feature vectors with the additional features of gender and age\nand age features, which increased the average pain estimation performance by 0.55%. Us-\ning these demographic features individually also improved classification accuracy, albeit\npartially.\nMulti-Task Neural Network\nThe final experiments utilized a multi-task learning framework with the proposed Multi-Task\nNeural Network (MT-NN) outlined in 4.3.1. The classification results for MT-NN, incorpo-\nrating additional tasks of (1) gender estimation, (2) age estimation, and (3) simultaneous\ngender and age estimation, are detailed in Table 4.13. For comparison, the results from ear-\nlier experiments using the Single-Task Neural Network (ST-NN) method are also included\nin Table 4.13. We noted that the task of gender estimation alone performed less effectively\nthan the other tasks. In contrast, the combined gender and age estimation delivered the high-\nest performance across four tasks. Specifically, in the multi-class classification, it achieved\n30.24%, and in NP vs. P 1, it reached 62.8%, marking the best results of any method pre-\nsented in this study. In NP vs. P 3and NP vs. P 4, the combined tasks outperformed the\nindividual gender and age tasks but were slightly less effective than the ST-NN approaches\nusing augmented features. Interestingly, in NP vs P 2, the age estimation task alone excelled,\nachieving 63.97%, the highest result recorded in this study.\nComparing the overall performances of MT-NN with the ST-NN approaches (using both\noriginal and augmented feature vectors), there is a noticeable improvement of 0.71% and\n0.39%, respectively, in average pain estimation accuracy across all tasks. Figure 4.9 visually\ncompare each neural network approach used in this study, encompassing multi-class and\nbinary classification tasks.4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 71\nTable 4.13: Comparison of results adopting the MT-NN approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAll MT-NN T(G) 61.72 63.39 65.95 68.99 30.00\nAll MT-NN T(A) 62.72 63.97 65.40 69.28 29.79\nAll MT-NN T(GA) 62.82 63.68 66.12 69.40 30.24\nT(G): MT-NN with the additional task of gender estimation T(A): MT-NN with the additional task of age estimation T(GA):\nMT-NN with the additional task of gender and age estimation\nComparison with Existing Approaches\nIn this section, we benchmark the results achieved using the Multi-Task Neural Network\n(MT-NN), which incorporated additional tasks of gender and age estimation against relevant\nstudies. These comparative studies also utilized electrocardiography signals from Part A of\ntheBioVid database with all 87participants. To ensure a fair comparison, they followed the\nsame evaluation protocol, specifically the leave-one-subject-out (LOSO) cross-validation.\nThe comparative results are detailed in Table 4.14 and include research that employed hand-\ncrafted features with traditional machine learning algorithms [35] [267], end-to-end deep\nlearning models [269] [235], and finally, hybrid approaches combine hand-crafted features\nwith deep learning classifiers [266]. Our approach, which leverages hand-crafted engineered\nECG features and a high-dimensional mapping from the encoder in combination with multi-\ntask learning neural networks, demonstrated superior performance across all pain estimation\ntasks, whether in binary or multi-class classification settings.\n4.3.4 Discussion\nWe explored multi-task learning neural networks for automatic pain estimation from electro-\ncardiography signals. By implementing the Pan-Tompkins algorithm to identify QRS com-\nplexes, we extracted features associated with inter-beat intervals (IBIs). Numerous experi-\nments were conducted to explore how gender and age influence pain perception, highlighting\ntheir significant impact. Additionally, we introduced two approaches to enhance pain esti-\nmation results by leveraging demographic information. Firstly, we augmented the original\nfeature vectors by incorporating the subjects\u2019 demographic data, improving classification\naccuracy. Secondly, we employed a multi-task learning neural network that combined the72 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(a) Binary classification\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(b) Multi-class classification\nFigure 4.9: Comparison of performances utilizing various neural networks approaches.\nTable 4.14: Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2).\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nGkikas et al. [35]:52.38 52.78 55.37 58.62 23.79\nHuang et al. [269]\u2039d- - - 65.00 28.50\nMartinez and Picard [270]\u00b8- - - 57.69 -\nThiam et al. [235]\u203949.71 50.72 52.87 57.04 23.23\nWernel et al. [267]:48.70 51.60 56.50 62.00 -\nThis study\u00b862.82 63.68 66.12 69.40 30.24\n::hand-crafted features and classic machine learning \u2039: end-to-end deep learning \u00b8: hand-crafted features with deep\nlearning classification algorithms d: pseudo heart rate gain extracted from visual modality\ntasks of pain, gender, and age estimation. This approach yielded superior results compared\nto methods previously discussed in this chapter and other related research. These findings\nindicate that domain-specific features can achieve excellent outcomes when combined with\nwell-designed deep-learning architectures and demographic factors.4.4. SUMMARY 73\n4.4 Summary\nIn this chapter, we examine the impact of age and gender on pain perception using ECG\nsignals to extract relevant features. Our study involved a series of experiments where subjects\nwere categorized into different groups based on gender (males and females) and age (20-35,\n36-50, and 51-65 years). Additionally, we created combined groups that segregated age\ngroups within each gender. Our findings from both approaches provided strong evidence\nof significant differences in pain perception among these groups. Notably, we observed\na12.5%disparity in pain sensitivity between young females and older males. Generally,\nour results confirm that females exhibit higher pain sensitivity than males, aligning with\nfindings from other studies in pain research. A critical discovery from our study is that pain\nsensitivity appears to decrease with age, which may increase the risk of unnoticed injuries.\nWe presented two methods of incorporating demographic information into our models from a\ncomputational perspective. First, we augmented the feature vectors derived from ECGs with\ndemographic data. Second, we utilized a multi-task neural network approach to estimate\npain, gender, and age simultaneously. Both methods demonstrated improved performance\ncompared to the standard approach, indicating that integrating demographic information can\nenhance the accuracy of automatic pain assessment systems.\nWe recommend that clinical pain assessment tools be designed for specific demographic\ngroups to account for the distinct ways pain manifests across different populations. Ad-\nditionally, we emphasize to researchers developing new pain databases the importance of\nincluding demographic factors and information on the social context and psychological con-\nditions of subjects to enhance the quality and applicability of the data collected. Our study\nfocused on analyzing pain sensation through biosignals, specifically ECGs. We propose that\nfurther research should explore pain expressivity through visual mediums such as video. As\npreviously discussed in Section 2, the expression of pain is a crucial and complex issue. Peo-\nple vary in expressiveness; for various reasons, they might exaggerate or even feign pain,\nmaking accurate assessment challenging.74Chapter 5\nOptimization: Balancing Efficiency and Per-\nformance\nContents\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the studies published in [37, 38]. As outlined in\n3.6.3, research in automatic pain assessment has rarely considered real-world situations. For\nexample, [231] implemented their study in an emergency triage setting, while [234] tested\ntheir approach within IoT devices. Additionally, we highlighted that the scarcity of stud-\nies exploring pain estimation in real-world settings or unconventional contexts suggests that\ncurrent methodologies might not be entirely suitable for practical environments like clinics\nor hospitals due to issues with generalization or operational factors such as efficiency and\ninference time. For these reasons, this chapter\u2019s objective is to explore approaches that (i)\n7576 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nutilize modalities readily available and applicable in the market and (ii)examine the impact\nof model size and computational cost on performance. In this context, our methodologies\nand experiments exclusively utilize RGB videos, a universally available modality, particu-\nlarly on mobile devices. Additionally, we incorporate heart rate data, which is commonly\naccessible from various types of wearable technology. It is important to note that although\nwe employ established pain datasets in our experiments, the videos and heart rate data ex-\ntracted from ECGs are akin to those that could be obtained from smartphones and wearables,\nserving as a proof of concept for application in real-world environments. Furthermore, with\nrespect to efficiency and the speed of inference, our goal is to develop the most compact pain\nassessment frameworks possible, ensuring they maintain adequate performance levels.\nNumerous studies highlight the capabilities of automated systems that utilize behavioral\nor physiological modalities for pain assessment [271]. Sario et al. [34] demonstrate the fea-\nsibility of accurately detecting and quantifying pain through facial expressions, establishing\ntheir value in clinical settings. The use of multimodal sensing appears especially promising,\noffering increased accuracy in pain monitoring systems [22]. An important aspect of pain\nmonitoring involves wearable devices that record biopotentials to estimate pain levels. Few\nstudies have investigated the use of mainstream, wearable technology for this purpose, possi-\nbly due to a research preference for more costly, highly precise medical equipment. Leroux\net al. [32] state, \u201cThe challenge is not whether wearable devices will provide useful clinical\ninformation but rather when we will start to use them in practice to enhance the field of pain. \u201d\nAdditionally, Claret et al. [272] explore the potential of using cardiac signals from wearable\nsensors for automatic emotion recognition, confirming the effectiveness of such methods.\nIn this chapter, our deep learning approaches are founded on transformer-based archi-\ntectures. Convolutional Neural Networks (CNNs) have been the cornerstone of mainstream\nneural architectures in computer vision (CV), especially in the field of automatic pain as-\nsessment using images and videos, as we discussed in Section 3. Inspired by the success\nof transformer architecture in natural language processing (NLP), where the self-attention\nmechanism is a fundamental element [273], researchers have developed similar models for\nvisual tasks. The introduction of Vision Transformers (ViT) [274] has established a new\nparadigm in the computer vision domain. This has led to a plethora of new approaches based\non ViT, such as the Transformer in Transformer (TNT) [275], which enhances local feature\nrepresentation by subdividing image patches into smaller sub-patches. While transformer-\nbased models have shown impressive results and offer great flexibility, they tend to scale\npoorly with input size and incur higher computational costs due to the self-attention layers\nthat compute interactions between all input pairs. Efforts to mitigate these challenges in-\nclude replacing self-attention with cross-attention [276] or combining both techniques [277]\nto improve the efficiency and reduce the complexity of these architectures.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 77\nFigure 5.1: The application of face alignment illustrates landmarks in 2D (left) and 3D (right) space.\n5.2 Video Analysis with Vision Transformers\nWe introduce a framework incorporating a vision transformer as a module extracting spatial\nfeatures for individual video frames, combined with a transformer-based model equipped\nwith cross and self-attention blocks, extracting temporal features from the video feature\nsequences. This configuration enables the effective utilization of the temporal dimensions of\nvideo data to deliver more accurate and reliable estimation of the continuous nature of pain.\n5.2.1 Methodology\nThis section outlines the preprocessing methods employed, the design of our framework, the\nimplementation details concerning the training procedure, and the database used.\nPre-processing\nBefore processing videos for pain estimation, applying face detection and alignment was\ncrucial to enhance performance and computational efficiency. We utilized the well-known\nface detector MTCNN [278] in combination with the Face Alignment Network (FAN) [279],\nwhich leverages 3D landmarks. This 3D approach is critical for addressing our specific chal-\nlenges, as head movements tend to increase, particularly during instances of high-intensity\npain, which can lead to inaccurate alignments with 2D methods. Additionally, it should be\nnoted that all experiments were carried out using video frames with a resolution of 224\u02c6224\npixels. Figure 5.1 illustrates the facial alignment process applied to a video frame.\nTransformer-based Framework\nOur framework is composed of two primary components: the \u201cspatial feature extraction\nmodule\u201d , specifically a TNT (Transformer in Transformer) model, and the \u201ctemporal fea-78 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nture extraction module\u201d , which is a transformer with both cross and self-attention blocks.\nThis framework, illustrated in Figure 5.2, includes approximately 24million parameters and\nperforms operations at 4.2GFLOPS.\nSpatial feature extraction module :Each frame is initially divided into npatches represented\nasFk\u201crF1\nk, F2\nk, . . . , Fn\nksPRn\u02c6p\u02c6p\u02c63, where p\u02c6pdenotes the resolution of each patch ( i.e.,\n16\u02c616) and 3represents the number of color channels. These patches are then subdivided\nintomsub-patches to facilitate the model\u2019s learning of global and local features. Each frame\nis thus transformed into a sequence of patches and sub-patches:\nFk\u00d1rFk,n,1, Fk,n,2, . . . , F k,n,ms, (5.1)\nwhere Fk,n,mPRs\u02c6s\u02c63is the m-th sub-patch of the n-th patch of the k-th frame, with each\nsub-patch having a resolution of s\u02c6s(i.e.,4\u02c64). Following this, the patches and sub-\npatches undergo a linear projection and are transformed into embeddings ZandY. Position\nembeddings are then added to retain spatial information:\nZ0\u00d0Z0`Epatch, (5.2)\nwhere Epatchare the position encodings for the patches. Correspondingly, for each sub-patch\nwithin a patch, a position encoding is also added:\nY0\ni\u00d0Y0\ni`Esub-patch , (5.3)\nwhere Esub-patch are the sub-patch position encodings and i\u201c1,2, . . . , m denotes the index of\na sub-patch. These sub-patches are then processed through an \u201cInner Transformer Encoder\u201d ,\nwhich consists of two multi-head self-attention blocks, crucial for dot product attention. The\nattention mechanism is defined as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n, (5.4)\nwhere QPRM\u02c6D, KPRM\u02c6C,andVPRM\u02c6C(Mis the input dimension, CandDare\nchannel dimensions) are projections of the corresponding input and represent the Query, Key,\nand Value matrices. They defined as Q\u201cXW Q,K\u201cXW K, and V\u201cXW V, where W\nare the learnable weight matrices and Xis the input. The output embedding from the \u201cInner\nTransformer Encoder\u201d is then added to the patch embedding and forwarded to the \u201cOuter\nTransformer Encoder\u201d . This encoder comprises three multi-head self-attention blocks, and\nits output is a feature vector d\u201c192. The \u201cspatial feature extraction module\u201d as a whole\nencompasses a depth of 12blocks.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 79\nTable 5.1: Training details for the automatic pain assessment.\nEpochs Optimizer Learning\nRateLR decay Weight\ndecayWarmup\nepochs\n200 AdamW 1e-4 cosine 0.1 5\nLabel\nSmoothingDropPath Attention\nDropOutLoss\nfunctionAugmentation methods\n0.1 0.1 0.1 Cross\nEntropyAugMix [281] &\nTrivialAugment [282]\nDropPath applied to the \u201cspatial feature extraction module\u201d , Attention DropOut applied to the \u201ctemporal\nfeature extraction module\u201d\nTemporal feature extraction module :The extracted embeddings of each input video frame\nare concatenated into a unified vector D, representing the entire video as V\u00f1D\u201c\npd1\"d2\", . . . , dkq. This vector is then processed through the temporal module, a transformer\narchitecture consisting of 1cross-attention and 2self-attention mechanisms, each followed\nby a fully connected neural network (FCN). The introduction of cross-attention, which em-\nploys asymmetry in the attention mechanism, helps reduce computational complexity and\nincrease the model\u2019s efficiency. Specifically, rather than projecting the input with dimen-\nsions M\u02c6D, theQin cross-attention is a learned matrix with dimensions N\u02c6D, where\nN\u0103M. This module\u2019s self-attention components function as detailed in Equation 7.3, with\nthe cross and self-attention units comprising 1and8heads, respectively. In addition, we\nincorporate Fourier feature position encoding [277].\nTraining Details: Before starting the automatic pain estimation training process, we pre-\ntrained the \u201cspatial feature extraction module\u201d using the VGGFace2 dataset [280], incorpo-\nrating over three million facial images from more than nine thousand individuals. Table 7.3\ndetails the hyperparameters of our method and the applied augmentation techniques.\nDatabase Details: For this approach, we used the publicly available BioVid dataset [109],\nas described in the previous chapters.\n5.2.2 Experiments\nIn this section, we detail the experiments conducted for pain estimation. Our experiments\nwere carried out in both binary and multi-level classification formats. Specifically, we con-\nducted binary classification tasks: (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3,(iv)NP vs. P 4,\nand(v)a multi-level pain classification utilizing all available pain classes from the database.\nWe employed the leave-one-subject-out (LOSO) cross-validation method as our evaluation80 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nprotocol. Additionally, the classification metrics used in this study include micro-average ac-\ncuracy, macro-average precision, macro-average recall (sensitivity), and macro-average F1\nscore.\n5.2.3 Results\nPain Estimation\nIn evaluating pain estimation tasks, we noted the following results: For the NP vs. P 1task,\naccuracy reached 65.95%, with precision almost identical at 65.90%. The F1 score was\nFigure 5.2: An overview of our proposed transformer-based framework for automatic pain as-\nsessment.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 81\nTable 5.2: Results on the pain estimation tasks.\nMetricTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAccuracy 65.95 66.87 69.22 73.28 31.52\nPrecision 65.90 66.89 69.18 73.31 31.48\nRecall 67.85 68.34 70.84 74.75 29.94\nF1 65.04 66.19 68.54 72.75 27.82\nNP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain P 4: very severe pain MC: multi-level\nclassification\nslightly lower at 65.04%, and the recall stood out at 67.85%. In the NP vs. P 2task, accuracy\nincreased to 66.87%, and all related metrics improved, with the F1 score climbing by over\n1.15%, highlighting enhanced detection of true positives. The results were notably better\nfor the NP vs. P 3task, with an accuracy of 69.22% and a sensitivity of 70.84%. This is\nexpected as the pain at this level is considered severe, eliciting more pronounced responses\nfrom subjects. In the highest pain task, NP vs. P 4, the recall was particularly high at 74.75%,\nwith an accuracy of 73.28%, demonstrating that the detection of very severe pain is relatively\nmore straightforward due to the pain reaching tolerance thresholds, making it more visibly\nevident through subjects\u2019 facial expressions. However, in the multi-level classification task,\nperformance metrics were lower, illustrating the complexity of estimating all pain levels\nconcurrently; accuracy was only 31.52%, with a recall of 29.94%, pointing to significant\nchallenges in accurately identifying true positives across multiple pain levels.\nIt should be noted that our framework, encompassing both the architectural and procedu-\nral aspects of training, was consistent across all binary and multi-level classification tasks.\nThis was done to evaluate the generalization potential of our approach across all possible\nscenarios provided by the database, akin to real-world clinical settings. The detailed classifi-\ncation outcomes are presented in Table 5.2.\nVideo Sampling\nIn this section, we explore the impact of video frame sampling on automatic pain estimation.\nExperiments detailed in Section 5.2.3 utilized all available frames ( 138) from each video.\nSubsequent experiments employed frame sampling with strides of 2,3, and 4. Starting with\nall138frames, the video feature representation Dhas dimensions 138\u02c6192, totaling 26,496.\nA stride of 2reduces this to 69frames, with Dhaving dimensions 69\u02c6192and totaling\n13,248. With strides 3and4, the frame counts reduce to 46and35, resulting in Dsizes82 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.3: Results for the pain estimation tasks using various numbers of input frames.\nNumber of\nFramesTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n138 65.95 66.87 69.22 73.28 31.52\n69 65.76 66.74 69.15 73.25 31.29\n46 65.66 66.70 68.50 71.78 31.20\n35 65.40 66.12 68.32 72.01 30.80\nFigure 5.3: The impact of the number of input frames on accuracy (left) and on runtime in\nmilliseconds (right). Runtime calculated during inference on a NVIDIA RTX-3090 .\nof8,832and6,720, respectively. Table 5.3 displays the classification accuracies achieved\nwith these varying frame counts for each pain estimation task. Concurrently, Figure 5.3\ndemonstrates how the number of frames affects mean accuracy across the five tasks and\nmean runtime during inference. We noted a performance increase of approximately 1.38%\nwhen using 138frames compared to 35frames. Additionally, the runtime increased by a\nfactor of 3. Despite the longer runtime, each sampling rate allows for real-time automatic\npain estimation when necessary.\nInterpretation\nResearch in deep learning, particularly relevant to healthcare, increasingly focuses on model\ninterpretability to explain decision-making processes. This is crucial for enhancing the trans-\nparency of models, a key factor for their acceptance and integration into clinical settings. In\nour study, we implemented the technique described in [283] to generate relevant maps illus-\ntrating which facial areas our model\u2014the \u201cspatial feature extraction module\u201d \u2014focuses on.\nAs shown in Figure 5.4, the model\u2019s attention is distributed over \u201carbitrary\u201d areas at the on-\nset of a facial expression sequence. However, as the expression of pain intensifies, the focus\nsharpens on specific regions indicative of pain. It is important to note from our relevance5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 83\nFigure 5.4: Relevance Maps.\nmaps that no universal facial expressions are unique to pain. However, there is a noticeable\nconcentration on areas like the mouth and eyes.\nComparison with existing methods\nIn this section, we present a comparison of our results achieved using a transformer-based\nframework that utilizes all available video frames against other studies that also employed\nPart A of the BioVid database with all 87subjects, following the same leave-one-subject-out\n(LOSO) cross-validation protocol. This ensures objective and accurate comparisons, with\nresults detailed in Table 5.4. The studies compared fall into three main categories: i) those\nfocusing exclusively on pain detection (NP vs. P 4),ii) those examining both pain detection\nand multi-level pain estimation, and iii) those that cover all major pain-related tasks.\nOur method, tested across all tasks, recorded the highest performance metrics in bi-\nnary and multi-level pain estimations. Studies limited to specific aspects of pain detec-\ntion or multi-level pain estimation often yielded comparable or superior results, as indicated\nin [219], [180], and [284]. This highlights that while focused studies often show high perfor-\nmance, the broader impact lies in developing systems that perform well across all potential\nscenarios.\n5.2.4 Discussion\nThis research examined the application of transformer-based architectures for automatic pain\nestimation through video analysis. Our framework employed exclusively transformer mod-\nels, leveraging the spatial and temporal aspects of the video frames. The experiments demon-\nstrated the effectiveness of our approach in assessing pain, showing strong generalization\nacross various pain estimation tasks with notable results, particularly with low-intensity pain\nwhere facial expressions are less apparent. Additionally, the framework demonstrated high\nefficiency, suitable for real-time applications. A significant contribution of our work includes\ndeveloping relevance maps highlighting facial areas the model focuses on. We advocate for\ncontinued efforts within the affective computing field to enhance the interpretability of these\ndeep-learning methods.84 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.4: Comparison of studies utilizing BioVid , RGB videos, and LOSO validation.\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nThiam et al. [219] - 69.25 -\nTavakolian et al. [180] - - - 71.02 -\nPatania et al. [284] - - - 73.20 -\nHuang et al. [269] - - - 77.50 34.30\nXinet al. [141] - - - 86.65 40.40\nZhi and Wan [217] 56.50 57.10 59.60 61.70 29.70\nWerner et al. [248] 53.30 56.00 64.00 72.40 30.80\nOur approach 65.95 66.87 69.22 73.28 31.52\n5.3 Video & Heart Rate Analysis with Transformer Architectures\nWe introduce a proof of concept for an automatic pain assessment framework that integrates\nfacial video data captured by an RGB camera with heart rate signals. We build and extend\nour previous analysis in 5.2. Our main objectives include (1) evaluating the effectiveness and\nlimitations of video and heart rate data as standalone modalities in an unimodal setting, (2)\nexploring the efficacy of combining behavioral (video) and physiological (heart rate) markers\nto overcome challenges associated with their reliance on different sensing technologies and\ninformation representations, and (3) analyzing the performance and efficiency of recently\nintroduced transformer-based architectures.\n5.3.1 Methodology\nThis section details the preprocessing methods for video and ECG, the design of the proposed\nframework, the augmentation techniques developed, and the specifics of implementing the\npretraining process.\nPre-processing\nPreparatory processing was essential before feeding data into the pain assessment framework,\nparticularly for the raw ECG data used to compute heart rate. We focused on exploring heart\nrate as the primary feature due to its benefits: It\u2019s readily obtainable from wearable devices,\ncost-effective, and easily accessible. These advantages position heart rate as a potentially\nvaluable feature for automated pain assessment.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 85\nVideo Preprocessing: Video preprocessing included face detection to isolate the facial re-\ngion using the MTCNN face detector [278], which employs multitask cascaded convolu-\ntional networks to predict facial and landmark locations. Predicting landmarks facilitates\nface alignment, which is crucial for accurate facial analysis. However, we observed that\nface alignment reduced the expressiveness linked to head movements, a common behavioral\nindicator of pain. Consequently, face alignment was omitted from our proposed pipeline.\nAdditionally, the resolution of frames post-face detection was standardized at 448\u02c6448\npixels.\nECG preprocessing & analysis: Similar to Section 4, we utilize the Pan-Tompkins [262] al-\ngorithm to detect the QRS complex, the most prominent wave complex in an ECG signal.\nThis algorithm operates in two phases: preprocessing and decision-making. The preprocess-\ning phase focuses on noise removal, artifact elimination, signal smoothing, and enhancing\nthe QRS slope. The decision-making phase involves initial QRS detection using adaptive\nthresholds, a retrospective search to identify any missed QRS complexes, and a method for\ndistinguishing T waves. Following the accurate detection of R waves, the estimation of inter-\nbeat intervals (IBIs) was conducted, leading to the extraction of key features. Specifically,\nwe calculated the mean of the IBIs as follows:\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (5.5)\nwhere nis the total number of IBIs, and RRidenotes the consecutive R\u00b4Rintervals.\nSubsequently, the heart rate was calculated using the following formula:\nHR\u201c60\u00a8FS\n\u00b5, (5.6)\nwhere FSdenotes the sampling frequency of the ECG recording.\nFramework architecture\nThe proposed framework, as illustrated in Figure 5.5, consists of four key components: the\nSpatial-Module that extracts embeddings from video data, the Heart Rate Encoder which\nmaps heart rate signals into a higher dimensional space, the AugmNet that generates aug-\nmentations in the latent space, and the Temporal-Module performs with the final assessment\nof pain.\nSpatial-Module: The architecture for this module draws inspiration from the Transformer\nin Transformer approach as detailed by [275]. The process begins with the initial video86 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(a) Video analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(b) ECG analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(c) Fusion analysis pipeline.\nFigure 5.5: Outline of the proposed framework.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 87\nframe at a resolution of 448\u02c6448pixels, segmented into 4quadrants, each at 224\u02c6224\npixels. This tiling method, which maximizes the utilization of the frame\u2019s resolution, is\ninfluenced by approaches seen in satellite imaging analysis. Our framework incorporates the\n4tiles and the original frame\u2014resized to 224\u02c6224pixels\u2014into our analysis pipeline. Thus,\neach video frame transforms into 5distinct images, denoted as:\nFk\u201crFk,1, Fk,2, . . . , Fk,ts, (5.7)\nwhere kstands for the frame number, and tencompasses the tile count, including the resized\nfull frame. Each tile is initially split into npatches, expressed as:\nFk,t\u201crFk,t,1, Fk,t,2, . . . , Fk,t,nsPRn\u02c6p\u02c6p\u02c63, (5.8)\nwhere p\u02c6pspecifies the resolution of each patch ( 16\u02c616), and 3denotes the RGB channels.\nThese patches are further segmented into msub-patches, allowing the model to capture the\nimage\u2019s global and localized features. Each tile from a frame thus transitions into a sequence\nof patches and sub-patches, represented as\nFk,t\u201crFk,t,n, 1, Fk,t,n, 2, . . . , Fk,t,n,ms.\nConsequently, each video frame is characterized by:\nFk\u00d1\u201c\nFk,t,n,m|tPr1,5s, nPr1,196s, mPr1,16s\u2030\n, (5.9)\nwhere Fk,t,n,mPRs\u02c6s\u02c63defines the m-th sub-patch within the n-th patch of the t-th tile\nfor the k-th frame, with each sub-patch having a resolution of s\u02c6s(4\u02c64). Each frame\nconsists of 5image representations, encompassing 196patches, and each patch contains 16\nsub-patches. The patches and sub-patches are then linearly projected into embeddings Zand\nY. Positional embedding is applied to maintain spatial information, employing 1D learnable\nposition encodings:\nZ0\u00d0Z0`Epatch, (5.10)\nwhere Epatch indicates the position encoding. Each sub-patch also receives its specific posi-\ntional encoding:\nYi\n0\u00d0Yi\n0`Esub\u00b4patch, (5.11)\nwhere Esub\u00b4patch denotes the positional encodings for sub-patches, and irepresents the index\nof a sub-patch within a patch. The sub-patches are processed in the Inner Encoder , which88 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nconsists of four self-attention heads [285], utilizing dot product attention:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n. (5.12)\nThe output from the Inner Encoder integrates into the patch embedding, advancing to the\nOuter Encoder , which mimics the Inner Encoder with ten self-attention heads. The Spatial-\nModule consists of twelve parallel blocks, generating embeddings of dimensionality d\u201c100.\nFor each input video frame, 5distinct output embeddings of dimensionality 100are produced\nand combined to form a comprehensive frame representation:\nD\u201cdFullFrame`pdTile1`dTile2`dTile3`dTile4q\u00a8c, DPR100, (5.13)\nwhere cadjusts the contribution from the tile embeddings, subsequently, the embedding for\neach frame, D, is concatenated with those of other frames to construct a complete video\nrepresentation:\nVD\u201crD1}D2}. . .}Dfs, VDPRN, (5.14)\nwhere frepresents the total number of frames in the video, and Ndenotes the dimensionality\nof the final video embedding.\nHeart Rate Encoder: As mentioned in Section 5.3.1, heart rate is computed from the origi-\nnal ECG every second, producing a heart rate vector of length h\u201c\u03b8for recordings lasting\n\u03b8seconds. It should be noted that when beats per minute (BPM) fall below 60within any\n1-second segment of the ECG, making direct heart rate calculation impractical, the method\naverages heart rates from the data points immediately before and after to maintain a uniform\nset of \u03b8data points. The Heart Rate Encoder , which is part of a transformer-based archi-\ntecture similar to the Inner andOuter Encoders , utilizes one cross-attention head instead\nof self-attention followed by a fully connected neural network (FCN). This use of cross-\nattention introduces an asymmetry that reduces computational load and increases efficiency.\nUnlike traditional input projections that are M\u02c6Din dimension, as detailed in Section\n5.3.1, the Qmatrix in cross-attention is a learnable matrix sized N\u02c6Dwhere N\u0103M. The\nencoder\u2019s internal embeddings are set to a dimensionality of 512and contain only a single\nblock depth. Fourier feature position encodings [277] are also implemented to handle posi-\ntional information. The main goal of this encoder is to transform the initial heart rate vector\nhinto a more complex and richer feature space,\nhPR\u03b8\u00d1EhPR2048,5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 89\nwhere Ehrepresents the enhanced output embedding of this encoder. In the next step, the\noutput from the heart rate encoder is expanded dimensionally via a bicubic interpolation\nmodule. This process enhances the original heart rate\u2019s feature representation, allowing it to\nintegrate smoothly with the video\u2019s embedding representation through addition. The need\nfor identical dimensions in both embedding vectors is critical and is adeptly addressed by this\ninterpolation module. This non-learning-based approach proves to be efficient and effective\nfor encoding purposes. Additionally, interpolation provides the flexibility to dynamically set\nthe dimensionality of the final output embedding, unlike the fixed dimensions typically seen\nin neural network-based methods. Specifically:\nBh\u201c3\u00ff\ni\u201c03\u00ff\nj\u201c0aijpEhq\u00a8px\u00b4x0pEhqqi\u00a8py\u00b4y0pEhqqj, (5.15)\nwhere aijare the coefficients used for interpolation, and Bhis the resulting vector from the\nbicubic interpolation process. The dimension of BhisN, which is the same as that of VD.\nAugmNet: Inspired by recent developments in the augmentation literature [286], employs\na learning-based technique to identify augmentation patterns within the latent space. Un-\nlike conventional methods that perform image augmentations ( e.g., rotation, cropping) in the\npixel space, AugmNet universally applies transformations to the embeddings. This method\neliminates the necessity for specific transformations tailored individually to each modality,\ne.g., image, signal, and text. Incorporating this module within the automatic pain assessment\nframework helps to regularize the learning process and address overfitting issues. Moreover,\ncorrupting the input embeddings compels the following model, especially the Temporal-\nModel , to derive more precise and representative features, thereby improving performance\nin the pain assessment task. The modality-agnostic method effectively applies to embedding\nrepresentations from any original modality, including video and heart rate signals. AugmNet\nadopts an encoder-decoder architecture, where both the encoder and decoder consist of only\n2fully connected layers with the ELU nonlinear activation function applied after each layer.\nFor a session lasting \u03b8seconds, it produces \u03b8\u02c6frames per second frames and \u03b8\u02c6FS\ndata points for video and ECG, respectively. In the video analysis pipeline, the Spatial-\nModule constructs an embedding representation, VD(5.14), from the original video, dimen-\nsioned at d\u02c6FPS\u201cN. In the ECG analysis pipeline, a feature vector with dimension \u03b8\nis created after extracting the heart rate, one data point per second. The Heart Rate Encoder\nand bicubic interpolation then produce an embedding, Bh(5.15), with dimension N. The fu-\nsion of video and heart rate embeddings at the session level is performed, where VDandBh\nare merged by addition, integrating the data from the initial input modalities. The resulting90 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncombined embedding is then processed by AugmNet :\nAD\u00d0AugmNetpVD`Bhq (5.16)\nP\u00d0AD`pVD`Bhq, (5.17)\nwhere Prepresents the transformed embedding vector, serving as input for the final module,\ntheTemporal-Module .AugmNet is active only during training as a standard augmentation\nmethod and remains inactive during inference.\nTemporal-Module: Like the Heart Rate Encoder , this component operates on a transformer-\nbased architecture. It employs a combination of multi-head cross-attention and multi-head\nself-attention mechanisms. The architecture consists of one multi-head cross-attention block\nwith a single attention head and three subsequent multi-head self-attention blocks, each fea-\nturing eight attention heads. An FCN follows each attention block. The internal embed-\ndings in this module have a dimensionality of 128and encompass a single block in depth.\nThe position encoding method used here mirrors that of the Heart Rate Encoder , utilizing\nFourier feature position encoding. This module processes the input embedding Por the sum\npVD`BhqifAugmNet is inactive, to derive the final classification outcome. The learning\nerror is computed during this phase, and the framework undergoes further training.\nSupplementary augmentation methods\nWe also introduced additional augmentation strategies alongside the AugmNet module, which\napplies learned transformations to embeddings. The initial technique, dubbed Basic , com-\nbines polarity inversion and noise addition to manipulate the original inputs by reversing\ntheir polarity and injecting noise. Another technique, named Masking , involves nullifying el-\nements within the embeddings using randomly sized and placed masks that zero out 10\u00b420%\nof the embedding elements. These methods function within the latent space, similar to Augm-\nNet.\nPre-training\nBefore starting the training for automatic pain assessment, we individually pretrained all\nmodules except AugmNet . The Spatial-Module underwent a dual-phase pretraining process.\nInitially, it was pre-trained on VGGFace2 [280], a dataset designed for facial recognition\nto learn basic facial features. This was followed by an advanced training phase involving\nemotion recognition datasets in a multi-task learning framework. These datasets include the\nwidely used AffectNet [287], Compound Facial Expressions of Emotions Database [288],\nRAF Face Database basic [289], and RAF Face Database compound [289], enabling the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 91\nTable 5.5: Datasets utilized for the pre-training process of the framework.\nDataset # samples # classes Task\nVGGFace2 [280] 3.31M 9,131 Face\nAffectNet [287] 0.40M 8 Emotion\nCompound FEE-DB [288] 6,000 26 Emotion\nRAF-DB basic [289] 15,000 7 Emotion\nRAF-DB compound [289] 4,000 11 Emotion\nECG HBC Dataset [291] 0.45M 5 Arrhythmia\nTask: all tasks involve classification\nmodule to adapt to specific emotional expressions related to pain manifestations. During\nthis phase, the model is trained across these datasets simultaneously, employing a multi-task\nlearning loss approach as suggested by [290], where learned weights scale the individual\nlosses to account for the homoscedastic uncertainty of each task:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s`rew3LS3`w3s`rew4LS4`w4s,\nwhere LSdenotes the loss for each dataset and ware the weights adjusting the learning focus\nto optimize the overall loss Ltotal. The Temporal-Module is trained solely on the VGGFace2\ndataset. For this module, images are converted into 1D vectors prior to processing. The\nHeart Rate Encoder is pre-trained using the ECG Heartbeat Categorization Dataset [291],\nwhich includes heartbeat signals from the MIT-BIH Arrhythmia Dataset [292] and the PTB\nDiagnostic ECG Database [293] [294]. Table 5.5 provides a detailed list of the datasets used\nin our training process.\nDataset Details\nIn this study, to assess the developed framework, we utilized the BioVid Heat Pain Database\n[109], which includes facial videos, electrocardiograms, electromyograms, and skin conduc-\ntance levels from 87healthy individuals ( 44males and 43females, aged 20\u00b465). The\ndataset employs a thermode to induce varying pain levels in the participants\u2019 right arm. Ini-\ntially, each participant\u2019s pain threshold (the transition from heat sensation to pain) and pain\ntolerance (the point at which pain becomes unbearable) were determined. These thresholds\ndelineated the minimum and maximum pain levels, along with two intermediary levels, form-\ning five distinct pain intensities: No Pain (NP), Mild Pain (P 1), Moderate Pain (P 2), Severe\nPain (P 3), and Very Severe Pain (P 4). Pain stimuli temperatures ranged from P 1to P 4, capped\nat50.5\u00b0C. Each subject experienced 20stimulations at each of the four intensities (P 1to P 4).\nEach stimulation lasted 4seconds, interspersed with recovery intervals of 8to12seconds.92 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.6: Training details for the automatic pain assessment.\nOptimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsBatch\nsize\nAdamW 1e-4 cosine 0.1 50 32\nThis protocol, along with 20baseline measurements at NP ( 32\u00b0C), culminated in randomly\nordered 100stimulations per participant. The data was preprocessed to capture 5.5-second\nwindows starting 1-second after reaching the target temperature for each stimulation. This\nprocess produced 8,700samples, each 5.5seconds in duration, distributed evenly across the\nfive classes and modalities for all 87subjects.\n5.3.2 Experiments\nThe study leveraged videos and electrocardiograms from Part A of the BioVid dataset, using\nall available samples from the 87participants. The videos were recorded at a rate of 25\nframes per second (FPS), and the electrocardiogram (ECG) signals were sampled at 512\nHertz (Hz). Each recording session lasted 5.5seconds, generating 138video frames and\nECG vectors containing 2,816elements each, then converted into heart rate vectors of 5\ndata points. The complete set of frames and data points from both videos and cardiac signals\nwas utilized in the experiments. The experimental approach included iterative refinement\nof techniques, with the most effective combinations selected for extended training periods\nranging from 500to800epochs to improve feature extraction and performance outcomes.\nTable 5.6 details the training configurations for the automatic pain assessment tasks.\nPain assessment experiments were structured around binary and multi-level classification\nsetups, testing each modality individually and in combination. The binary classification task\ndifferentiated between No Pain (NP) and Very Severe Pain (P 4), whereas the multi-level\nclassification (MC) involved categorizing all pain intensities available in the dataset. The\nevaluation strategy adopted was the leave-one-subject-out (LOSO) cross-validation, and the\nassessment metrics included accuracy, precision, recall (sensitivity), and F1 score. Notably,\na consistent training regimen was applied across both the binary (NP vs. P 4) and multi-level\n(MC) classification tasks without varying the training schedule or optimization strategies.\n5.3.3 Results\nVideo modality\nExperiments related to the video modality explored the effects of pretraining on the Spatial-\nModule , the video analysis pipeline\u2019s performance, particularly the impact of tiling, and the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 93\nTable 5.7: Results utilizing the video modality.\nEpochsPretraining stage Pipeline Augmentations Task\n1st2ndFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n500 \u2713 - \u2713 - \u2713 - - 72.56 31.22\n500 - \u2713 \u2713 - \u2713 - - 74.25 33.34\n500 - \u2713 - \u2713 \u2713 - - 68.07 31.49\n500 - \u2713 \u2713 \u2713 \u2713 - - 65.11 27.84\n500 - \u2713 \u2713 \u2713c\u2713 - - 74.86 33.86\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 - 73.05 32.14\n500 - \u2713 \u2713 \u2713c\u2713 - \u2713 74.83 33.73\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 73.16 32.87\n800 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 77.10 35.39\nStage: referring to pretraining process for Spatial-Module Mask: Masking c: constant-coefficient applied exclusively to the tiles NP:\nNo Pain P 4: Very Severe Pain MC: multiclass pain level\nimplementation of new augmentation techniques. These experiments are detailed in Table\n5.7. Performance enhancements are evident when comparing the first and second pretraining\nstages of the Spatial-Module . For instance, in the NP vs. P 4task, initial pretraining alone\nachieved 72.56% accuracy, while including the second emotion-focused pretraining stage\nincreased accuracy to 74.25%. This trend is also notable in the multi-level classification,\nwhere the second stage added 1.12% to the performance, totaling 33.34%. Further experi-\nments assessed the effect of using tiles in the video representation. Initially, employing four\ntiles led to a performance decrease of over 6%in the binary classification task and 1.85% in\nthe multi-level task. This reduction likely results from the localized information in each tile,\nwhich may capture irrelevant details like non-expressive facial areas or background elements.\nIncluding the resized full-frame ( 224\u02c6224pixels) alongside tiles further decreased accuracy\nto65.11% and27.84% for binary and multi-level tasks, respectively. However, introducing\na coefficient ( c\u201c0.1) to adjust the tile embeddings restored some performance, achieving\n74.86% and33.86% in respective tasks.\nThe integration of two augmentation techniques, Masking andAugmNet , along with the\nBasic method, was then tested. Masking reduced performance by 1.81% and1.72%, and\nAugmNet showed smaller declines of 0.03% and0.13%. Using both techniques together\nresulted in better outcomes than Masking alone but did not independently surpass the per-\nformance of AugmNet . Despite these initial results, combining all augmentation methods\nproved advantageous for extended training periods. This approach addresses the risk of\noverfitting through a robust regularization strategy. Ultimately, this comprehensive strategy\nled to final accuracy rates of 77.10% and35.39% for binary and multi-level classifications,\ndemonstrating its effectiveness in an unimodal, vision-based pain assessment framework.94 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nHeart rate modality\nThe experiments concerning the heart rate modality explore the use of the encoder and vari-\nous augmentation methods. Table 5.8 details all experiments involving the heart rate modal-\nity. Initially, using the original heart rate vectors with a dimensionality of h\u201c5, we achieved\nclassification scores of 61.70% for NP vs P 4and27.60% for the multi-level task. After apply-\ning the Heart Rate Encoder to map these vectors to a higher-dimensional space of h\u201c2048 ,\nwe noted a slight improvement: a 0.23% increase for the binary task and 0.08% for the\nmulti-level task. Despite the considerable increase in embedding size, this modest enhance-\nment suggests that the intrinsic information within the limited heart rate data points does not\nsignificantly enhance the feature representation. Nonetheless, the encoder\u2019s use is vital for\nproducing larger embeddings, especially for our multimodal approach integrating video and\nheart rate data, which will be discussed in subsequent sections.\nWe also tested augmentation methods on the heart rate data. Applying Masking yielded\na slight improvement of 0.02% for the binary task and 0.05% for the multi-level task. Imple-\nmenting AugmNet further enhanced performance to 62.09% and28.11% for binary and multi-\nlevel tasks, respectively. However, combining all augmentations decreased performance to\n61.87% and27.96%. During an extended 800-epoch training period, we achieved 64.87%\naccuracy for the binary task and 29.81% accuracy for the multi-level task using all augmen-\ntations. Despite these gains, we found that augmentations pose more challenges for accurate\nheart rate classification than video. Therefore, we repeated the extended training without\nBasic andMasking , keeping only AugmNet , which improved binary task performance to\n67.04% and multi-level to 31.22%. This reduction in heart rate embedding corruption sig-\nnificantly enhanced performance. The differing effects of augmentations between heart rate\nand video modalities highlight the challenges of using a single, isolated feature in a machine\nlearning system. We infer that heart rate embeddings with limited informational content\nare more vulnerable to significant performance degradation from augmentations than richer\nvideo embeddings.\nMultimodality\nThe results of integrating video and heart rate modalities are detailed in Table 5.9. Based on\nthe insights gained from separate experiments with each modality, we extended the training\nduration to 800epochs. For this integrated approach, we utilized the tiles with a coefficient\nc\u201c0.1and applied AugmNet as the sole augmentation method. This fusion strategy re-\nsulted in a classification accuracy of 82.74% for NP vs. P 4and39.77% for the multi-level\nclassification task. These results mark a substantial enhancement, with improvements of\n5.64% and15.70% over the individual performances of the video and heart rate modalities,\nrespectively, for the binary classification. Similarly, for the multi-level classification, the in-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 95\nTable 5.8: Results utilizing the heart rate modality.\nEpochsHR\nEncoderAugmentations Task\nBasic Mask AugmNet NP vs P 4 MC\n500 - \u2713 - - 61.70 27.60\n500 \u2713 \u2713 - - 61.93 27.68\n500 \u2713 \u2713 \u2713 - 61.95 27.73\n500 \u2713 \u2713 - \u2713 62.09 28.11\n500 \u2713 \u2713 \u2713 \u2713 61.87 27.96\n800 \u2713 \u2713 \u2713 \u2713 64.84 29.81\n800 \u2713 - - \u2713 67.04 31.22\nTable 5.9: Results utilizing the video & the heart rate modality.\nEpochsHR\nEncoderPipeline Augmentations Task\nFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n800 \u2713 \u2713 \u2713c- - \u2713 82.74 39.77\ntegrated approach shows a 4.38% and8.55% increase compared to the standalone modalities.\nThe combination of these two pivotal modalities significantly boosts the efficacy of the pain\nassessment process, outperforming the results obtained by each modality on its own.\nComparison with existing methods\nIn this section, a comparative analysis is performed to evaluate the performance of our\nmethod against other existing approaches documented in the literature. This evaluation\nis based on Part A of the BioVid dataset, including all 87participants. The same evalua-\ntion protocol\u2014leave-one-subject-out (LOSO) cross-validation\u2014is adhered to for all com-\nparisons to ensure fairness and accuracy. Our method is contrasted with both unimodal and\nmultimodal approaches, divided into (1) video-based, (2) ECG-based, and (3) multimodal\nstudies regardless of the modalities involved. The outcomes are summarized in Table 5.10.\nFor video-based studies, our approach, achieving 77.10% in binary and 35.39% in multi-\nlevel classification tasks, is recognized as one of the highest-performing methods. It exceeds\nthe average results of comparative studies by about 4.7%for binary and 3.4%for multi-level\ntasks. Regarding ECG-based studies, our method shows superior performance, exceeding\nthe average by 8.5%and18.1%for binary and multi-level classifications, respectively. Re-\nmarkably, it records the highest classification accuracy in the multi-level task at 31.22%.96 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nThese findings underscore the effectiveness of using heart rate data extracted from ECG\nas a standalone feature, establishing our method\u2019s capability to assess pain accurately and\nachieve state-of-the-art results. In multimodal studies, our approach records an impressive\n82.74% accuracy in the NP vs. P 4task, making it one of the top-performing methods. It\nis slightly outperformed by studies [269] and [243], which achieved 88.10% and83.99%,\nrespectively. For multi-level tasks, comparisons are scarce; however, study [269] reached\n42.20%, and [235] reported 36.54%, positioning our method favorably within this context.\nInference time\nWe explored several methodologies in this study, including a video-based approach, video\nincorporating tiles, a heart rate-based approach, heart rate analysis with an encoder, and a\ncombined multimodal strategy. Figure 5.6 illustrates each method\u2019s inference time in sec-\nonds and the corresponding average accuracy performances across binary and multi-level\ntasks. Table 5.11 details the number of parameters and the computational cost of floating-\npoint operations (FLOPS) for each component. Inference tests were conducted on an Intel\nCore i7-8750H CPU, including the time for face detection in each frame but excluding the\nextraction of heart rate from electrocardiography, focusing on the potential use of automati-\ncally provided cardiac features from wearables.\nThe inference time for the video modality employing the standard pipeline is approxi-\nmately 26seconds. Utilizing the tile pipeline increases inference time significantly, soaring\nto about 130seconds due to processing five image representations per frame\u2014one full frame\nand four tiles\u2014compared to a single image representation in the non-tiled approach. In the\ncontext of heart rate signals, completing a pain assessment requires only 1.2seconds. With\nthe integration of the Heart Rate Encoder , the processing time remains virtually unchanged,\nshowing a negligible increase of less than half a second, highlighting this specific encoder\u2019s\nefficiency. Lastly, the comprehensive multimodal framework incorporating the tiles and the\nHeart Rate Encoder demands about 131seconds, illustrating the increased complexity and\ncomputational requirements when combining multiple modalities.\nInterpretation\nImproving model interpretability is essential for their acceptance and integration into clin-\nical settings. This study generates attention maps from both the Spatial-Module and the\nTemporal-Module , as illustrated in Figure 5.7. For the Spatial-Module , attention maps are de-\nrived from the last fully connected layer\u2019s weight contributions, which are then interpolated\nonto the images to highlight the model\u2019s focal areas. Figure 5.7a displays an original frame\nsequence along with three attention map variations: (1) post-initial pretraining, (2) after the\nsecond pretraining phase, and (3) post-training on the BioVid . Based on face recognition5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 97\nTable 5.10: Comparison of studies utilizing BioVid & LOSO validation, reported on accuracy %.\nStudy ModalityMethod Task\nFeatures Machine Learning Params (M) FLOPS (G) NP vs P 4 MC\n[295] Video optical flow RF - - 70.20 -\n[217] Video raw SLSTM - - 61.70 29.70\n[269] Video raw 3D CNN, - - 77.50 34.30\n[219] Video raw 2D CNN, biLSTM - - 69.25 -\n[180] Video raw 2D CNN 25.00\u00154.00 71.00 -\n[296] Video facial action\ndescriptorsDeep RF - - 72.40 30.80\n[296] Video facial 3D distances Deep RF - - 72.10 30.30\n[284] Video fiducial points GNN - - 73.20 -\n[135]:Video raw 2D CNN - - 71.30 37.60\n[211]:Video raw 2D CNN, GRU 150.00\u0015- 73.90 39.10\n[37] Video raw Transformer 24.00 4.20 73.28 31.52\n[267] Video facial landmarks,\n3D distancesRF 71.60 -\nOur Video raw Transformer 4.20\u00101.62 77.10 35.39\n[235] ECG raw 1D CNN 1.80\u0015- 57.04 23.23\n[270] ECG domain-specific\u00b8LR - - 57.69 -\n[35] ECG domain-specific\u00b8SVM - - 58.39 23.79\n[269] ECG heart rate\u20393D CNN - - 65.00 28.50\n[267] ECG domain-specific RF - - 62.00 -\n[36] ECG domain-specific FCN 4.09\u00120.40 69.40 30.24\n[297] ECG domain-specific\u00b8SVM - - 63.50 -\nOur ECG heart rate Transformer 6.03\u00101.25 67.04 31.22\n[235] ECG, EMG, GSR raw 2D CNN 10.00\u0015- 76.72 36.54\n[270] ECG, GSR domain-specific\u00b8SVM - - 72.20 -\n[269] Video1, ECG2raw1, heart rate2\u20393D CNN - - 88.10 42.20\n[267] ECG1, EMG1,\nGSR1domain-specific1\u00b8RF - - 74.10 -\n[267] Video1,ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 77.80 -\n[297] Video1, ECG2,\nGSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 78.90 -\n[297] Video1, ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8SVM - - 76.60 -\n[243] ECG, EMG, GSR raw DDCAE 4.00\u0015- 83.99 -\nOur Video1,ECG2raw1, heart rate2Transformer 8.60\u00102.44 82.74 39.77\nM: Millions G: Giga :: reimplemented for pain intensity estimation on BioVid by [269]\u2039: pseudo heart rate gain \u00b8: numerous\nfeatures \u0015: parameter count estimated from provided paper details \u0010:AugmNet excluded from parameter count, not used in inference\n\u0012: parameter count not mentioned in study, provided directly by authors -: missing value RF: Random Forest AE-ATT: Autoencoder\nAttention SVM: Support Vector Machines LR: Logistic Regression98 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nVideo Video\n(Tiles)Heart rate Heart rate\n(Encoder)Multimodal0102030405060Accuracy (%)Average Accuracy\n020406080100120\nInference Time (s)\nInference Time\nFigure 5.6: Comparison of mean accuracy and inference period for unimodal and multimodal\nstrategies across NP versus P 4and MC tasks. The diagram adopts a dual-y-axis\nconfiguration\u2014accuracy measurements on the left and time metrics on the right\u2014\nto outline the balance between performance efficacy and computational load, cate-\ngorizing the methodologies along the x-axis.\nTable 5.11: Module parameters and computational cost in FLOPS for the proposed framework.\nModule Params (M) FLOPS (G)\nSpatial-Module 2.57 1.19\nHeart Rate Encoder 4.40 0.82\nAugmNet 1.02 0.02\nTemporal-Module 1.63 0.43\nTotal 9.62 2.46\ntasks, the Spatial-Module produces maps focusing broadly on the facial area, particularly\nthe zygomatic, buccal, oral, mental, and nasal regions. The second stage, oriented towards\nmulti-task emotion recognition, refines the focus, sharpening attention on specific facial ar-\neas, highlighted in the first stage but with greater clarity and emphasis. After training on the\nBioVid for pain assessment, the attention maps show further refined focus on specific facial\nareas, with reduced attention to less relevant regions, ensuring concentrated focus on key\nareas. These maps consistently demonstrate the model\u2019s capability to adjust its focus based\non pain-related facial expressions.\nAttention maps from the Temporal-Module , based on input embeddings, illustrate the\nweight contributions in the module\u2019s final layer, forming easy-to-visualize rectangular pat-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 99\nterns. Figure 5.7b shows examples for three scenarios: (1) video embedding, (2) heart rate\nembedding, and (3) a combined embedding of video and heart rate. The attention maps ex-\nhibit a grid-like pattern, possibly due to the Fourier position encoding used, akin to those\nseen in perceiver-like architectures. The video embedding map shows intense attention\nacross the input. In contrast, the heart rate embedding map focuses attention more sparsely,\nwith a notable concentration in specific areas indicated by intense red coloration. The com-\nbined embedding map displays moderate intensity, consistent with the blended nature of the\ninput. These maps tend to emphasize the latter part of the session, aligning with the timing\nof pain manifestation towards the session\u2019s end, indicating the model\u2019s responsiveness to\nreal-time pain expressions.\n5.3.4 Discussion\nThis research introduced a multimodal framework that integrates video and heart rate sig-\nnals to assess pain automatically. Our innovative approach includes four main modules,\neach characterized by effectiveness and efficiency. The Spatial Module , particularly no-\ntable for its compact size of only 2.57million parameters, ranks as one of the most efficient\nvision-based models in automatic pain assessment literature. Despite limited comparative\nstudies, our model has shown it can match or exceed the performance of larger models. Its\nhigh efficiency and robust performance are primarily due to a thorough pretraining regime\non datasets related to affective responses, crucial for enhancing model capabilities in pain\nestimation tasks. The Heart Rate Encoder , with 4.40million parameters, excels at transform-\ning heart rate data into complex, high-dimensional embeddings, which integrate seamlessly\nwith video data during inference, all within under half a second. This quick processing\nunderscores the encoder\u2019s efficiency, supported by bicubic interpolation that modifies input\ndynamically to achieve variable output dimensions without predefined constraints. AugmNet ,\na novel augmentation module, learns to modify latent space representations directly, prevent-\ning the need for specific augmentation techniques designed for each data type. However, this\nmodule requires careful application to avoid overfitting and other training challenges. The\nTemporal-Module , consisting of 1.63million parameters, is crucial for final pain level as-\nsessments. It leverages a mix of cross- and self-attention mechanisms to enhance efficiency\nand accuracy, demonstrating the potential of transformers in streamlined settings contrary to\ntheir typical use in large-scale applications.\nOur experiments demonstrate that videos are invaluable for discerning individual pain ex-\nperiences by capturing diverse behavioral indicators like facial expressions, eye movements,\nand even slight color changes under stress. Utilizing video data, our methodology reached\nan accuracy of 77.10% in binary classification, effectively distinguishing between no pain\nand very severe pain scenarios. Moreover, it achieved 35.39% accuracy in a multi-level clas-100 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n(a) Attention maps from the Spatial-Module .\n(b) Attention maps from the Temporal-Module .\nFigure 5.7: Regions highlighted in yellow and red denote areas of significant attention. (a)\n(1strow) Sequence of original frames. (2ndrow) Derived from the Spatial-Module\nafter initial stage pretraining. (3rdrow) Derived from the Spatial-Module post sec-\nond stage pretraining. (4throw) Derived from the Spatial-Module trained on the\nBioVid dataset. (b) (1strow) Derived from the Temporal-Module incorporating\nvideo embeddings. (2ndrow) Derived from the Temporal-Module with heart rate\nembeddings. (3rdrow) Derived from the Temporal-Module using a combined em-\nbedding of video and heart rate.5.4. SUMMARY 101\nsification spanning five distinct pain intensities. The heart rate signal, tested as a standalone\nfeature from electrocardiography, showed that remarkable outcomes are possible with this\nsingle data feature, which is pivotal for validating the feasibility of heart rate as a viable\npain indicator. This is crucial as heart rate data is readily accessible from most wearable\ntechnologies, reducing the need for specialized algorithms to handle cardiac signals or raw\nbiosignals, thereby conserving both time and computational resources. Solely using heart\nrate, our model excelled, registering accuracies of 67.04% and31.22% for binary and multi-\nlevel classifications, respectively, among the highest reported. Incorporating video and heart\nrate data, our multimodal method yielded superior results\u2014 82.74% and39.77% for binary\nand multi-level classifications, respectively. These figures significantly enhance video-only\nresults by roughly 9%and heart rate-only outcomes by about 24%. Furthermore, with a total\nparameter count of just 9.62million, our approach stands out for its efficiency and effec-\ntiveness. Overall, this study showcases the synergy achievable by merging video and heart\nrate data, leading to a system outperforming its unimodal counterparts and emphasizing the\npotential of integrated multimodal pain assessment tools.\nUsing attention maps generated by the Spatial-Module , our framework analysis iden-\ntified crucial facial areas like the zygomatic and oral regions as significant for automatic\npain assessment. These maps demonstrated that different pretraining stages refine the focus,\nshowing more targeted attention with specialized training. Similarly, attention maps from\ntheTemporal-Module focused on the latter part of the input image, corresponding to where\npain manifestations are typically observed in the particular dataset.\n5.4 Summary\nThis chapter explores the interplay between model efficiency and performance in automatic\npain assessment tasks. We also aimed to mirror real-world conditions by leveraging read-\nily accessible and applicable modalities without relying on costly precision medical devices.\nConsequently, we utilized RGB videos with a resolution of approximately 1080x1080 and\nheart rate data. The videos utilized are of medium quality, comparable to those captured with\nmobile phone cameras, and the heart rate data simulates readings from wearable devices. It\nis important to note that wearables across various price ranges automatically provide heart\nrate information. Consequently, exploring the potential of using this readily available modal-\nity for pain assessment is crucial. This proof of concept is significant as it could enable\ncost-effective and accessible pain assessment solutions without dependence on specialized\nmedical equipment. Additionally, our study focuses on developing compact and efficient\nmodels that maintain robust performance.\nThe experiments detailed in this chapter reveal that reducing the number of frames used\nin a video-based pipeline by a factor of four minimizes the accuracy loss, under 1.5%, while102 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncutting inference time threefold, facilitating near real-time pain assessment. Furthermore,\nour initially proposed framework, which incorporates 24million parameters and operates at\n4.3GFLOPS, demonstrated exceptionally high performance. In the subsequent experiments,\nwe showcased that heart rate data alone can be effectively used for pain assessment, achiev-\ning impressive results. This finding underscores the practical utility of data available from\nwearables. Additionally, combining video data with heart rate information yielded the high-\nest accuracy in our tests, illustrating that an integrated approach using both behavioral and\nphysiological modalities can significantly enhance performance. Additionally, we demon-\nstrated that creating one of the smallest models documented, with only 9.62million param-\neters and 2.46GFLOPS, allowed us to achieve top-tier results. This highlights that large\nmodels, commonly favored in the current era of AI, are not always necessary for effective\nperformance. However, it is important to note that extensive multi-stage pretraining across\nvarious datasets greatly aided this framework\u2019s success, which was critical in achieving such\nhigh efficiency and effectiveness.\nIn this chapter, we aimed to explore the effectiveness of compact models in achieving\nhigh performance with rapid inference times. We exclusively used heart rate data, mimicking\nthe information typically available from wearable devices, though our experiments did not\nextend to real-world conditions. Instead, they relied on the sole publicly accessible dataset\nexplicitly designed for pain assessment, including facial videos and cardiac signals collected\nin highly controlled laboratory settings. Participants were positioned facing forward un-\nder optimal lighting conditions, with physiological sensors precisely affixed. Recognizing\nthe potential challenges of applying these findings to clinical settings is essential. Issues\nlike inconsistent lighting, unforeseen facial movements, occlusions, or difficulties with sen-\nsor placement must be meticulously addressed to tailor these systems for real-world use.\nMoreover, depending solely on heart rate as a cardiac feature could be restrictive in more\ndemanding scenarios, highlighting the necessity to integrate multiple extracted features or\nuutilizeraw biosignals for comprehensive assessments.Chapter 6\nSynthetic Data: The Role of Thermal Imag-\ning\nContents\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n6.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the study published in [39]. In recent years, syn-\nthetic data generation has gained traction as a viable approach to addressing data scarcity\nand privacy issues while also meeting the requirements for training AI algorithms on unbi-\nased data with adequate sample size and statistical robustness. Additionally, synthetic data\ncan increase the availability and diversity of real data, particularly in rare modalities, which\ncan be essential for training AI-driven diagnostic and predictive models. This enhance-\nment supports healthcare research and improves patient outcomes. [298]. In the literature\non automatic pain assessment, no studies have been reported concerning creating synthetic\nmodalities. This chapter introduces the generation process of synthetic thermal videos using\nGenerative Adversarial Networks (GANs).\nRegarding thermal modality, in recent years, the field of affective computing research\nhas increasingly adopted thermal imaging techniques [299]. This shift was motivated by\n103104 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nstudies showing that stress and cognitive load significantly affect skin temperature [300], at-\ntributable to the autonomic nervous system\u2019s (ANS) regulation of physiological signals such\nas heart rate, respiration rate, blood perfusion, and body temperature. These signals are vital\nindicators of human emotions and affect [299]. Moreover, muscle contractions can alter fa-\ncial temperature by transferring heat to the facial skin [301]. Consequently, thermal imaging\nhas emerged as a viable method for recording transient facial temperatures [302]. Research\nby the authors in [303] on thermal imaging and facial action units to evaluate emotions, such\nas frustration, boredom, and enjoyment, indicated that a multimodal approach yielded the\nmost accurate results. Within pain research, thermal imaging has been explored in limited\nstudies. For instance, [304] reported increased facial temperature in response to painful stim-\nuli, suggesting thermal cameras as effective tools for monitoring pain. Another study [305]\nintroduced a pain dataset consisting of RGB, thermal, and depth videos, finding that while\nthe RGB modality slightly outperformed the others, integrating all modalities provided the\nbest results. This prompted us to explore the specific modality of thermal imagery through\nthe prism of synthesis using generative deep learning.\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks\nWe present a process of creating synthetic thermal videos using GANs in unimodal and\nmultimodal settings alongside RGB video modalities. Integrating a Vision Multilayer Per-\nceptron (MLP) model with a transformer-based module is at the core of our automatic pain\nassessment framework. Key contributions of this research include (1) generating synthetic\nthermal videos to enhance pain assessment as an additional vision modality, (2) assessing\nthe effectiveness of RGB and synthetic thermal videos as standalone modalities, (3) exam-\nining the utility of thermal-related information for pain assessment, and (4) evaluating the\nperformance and implications of the newly developed Vision-MLP architectures.\n6.2.1 Methodology\nThis section outlines the generation of synthetic thermal videos, which will be utilized sub-\nsequently and incorporated into an automatic assessment pipeline.\nSynthetic Thermal Videos\nAn image-to-image translation (I2I) approach has been utilized to create synthetic thermal\nvideos. I2I generative models are designed to bridge different image domains by learning the\ndata distributions inherent to each domain. Here, the source domain comprises RGB images;\nthe target domain is thermal images. In this research, conditional generative adversarial\nnetworks (cGANs) [306] were employed and trained in a supervised manner using paired6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 105\nimages. Fig. 6.1 provides a high-level overview of the method. The generator Gproduces\nimages that appear realistic, whereas the discriminator Dworks to differentiate between\ngenuine and synthetic images through the following minimax game:\nmin\nGmax\nDLcGANpG, Dq, (6.1)\nwhere the objective function LcGANpG, Dqis defined as:\nEx,yrlogDpx, yqs`Ex,zrlogp1\u00b4Dpx, Gpx, zqqqs, (6.2)\nwithxdenoting the actual data, yindicating the target data, and zrepresenting the random\nnoise vector. Here, Gseeks to minimize the objective function, whereas Doperates in\nopposition, striving to maximize it. Additionally, we incorporated the Wasserstein gradient\npenalty (WGAN-GP) [307] to enhance the stability of training. The overall objective is\narticulated as:\nLcGANpG, Dq`\u03bbE\u02c6x,yrp}\u2207\u02c6xDp\u02c6x, yq}2\u00b41q2s, (6.3)\nwhere \u03bbis the penalty coefficient. In the architectural design of our proposed method, which\ndraws inspiration from [308], the generator Gis divided into three distinct modules: an en-\ncoder, which includes two convolutional layers that downsample the input; a middle ResNet\nmodule, featuring nine residual blocks, each equipped with two convolutional layers; and a\ndecoder that upsamples the feature maps back to the final resolution ( i.e.,256\u02c6256) for the\nsynthetic sample. The discriminator D, based on the approach outlined in [309], employs a\npixel-level PatchGAN strategy using 1\u02c61kernels and consists of two convolutional layers.\n6.3 Combination of RGB and Synthetic Thermal Videos\n6.3.1 Methodology\nThis section presents the structure of the proposed automatic pain assessment framework,\nthe augmentation techniques developed, the pre-processing methods employed, and the pre-\ntraining strategy for the modules.\nFramework Architecture\nThe proposed framework consists of two primary modules: a Vision-MLP model that acts\nas a spatial embedding extractor for individual video frames and a transformer-based model\nthat functions as a temporal module, using the embedded representations of the videos for\ntemporal analysis and final pain assessment. Fig. 6.2 displays the modules and their main\ncomponents.106 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nDecoder\n\u2026Encoder\n\u2026Residual blocks\nDiscriminator\nGenerator\nAuthentic/Synthetic?\nFigure 6.1: Illustration of the procedure for creating thermal images, featuring the architecture\nof the Generator G(Encoder, mid-stage ResNet, Decoder), and the Discriminator\nD.\nVision-MLP: MLP-like models have recently emerged as a novel class of vision models,\nproviding an alternative to traditional Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViT). These models are characterized by their straightforward architectures,\nwhich consist of fully connected layers combined with activation functions. They possess\na lower level of inductive bias and rely on basic matrix multiplication operations. Our\nmethodology is grounded in the principles outlined in [310] that introduced the Vision-\nMLP, and [311] that incorporates a wave representation for the patches (also known as\ntokens). Each video frame is initially partitioned into nnon-overlapping tokens Fm\u201c\nrfm,1, fm,2, . . . , f m,nsPRn\u02c6p\u02c6p\u02c63, where pspecifies the resolution of each token, i.e.,16\u02c616\npixels, and 3represents the number of color channels. Each token is subsequently linearly\nprojected into a dimension d\u201c768prior to entering the Vision-MLP (refer to Fig. 2a). The\nfirst principal sub-module, the Channel-Mixer (Fig. 2c), operates independently on each\ntoken fj, enabling interactions among different channels, and is formulated as:\nChannel-Mixer pfj, Wcq\u201cWcfj (6.4)\nwhere Wcdenotes the weight matrix with learnable parameters, and j\u201c1,2, . . . , n . Fol-\nlowing this, the next significant sub-module, the Token-Mixer (Fig. 2b), facilitates commu-\nnication among various tokens, aiding in the extraction of features from different spatial6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 107\nVision-MLPLinear ProjectionToken MixerChannel Mixer\nNormNorm2x\n3xStage-1Token MixerChannel Mixer\nNormNorm4xStage-2Token MixerChannel Mixer\nNormNorm18xStage-3Token MixerChannel Mixer\nNormNorm 3xStage-4\nInput Imagea\nChannel-MixerMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLPChannelsTokensSkip-connectionscMLP\nFully-connectedGELUFully-connectedd\nChannel-MixerWave-BlockWave-Block\nToken-MixerTokensChannels\n......FCNb\nTransformer\n\u2026\nVision-MLP\nWeighted Fusion\nPain Assessment\nCross-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN1x\n8x\n8x4x\n8xe\nFigure 6.2: Representation of the proposed framework, illustrating its components and their\nmain functions: (a)The Vision-MLP module, tasked with extracting feature em-\nbeddings from video frames. (b)The Token-Mixer , an important sub-module of\nVision-MLP , generates the wave representation for the tokens. (c)The Channel-\nMixer , a crucial sub-module within Vision-MLP .(d)The MLP, a core component\nof the Channel-Mixer .(e)The fusion procedure that combines RGB and synthetic\nthermal embeddings, succeeded by the Transformer module, which conducts the\nfinal pain assessment.\nlocations. Typically, in MLP-based models, the token mixers are defined as:\nToken-MixerpF, Wtqj\u201c\u00ff\nkWt\njkdfk, (6.5)\nwhere Wtis the corresponding weight matrix for the tokens, and drepresents element-\nwise multiplication. Our proposed approach modifies tokens into wave-like representations\nto dynamically adjust the interactions between tokens and weights based on their semantic\ncontent. To depict a token fjas a wave \u02dcfjvia a wave function, both amplitude and phase\ninformation is necessary:\n\u02dcfj\u201c|fj|dei\u03b8j. (6.6)\nHere, iis the imaginary unit satisfying i2\u201c \u00b41. The term|fj|indicates the amplitude of\nthe signal, while ei\u03b8jis a periodic function, with \u03b8jrepresenting the phase of the signal. The108 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nabsolute value operation is typically omitted and substituted with 6.4 for simplicity. Each to-\nken\u2019s phase \u03b8jreflects its position within the wave cycle and can, therefore, be characterized\nusing fixed parameters, which are adjustable during the training phase. As such, 6.4 is also\napplied for phase estimation. Given that 6.6 characterizes a wave within the complex domain,\nthe Euler formula is employed to embed tokens within the neural network architecture:\n\u02dcfj\u201c|fj|dcos\u03b8j`i|fj|dsin\u03b8j. (6.7)\nCombining 6.5 and 6.7, a token is represented as:\nfj\u201c\u00ff\nWt\njkfkdcos\u03b8k`Wi\njkfkdsin\u03b8k (6.8)\n\u00f9\u00f1\u00ff\nWt\njkfkdcospWcfkq`Wi\njkfkdsinpWcfkq (6.9)\nwhere Wt,Wc, and Wiare the learnable weight matrices. The described process focuses\non wave-like representations and is conducted within the Token-Mixer , particularly in the\nWave-Block . The Token-Mixer architecture consists of three blocks: two Wave-Blocks and\noneChannel-Mixer operating in parallel. The Vision-MLP module is organized into four\nstages, each featuring a sequence of a Token-Mixer and a Channel-Mixer block, preceded\nby a normalization layer. The depth of parallel blocks in each stage is 3,4,18, and 3, re-\nspectively, allowing for hierarchical embeddings extraction with corresponding dimensions\nacross stages 64,128,320, and 100.\nFusion: For each input frame, the Vision-MLP extracts an embedding of dimensionality d\u201c\n100. The embeddings from the individual frames of a specific video are then concatenated\nto form a comprehensive embedding representation of the video:\nVD\u201crd1}d2}\u00a8\u00a8\u00a8} dms,VDPRN, (6.10)\nwhere mrepresents the number of frames in the video, and Nis the dimensionality of the\nultimate embedding. Following this, the embeddings from the RGB and synthetic thermal\nvideos are combined through a weighted fusion process:\nVFused\u201cw1\u00a8VRGB`w2\u00a8VThermal ,VFusedPRN. (6.11)\nThis fusion strategy integrates the respective embeddings using learned weights w1andw2,\nwhich adjust the influence of the RGB and thermal embeddings, respectively. This weighted\nsummation achieves an effective integration, capturing the relevance of each modality in the\nresultant fused representation VFused .6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 109\nTransformer: The fused embeddings are then input into a transformer-based module that\nincludes self-attention and cross-attention blocks (Fig. 2e). The self-attention mechanism is\nexpressed as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dk\u02d9\nV. (6.12)\nIn this formulation, QPRM\u02c6C,KPRM\u02c6C, and VPRM\u02c6Care the Query, Key, and Value\nmatrices respectively, where Mindicates the input dimension, and Crepresents the channel\ndimension. The cross-attention mechanism also utilizes a dot product operation; however,\nQfor cross-attention is dimensioned N\u02c6Crather than M\u02c6C, with N\u0103Mproviding\na reduction in computational cost. Each self-attention and cross-attention block features 1\nand8attention heads, respectively, and the entire Transformer module consists of 4parallel\nblocks. The output embeddings, with a dimensionality of 340, are used to perform the final\npain assessment via a fully connected neural network.\nAugmentation Methods: Two augmentation techniques have been implemented within the\nframework. Firstly, the method known as Basic is utilized, which combines polarity in-\nversion with the addition of noise. This approach modifies the original input embedding by\ninverting the polarity of data elements and adding random noise from a Gaussian distribution,\nthus introducing variability and perturbations. Secondly, the Masking technique involves ap-\nplying zero-valued masks to the embeddings, effectively eliminating sections of the vectors.\nThe dimensions of these masks are randomly determined, ranging from 10% to 50% of the\nembedding\u2019s total dimensions, and are randomly positioned within the embeddings.\nPre-processing: The pre-processing involves face detection to isolate the facial region. The\nMTCNN face detector [278] is used, which employs multitask cascaded convolutional neural\nnetworks to identify faces and landmarks. It is essential to highlight that the face detector\nwas applied exclusively to the individual RGB frames, and the coordinates of the detected\nface were then applied to the corresponding synthetic thermal frames. The resolution of all\nframes was standardized at 224\u02c6224pixels.\nPre-training: For the I2I approach, the SpeakingFaces dataset [312] was employed to train\nthe proposed GAN model for translating RGB to synthetic thermal videos. Additionally, be-\nfore commencing the automatic pain assessment training, the Vision-MLP andTransformer\nmodules underwent pre-training. The Vision-MLP was subject to a three-stage pre-training\nstrategy: initially, it was trained on DigiFace-1M [313] to acquire basic facial features. It\nwas then trained on AffectNet [287] and RAF Face Database basic [289] to learn features\nrelated to basic emotions through multi-task learning. Lastly, the Compound Facial Expres-\nsions of Emotions Database [288] and the RAF Face Database compound [289] were used110 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.1: Datasets utilized for the pretraining process of the framework.\nDataset # samples # classes Task\nSpeakingFaces [312] 4.58M 142 Face\u0015\nDigiFace-1M [313] 1.00M 10,000 Face\u0010\nAffectNet [287] 0.40M 8 Emotion\u0010\nCompound FEE-DB [288] 6,000 26 Emotion\u0010\nRAF-DB basic [289] 15,000 7 Emotion\u0010\nRAF-DB compound [289] 4,000 11 Emotion\u0010\n\u0015: includes face image pairs for the I2I task \u0010: includes images for face or emotion\nrecognition tasks\nto learn features of compound emotions in a similar multi-task framework. The multi-task\nlearning process is represented as:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s, (6.13)\nwhere LSis the loss for the specific task associated with different datasets, and ware the\nlearned weights that guide the learning process in minimizing the collective loss Ltotal, in-\ntegrating all the individual losses. The Transformer was pre-trained solely on the DigiFace-\n1M[313], adapting the input images into 1D vectors to suit its architectural needs. Table 6.1\noutlines the datasets utilized in the pre-training phase.\n6.3.2 Experiments\nThe proposed framework was assessed using the BioVid Heat Pain Database [109], which\ncomprises facial videos, electrocardiograms, electromyograms, and skin conductance levels\nfrom 87healthy subjects. Participants underwent heat-induced pain via a thermode on their\nright arm at five different intensities: no pain (NP), mild pain (P 1), moderate pain (P 2), se-\nvere pain (P 3), and very severe pain (P 4). Each level was applied 20times to each subject,\nresulting in 100samples per modality and a total of 1740 samples per class. Experiments\nwere structured around binary and multi-level classification schemes to assess pain, analyz-\ning each modality individually and collectively. Binary classification aimed to distinguish\nbetween no pain (NP) and very severe pain (P 4), while multi-level classification (MC) was\ntasked with categorizing all levels of pain intensity present in the dataset. The leave-one-\nsubject-out (LOSO) method was utilized for validation, and accuracy served as the metric\nfor performance evaluation. Table 6.2 outlines the training details for the automatic pain\nassessment, including parameter number and the computational cost measured in floating-\npoint operations (FLOPS) for each module.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 111\nTable 6.2: Training specifications, and number of parameters and FLOPS for each module.\nTraining Details Vision-MLP Transformer\nOptimizer: AdamW Params: 7.35 M Params: 7.96 M\nLearning rate: 2e-5 FLOPS: 30.95 G FLOPS: 30.90 G\nLR decay: cosine\nWeight decay: 0.1\nWarmup epochs: 5\nBatch size: 32\nTotal Params: 15.31 Millions FLOPS: 61.85 Giga\nTable 6.3: Results utilizing the RGB video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.37 30.23\n200 \u2713 20-50 0.7 70.26 28.50\n300 \u2713 30-50 0.9 70.05 30.02\nMasking: indicates the percentage of the input embedding to which zero-value masking is applied\nP(Aug): represents the probability of applying the augmentation methods of Basic & Masking NP: No\nPain P 4: Very Severe Pain MC: multiclass pain level\n6.3.3 Results\nRGB Videos\nWithin the context of the RGB video modality, we recorded an accuracy of 69.37% for the\nbinary classification task (NP vs. P 4) and 30.23% for the multi-class classification (MC).\nThe use of the Masking augmentation method, which obscured 20\u00b450% of the input em-\nbeddings, yielded a slight increase in binary classification accuracy by 0.89%. However, it\nled to a reduction in multi-class classification accuracy. By extending the training period to\n300epochs, modifying the Masking method to cover 30\u00b450% of the embeddings, and ap-\nplying a 90% probability to both augmentation methods, the accuracies improved to 70.05%\nand30.02% for the binary and multi-class tasks, respectively. This represents an average\naccuracy gain of just under 0.5%. The classification outcomes are detailed in Table 6.3.\nSynthetic Thermal Videos\nIn the synthetic thermal modality experiments conducted under identical conditions, the ini-\ntial accuracies were 69.97% for the binary classification and 30.04% for the multi-class clas-\nsification. Enhancing the intensity of the masking method yielded modest gains in accuracy112 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.4: Results utilizing the synthetic thermal video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.97 30.04\n200 \u2713 20-50 0.7 70.20 30.50\n300 \u2713 30-50 0.9 70.69 29.60\nof0.23% for the binary classification and 0.46% for the multi-class classification. Subse-\nquent final accuracies were 70.69% for the binary classification and 29.60% for the multi-\nclass classification, reflecting an average increment of 0.28%. This discrepancy likely arises\nfrom the challenge of detecting nuanced facial changes linked to low-level pain, exacerbated\nby more aggressive augmentation that potentially reduces performance. The summarized\nresults are presented in Table 6.4.\nAdditional Analysis on RGB & Synthetic Thermal Videos\nThe results from the previous section showed a surprising similarity in performance between\nthe RGB and synthetic thermal modalities. Specifically, the RGB modality achieved maxi-\nmum accuracies of 70.26% for the NP vs. P4 task and 30.23% for the MC task. Similarly,\nthe synthetic thermal modality reached top accuracies of 70.69% and30.50% for the same\ntasks, respectively. On average, the thermal video performances were about 1%higher than\nthose for the RGB modality. This was unexpected, considering the synthetic modality was\ninitially considered inferior to the original. This prompted further investigation into why\nsynthetic modalities might perform comparably to or better than the original RGB modality.\nA key question involved the relevance and efficacy of the thermal-related data featured in the\nsynthetic videos. The theory proposed that minimizing facial expressions in thermal videos\ncould enhance the clarity of thermal data assessment.\nGaussian blurring was incrementally applied to RGB and synthetic thermal videos, as\nshown in Fig. 6.3, with kernel sizes increasing from 0to191. According to Table 6.5, at\na kernel size of k\u201c0, a performance differential of 0.47%, favoring the thermal modality,\nconfirms prior findings. This gap slightly expands to 0.49% atk\u201c41. Remarkably, at\nk\u201c91, the disparity enlarges to 2.13% and increases to 5.90% atk\u201c191, where blurring\nis most intense. The results reveal that as facial expressions become less visible through\nblurring, synthetic thermal videos consistently outperform RGB videos, with respective ac-\ncuracies of 66.24% versus 60.34%. As blurring intensifies from k\u201c0tok\u201c191, accuracy\nrates for synthetic thermal and RGB modalities decrease by 1.81% and7.13%, respectively.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 113\nFigure 6.3: Gradual blurring of RGB and synthetic thermal facial images: a series displaying\nvarying levels of Gaussian blur applied, with kernel sizes gradually increased from\nk\u201c0(no blur) to k\u201c191(extensively blurred).\nTable 6.5: Results utilizing the RGB & the synthetic thermal video.\nEpochs Modality BlurAugmentations Task\nBasic Masking P(Aug) NP vs P 4\n100 RGB 0 \u2713 10-20 0.7 67.47\n100 Thermal 0 \u2713 10-20 0.7 68.05\n100 RGB 41 \u2713 10-20 0.7 66.61\n100 Thermal 41 \u2713 10-20 0.7 67.10\n100 RGB 91 \u2713 10-20 0.7 64.80\n100 Thermal 91 \u2713 10-20 0.7 66.93\n100 RGB 191 \u2713 10-20 0.7 60.34\n100 Thermal 191 \u2713 10-20 0.7 66.24\nBlur: Gaussian blurring with kernel sizes k\nThis suggests that critical information, such as visually represented facial temperature in\nthe synthetic modality, is unaffected or slightly impacted. Fig. 6.4 displays the embedding\ndistribution for the RGB and synthetic thermal modalities at k\u201c0andk\u201c191, highlight-\ning a distinct variation in distribution patterns, albeit with ambiguous data point separation.\nFork\u201c191, while RGB embeddings tend to cluster and potentially overlap, many points\nconspicuously stray from the central mass without any distinct arrangement. Conversely, the\ndata points for the synthetic modality spread more uniformly, possibly indicating better class\ndifferentiation.114 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nRGB-0\nThermal-0 Thermal-191RGB-191\nNP\nP4\nFigure 6.4: Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain) classes\nin RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191(heavily\nblurred).\nFusion\nThree fusion strategies were assessed for multimodal analysis involving RGB and synthetic\nthermal video data. Initially, the fusion strategy referenced in 6.11 utilized learned weights\nw1andw2to scale the contributions of each modality. A second strategy incorporated an\nadditional weight, w3, modifying the formula to w3\u00a8pw1\u00a8VRGB`w2\u00a8VThermalq. The\nthird method bypassed learned weights altogether, directly combining the embedding vectors\nfrom the modalities. The results, detailed in Table 6.6, indicate that omitting learned weights\nachieved accuracies of 64.92% and26.40% for the binary and multi-class tasks, respectively.\nThe introduction of w3reduced 0.5%in accuracy for both tasks. The strategy using only\nweights w1andw2yielded the best outcomes, with accuracies of 65.08% and26.50% for\nthe binary and multi-class tasks, respectively. By maintaining the use of weights w1and\nw2and increasing the training duration from 100to300epochs, while consistent with the\naugmentation settings, accuracies improved to 69.50% and29.80% for the binary and multi-\nclass tasks, respectively. Further prolonging the training to 500epochs, with no evidence\nof overfitting, led to further improved performances, with final accuracies of 71.03% and\n30.70% for the respective tasks.6.4. SUMMARY 115\nTable 6.6: Results utilizing the fusion of RGB & synthetic thermal video.\nEpochsFusion\nweightsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n100 \u2013 \u2713 10-20 0.7 64.92 26.40\n100 W2 \u2713 10-20 0.7 65.08 26.50\n100 W3 \u2713 10-20 0.7 64.42 25.90\n300 W2 \u2713 10-20 0.7 69.50 29.80\n500 W2 \u2713 10-20 0.7 71.03 30.70\nW2: utilization of [w 1,w2] W3: utilization of [w 1,w2,w3]\nComparison with Existing Methods\nThis section compares the proposed method with other methodologies reported in the liter-\nature. We utilize Part A of the BioVid dataset, which includes all 87subjects, employing\nthe LOSO cross-validation protocol for validation. The results are displayed in Table 6.7.\nOur vision-based approach, which leverages RGB and synthetic thermal modalities, shows\nperformances comparable to or surpass those of prior studies. Relative to results in the lit-\nerature from [180, 217, 219, 295], our method achieved higher accuracy in both binary and\nmulti-level tasks. Notably, the research in [296] recorded accuracies of 72.40% and30.80%,\nmarking an improvement of 1.37% and0.10% over our method, respectively. The highest\nreported results are from [37], which utilized a transformer-based architecture.\nAdditionally, Table 6.8 compares our results with those from [305], where the authors\nintroduced the MIntPAIN dataset comprising both RGB and thermal videos for automatic\npain assessment across five intensity levels. Our analysis revealed that the accuracies of\nthe RGB and thermal modalities were closely matched at 18.55% and18.33%, respectively,\nwhich parallels our observations of similar performance between RGB and synthetic ther-\nmal modalities. By integrating these modalities, the authors in [305] reported a significant\nperformance increase of 30.77%, surpassing our modest gains. It should be emphasized that\nin [305], the performance levels of the individual modalities were below the random guess\nprediction threshold of 20%. It was only through their combination that performance was\nelevated above this threshold.\n6.4 Summary\nThis chapter investigated the creation of synthetic thermal imagery via GAN models to as-\nsess its utility in automatic pain evaluation. Additionally, a novel framework incorporating\naVision-MLP and supported by a Transformer module as the core of the assessment sys-\ntem was introduced. The experiments demonstrated the effectiveness of the synthetic ther-116 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.7: Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.\nStudy MethodTask\nNP vs P 4 MC\nWerner et al. [296] Deep RF 72.40 30.80\nWerner et al. [295] RF 70.20 \u2013\nZhiet al. [217] SLSTM 61.70 29.70\nThiam et al. [219] 2D CNN, biLSTM 69.25 \u2013\nTavakolian et al. [180] 2D CNN 71.00 \u2013\nGkikas et al. [37] Vision-Transformer 73.28 31.52\nOur Vision-MLP 71.03 30.70\nTable 6.8: Comparison with the MIntPAIN dataset.\nStudy Dataset ModalityTask\nMC\nHaque et al. [305] MIntPAINRGB 18.55\nThermal\u02dd18.33\nFusion 30.77\nOur BioVidRGB 30.02\nThermal\u203929.69\nFusion 30.70\n\u02dd:real\u2039: synthetic\nmal modality, which achieved performances comparable to or better than the original RGB\nmodality. This research also delved into the factors contributing to this effectiveness, par-\nticularly the role of temperature color representations in the analysis. Furthermore, various\nfusion techniques were used to evaluate the combination of the two vision modalities, high-\nlighting the potential for performance improvements over single-modality approaches. It\nis important to note that further enhancements and experimental work, especially with the\nmultimodal approach, could improve outcomes. The generation and integration of synthetic\nmodalities, such as thermal imagery, within an automatic pain assessment framework exhibit\nconsiderable promise, warranting additional exploration and research.Chapter 7\nGeneral-Purpose Models\nContents\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n7.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n7.1 Chapter Overview: Introduction & Related Work\nThis chapter discusses the approaches and findings from [40] and [41]. In recent years,\nthe literature on deep learning has shown a trend towards adopting general-purpose models.\nThese models are characterized by architectures not tied to specific modalities or pre-training\non datasets derived solely from a single domain. We will explore two approaches: first, a\nmodality-agnostic pipeline for automatic pain assessment, and second, the development of a\nfoundation model explicitly applied for automatic pain assessment tasks.\nRegarding modality-agnostic approaches, in 5.3, we introduced the concept specifically\nfor augmentations. These augmentations were designed not directly for the image or biosig-\nnal space but for the latent space. Thus, regardless of the original input modality, whether\n117118 CHAPTER 7. GENERAL-PURPOSE MODELS\nimages, biosignals, or others, the augmentations were applied to their feature representa-\ntions. This chapter advances our work by developing a modality-agnostic multimodal fusion\npipeline evaluated in pain assessment tasks. The literature on modality-agnostic approaches\nremains limited. In [314], the researchers pursued a novel approach by exploring modality-\nagnostic representations through knowledge distillation for semantic segmentation. Their\ngoal was to reduce the modality gap and diminish semantic ambiguity, enabling the com-\nbination of various modalities under any visual conditions. In [315], the authors addressed\nthe persistent challenges of temporal asynchrony and modality heterogeneity in multimodal\nsequence fusion, often leading to performance bottlenecks. To overcome these issues, they\ndeveloped a strategy integrating modality-exclusive and modality-agnostic representations\nfor multimodal ideo sequence fusion. This approach enabled them to capture reliable con-\ntext dynamics within individual modalities and enhance distinctive features across modality-\nexclusive spaces. Additionally, they designed a hierarchical cross-modal attention module\nto uncover significant element correlations across different modalities within the modality-\nagnostic space.\nThe literature on foundation models is considerably more extensive. With the emerging\nparadigm of building AI systems around foundation models, there has been a shift toward\ncreating more adaptable and scalable systems that generalize across various tasks and do-\nmains. A foundation model is defined as any model trained on vast datasets, often through\nextensive self-supervision, which can then be adapted\u2014such as through fine-tuning\u2014to a\nwide range of downstream tasks. Despite their reliance on traditional deep learning and\ntransfer learning techniques, the extensive scale of foundation models fosters the develop-\nment of new capabilities and enhances effectiveness across many tasks [316]. Numerous\nexamples have surfaced recently in academic literature. For instance, the SAM model [317],\na foundation model for image segmentation, was initially trained from scratch on 11million\nimages. In later studies [318, 319], researchers have adapted SAM for medical imaging by\noptimizing it for smaller, specialized datasets. Additionally, a notable paradigm shift has oc-\ncurred with the introduction of generalist models [320], a novel class of foundation models\ntrained simultaneously on various tasks under a unified learning strategy, typically super-\nvised. This approach is particularly advantageous in computer vision, enabling handling\ndiffering embedding representations across tasks and various visual modalities [321].\nTo the best of our knowledge, there are no modality-agnostic or foundation models in\nthe field of automatic pain assessment. Currently, the majority of the approaches utilize\npre-trained models; however, these models typically adhere to the traditional methodology\nof pre-training on a general, large-scale dataset and then fine-tuning for the specific task of\npain assessment. Studies such as those detailed in [305, 322] rely on transfer learning tech-\nniques derived from facial recognition datasets. The majority of these studies are reviewed\nin Chapter 3.7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 119\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment\nWe present a modality-agnostic multimodal framework that leverages both video and fNIRS\ndata. The pipeline operates on a dual Vision Transformer (ViT) format, which obviates the\nnecessity for modality-specific architectures or expansive feature engineering. It interprets\nthe inputs as unified images through 2D waveform representations.\n7.2.1 Methodology\nThis section outlines the pipeline for the proposed multi-modal automatic pain assessment\nframework, describing the model architectures, pre-processing methods, pre-training strate-\ngies, and augmentation techniques.\nFramework Architecture\nThe proposed framework, Twins-PainViT , includes two modules: PainViT\u20131 andPainViT\u20132 .\nBoth models share uniform architectures and parameters and follow to identical pre-training\nprotocol. PainViT\u20131 is tasked with processing the individual input video frames and the\nvisualized fNIRS waveforms, serving as an embedding extractor. Meanwhile, PainViT\u20132\nmanages the visual representation of these embeddings and performs the final pain classifi-\ncation task.\nPainViT: Vision Transformers (ViTs) [323] have established themselves as a leading frame-\nwork in computer vision tasks, recognized for their impressive performance. However,\ndespite their effectiveness, transformer-based models encounter scalability challenges with\nlarger input sizes, significantly increasing computational demands. This inefficiency mainly\nstems from the element-wise operations in the multi-head self-attention mechanism. Efforts\nto improve the efficiency and simplify the architecture of transformer-based models have\nincluded modifications to the self-attention module and overall structural adjustments [324]\n[325]. Our methodology builds on the principles of [326], which incorporates hierarchical\narchitectures into vision transformers, and [327], which introduces mechanisms to enhance\nefficiency and processing speed.\nPainViT\u2013block: A block consists of two key elements: the Token-Mixer and the Cascaded-\nAttention . The architecture places the Cascaded-Attention module centrally, combined with\naToken-Mixer module before and after it. For every input image I, overlapping patch em-\nbedding is utilized, generating 16\u02c616patches. Each patch is then projected into a token\nwith a dimensionality of d.120 CHAPTER 7. GENERAL-PURPOSE MODELS\nToken-Mixer: To better integrate local structural information, the token Tis processed\nthrough a depthwise convolution layer:\nYc\u201cKc\u02daTc`bc. (7.1)\nHere, Ycdenotes the output of the depthwise convolution for channel cof the token Tc.Kc\nrepresents the convolutional kernel for channel c,Tcindicates the c-th channel of the token\nT, and bcis the bias term added to the convolution output for channel c. The symbol \u02da\nindicates the convolution operation. After the depthwise convolution, batch normalization is\nthen applied to the output:\nZc\u201c\u03b3c\u02dc\nYc\u00b4\u00b5Ba\n\u03c32\nB`\u03f5\u00b8\n`\u03b2c. (7.2)\nHere, Zcrepresents the batch-normalized output for channel cof the token T. The learnable\nparameters \u03b3cand\u03b2care specific to channel c, adjusting the scale and shift of the normalized\ndata. \u00b5Bdenotes the batch mean of Yc,\u03c32\nBindicates the batch variance of Yc, and \u03f5is a\nsmall constant included for numerical stability to prevent division by zero. Subsequently, a\nfeed-forward network (FFN) is used to enhance communication between different feature\nchannels:\n\u03a6FpZcq\u201cW2\u00a8ReLUpW1\u00a8Zc`b1q`b2. (7.3)\nHere, \u03a6FpZcqrepresents the output of the feed-forward network for the input Zc. The weight\nmatrices for the first and second linear layers are denoted by W1andW2, respectively; b1\nandb2are the corresponding bias terms for these layers. The activation function employed\nhere is ReLU.\nCascaded-Attention Respecting the attention mechanism, it includes a single self-attention\nlayer. For each input embedding:\nXi`1\u201c\u03a6ApXiq. (7.4)\nTheXirefers to the entire input embedding for the i-thPainViT-block . The Cascaded-\nAttention module uses a cascaded mechanism that splits the full input embedding into smaller\nsegments, with each segment directed to a specific attention head. This method distributes\nthe computational load across the heads, improving efficiency by managing long input em-\nbeddings more effectively. The attention mechanism operates as follows:\nrXij\u201cAttnpXijWQ\nij, XijWK\nij, XijWV\nijq, (7.5)\nrXi`1\u201cConcatrrXijsj\u201c1:hWP\ni, (7.6)7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 121\nTable 7.1: Number of parameters and FLOPS for the components of the proposed Twins-PainViT .\nModule Params (M) FLOPS (G)\nPainViT\u20131 16.46 0.59\nPainViT\u20132 16.46 0.59\nTotal 32.92 1.18\nwhere each j-th head is responsible for computing the self-attention of Xi,j, thej-th segment\nof the full input embedding Xi. This embedding is organized as rXi1, Xi2, . . . , X ihs, with j\nranging from 1 to h, where his the total number of heads. The projection layers WQ\nij,WK\nij,\nandWV\nijtransform each segment of the input embedding into distinct subspaces for queries,\nkeys, and values, respectively. The process concludes with WP\ni, a linear layer that combines\nthe outputs of all heads, restoring them to the original input dimensionality. Additionally, the\ncascaded design aids in developing more complex representations for the Q,K, andVlayers.\nThis enhancement occurs as the output from each head is fed into the subsequent head,\nallowing for a progressive accumulation of information throughout the layers. Specifically:\nX1\nij\u201cXij`rXipj\u00b41q. (7.7)\nHere, X1\nijdenotes the sum of the j-th input segment Xijand the output \u02dcXipj\u00b41qfrom thepj\u00b4\n1q-th head. This summation is the new input embedding for the j-th head in the self-attention\ncomputation. Additionally, depthwise convolution is applied to each Qin every attention\nhead, enhancing the self-attention process\u2019s ability to capture both global representations\nand local details.\nThe architecture consists of three PainViT\u2013blocks with depths of 1,3, and 4. This hier-\narchical design progressively reduces the number of tokens by subsampling the resolution\nby2\u02c6at each stage, enabling the extraction of embeddings with dimensions dacross the\nblocks, particularly 192,288, and 500. Each block also features a multihead self-attention\nmechanism, employing 3,3, and 4heads, respectively. Fig. 1(a-d) depicts the PainViT archi-\ntecture and its core components, while Table 7.1 details the number of parameters and the\ncomputational costs measured in floating-point operations (FLOPS).\nEmbedding extraction & Fusion\nFor every frame in a video, V\u201crv1, v2, . . . , v ns,PainViT\u20131 generates a corresponding em-\nbedding. These embeddings are combined to produce a composite feature representation of\nthe video. Similarly, for each fNIRS signal channel, C\u201c rc1, c2, . . . , c ms,PainViT\u20131 ex-\ntracts embeddings which are then compiled to form a complete representation of the fNIRS122 CHAPTER 7. GENERAL-PURPOSE MODELS\nsignal. The following equations outline this procedure:\nEV\u00d0n\u00ff\ni\u201c1PainViT\u20131pviq, (7.8)\nEC\u00d0m\u00ff\ni\u201c1PainViT\u20131pciq, (7.9)\nwhere EVandECrepresent the embedding representations for the video and fNIRS, re-\nspectively. After these embeddings are extracted, EVandECare visualized as waveform\ndiagrams. The waveforms from both modalities\u2014video and fNIRS\u2014are then combined into\na single image with a resolution of 224\u02c6224. This integrated visual representation is input\ninto PainViT\u20132 for final pain assessment. (Fig. 1e) provides a high-level overview of the\nmultimodal proposed pipeline.\nPre-processing\nThe preprocessing steps include face detection in video frames and generating waveform\ndiagrams from the original fNIRS data. The MTCNN face detector [278], which uses a\nseries of cascaded convolutional neural networks, was employed to identify faces and facial\nlandmarks with the faces set at a resolution of 224\u02c6224pixels. All fNIRS channels are used\nto create waveform diagrams, which visually represent the signal\u2019s wave shape and form\nover time, displaying amplitude, frequency, and phase. This method offers a straightforward\napproach to visualizing a signal without requiring transformations or complex computations\ntypical of spectrograms, scalograms, or recurrence plots. Similarly, embeddings extracted\nbyPainViT\u20131 are visualized using waveform diagrams. Although these embeddings are not\nsignals per se, the 1D vectors can still be plotted in a 2D space for further analysis or use by\ndeep-learning vision models. Waveform diagrams created from fNIRS data and embeddings\nare formatted as images with a 224\u02c6224pixels resolution. Fig. 7.2 shows waveform\nrepresentations of channel-specific fNIRS signals, an embedding extracted from a video,\nand an embedding derived from a channel-specific fNIRS sample.\nPre-training\nBefore the automatic pain assessment training, the Twins-PainViT models underwent pre-\ntraining using a multi-task learning approach. This pre-training incorporated four datasets de-\nsigned for emotion recognition tasks. The AffectNet [287] and RAF-DB basic [289] datasets\nsupplied facial images to train on basic emotions, whereas the Compound FEE-DB [288]\nandRAF-DB compound [289] datasets focused on complex emotional states. In addition,\nfive datasets containing various biosignals were utilized. The EEG-BST-SZ [328] dataset in-7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 123\nTable 7.2: Datasets utilized for the pretraining process of the framework.\nDataset # samples # classes Modality\nAffectNet [287] 0.40M 8 Facial Images\nRAF-DB basic [289] 15,000 7 Facial Images\nRAF-DB compound [289] 4,000 11 Facial Images\nCompound FEE-DB [288] 6,000 26 Facial Images\nEEG-BST-SZ [328] 1.5M 2 EEG\nSilent-EMG [329] 190,816 8 EMG\nBioVid [109] 8,700 5 ECG\nBioVid [109] 8,700 5 EMG\nBioVid [109] 8,700 5 GSR\nEEG: electroencephalogram EMG: electromyogram ECG: electrocardiogram GSR: galvanic\nskin response\ncludes electroencephalograms for schizophrenia analysis, and the Silent-EMG [329] contains\nelectromyograms that help pinpoint the origin of EMG activities, such as from the throat or\nmid-jaw areas. The BioVid [109] dataset provided electrocardiogram, electromyogram, and\ngalvanic skin response samples for pain assessment. All biosignals were represented as\nwaveforms, as detailed in 7.2.1. The equation defines the multi-task learning framework:\nLtotal\u201c9\u00ff\ni\u201c1rewiLSi`wis, (7.10)\nwhere LSirepresents the loss for each specific task from the various datasets, and wiare\nthe learned weights that drive the learning process to minimize the combined loss Ltotal,\nconsidering all individual losses. Table 7.2 outlines the datasets involved in the pre-training\nphase.\nAugmentation Methods & Regularization\nSeveral augmentation methods have been employed to train the proposed framework. For the\npre-training process, RandAugment [330] and TrivialAugment [282] were used, along with\nauxiliary noise from a uniform distribution and a custom MaskOut technique that masks\nout random square sections of input images. In the automatic pain assessment task, Aug-\nMix[281] is used in addition to the previously mentioned methods. Moreover, Label Smooth-\ning[331] and DropOut [332] are implemented as regularization techniques to optimize the\ntraining outcome.124 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.3: Training details for the automatic pain assessment.\nOptimizer LR LR\ndecayWeight\ndecayEpochs Warmup\nepochsCooldown\nepochsBatch\nsize\nAdamW 2e-5 cosine 0.1 100 10 10 32\nLR: learning rate\n7.2.2", "Experiments": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3", "Results": ". . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research\nDirections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4 Demographic Variables: Their Role and Impact 53\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4", "Discussion": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4", "ECG Analysis with Multitask Neural Networks": ". . . . . . . . . . . . . . . 64\n4.3.1", "Summary": "of Automatic Pain Assessment Methods . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . 49\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research\nDirections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4 Demographic Variables: Their Role and Impact 53\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n5 Optimization: Balancing Efficiency and Performance 75\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n6 Synthetic Data: The Role of Thermal Imaging 103\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7 General-Purpose Models 117\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n8", "Optimization: Balancing Efficiency and Performance": "75\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n6 Synthetic Data: The Role of Thermal Imaging 103\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7 General-Purpose Models 117\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n8 Conclusions, Perspectives and Future Work 157\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nBibliography 163\nAppendix 201\nAcronyms 209List of Figures\n2.1 The spinothalamic tract (STT) [43]. Pain, temperature, and some touch affer-\nents end in the posterior horn, where second-order fibers cross the midline\nto form the spinothalamic tract, ascending to the thalamus and projecting to\nvarious cortical areas. Along the way, collaterals connect to the reticular for-\nmation. Due to the rostral inclination of fibers in Lissauer\u2019s tract, cordotomy\nmust be performed several segments above the pain level for effective relief. 12\n2.2 Pain classification [48]: (A)Nociceptive pain , which results from detecting\npotentially harmful stimuli and serves a protective function. (B)Inflamma-\ntory pain is linked to tissue damage and immune cell infiltration, increas-\ning pain sensitivity during healing. (C)Pathological pain is a disease state\ncaused by either nervous system damage (neuropathic) or abnormal nervous\nsystem function (dysfunctional). . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 The number of studies utilizing these specific datasets. Note that various\nstudies used multiple datasets to conduct their experiments. . . . . . . . . . 25\n4.1 The PQRST waveform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure. . 55\n4.3 The signal preprocessing using the Pan-Tompkins algorithm. . . . . . . . . 57\n4.4 Results for the Gender Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Results for the Age Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.6 Results for the Gender-Age Scheme . . . . . . . . . . . . . . . . . . . . . . 64\n4.7 The proposed MTL network: The sizes of the extracted vectors for the net-\nwork are as follows: for the Pain classifier, n\u02c61, where nis the number of\npain estimation tasks ( e.g.,2for binary classification, 5for multi-class clas-\nsification); for the Age classifier, 36\u02c61, where 36represents the possible\nage values of the subjects; for the Gender classifier, 2\u02c61, corresponding to\nthe two possible gender categories ( i.e., males and females). . . . . . . . . 66\n4.8 Results for the proposed Schemes. . . . . . . . . . . . . . . . . . . . . . . 69\n4.9 Comparison of performances utilizing various neural networks approaches. 72\nxix5.1 The application of face alignment illustrates landmarks in 2D (left) and 3D\n(right) space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 An overview of our proposed transformer-based framework for automatic\npain assessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 The impact of the number of input frames on accuracy (left) and on runtime\nin milliseconds (right). Runtime calculated during inference on a NVIDIA\nRTX-3090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Relevance Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5 Outline of the proposed framework. . . . . . . . . . . . . . . . . . . . . . 86\n5.6 Comparison of mean accuracy and inference period for unimodal and multi-\nmodal strategies across NP versus P 4and MC tasks. The diagram adopts a\ndual-y-axis configuration\u2014accuracy measurements on the left and time met-\nrics on the right\u2014to outline the balance between performance efficacy and\ncomputational load, categorizing the methodologies along the x-axis. . . . . 98\n5.7 Regions highlighted in yellow and red denote areas of significant attention.\n(a) (1strow) Sequence of original frames. (2ndrow) Derived from the\nSpatial-Module after initial stage pretraining. (3rdrow) Derived from the\nSpatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module trained on the BioVid dataset. (b) (1strow) Derived from\ntheTemporal-Module incorporating video embeddings. (2ndrow) Derived\nfrom the Temporal-Module with heart rate embeddings. (3rdrow) Derived\nfrom the Temporal-Module using a combined embedding of video and heart\nrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.1 Illustration of the procedure for creating thermal images, featuring the archi-\ntecture of the Generator G(Encoder, mid-stage ResNet, Decoder), and the\nDiscriminator D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2 Representation of the proposed framework, illustrating its components and\ntheir main functions: (a)The Vision-MLP module, tasked with extracting\nfeature embeddings from video frames. (b)TheToken-Mixer , an important\nsub-module of Vision-MLP , generates the wave representation for the tokens.\n(c)The Channel-Mixer , a crucial sub-module within Vision-MLP .(d)The\nMLP, a core component of the Channel-Mixer .(e)The fusion procedure\nthat combines RGB and synthetic thermal embeddings, succeeded by the\nTransformer module, which conducts the final pain assessment. . . . . . . . 107\n6.3 Gradual blurring of RGB and synthetic thermal facial images: a series dis-\nplaying varying levels of Gaussian blur applied, with kernel sizes gradually\nincreased from k\u201c0(no blur) to k\u201c191(extensively blurred). . . . . . . 1136.4 Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain)\nclasses in RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191\n(heavily blurred). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1 PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer hav-\ning varying depths, showcasing how token resolution decreases at each stage;\n(b)Composition of the Token-Mixer module, featuring elements like depth-\nwise convolution (DWConv) and batch normalization; (c)Architecture of the\nFeed-Forward Network (FFN) within the Token-Mixer ;(d)The Cascaded\nAttention mechanism implemented across multiple heads, illustrating how\noutputs from preceding heads are incorporated to refine the self-attention\nprocess, culminating in the final output projection; (e)Configuration of the\nproposed multimodal pipeline, employing videos and fNIRS. The embed-\ndings from PainViT\u20131 are represented as waveform diagrams, which are\nmerged into a single diagram that illustrates both modalities before entering\nPainViT\u20132 for final pain evaluation. . . . . . . . . . . . . . . . . . . . . . . 151\n7.2 Waveform illustrations for various data types: (a)original fNIRS signal,\n(b)video embedding derived from PainViT\u20131 , and (c)fNIRS embedding\nobtained from PainViT\u20131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n7.3 Attention maps from the PainViT\u20132 . . . . . . . . . . . . . . . . . . . . . . 152\n7.4 Overview of primary models and their components outlined in this research:\n(a)PainFormer is structured hierarchically into four stages, incorporating\nSpectral andSelf-Attention Layers to extract embeddings from the inputs;\n(b)The Spectral Layer , a key element of PainFormer , uses FFT to ana-\nlyze frequency-specific data along with a learnable filter Kto highlight\ncritical frequencies; (c)The Self-Attention Layer , crucial for PainFormer ,\nenables parallel processing of features and their interconnections; (d)The\nEmbedding-Mixer , employing both cross and self-attention mechanisms, func-\ntions as the component for the final classification of embeddings in pain as-\nsessment; (e)TheVideo-Encoder , designed for compact and efficient encod-\ning, compresses video data into a reduced dimensional form; (f)TheMLP-1\nis part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)TheMLP-3 configuration is integrated into the Embedding-Mixer\nandVideo-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n7.5 Examples of different vision modalities in frame samples: (a)RGB frame,\n(b)synthetic thermal frame, and (c)depth estimation frame. . . . . . . . . 153\n7.6 Examples of different visual representations for biosignals: (a)waveform ,\n(b)spectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD . . 1547.7 An overview of the presented framework. PainFormer , the foundational\nmodel, excels in deriving high-quality embeddings from a diverse array of\nbehavioral and physiological modalities. The evaluation of RGB, thermal,\nand depth videos, alongside various representations of ECG, EMG, GSR,\nand fNIRS such as waveforms and spectrograms, underscores the rich infor-\nmation captured within these embeddings. Leveraging the embeddings from\nPainFormer facilitates the creation of various and diverse unimodal and mul-\ntimodal pipelines designed for the pain assessment task. Each pipeline can\nbe customized to suit the specific modalities involved, dataset characteristics,\nand the demands of the intended application or clinical setting. Our assess-\nments included the development and implementation of several pipelines\nin both unimodal and multimodal contexts, achieving leading-edge results\nacross various modalities and data representations. . . . . . . . . . . . . . 154\n7.8 Attention maps from the PainFormer :(a)(1strow) frames from RGB, ther-\nmal, and depth video modalities; (a)(2ndrow) corresponding attention maps;\n(b)(1strow) attention maps for ECG and EMG; (b)(2ndrow) attention maps\nfor EDA and fNIRS modalities. . . . . . . . . . . . . . . . . . . . . . . . . 155\n1 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n2 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n3 Additional attention maps from the PainViT\u20132 (refer to Section 7.2). . . . . 207List of Tables\n3.1 Most commonly utilized pain databases. . . . . . . . . . . . . . . . . . . . 24\n3.2 Vision-based studies with static analysis. . . . . . . . . . . . . . . . . . . . 32\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 33\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 34\n3.3 Vision-based studies with temporal utilization. . . . . . . . . . . . . . . . . 39\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 40\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 41\n3.4 Touch sensor-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Audio-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6 Multimodal-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.7 Interpretation approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1 Results for the Basic Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Results for the Gender Scheme (1). . . . . . . . . . . . . . . . . . . . . . . 60\n4.3 Results for the Age Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Results for the Gender-Age Scheme (Males) (1). . . . . . . . . . . . . . . . 62\n4.5 Results for the Gender-Age Scheme (Females) (1). . . . . . . . . . . . . . . 63\n4.6 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (1). 63\n4.7 Hyper-parameters used in our approach. . . . . . . . . . . . . . . . . . . . 65\n4.8 Results for the Basic Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . 68\n4.9 Results for the Gender Scheme (2). . . . . . . . . . . . . . . . . . . . . . . 68\n4.10 Results for the Age Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.11 Results for the Gender-Age Scheme (2). . . . . . . . . . . . . . . . . . . . 68\n4.12 Comparison of results adopting the feature augmentation approach. . . . . . 70\n4.13 Comparison of results adopting the MT-NN approach. . . . . . . . . . . . . 71\n4.14 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2). 72\n5.1 Training details for the automatic pain assessment. . . . . . . . . . . . . . 79\n5.2 Results on the pain estimation tasks. . . . . . . . . . . . . . . . . . . . . . 81\n5.3 Results for the pain estimation tasks using various numbers of input frames. 82\n5.4 Comparison of studies utilizing BioVid , RGB videos, and LOSO validation. 84\n5.5 Datasets utilized for the pre-training process of the framework. . . . . . . . 91\nxxiii5.6 Training details for the automatic pain assessment. . . . . . . . . . . . . . 92\n5.7 Results utilizing the video modality. . . . . . . . . . . . . . . . . . . . . . 93\n5.8 Results utilizing the heart rate modality. . . . . . . . . . . . . . . . . . . . 95\n5.9 Results utilizing the video &the heart rate modality. . . . . . . . . . . . . . 95\n5.10 Comparison of studies utilizing BioVid &LOSO validation, reported on ac-\ncuracy %. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.11 Module parameters and computational cost in FLOPS for the proposed frame-\nwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.1 Datasets utilized for the pretraining process of the framework. . . . . . . . 110\n6.2 Training specifications, and number of parameters and FLOPS for each mod-\nule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.3 Results utilizing the RGB video. . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Results utilizing the synthetic thermal video. . . . . . . . . . . . . . . . . . 112\n6.5 Results utilizing the RGB &the synthetic thermal video. . . . . . . . . . . . 113\n6.6 Results utilizing the fusion of RGB &synthetic thermal video. . . . . . . . 115\n6.7 Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.116\n6.8 Comparison with the MIntPAIN dataset. . . . . . . . . . . . . . . . . . . . 116\n7.1 Number of parameters and FLOPS for the components of the proposed Twins-\nPainViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.2 Datasets utilized for the pretraining process of the framework. . . . . . . . 123\n7.3 Training details for the automatic pain assessment. . . . . . . . . . . . . . 124\n7.4 Results utilizing the video modality & Addition method. . . . . . . . . . . . 125\n7.5 Results utilizing the video modality & Concatenation method. . . . . . . . . 125\n7.6 Results utilizing the HbR & Addition method. . . . . . . . . . . . . . . . . 126\n7.7 Results utilizing the HbR & Concatenation method. . . . . . . . . . . . . . 126\n7.8 Results utilizing the HbO & Addition method. . . . . . . . . . . . . . . . . 126\n7.9 Results utilizing the HbO & Concatenation method. . . . . . . . . . . . . . 127\n7.10 Results utilizing the HbR, HbO & Addition method. . . . . . . . . . . . . . 127\n7.11 Results utilizing the videos, HbO & Addition method. . . . . . . . . . . . . 127\n7.12 Results utilizing the videos, HbO & Single Diagram method. . . . . . . . . 128\n7.13 Comparison with the validation baseline provided by the AI4PAIN challenge\norganizers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.14 Number of parameters and FLOPS for the modules of the proposed framework.130\n7.15 Details of the PainFormer\u2019s architecture. . . . . . . . . . . . . . . . . . . . 132\n7.16 Datasets utilized for the multitask learning-based pretraining process of the\nframework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347.17 Training details of the proposed framework. . . . . . . . . . . . . . . . . . 135\n7.18 Results utilizing the video modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.19 Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.20 Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.21 Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.22 Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and\nF1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.23 Results on the validation set of AI4Pain dataset, multilevel classification task,\nreported on accuracy, recall and F1 score. . . . . . . . . . . . . . . . . . . 144\n7.24 Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.25 Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4\ntask and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.26 Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs.\nP4task and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.27 Comparison of studies on the testing set of AI4Pain dataset. . . . . . . . . . 148\n1 Results utilizing the video modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n2 Results utilizing the heart rate modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n3 Results utilizing the video &the heart rate modality reported on precision,\nrecall and F1 score (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . 202\n4 Results utilizing the RGB video modality, reported on recall and F1 score\n(refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n5 Results utilizing the synthetic thermal video modality, reported on recall and\nF1 score (refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . 203\n6 Results utilizing the fusion of RGB &synthetic thermal video modality, re-\nported on recall and F1 score (refer to Section 6.2). . . . . . . . . . . . . . 204\n7 Results of the proposed approaches, reported on macro-averaged precision,\nrecall and F1 score (refer to Section 7.2). . . . . . . . . . . . . . . . . . . . 204Chapter 1\nIntroduction\nContents\n1.1 Context and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Scope and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions \u2013 Peer-review Publications . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.1 Context and Motivation\nPain is a complex and deeply personal experience that is subjective by nature. Traditionally,\nit has been described in terms of its sensory dimension [1]. However, extensive research\nhas highlighted the importance of affective, cognitive and social aspects in shaping this ex-\nperience [2]. Studies have explored physiological, psychological, and socio-environmental\nfactors that contribute to the experience of pain. It is understood as a result of biological evo-\nlution and as influenced by psychological and social factors. As Ridell et al. [3] noted, \u201cPain\nis a synthesis\u2013a sum that is greater than its parts. \u201d The brain\u2019s ability to alter the perception\nof sensory inputs through the interplay of emotion, cognition, and social processes is signifi-\ncant. Although natural systems establish the initial biological framework for pain perception,\nthis structure is highly adaptable, particularly in humans. Throughout a person\u2019s life, both\nbiological developments and personal experiences significantly reshape this framework.\nA key question driving pain research across biological, psychological, and computational\nfields is why this topic of pain is meaningful and important. This question also forms the\nbasis for initiating this thesis, highlighting the broader relevance of studying pain. Williams\nand Kappesser [4] provide a compelling explanation, stating, \u201cWe care because we are wired\nto care: to attend to other people\u2019s expression of pain and to understand its meaning; to feel\ndistress in relation to their distress; and to be motivated to reduce their distress, and ours,\nif we are able to do so. \u201d This highlights the intrinsic human response to empathize and\n12 CHAPTER 1. INTRODUCTION\nalleviate pain, underlining the fundamental importance of this research area. Indeed, from a\nDarwinian perspective, pain serves a crucial role. The manifestation of pain in humans and\nthe reactions it elicits are examined through an evolutionary lens. Pain facilitates recovery by\npromoting responses to harmful stimuli and behaviors that demonstrate the adverse nature\nof painful experiences, common among animals. Specifically, the facial expression of pain,\nwhich communicates discomfort directly to those nearby, is universally recognized across\ndifferent ages, ethnicities, roles, and relationships. Evidence from healed major fractures\n[5, 6] suggests that injured members of hominid groups were not left to fend for themselves\nbut were supported through their recovery, indicating the fundamental importance of pain\nexpression in our evolutionary history.\nPain is a widespread health concern globally, affecting up to 30% of the adult popula-\ntion [7] and between 83% and93% of elderly adults in residential care [8]. The Global Bur-\nden of Disease (GBD) study identifies pain as the primary cause of years lived with disability\n(YLD) [9], with major contributors including chronic back pain, musculoskeletal disorders,\nand neck pain [10]. Pain impacts individuals and poses significant clinical, economic, and\nsocial challenges. In the United States, the economic and healthcare costs related to pain\ndue to reduced work productivity range from $560 to$635 billion annually, surpassing the\ncosts associated with heart disease, cancer, and diabetes combined [11]. In Europe, chronic\npain\u2019s direct healthcare costs and indirect socioeconomic impacts account for 3%to10% of\nthe GDP [12]. In Australia, the average annual cost for individuals among the 15.4%living\nwith chronic pain ranges from AU $22,588to AU $42,979, including non-financial costs [13].\nBeyond direct effects on health, pain contributes to a range of adverse outcomes, such as opi-\noid dependency, drug overuse, addiction, declining social relationships, and psychological\ndisorders [14]. In the last two decades, prescription opioid use has surged in the United\nStates, where overdose deaths have increased more than fourfold from 1999 to2016 [15].\nAdditionally, side effects from these opioids, like lethargy, depression, anxiety, and nausea,\nseverely impact workforce productivity and overall life quality [16].\nAccurate pain assessment is crucial for early diagnosis, disease progression monitoring,\nand treatment effectiveness evaluation, particularly in managing chronic pain [17]. This criti-\ncal role has resulted in pain being recognized as \u201cthe fifth vital sign\u201d in nursing literature [18].\nPain assessment is also fundamental in physiotherapy, where therapists apply external stim-\nuli and need to gauge the patient\u2019s pain levels accurately [19]. Objective evaluation of pain is\nessential to provide appropriate care, especially for vulnerable populations who may not be\nable to communicate their pain effectively, such as infants, young children, individuals with\nmental health issues, and the elderly. Various methods are used for pain assessment, with\nself-reporting\u2013where individuals describe their pain experiences\u2013considered the gold stan-\ndard [20]. Pain evaluation methods in clinical environments include quantifiable measures\nlike the Numeric Pain Rating Scale (NPRS), Visual Analogue Scale (V AS), and quantitative1.1. CONTEXT AND MOTIVATION 3\nsensory testing techniques such as the pressure pain detection threshold (PPDT) [21]. Behav-\nioral indicators are also crucial and include facial expressions ( e.g., grimacing, open mouth,\nlifted eyebrows), vocalizations (like crying, moaning, or screaming), and movements of the\nbody and head [22]. Physiological measures such as electrocardiography (ECG), electromyo-\ngraphy (EMG), galvanic skin responses (GSR), and respiration rates further contribute to\nunderstanding pain\u2019s physiological aspects [17]. Additionally, brain monitoring techniques\nlike near-infrared spectroscopy (fNIRS) have effectively detected changes in hemodynamic\nactivity associated with pain stimuli [23].\nCaregivers and family members often determine the presence or absence of pain in pa-\ntients by observing their behavioral or physiological responses [17]. However, accurately\nassessing pain poses a significant challenge for clinicians [24], especially with nonverbal\npatients such as the elderly, who may have reduced expressive abilities or may be reluctant\nto communicate pain [25]. Extensive research indicates that pain manifestations vary signif-\nicantly across different genders and ages, adding to the complexity of its assessment [26].\nFurther complicating the assessment process are the heightened workload and fatigue ex-\nperienced by nursing staff due to the demands of patient monitoring [27]. Technological\nsolutions are necessary for continuous patient monitoring. Nevertheless, concerns remain\nabout the objectivity and accuracy of these observations, as inadequately trained or biased\nobservers may struggle to assess pain [28] accurately. Even among trained observers, in-\nterpretations of behaviors can vary [22], and social and interpersonal dynamics can signif-\nicantly affect the pain assessment process, influencing both the evaluators\u2019 judgments and\nthe patients\u2019 expressions of pain [29]. Additionally, the presence of an observer can lead pa-\ntients to modify their behavior [30], and expressing pain through scales and measurements\ncan be challenging [31]. While self-reporting is used because pain is inherently subjective,\nrelying solely on a one-dimensional pain score fails to capture this complex phenomenon,\noften leading to inadequate pain management [32].\nGiven the challenges described above, scientific computing (SC) researchers have fo-\ncused on developing models and algorithms to enhance automatic pain recognition systems\nover the last two decades. Their goal is to accurately determine the presence and intensity\nof pain by analyzing physiological and behavioral indicators. Adopting deep learning and\nartificial intelligence (AI) techniques has expanded these automatic methods, designed to\ninterpret the complex and varied nature of pain [17]. Numerous studies have underscored\nthe effectiveness of automated systems that utilize behavioral or physiological modalities\nfor pain assessment [33]. Sario et al. [34] have shown the capability of these systems to\naccurately recognize pain through facial expressions, proving their utility in clinical envi-\nronments. Multimodal sensing has shown particular promise, offering enhanced accuracy\nin pain detection systems [22]. Furthermore, including temporal aspects in these modalities\nhas proven to significantly improve the accuracy of pain assessments [17].4 CHAPTER 1. INTRODUCTION\n1.2 Scope and Challenges\nAlthough considerable research has been conducted on automatic pain assessment, studies\nhave yet to explore factors like demographics and social aspects from a computational angle.\nFurthermore, despite the existence of deep learning-based methods, the approaches we ob-\nserve are often outdated and repeatedly recycled. For these reasons, we aimed to address two\nissues by (i)attempting to evaluate the social or demographic context, which significantly\nimpacts and influences pain sensation and perception, and (ii)introducing innovative deep\nlearning methods inspired by the latest developments in AI and generative AI literature. We\nbelieve these approaches can forge new paths in pain research, enhance the accuracy of rec-\nognizing this complex phenomenon, and, ultimately, be adopted in real-world scenarios to\nassist those in need. Additionally, (iii)recognizing the skepticism towards new technologies\namong clinicians and the general public, especially regarding the limited understanding of\nhow deep learning models function, we have devoted a portion of our research to interpret-\ning these models to offer some level of explanation and help the adoption process of them in\nclinical settings.\nNevertheless, this thesis initially faced challenges related to our objectives and goals as\nthe research progressed. The availability of pain datasets (to be discussed in the next chapter)\nis limited. Only a few datasets are available, and crucially, they are limited in size. This re-\nstriction poses a significant challenge for developing deep learning models, which typically\nrequire a large volume of data. In automatic pain assessment, researchers who develop deep\nlearning methods typically confront a decision: either train their models from scratch, which\ncan introduce performance limitations, or employ pre-trained models. These pre-trained\nmodels are generally trained on broadly available image datasets that include a variety of\nsubjects like animals and objects, or they rely on older architectures that were trained explic-\nitly on facial datasets. In this thesis, we addressed these issues by independently pre-training\nour deep-learning models using diverse datasets related explicitly to human facial images\nand biosignals. This strategy allowed us to design specific architectures to meet our unique\nneeds for each scenario, free from the constraints of relying on models developed and trained\nby others. Furthermore, we explored and evaluated several pre-training techniques to assess\ntheir effectiveness in pain assessment applications.\nRegarding, our objective to explore methods that utilize various modalities individually\nand in combination in a multimodal manner further constrains our dataset options. More-\nover, as previously outlined, our interest in the sociodemographic aspects of pain necessitates\ndatasets that include this information type, intensifying our challenges. For these reasons,\nthis thesis focuses specifically on examining the impact of age and gender on pain. In addi-\ntion, led us to utilize two pain datasets that most closely match the characteristics necessary\nfor our research, particularly in terms of demographic elements and multimodality.1.3. CONTRIBUTIONS \u2013 PEER-REVIEW PUBLICATIONS 5\n1.3 Contributions \u2013 Peer-review Publications\nThis section outlines the publications and projects produced during the Ph.D. research on\nautomatic pain assessment, where I was the first author.\n1.Automatic assessment of pain based on deep learning methods: A systematic re-\nview [17]\nThis systematic literature review (SLR) was conducted at the start of this Ph.D. re-\nsearch. This paper aims to explore the surge in recent years of deep learning algorithms\nadopted by researchers to encode the multidimensional nature of pain into meaning-\nful features. Specifically, this systematic review examines the models, methods, and\ndata types used to establish the foundation for deep learning-based automatic pain\nassessment systems. It identified relevant original studies from digital libraries such\nasScopus ,IEEE Xplore , and ACM Digital Library , following defined inclusion and\nexclusion criteria for studies published until December 2021 . The findings highlight\nthe critical role of multimodal approaches in automatic pain estimation, particularly\nin clinical environments, and emphasize the substantial gains observed with the inclu-\nsion of temporal exploitation of modalities. The review also recommends selecting\nhigh-performing deep learning architectures and methods, encouraging the adoption\nof robust evaluation protocols and interpretability techniques to deliver reliable and\nunderstandable outcomes. Additionally, it underscores the current limitations of exist-\ning pain databases in adequately supporting the development, validation, and practical\napplication of deep learning models as decision-support tools in real-world settings.\nFurthermore, we believe this paper is valuable not only for this Ph.D. project but also\nfor other practitioners and researchers in the field.\n2.Automatic Pain Intensity Estimation based on Electrocardiogram and Demographic\nFactors [35]\nThis study investigated the relationship between gender, age, and pain sensation and\ntheir effects on the automatic pain assessment process. By analyzing physiological\nsignals, particularly electrocardiography (ECG), we estimated pain intensity and ex-\namined the influence of these demographic factors. Utilizing the Pan-Tompkins algo-\nrithm for feature extraction and applying well-established classification methods, we\nexplored the correlation between gender, age, and pain manifestation.\n3.Multi-task Neural Networks for Pain Intensity Estimation Using Electrocardiogram\nand Demographic Factors [36]\nInspired by the previous study, this research further explored the influence of gender\nand age on pain perception. In this work, we analyze electrocardiography signals\nto uncover variations in pain perception across different demographic groups. We6 CHAPTER 1. INTRODUCTION\nleveraged these insights by developing a novel multi-task neural network for automatic\npain estimation, incorporating age and gender data for each individual. The study\ndemonstrated the advantages of this approach compared to other existing methods.\n4.A Full Transformer-based Framework for Automatic Pain Estimation using Videos\n[37]\nThis study introduced an innovative full transformer-based framework featuring a Trans-\nformer in Transformer (TNT) model combined with cross-attention and self-attention\nblocks. We achieved state-of-the-art performance using video data from the BioVid\ndatabase, demonstrating the model\u2019s effectiveness, efficiency, and strong generaliza-\ntion across primary pain estimation tasks.\n5.Multimodal automatic assessment of acute pain through facial videos and heart rate\nsignals utilizing transformer-based architectures [38]\nThis study presented a multimodal automatic acute pain assessment framework, inte-\ngrating video and heart rate signals. The framework consists of four key modules:\ntheSpatial Module , which extracts embeddings from videos; the Heart Rate Encoder ,\nwhich maps heart rate signals into a higher-dimensional space; the AugmNet , which\ngenerates learning-based augmentations in the latent space; and the Temporal Mod-\nule, which leverages the video and heart rate embeddings for the final assessment.\nThe Spatial Module undergoes a two-stage pre-training process: first, it learns uni-\nversal facial features through face recognition, followed by emotion recognition in a\nmultitask learning approach, enabling high-quality embeddings for pain assessment.\nExperiments with facial videos and heart rate data extracted from electrocardiograms\nin the BioVid database, alongside direct comparisons to 29studies, demonstrate state-\nof-the-art performance in unimodal and multimodal settings while maintaining high\nefficiency. In the multimodal setting, the framework achieved 82.74% accuracy for bi-\nnary pain classification and 39.77% for multi-level pain classification, using only 9.62\nmillion parameters across the entire framework.\n6.Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-\nMLP Architecture [39]\nThis paper introduced synthetic thermal videos generated by Generative Adversarial\nNetworks , which are integrated into the pain recognition process to assess their effec-\ntiveness. The framework employs a Vision-MLP andTransformer -based module, lever-\naging RBG and synthetic thermal videos in unimodal and multimodal settings. Exper-\niments conducted using facial videos from the BioVid database highlighted synthetic\nthermal videos\u2019 effectiveness and showcased their potential benefits in pain recognition\ntasks.1.4. THESIS OUTLINE 7\n7.Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for\nMultimodal Automatic Pain Assessment using Facial Videos and fNIRS [40]\nThis study was submitted to the First Multimodal Sensing Grand Challenge for Next-\nGen Pain Assessment (AI4PAIN) . The proposed multimodal framework leverages fa-\ncial videos and fNIRS, offering a modality-agnostic approach that eliminates the need\nfor domain-specific models. Utilizing a dual ViT configuration and waveform repre-\nsentations for both fNIRS and the extracted embeddings from the two modalities, the\nmethod demonstrates its effectiveness, achieving an accuracy of 46.76% in the multi-\nlevel pain assessment task.\n8.PainFormer: a Vision Foundation Model for Automatic Pain Assessment [41]1\nThis study introduces PainFormer , a vision foundation model built on multi-task learn-\ning principles and trained across 14distinct tasks and datasets comprising 10.9million\nsamples. As an embedding extractor for various input modalities, PainFormer provides\nfeature representations to the Embedding-Mixer , a transformer-based module respon-\nsible for conducting the final pain assessment. Extensive experimentation using both\nbehavioral modalities\u2013including RGB, synthetic thermal, and estimated depth videos\u2013\nand physiological modalities like ECG, EMG, GSR, and fNIRS revealed PainFormer \u2019s\nability to extract high-quality embeddings from diverse inputs. Tested on the BioVid\nandAI4Pain datasets and compared to more than 60existing methods, the framework\ndemonstrated state-of-the-art performance in unimodal and multimodal settings, posi-\ntioning itself as a step toward developing general-purpose models for automated pain\nevaluation.\n1.4 Thesis Outline\nThe dissertation is organized into the following chapters:\nChapter 2 introduces the foundational concepts of pain from biological, psychological, and\nclinical perspectives.\nChapter 3 reviews existing literature on automatic pain assessment using deep learning\nmethods and details the pain datasets used.\nChapter 4 outlines and proposes methods for evaluating demographic variables, their uti-\nlization, and their integration into an automatic pain assessment framework.\nChapter 5 discusses methods that utilize video and wearable device data, exploring the\ntrade-offs between efficiency and accuracy. It also proposes efficient, fast, effective models\nsuitable for real-world applications.\nChapter 6 explores synthetic data in pain assessment and introduces synthetic thermal im-\n1Under Review8 CHAPTER 1. INTRODUCTION\nagery techniques to enhance performance in automatic pain recognition.\nChapter 7 discusses general-purpose models, introduces a modality-agnostic framework,\nand presents the first foundation model used in automatic pain assessment.\nChapter 8 concludes the thesis with a final discussion, offering perspectives and ideas for\nfuture research in automatic pain assessment.Chapter 2\nClinical Pain Assessment\nContents\n2.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . . . . 20\n2.1 Chapter Overview\nThis chapter provides an anatomical and physiological overview of pain, focusing on the\nmechanisms responsible for generating, transmitting, processing, and interpreting pain sig-\nnals. It examines the various types of pain and explores the actions and expressions typically\nassociated with pain. Additionally, it reviews current pain assessment methods used in clin-\nical settings for adults, children, and newborns. The chapter also discusses developing and\nvalidating existing clinical pain assessment tools. This foundational knowledge is essen-\ntial for understanding the development and validation of computer-assisted pain assessment\n910 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nmethods discussed in later chapters. Finally, it highlights the challenges faced in clinical\npain assessment and underscores the need for automated pain assessment techniques.\n2.2 Biology of Pain\nPain, according to the International Association for the Study of Pain (IASP) [42], is \u201can\nunpleasant sensory and emotional experience associated with actual or potential tissue dam-\nage, or described in terms of such damage\u201d. Biologically, pain is an undesirable sensation\noriginating from the peripheral nervous system. Its fundamental function is to engage sen-\nsory neurons, notifying the organism of potential harm and playing a vital role in recognizing\nand responding to threats [43].\nThe transmission of a noxious stimulus from the periphery to the central nervous sys-\ntem involves a complex pathway through the spinal cord, resulting in the physical sensation\nof pain and a corresponding emotional response and memory. This process culminates in\nthe perception of pain. The initial stage of pain processing occurs when a stimulus at noci-\nceptive sensory fibers in the periphery is converted into an action potential. A nerve signal\nis generated if the stimulus is strong enough to surpass the action potential threshold [44].\nThis signal travels along the primary afferent fiber toward the central nervous system. As the\nstimulus intensity grows, more nerve fibers and areas of the nervous system are engaged [44].\nDue to their branching nature, primary afferent fibers typically relay information from sev-\neral pain receptors. These fibers and their receptors comprise a sensory unit, which gathers\ndata from a specific receptive field [44]. When receptive fields are larger and overlap with\nnearby fields, it becomes more challenging for the sensory system to locate the source of pain\naccurately. The primary afferent neuron is a pseudounipolar neuron that splits into a periph-\neral and central axon. The cell bodies of these neurons are located in the peripheral nervous\nsystem, within the posterior or cranial root ganglia. The peripheral axon extends to the skin,\nmuscles, tendons, or joints, branching into terminal fibers that connect with somatosensory\nreceptors. In contrast, the central axon leads to the central nervous system [45].\nPeripheral somatosensory fibers are categorized into three main groups. The first group\nincludes A\u00b4\u03b1,A\u00b4\u03b2,A\u00b4\u03b3fibers, large, myelinated fibers that rapidly conduct sig-\nnals [46]. These fibers involve touch and proprioception but are not associated with pain\nperception. The second group consists of A\u00b4\u03b4fibers, which are smaller and slower con-\nducting. Certain A\u00b4\u03b4fibers play a key role in pain sensation, with some responding only\nto intense mechanical stimuli and others reacting to noxious and non-noxious heat. The\nthird group comprises Cfibers, which are small, unmyelinated, and conduct signals very\nslowly. Most Cfibers are polymodal for pain perception, responding to various noxious\nmechanical, thermal, and chemical stimuli. These fibers are mainly linked to burning pain\nsensations [43]. The sensation of pain, known as nociception, is primarily facilitated by2.3. CLASSIFICATION AND CHARACTERISTICS OF PAIN 11\nvarious intracellular and extracellular molecular messengers. When activated by a specific\nstimulus, nociceptors relay information through glutamate, an excitatory neurotransmitter.\nAdditionally, inflammatory mediators are released at the injury site, further stimulating no-\nciceptor activation by releasing chemicals such as neurotransmitters ( e.g., serotonin), lipids\n(e.g., prostaglandins), peptides ( e.g., bradykinin), and neurotrophins ( e.g., nerve growth fac-\ntor) [46]. There are ascending tracts responsible for transmitting sensory information from\nthe periphery to the central nervous system. Fibers that convey two-point discrimination, tac-\ntile information, pressure, vibration, and proprioception ascend via the dorsal column of the\nspinal cord, forming the gracile and cuneate fasciculi. Fibers transmitting pain, temperature,\nand crude touch from somatic and visceral structures travel through the lateral spinothalamic\ntract. The anterior spinothalamic tract also transmits pain, temperature, and touch informa-\ntion to the brainstem and diencephalon (Figure 2.1) [47].\n2.3 Classification and Characteristics of Pain\nAccording to neurobiologist Clifford Woolf [48], pain can be classified into three categories\nbased on its function and characteristics: nociceptive ,inflammatory , and pathological pain.\nThese classes and their respective functions are illustrated in Figure 2.2.\nNociceptive pain (refer to Figure. 2.2(A)), arising from tissue damage, is a high-threshold\npain that activates only in response to intense stimuli [49], serving as a vital warning signal\nto the body. The neurobiological system responsible for nociceptive pain evolved from the\nability of even the most primitive nervous systems to detect impending or actual tissue dam-\nage caused by external stimuli. Its protective role requires immediate attention and action,\nachieved through the withdrawal reflex it initiates, the unpleasant sensation it produces, and\nthe emotional distress it triggers. Nociceptive pain demands avoidance in the present mo-\nment, and when activated, it overrides most other neural processes [48].\nInflammatory pain (refer to Figure. 2.2(B)) is also protective and adaptive, increasing\nsensory sensitivity following tissue damage to aid healing by discouraging movement and\ncontact with the injured area. This heightened sensitivity, or tenderness, helps prevent further\nharm and supports recovery, as seen after surgical wounds or inflamed joints where normally\nnon-painful stimuli now cause pain. It is triggered by immune system activation in response\nto tissue injury or infection. Despite its adaptive role, this pain often needs to be alleviated in\npatients with persistent inflammation, such as in rheumatoid arthritis or severe injuries [48].\nPathological pain (Figure. 2.2(C)) is maladaptive, arising from abnormal nervous sys-\ntem functioning and not serving a protective role. Unlike nociceptive and inflammatory pain,\npathological pain is a disease state of the nervous system itself. It may occur following\nnerve damage (neuropathic pain) or in conditions without apparent damage or inflammation\n(dysfunction l pain). Examples of dysfunctional pain include fibromyalgia, irritable bowel12 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFIGURE\n 2: Spinothalamic tract.\nPain, temperature, and some touch and pressure afferents end in the posterior horn. Second- or\nhigher-order fibers cross the midline, form the spinothalamic tract, and ascend to the ventral\nposterolateral (VPL) nucleus of the thalamus (and also to other thalamic nuclei not shown).\nThalamic cells then project to the somatosensory cortex of the postcentral gyrus, to the insula,\nand to other cortical areas (also not shown). Along their course through the brainstem,\nspinothalamic fibers give off many collaterals to the reticular formation (RF). The inset to the left\nshows the lamination of fibers in the posterior columns and the spinothalamic tract in a leg-\nlower trunk-upper trunk-arm sequence. The inset to the right shows the longitudinal formation\nof the spinothalamic tract. Primary afferents ascend several segments in Lissauer\u02bcs tract before\nall their branches terminate; fibers crossing to join the spinothalamic tract do so with a rostral\ninclination. As a result, a cordotomy incision at any given level would spare most of the\ninformation entering the contralateral side of the spinal cord at that level, and to be effective,\nthe incision must be made several segments rostral to the highest dermatomal level of pain.\n2017 Khalid et al. Cureus 9(10): e1754. DOI 10.7759/cureus.1754\n5\n of \n14\nFigure 2.1: The spinothalamic tract (STT) [43]. Pain, temperature, and some touch afferents\nend in the posterior horn, where second-order fibers cross the midline to form the\nspinothalamic tract, ascending to the thalamus and projecting to various cortical\nareas. Along the way, collaterals connect to the reticular formation. Due to the ros-\ntral inclination of fibers in Lissauer\u2019s tract, cordotomy must be performed several\nsegments above the pain level for effective relief.\nsyndrome, tension headaches, and temporomandibular joint disease, where significant pain\nexists without an apparent noxious stimulus or peripheral pathology. Pathological pain, a\nlow-threshold pain primarily driven by amplified sensory signals in the central nervous sys-\ntem, is the clinical pain syndrome with the greatest unmet need. To analogize, while nocicep-\ntive pain acts as a fire alarm for intense heat, and inflammatory pain reacts to warm tempera-\ntures, pathological pain is a false alarm triggered by a system malfunction. Thus, treatment\nmust specifically target the underlying mechanisms causing each type of pain [48].\nPain from a time-duration perspective can be categorized by duration into acute and2.4. PAIN INDICATORS 13\nchronic , with chronic pain persisting or recurring for more than three months [50]. Acute\npain is typically related to identifiable physiological damage from injury, surgery, illness,\ntrauma, or medical procedures and generally subsides once the underlying cause is resolved.\nHowever, if untreated, it may develop into chronic pain. Acute pain is further classified\nintoprocedural pain, caused by medical interventions such as muscular injections [51], and\npostoperative pain, which occurs after surgery and is a significant concern for both patients\nand healthcare providers. Effective management is crucial to aid recovery and prevent the\ntransition to chronic pain [52]. Chronic pain manifests in various forms, including chronic-\nrecurrent pain, like migraine headaches, and chronic-continuous pain, such as persistent low\nback pain [53].\n2.4 Pain Indicators\nPain can manifest in numerous ways and is often shaped by individual characteristics and\nenvironmental influences. Various human expressions, actions, and bodily responses have\nbeen linked to pain, serving both communicative and coping purposes. These pain indicators\nare generally categorized into three primary groups: (i)behavioral, (ii)physiological, and\n(iii)biochemical. While these indicators are universally present, certain expressions are more\nprominent in specific groups. For instance, crying is a common pain response across all age\ngroups but is more frequently observed in younger infants. This may be due to contextual\nfactors\u2014such as culture, social status, age, and ego\u2014influencing how pain is expressed\nover time. Adults, for example, may suppress crying in favor of other vocalizations, such as\ngroans and moans, as crying could be perceived as inappropriate in certain contexts. These\nmediating factors are often considered when interpreting pain indicators. The following\nsections will delve into each of these three categories [51].\n2.4.1 Behavioral Indicators\nBehavioral indicators such as facial expressions ( e.g., grimacing, open mouth, raised eye-\nbrows), vocalizations ( e.g., crying, moaning, screaming), and various bodily movements\n(e.g., changes in posture, signs of tension) are vital markers used in assessing pain [22].\nFacial expressions and limb movements in response to acute pain are typically rapid and\ninvoluntary. Facial reactions include brow bulging, eye squeezing, nasolabial furrow forma-\ntion [54], grimacing, clenched teeth, jaw-dropping, and tightened lips [55]. Body movements\nassociated with pain include bracing (gripping an object or the affected area during move-\nment), rubbing (massaging the painful area), restlessness (constant shifting of position) [55],\nand knee flexion [56]. Non-verbal vocalizations such as groaning, moaning, sighing, crying,\nand gasping [57] also indicate pain. Verbal expressions like \u201couch\u201d ,\u201cstop\u201d ,\u201cthat hurts\u201d ,14 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFigure 2.2: Pain classification [48]: (A)Nociceptive pain , which results from detecting po-\ntentially harmful stimuli and serves a protective function. (B)Inflammatory pain is\nlinked to tissue damage and immune cell infiltration, increasing pain sensitivity dur-\ning healing. (C)Pathological pain is a disease state caused by either nervous sys-\ntem damage (neuropathic) or abnormal nervous system function (dysfunctional).\n\u201cthat is enough\u201d , and even cursing [55] also serve as pain indicators. Interestingly, swearing\nhas been found to significantly alleviate pain, although its effect diminishes with frequent\nuse over a short period [58, 59].2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 15\n2.4.2 Physiological Indicators\nVital signs can reflect the state of the central nervous system, and since pain is mediated\nthrough this system, trends in vital signs can provide insights into pain levels. Clinical stud-\nies [60, 61] have examined physiological changes in response to pain and established em-\npirical solid evidence linking pain to vital sign alterations. However, as vital signs can also\nchange due to other non-pain-related pathological conditions, it is recommended that they be\nassessed alongside behavioral pain indicators for accuracy. Physiological pain responses are\nconsidered more reliable than behavioral signals, as they cannot be consciously controlled\nor altered. Physiological measurements such as electrocardiography (ECG), electromyogra-\nphy (EMG), galvanic skin responses (GSR), and respiration rate provide critical insights into\nthe body\u2019s reaction to pain [17]. In addition, brain monitoring techniques like near-infrared\nspectroscopy (fNIRS) have demonstrated the ability to detect pain-related hemodynamic\nchanges [23]. At the same time, functional magnetic resonance imaging (fMRI) has been\nexplored for assessing pain in both normal and pathological conditions [62].\n2.4.3 Biochemical Indicators\nCompared to other pain indicators, biochemical changes are the most precise and sensitive\nreactions to pain. However, their routine use in pain assessment is restricted due to the\ninvasive nature of measurement techniques [63]. These biochemical responses are most\nevident during surgical procedures with limited anesthesia, leading to increased levels of\nendorphins, norepinephrine, cortisol, growth hormones, renin, glucagon, aldosterone, and\ncatecholamines, along with a decrease in insulin levels [60].\n2.5 Sociodemographic and Psychological Variables\nIn1965 , Melzack and Wall [64] introduced the \u201cGate Control Theory\u201d , which interprets pain\nfrom two perspectives. The first involves the mechanisms of nociceptive signal transmission\nand modulation, while the second emphasizes pain as a psychophysiological phenomenon\narising from the interaction between physiological and psychological factors [53]. Observa-\ntions, empirical research, and theoretical models increasingly suggest that a comprehensive\nunderstanding of pain requires a biopsychological approach. It is also becoming apparent\nthat, although pain is often regarded as private and subjective, it is also fundamentally a so-\ncial experience [53]. Pain is not solely explained by biomedical components ( e.g., muscle\ndamage) but also involves psychological ( e.g., cognitive, affective) and social factors ( e.g.,\nfriends, family, health professionals), leading to what is known as a biopsychosocial sensa-\ntion [65]. Numerous factors contribute to how painful experiences are expressed and per-\nceived, varying wildly due to social and personal biases. These factors prompted Williams16 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nand Craig [2] to define pain as \u201ca distressing experience associated with actual or potential\ntissue damage with sensory, emotional, cognitive, and social components. \u201d\n2.5.1 Sex and Gender\nSeveral studies have explored the relationship between gender and pain expressiveness, as\nwell as variations in pain reporting. Research indicates that women generally exhibit a lower\npain threshold compared to men. A meta-analysis by Boerner et al. [66] on gender differ-\nences in children and adolescents found that girls over the age of 12reported higher pain\nintensity in response to cold-induced pain than boys. Furthermore, multiple studies suggest\nthat women tend to describe a greater degree of pain compared to men. In addition to bi-\nological differences, psychological aspects linked to gender also play a role. For instance,\nindividuals with a masculine identity may be less inclined to express or report their pain or\nseek assistance [67].\nMoreover, the manifestation of pain is not only influenced by the individual\u2019s gender but\nalso by dyadic interactions between people of different sexes. Levine and Desimone [68]\nconducted one of the initial studies on this phenomenon, showing that male participants in\na cold pressure experiment reported lower pain intensity when a female experimenter was\npresent. Similarly, McClelland and McCubbin [69] found that female participants expressed\nand reported higher pain levels when accompanied by a female friend. This dynamic also\nextends to patient-healthcare provider interactions. In studying health records, Vigil and\nAlcock [70] discovered that when the pain intensity was reported as high, the patients ( i.e.,\nmen and women) were examined by a female doctor or nurse. Additionally, studies exam-\nining gender differences among physicians in pain treatment options revealed that female\npatients were more likely to receive prescriptions for more potent drugs, such as analgesics,\nand female physicians were more likely to prescribe medications. Extensive research has\nalso shown that both lay observers and healthcare professionals tend to estimate higher pain\nlevels for female patients compared to male patients [71]. Hooper et al. [72] further noted\nthat clinicians communicate more effectively with female patients, often displaying greater\nempathy. Gender roles, beliefs, and expectations play a significant role in understanding\nhow social factors influence the differences in pain perception and experience between men\nand women [73].\n2.5.2 Age\nAge plays a crucial role in pain assessment and management. At the same time, there are\nsignificant challenges, limitations, and biases related to the patient\u2019s age group. Two of the\nmost vulnerable groups, albeit for different reasons, are the elderly and infants.\nPain recognition and interpretation among the elderly, particularly by caregivers, often2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 17\npresent unique challenges. Older adults frequently exhibit stoicism and reluctance to ex-\npress their pain, while healthcare providers struggle to accurately assess the patient\u2019s pain,\nleading to inappropriate pain management decisions [74]. McPherson et al. [75] noted that\ndespite caregivers\u2019 accommodating and empathetic relationships with elderly patients, con-\nflicts still arise. Older patients may resist acknowledging their weaknesses and accepting\nhelp, which can cause them to conceal their pain. The situation becomes even more complex\nwhen dealing with dementia, a disorder encompassing a range of conditions ( e.g., Parkin-\nson\u2019s, Alzheimer\u2019s, Vascular dementia), characterized by abnormal brain changes that im-\npair cognitive and linguistic abilities. A person with dementia may find it challenging to\ncommunicate their pain verbally. However, non-verbal pain expressions remain intact even\nin moderate dementia, although such reactions can be exaggerated [76]. However, aggres-\nsive behavior and disturbances in dementia patients, often caused by pain, are frequently\nmisinterpreted as psychiatric symptoms, leading to improper medication that can have life-\nthreatening consequences [77]. Caregivers of dementia patients face additional challenges,\nnot only related to pain management but also in addressing dementia\u2019s impact on language\nand memory. Particularly in the later stages of dementia, patients encounter severe pain\ncommunication difficulties due to cognitive decline, necessitating that caregivers recognize\nbehavioral and contextual indicators of pain [74]. Age is also known to cause changes in\nskin characteristics, such as texture, rigidity, and elasticity, which impact the performance of\nemotional face recognition tasks [78].\nInfants represent another vulnerable age group where pain assessment requires special-\nized attention, particularly when they experience painful events. The first challenge is obvi-\nously their limited reporting ability to express their pain through language. Although crying\nmight appear to signal pain, this is an oversimplified and unreliable method, as crying can in-\ndicate a variety of situations, such as discomfort, hunger, or pain. Accurately discerning the\ntype of cry is only one part of the challenge; assessing pain in infants is far more complex and\ninfluenced by numerous factors, including the interpersonal relationships within their envi-\nronment. Riddell and Racine [79] found that through various distressing experiences, infants\ncan learn that specific signaling behaviors can prompt their caregiver\u2019s proximity. This at-\ntachment dynamic suggests that, to some extent, infants may consciously utilize pain-related\nbehaviors to elicit responses from their caregivers. Similarly, the context affects older chil-\ndren as well; for example, self-reports of pain tend to be significantly lower when a parent is\npresent compared to when the child is alone [80].\n2.5.3 Psychological Factors\nMultiple studies have revealed that several psychological factors are consistently linked with\npain-related behavior, including depression, pain-related fear, and catastrophizing. Research18 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nfocusing on the impact of depression and anxiety on pain-related behavior has been con-\nducted mainly on patient populations. These studies have shown that depressed individ-\nuals exhibit more pronounced protective and communicative behaviors compared to non-\ndepressed patients [81]. Similarly, numerous studies suggest that patients with higher levels\nof anxiety demonstrate more pain-related behaviors than those with lower anxiety levels [82].\nDespite the frequent coexistence of pain with psychological conditions, research indicates\nthat these patients often experience underestimation of their pain. For instance, De Ruddere\net al. [83] found that patients dealing with psychological stressors such as anxiety, depres-\nsion, and daily life challenges are often perceived by physiotherapists as experiencing less\nsevere pain, illustrating the influence of psychosocial factors on the patient\u2019s pain experience.\n2.5.4 Race and Culture\nPain expression is generally understood across ethnicities and cultures, though differences\nexist in how it is conveyed [4]. However, cultural variations and the nuances of facial ex-\npressions related to emotion are complex and necessitate deeper study. Additionally, racial\nand cultural biases significantly influence pain assessment, judgment, and interpretation.\nExtensive research highlights the impact of a patient\u2019s race as a sociodemographic factor\non observer responses. The most examined topic relates to the different responses toward\nCaucasian versus non-Caucasian individuals, particularly African Americans, who are more\nlikely to have their pain underestimated and undertreated by healthcare providers [84].\nEthnocultural factors are crucial in shaping how individuals perceive and express pain.\nFor example, Western cultures often emphasize conservative expressions and self-control,\nleading to restrained responses in personal pain experiences and in perceiving others\u2019 pain\n[3]. Differences also arise in coping mechanisms; African Americans, for instance, are more\nprone to catastrophizing pain events compared to European Americans [85]. Furthermore,\nevidence shows racial biases in pain treatment across various racial groups, with certain\ngroups being more sensitive to pain but receiving lower-quality treatment [86]. For exam-\nple, Cleeland et al. [87] found that minority cancer patients, mainly Black and Hispanic\nindividuals, were more likely to experience inadequate analgesia compared to non-minority\npatients.\n2.5.5 Observer\u2019s Impact on Pain\nThe variability in pain management stems from the interplay of various elements, including\nsociocultural, biomedical, and psychosocial factors, especially in cases of chronic pain [88].\nWhen it comes to the observer responsible for assessing a patient\u2019s pain, several characteris-\ntics directly influence the objectivity of their evaluation. The first and perhaps most critical\nfactor is the observer\u2019s experience level. One would expect that more experience leads to2.6. IMPACT OF INADEQUATE PAIN MANAGEMENT 19\nbetter and more accurate assessments, but studies show that even experienced healthcare\nproviders consistently underestimate pain, much like laypersons [28]. The greater the expe-\nrience, the more pronounced the underestimation tends to be. This may be due to desensitiza-\ntion caused by repeated exposure to pain events, as seen in the differences between internists\nand surgeons in their evaluation of postoperative pain, with surgeons often encountering se-\nvere pain more regularly [89]. Another significant factor is the observer\u2019s knowledge and\nbeliefs about pain. For example, [83] found that laypersons and healthcare professionals\nwithout physical signs of pain might view the patient\u2019s complaints less seriously. Proper\ntraining is also essential for adequate pain assessment, which is why the Department of\nHealth and Human Services (DHHS) initiated a strategic program to improve healthcare\nproviders\u2019 education and knowledge regarding pain management, following evidence of in-\nadequate training in the field [90].\n2.6 Impact of Inadequate Pain Management\nThe experience of pain, particularly persistent pain, can have detrimental effects on the indi-\nvidual and their surrounding environment. Thoughts about severe pain often lead to grief and\nfear, causing individuals to perceive pain as a threat and feel incapable of managing it. This\ncan prompt avoidance behaviors aimed at escaping perceived harm [91]. Studies have shown\nthat children with a catastrophizing mindset about pain struggle with daily activities, while\nadolescents with chronic pain tend to have fewer friends and may miss out on social and\nentertainment opportunities, putting them at greater risk of victimization [92]. These adoles-\ncents often feel isolated and lonely compared to their healthy peers, and they may experience\nanxiety in social interactions [93]. Parental reactions to their children\u2019s pain can further com-\nplicate the situation, as parents with catastrophic tendencies tend to engage in overprotective\nbehaviors that hinder the child\u2019s functioning and psychosocial development [94]. Addition-\nally, the family\u2019s overall dynamic is affected, with the patient\u2019s sadness, sleep disorders, and\nchanges in leisure activities impacting the household [74].\nOn a biological level, pain, particularly when experienced early and severely, can alter\nthe brain and nervous system. These early pain experiences can disrupt neurobiological\ndevelopment and affect how pain is processed later in life [95]. A growing body of research\nlinks chronic pain to changes in the medial prefrontal cortex, a region crucial to emotional\nprocessing. Chronic pain is associated with structural and biochemical alterations in this\nbrain area, suggesting that these changes play a role in the pathophysiology of chronic pain\n[96].20 CHAPTER 2. CLINICAL PAIN ASSESSMENT\n2.7 Pain Measurement Scales and Metrics\nIn clinical settings, self-reporting remains the gold standard for assessing pain, allowing in-\ndividuals to describe their pain\u2019s intensity and location. Various self-report scales have been\ndeveloped for different age groups, such as the visual analog scale (V AS) [97] and the verbal\nrating scale (VRS) [98]. Additionally, observation-based scales, where a third party eval-\nuates the pain\u2019s severity, include tools like the Prkachin and Solomon pain intensity scale\n(PSPI) [99] and the neonatal/infant pain scale (NIPS) [100]. However, some studies suggest\nthat patients may exaggerate their pain severity to prompt more aggressive treatment inter-\nventions [101], raising concerns about the accuracy of self-reported symptoms. Therefore,\nobjective pain measurement remains clinically crucial.Chapter 3\nAutomatic Pain Assessment\u2013A Literature\nReview\nContents\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database . . . . 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 49\n2122 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research Di-\nrections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.1 Chapter Overview\nThis chapter corresponds to the publication [17], a systematic literature review (SLR) con-\nducted at the start of this Ph.D. research. This review facilitated an understanding of au-\ntomatic pain assessment methods, particularly those based on deep learning, and the tech-\nniques and strategies employed. It enabled the identification and proposal of new approaches\nthat could enhance the effectiveness of pain recognition.\nAdditionally, it allowed for identifying gaps in the literature from other reviews con-\nducted on this specific research topic. Every existing systematic review on pain assessment\nwas identified and assessed, revealing several insights. The first review on automatic pain\nassessment, published by Prkachin in 2009 [102], did not cover papers on deep learning,\nas the practical implementations of deep architectures only began around 2012. Zamzmi et\nal.[103] focused their review exclusively on infants, omitting deep learning methods. In\n2018, Chen et al. [104] reviewed automated pain detection methods using the Facial Ac-\ntion Coding System (FACS), noting only three publications that employed deep learning\ntechniques. In 2019, Hassan et al. [105] included only seven papers that used deep learn-\ning methods in their review. Similarly, Werner et al. [106], also in 2019, discussed pain\nassessment without restrictions on modalities or age groups, finding fewer than ten papers\nthat reported on deep learning methods. In 2020, Al-Eidan et al. [107] published the first\nsystematic literature review titled \u201cDeep-Learning-Based Models for Pain Recognition: A\nSystematic Review\u201d, which included fifteen papers but was critiqued for having significant\nlimitations and incorrect information. It was noted that some papers analyzed might not be\nrelevant, and there was confusion between \u201cneural networks\u201d and \u201cdeep learning\u201d. For in-\nstance, while study [105] mentioned using neural network approaches, they did not provide\nevidence of using deep learning methods. Moreover, in the study [104], the authors devel-\noped a neural network with only two layers combined with handcrafted features, which does\nnot qualify as a deep learning method. Additionally, studies [103, 107] focused on detect-\ning protective movement behaviors in chronic pain patients, which deviates from the central\ntopic of automatic pain assessment. Several reviews and SLRs on automatic pain assessment\nhave been published, but none exclusively or adequately focus on deep learning methods.\nThis SLR aims to bridge this gap by thoroughly reviewing deep learning techniques used for\nautomatic pain assessment.3.2. MODALITIES AND HARDWARE FOR AUTOMATIC PAIN ASSESSMENT 23\n3.2 Modalities and Hardware for Automatic Pain Assessment\nCreating an automatic pain assessment system hinges on capturing the necessary input data\nthrough various information channels, referred to as modalities. These modalities are cat-\negorized into behavioral and physiological types. A system utilizing only one modality is\ntermed unimodal, whereas a multimodal system incorporates multiple modalities.\nKey behavioral modalities encompass facial expressions, body movements, gestures, and\nauditory signals. Researchers use a range of optical and light sensors to record images or\nvideo sequences of facial and body movements. Commonly, researchers employ color RGB\ncameras, but depth and thermal sensors are also used to enhance visual data. Motion capture\nsensors are also employed to track movements, and microphones are frequently employed\nto capture sound. On the physiological front, modalities often involve biosignals that detect\nelectrical activities from various tissues and organs. Techniques such as electrocardiogra-\nphy (ECG), electromyography (EMG), electrodermal activity (EDA), photoplethysmogra-\nphy (PPG), blood oxygen saturation (SpO2), near-infrared spectroscopy (NIRS), respiration\nrate, and skin temperature are commonly used to gauge pain. Multiple sensors can mea-\nsure several modalities simultaneously \u2014 for instance, strain sensors and cameras can track\nrespiration rates.\nBesides the sensors that gather input data, the computational hardware is crucial. Deep\nlearning-based systems operate in two phases: training and inference. The training phase is\nparticularly resource-intensive, necessitating a graphics processing unit (GPU). The trained\nmodel makes predictions on new data during inference, typically processed on a central\nprocessing unit (CPU). The choice of hardware depends on various factors, especially in\nreal-time scenarios where low latency is crucial, compared to offline settings where data\nprocessing can be deferred. Additionally, characteristics of the model, such as floating point\noperations per second (FLOPS) and total computational operations, are significant consider-\nations.\n3.3 Pain Databases\nAccess to data is crucial for evaluating methods and algorithms in automatic pain assess-\nment. However, only a few databases have explicitly been developed for automatic pain\nrecognition based on human behavioral and physiological changes. Unlike the extensive\ndata found in most facial expression databases, publicly accessible pain datasets often offer\nlimited samples and suffer from significant class imbalance. This primarily stems from the\nethical concerns associated with collecting pain data. Table 3.1 lists the principal databases\nreviewed in the studies. Figure 3.1 shows how frequently each database was used. Most\nresearch utilized publicly available datasets, with some studies exploring multiple datasets.24 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nFew studies used private datasets, mainly those aimed at detecting pain in neonates. The\nUNBC-McMaster Shoulder Pain Archive Database [108] is the most utilized, followed by\nTheBioVid Heat Pain Database [109]. The former contains 200facial videos of 25individ-\nuals with shoulder pain. At the same time, the latter includes facial videos and biopotentials\nof90healthy participants subjected to experimentally induced heat pain at four intensity\nlevels. The following subsections provide a brief description of some of these datasets.\nTable 3.1: Most commonly utilized pain databases.\nDatabase Modality Population Annotation\nGranularityAnnotation Labels\nUNBC-McMaster\nShoulder PainA[108]RGB video of face 25 adults with shoulder painFrame level\nSequence levelFACS\nV AS, OPI\nBioVidA[109] RGB video of face, EDA, ECG,\nEMG87 healthy adults Sequence level stimulus\n(calibrated per person)\nMIntPAINA[110] RGB-Depth-Thermal video of\nface20 healthy adults Sequence level stimulus\n(calibrated per person),\nV AS\niCOPEA[111] RGB photographs of face 26 healthy neonates Frame level pain, cry, rest, air puff,\nfriction\niCOPEvidA[112] Grayscale video of face 49 neonates Sequence level pain, no pain\nNPAD-IA[113] RGB video of face & body, HR,\nSpO2, BP, NIRS36 healthy neonates & 9 neonates\nwith tissue injured by surgerySequence level NIPS, N-PASS\nAPN-dbA[114] RGB video of face 112 healthy neonates Sequence level NFLAPS, NIPS, NFCS\nEmoPainN[115] video, audio, EMG, MoCap 22 adults with chronic pack pain &\n28 healthy adultsSequence level self-report, naive OPI\nSenseEmotionN\n[116]video of face, audio, EDA, ECG,\nEMG, RSP45 healthy adults Sequence level stimulus\n(calibrated per person)\nX-ITEN[117] RGB-Thermal video of face,\nRGB-Depth video of body, au-\ndio, EDA, ECG, EMG134 healthy adults Sequence level stimulus\n(calibrated per person)\nA: Publicly available by request, complete or part of the dataset N: Not yet available Modality: HR: heart rate SpO2: oxygen saturation rate BP:\nblood pressure NIRS: near-infrared spectroscopy MoCap: motion capture RSP: respiration rate EDA: electrodermal activity ECG: electrocardiogram EMG:\nelectromyogram Annotation Labels: FACS: Facial Action Coding System V AS: visual analogue scale OPI: observer pain intensity NIPS: neonatal infant\nscale N-PASS: neonatal pain, agitation and sedation scale NFLAPS: neonatal face and limb acute pain scale NFCS: neonatal facial coding system\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database\nTheUNBC McMaster Shoulder Pain Database [108] comprises 200video sequences show-\ning the facial expressions of 25subjects undergoing motion tests, including arm abduction\nand external and internal rotations. The data collection utilized both active and passive ap-\nproaches: in the active mode, subjects moved their affected arms to their bearable limit,\nwhile in the passive mode, a physiotherapist moved the subjects\u2019 arms. Each video sequence\ncontains about 60to700frames, totaling 48,398, with 82.71% of frames scoring a pain\nrating of zero, indicating a significant imbalance in the data. All frames are FACS-coded\nfor pain-related action units (AUs)\u2014AU4, AU6, AU7, AU9, AU10, AU12, AU20, AU25,\nAU26, AU27, and AU43\u2014with each AU coded for intensity from A to E, 0, or5, except\nfor AU43 (closed eyes), which is coded as either present or absent. Pain scores are assigned\nusing the PSPI metric based on the intensity of the AUs present. Additionally, the database3.3. PAIN DATABASES 25\n0815233038455360\nUNBC-McMasterBioVidEmoPainSenseEmotionX-ITEMIntPAINiCOPEiCOPEvidNPAD-IAPN-dbother\nTable 1Category AUNBC-McMaster59BioVid21EmoPain7SenseEmotion5X-ITE2MIntPAIN4iCOPE3iCOPEvid1NPAD-I5APN-db1other21\n1\nFigure 3.1: The number of studies utilizing these specific datasets. Note that various studies\nused multiple datasets to conduct their experiments.\nincludes 66facial landmarks per frame, determined by an active appearance model. Pain as-\nsessments also include self-reports using two Likert scales with 15options each and a visual\nanalog scale (V AS) from 1(no pain) to 10(extreme pain). One scale measures the sen-\nsory intensity from \u201cextremely weak\u201d to \u201cextremely intense\u201d , while the other assesses the\naffective-motivation aspect of pain from \u201cbearable\u201d to \u201cextremely excruciating\u201d. Indepen-\ndent observer pain intensity (OPI) ratings use a 6-point scale from 0(no pain) to 5(intense\npain). The UNBC database is currently the most extensively utilized dataset for automatic\npain recognition among publicly available resources.\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database\nTheBioVid dataset [109] is a prominent resource in pain research, comprising facial videos,\nelectrocardiograms, electromyograms, and galvanic skin response data from eighty-seven\npn=87qhealthy participants ( 44males and 43females, aged 20to65). The pain was in-\nduced using a thermode on the participants\u2019 right arm, with pain and tolerance thresholds\nestablished before data collection. These thresholds defined the range of pain from No Pain\n(NP) to Very Severe Pain (P 4), encompassing five levels of pain intensity. The temperatures\nfor the pain inductions ranged from P 1to P 4and did not exceed 50.5\u02ddC. Each participant\nunderwent 20inductions at each of four pain levels, with each induction lasting 4sfollowed\nby a recovery period of 8to12s. In addition, 20baseline measurements were taken at 32\u02ddC\n(NP), totaling 100stimulations per participant, randomly administered. Data processing\nsegmented these into 5.5sdurations starting 1safter the target temperature was reached, re-26 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsulting in 8,700samples across the five pain intensity classes, equally distributed among all\nmodalities for each participant. Video recordings were made at a frame rate of 25FPS, and\nbiosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\n3.3.3 The EmoPain Database\nTheEmoPain [115] dataset encompasses various pain indicators, including body movements,\naudio, biosignals, and postural and facial expressions. It features video and audio recordings\nof22patients ( 7male, 15female) exhibiting natural pain expressions while engaging in\nphysiotherapy-like exercises. These exercises, performed at regular and challenging levels,\ninclude a sitting-standing sequence, balancing on one leg for five minutes, and reaching for-\nward while standing. The video signals are captured in high resolution ( 1024\u02c61024 pixels)\nusing eight cameras positioned at various angles, enhanced by specialized lighting condi-\ntions. Audio is recorded with two microphones: an AKG C-1000S MKIII placed near the\ncameras and an AKG HC 577 L worn by the patients, both operating at a 48kHz sampling\nrate with bit Pulse Code Modulation. Body movements and postures are tracked using a mo-\ntion capture suit with 18sensors distributed across the body. Biosignals are monitored with\nfour sEMG sensors attached to the trapezius and lumbar para-spinal muscles. Additionally,\nthe dataset provides continuous frame-wise pain ratings for facial expressions by eight naive\nannotators and binary frame-wise annotations for protective behaviors by four experts, along\nwith coordinates from 26body nodes. Six annotated protective behaviors include stiffness,\nbracing, hesitation, limping, rubbing, and abrupt actions. Audio and EMG signals from the\neight activities per subject also contribute to multimodal pain recognition. Like the UNBC\ndatabase, EmoPain faces significant challenges due to data sparsity and imbalance\u2014only\n11.4%of frames show facial expressions of pain, and 8.6%show protective behaviors. This\nscarcity complicates pain recognition research, necessitating the development of methods\nthat efficiently utilize limited data to achieve optimal performance.\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database\nTheX-ITE [117] dataset is one of the largest pain datasets but is not publicly available. It\ninvolved 134healthy adults ( 67men and 67women) aged between 18and50. The aver-\nage age was 31.4years (SD = 9.7), with men averaging 33.4years (SD = 9.3) and women\n32.9years (SD = 10.2). Participants had no chronic pain, depression, psychiatric disorders,\nneurological conditions, headache syndromes, or cardiovascular disease, nor had they taken\npain medication or painkillers before the experiment. Pain stimuli were stimulated using the\nMedoc PATHWAY Model ATS for heat pain on the forearm and the Digitimer DS7A for elec-\ntrical pain on the index and middle fingers. Both modalities featured phasic stimuli (short,\n5seconds) and tonic stimuli (long, 60seconds), each in three intensities. After calibration,3.4. UNIMODAL STUDIES 27\nparticipants underwent a 90-minute stimulation phase where phasic stimuli were repeated\n30times in a randomized sequence with 8-12-second pauses. The tonic stimuli were applied\nonce per intensity, totaling six per participant, each followed by a five-minute pause. The\nhighest intensity tonic stimuli for heat and electrical pain were induced at the experiment\u2019s\nending, with the other stimuli randomly interspersed during the phasic period. Simultane-\nous to the pain stimulation, various sensors collected multimodal pain response data: frontal\nand side view RGB videos for facial expression and head pose analysis, audio for paralin-\nguistic response analysis, electrocardiogram (ECG) to monitor heart rate variability, surface\nelectromyography (EMG) to assess muscle activity in the trapezius, corrugator supercilii,\nand zygomaticus major, electrodermal activity (EDA) to measure sweating, video for body\nmovement analysis, and thermal video for facial temperature changes.\n3.3.5 The AI4Pain Database\nThe AI4Pain Grand Challenge 2024 [118] dataset is a recent contribution to the pain re-\nsearch field, tailored for sophisticated pain recognition tasks using fNIRS and facial video\ndata. This dataset involves sixty-five volunteers pn=65q, including 23females, with ages\nranging from 17to52years (mean age of 29.06years and a standard deviation of 8.28years).\nAlthough it captures physiological signals such as photoplethysmography (PPG), electroder-\nmal activity (EDA), and respiration (RESP), these signals are not publicly available yet. The\ndataset is segmented into three parts: training ( 41volunteers), validation ( 12volunteers),\nand testing ( 12volunteers). The experimental setup includes fNIRS data recorded with an\nArtinis device, measuring changes in oxygenated and deoxygenated haemoglobin concentra-\ntions across 24channels targeting the prefrontal cortex. The optodes configuration includes\n10sources and 8detectors spaced 30mm apart, using near-infrared light at 760nm and 840\nnm, sampled at 50Hz. Additionally, facial movements are captured by a Logitech Stream-\nCam at30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low Pain ,\nandHigh Pain . It features 65instances of No Pain (each lasting 60s),780instances of Low\nPain (each lasting 10s), and 780instances of High Pain (each lasting 10s). The No Pain\ninstances, recorded during baseline, serve as control data. The Low Pain instances reflect\nmild pain responses, and the High Pain instances capture significant pain, both derived from\na pain tolerance test and reflected in the corresponding neurological and behavioral data\nrecorded.\n3.4 Unimodal studies\nThis section presents the studies that utilized only one information channel to estimate the\nsubject\u2019s pain condition.28 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.1 Vision-based: Static Analysis\nThe first publicly available pain database that significantly contributed to the development\nof automatic pain assessment methods was the UNBC-McMaster Shoulder Pain Database .\nNumerous studies have employed this dataset. Pedersen [119] implemented the first deep\nlearning approach in 2015 to address the pain assessment problem, utilizing a 4-layer contrac-\ntive autoencoder. He combined the encoded representations with a support vector machine\n(SVM), achieving high performance in frame-level pain detection. A significant advance-\nment in vision-based pain recognition methods was the EmoPain challenge in 2020, which\nbecame the first international competition to compare machine learning methods for chronic\npain assessment. Egede et al. [120] presented the EMOPAIN 2020 Challenge , utilizing a\ndataset composed of features extracted via both handcrafted methods and deep-learned mod-\nels. They utilized facial landmarks, histogram of oriented gradients (HOG), and deep vectors\nfrom VGG-16 [121] and ResNet-50 [122], both pre-trained on the Aff-Wild dataset1. The au-\nthors report that combining hand-engineered features with deep learning cues led to the best\nperformance. Similarly, Yang et al. [123] extracted both low- and high-level features from lo-\ncal descriptors and the pre-trained VGG-16 CNN, combining them through weighted coeffi-\ncients. Semwal and Londhe [124] demonstrated that fusing deep-learned features with facial\nlandmarks is beneficial for multi-class pain estimation. Lakshminarayan et al. [125] com-\nbined deep-learned features with handcrafted ones\u2014namely features from VGG-16 [121]\nandResNet-50 [122], HOG, action unit occurrence and intensity, facial landmarks, and head\npose\u2014through a fully connected network. Their study found that combining VGG-16 with\nhandcrafted features lowered regression error, whereas [126] achieved maximal performance\nusing only VGG-16 features with a fully connected network.\nConversely, Semwal and Londhe [127] noted the limitations of traditional handcrafted\nfeature engineering and the computational expense of deep neural networks. As a solution,\nthey proposed a relatively shallow 4-layer CNN, which reduces computational costs due to\nfewer parameters while achieving performance comparable to deeper models. A different\napproach came from [128], where the authors focused on representing facial expressions\nas compact binary codes for pain intensity classification. Feature extraction was conducted\nusing a pre-trained model [129], with a fully connected network used to generate the binary\ncodes.\nSeveral studies utilized CNN ensemble designs with varying architectures to exploit fea-\nture diversity. Semwal and Londhe [130] combined predictions from three compact CNNs\u2014\nVGG-16 ,M-MobileNet [131], and GoogleNet [132]\u2014using the average ensemble rule, re-\nsulting in improved classification performance. Kharghanian et al. [133] developed a con-\nvolutional deep belief network (CDBN) using unsupervised feature learning. An SVM used\n1https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge3.4. UNIMODAL STUDIES 29\nthe extracted features to differentiate between two states for binary pain classification ( i.e.,\npain vs. no pain). Later, [134] added two layers to the CDBN, though the results were not\ndirectly comparable due to differing evaluation methods.\nSeveral papers suggest that because pain is predominantly expressed in specific facial\nregions, focusing on these areas rather than the whole face could improve model accuracy by\nreducing noise. Huang et al. [135] initially identified the left eye, right eye, nose, and mouth\nas key regions and utilized a multi-stream CNN for feature extraction, assigning learned\nweights to enhance attention on these regions. Xin et al. [136] employed a 9-layer CNN\nwith an attention mechanism to assign different weights to face regions, resulting in more\naccurate attention face maps and boosting prediction accuracy by up to 19%. Cui and Huang\n[137] introduced a multi-scale regional attention network (MSRAN), which uses multiple\ncropping regions from video frames. The framework includes self-attention and relation-\nattention modules to highlight pain-relevant regions and explore interrelationships. Li et\nal.[138] extended this concept by integrating contrastive and multi-task training through an\nautoencoder, building on the work of [139].\nOne challenge in pain intensity estimation is that individual facial features, such as face\nshape, can introduce significant variability in how pain is expressed. This makes it difficult\nto distinguish between adjacent intensity levels. To address this, Peng et al. [140] examined\nfacial shape information and developed a deep multi-task network to account for the rela-\ntionship between pain recognition and shape, which improved pain estimation performance.\nSimilarly, Xin et al. [141] proposed a novel multi-task framework that combines a CNN\nfeature learning module with an autoencoder attention component, also estimating subject\nidentity, as individual differences in pain manifestation are key. Their experiments achieved\nstate-of-the-art results on publicly available datasets.\nMost studies report results obtained from controlled laboratory settings, which typically\nfeature proper lighting, minimal head pose variability, and no occlusions. However, such\nconditions do not represent typical hospital environments. Semwal and Londhe [142] ad-\ndressed this by focusing on pain assessment in uncontrolled settings, developing a shallow\nCNN with three convolutional layers that performed comparably to deeper pre-trained mod-\nels. In a subsequent study [143], they introduced a more complex framework comprising\nthree modules that leveraged high-level spatial descriptors with both local and global geomet-\nric cues, achieving results comparable to models like GoogleNet [144] and VGG [121]. Lee\nand Wang [145] explored pain assessment in intensive care unit (ICU) settings, where par-\ntially occluded faces frequently complicate facial analysis. They developed a 4-layer CNN\ncombined with an extreme learning machine (ELM) for final estimation. Virrey and Cae-\nsarendra [146] used CNNs to classify sections of frames where pain was triggered, peaked,\nand subsided. Nugroho et al. [147] tackled pain detection in smart home-care settings, par-\nticularly for elderly patients, using relatively low-power mobile devices. They modified the30 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nOpenFace2library, based on pre-trained FaceNets [148], and showed that transfer learning\ncould enable real-time binary classification ( pain vs.no pain ), even on low-powered hard-\nware.\nResearchers like Dai et al. [149] and Menchetti et al. [150] have noted that most models,\nwhether deep or shallow, are trained on dataset-specific features rather than actual pain-\nrelated features. Moreover, most studies employ validation methods using the same dataset,\nwhile cross-dataset performance is rarely addressed, limiting real-world applicability. To\ntackle these issues, Dai et al. [149] combined pain and emotion detection datasets to develop\na real-time pain assessment system with better generalization capabilities. They emphasized\nthe importance of cross-corpus evaluation, real-time testing, and the need for well-balanced,\necologically valid pain datasets [151].\nSeveral studies have explored combining pain scales to improve prediction objectivity\nand reliability. Liu et al. [152] developed a two-stage personalized model trained using active\nappearance model (AAM) facial landmarks and multi-task learning, with visual analog scale\n(V AS) and observed pain index (OPI) as ground truth. Xu et al. [153] similarly reduced\nmean square error (MSE) by incorporating various pain scales with the VGG-Face model.\nHowever, Casti et al. [154] pointed out the limitations of original ground truth data due to\nsubjectivity and annotation inconsistencies. To address this, they re-annotated their dataset\nwith judgments from multiple experts, using multidimensional scaling to map frames to\nillumination-invariant 3D space, which they then fed into a pre-trained AlexNet [155].\nCelona and Manoni [156] investigated neonatal facial expressions to detect pain, achiev-\ning the highest accuracy when utilizing two pre-trained models: VGG-Face [157] and mapped\nLBP+CNN (MBPCNN) [158]. Similarly, Lu and Hao [159] found that pre-trained models\nwere crucial for small datasets like neonates, as training from scratch led to overfitting. They\nachieved optimal classification performance by fine-tuning the entire VGG-16 model [122].\nHowever, Zamzmi et al. [160] argue that most face recognition methods are tailored for\nadults and thus less applicable to infants. They developed a lightweight 2D CNN trained\nend-to-end and achieved high pain detection accuracy, but external validation on a different\nneonatal dataset revealed challenges with generalizability. In 2019, Brahnam et al. [112]\nintroduced the iCOPEvid neonatal video dataset, a significant contribution since the only\npublicly available neonatal pain dataset [111] previously contained only static images. Their\nexperiments showed that local descriptors based on the bag-of-features (BoF) approach out-\nperformed deep learning models like VGG-Face andResNet . Combining handcrafted and\ndeep-learned features offered only a marginal improvement in performance. In contrast, Za-\nmzmi et al. [161] found that the most effective approach for binary classification (pain vs.\nno pain) was the fusion of high-level features from VGG [162] and optical flow strains, with\n2http://cmusatyalab.github.io/openface3.4. UNIMODAL STUDIES 31\nnaive Bayes serving as the classifier. Celona and Brahnam [163] applied a Wasserstein gen-\nerative adversarial network with gradient penalty (WGAN-GP) [164], demonstrating that\ntraining set augmentation with synthetic samples improved classification performance. Ta-\nble 3.2 summarizes the vision-based studies focusing exclusively on the spatial dimension.32 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [112]F (RGB) texture\ndescriptors- FF 2D CNN`SVM SL C P O 49 k-fold iCOPEvid 79.80 AUC\n'15 [119]F (RGB) - - - AE SVM SeSL,\nSLC P PS 25 LOSO UNBC 86.10 ACC,\n96.50 AUC\n'20 [120]F (RGB) - - FF 2D CNN`NN SL R IC O 36 hold-out EmoPain 0.91 MAE;\n'18 [123]F (RGB) HOG,\nstatistics- FF 2D CNN`SVR SL R IC PS 25 LOSO UNBC 1.44 MSE;\n'21 [130]F (RGB) - - DF 2D CNN`- SL C ID PS 25 k-fold UNBC 93.87 ACC;\n'16 [133]F (RGB) - - - CDBN SVM UL C P PS 25 LOSO:UNBC 87.20 ACC;\n'21 [134]F (RGB) - - - CDBN SVM SL C P PS 25 LOSO UNBC 93.16 AUC\n'19 [135]F (RGB) - - FF 2D CNN - SL C ID1, IC PS 25 LOSO UNBC 88.191ACC\n'20 [136]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 51.10 ACC;\n'20 [140]F (RGB) - - FF 2D CNN`- SL R ID S 25 ? UNBC 79.94 ACC;\n'21 [142]F (RGB) - - - 2D CNN - SL C ID O 8 k-fold other 97.48 ACC;\n'19 [145]F (RGB) - - - 2D CNN ELM SL R IC PS 25 k-fold UNBC\u201a1.22 MSE;\n'19 [146]F (RGB) - - - 2D CNN - SL C TR, CL, DI PS 25 k-fold UNBC 60.00 ACC\n'19 [150]F (RGB) - - - 2D CNN`- SL C AUs-D PS 25, 43 k-fold UNBC & CK+1,\nWilkie97.701ACC;\n'17 [152]F (RGB) statistics - - NN GPM WSL R IC O, S 25 k-fold UNBC 2.18 MAE\n'20 [153]F (RGB) statistics - FF 2D CNN`NN SL R IC S 25 k-fold UNBC 1.95 MAE;\n'19 [154]F (RGB) LBP, MDS - - 2D CNN`- SL C ID O 25 hold-out UNBC 80.00 ACC\n'18 [159]F (RGB) - - - 2D CNN`- SL C ID O ? hold-out other 78.30 ACC\n`: Pre-trained model -:Not exist &: in Dataset indicates the utilization of cross-database training/validation ?: Not found :: The authors provide additional experiments with other validation methods \u201a: The authors\nutilized occluded facial images ;: The authors provide additional metrics Modality: F: face region Non deep features: LBP: local binary pattern MDS: multidimensional scaling Fusion: M: fusion of modalities E:\nfusion of deep learned features or hand-crafted features Deep models: AE: autoencoder RCNN: recurrent convolutional neural network CDBN: convolutional deep belief network CNN: convolutional neural network\nNN: neural network WGAN-GP: Wasserstein generative adversarial model with gradient penalty Non deep model: SVM: support vector machine GPM: Gaussian process regression model kNN: k-nearest neighbors\nNB: naive Bayes ELM: extreme learning machine Learning Method: SL: supervised learning SeSL: semi-supervised learning UL: unsupervised learning WSL: weakly supervised learning Classific./Regres.: C:\nclassification R: regression Objective: P: presence of pain ID: intensity in discrete scale IC: intensity in continuous scale TR: trigger CL: climax DI: diminishing AUs-D: Action Units detection GT: ground truth\nPS: Prkachin and Solomon S: self-report O: observer rating ST: stimulus Validation Method: LOSO: leave one subject out Metrics: AUC: Area Under the ROC Curve ACC: accuracy PPV: precision MSE: mean\nsquared error MAE: mean absolute error3.4. UNIMODAL STUDIES 33Table 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [124]F (RGB) facial landmarks - FF 2D CNN NN SL C, R ID, IC1P 25 LOSO:UNBC 0.171MSE;\n'20 [125]F (RGB) HOG, head pose,\nAUs intensity/\noccurrence, facial\nlandmarksFF - 2D CNN`NN SL R IC O 36 hold-out EmoPain 5.48 RMSE;\n'20 [126]F (RGB) - - - 2D CNN`NN SL R IC O 36 hold-out EmoPain 1.49 RMSE;\n'18 [127]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 92.00 ACC;\n'18 [128]F (RGB) statistics, distance\nmetrics- FF 2D CNN`- SL C, R ID, IC PS 25 LOSO UNBC 0.81 PCC,\n0.69 MSE\n'21 [137]F (RGB) - - FF 2D CNN`- SL C, R ID, IC P 25 LOSO UNBC 91.13 ACC,\n0.78 PCC,\n0.46 MSE\n'18 [138]F (RGB) - - - AE`- SL R IC PS 25 k-fold UNBC 0.33 MAE;\n'21 [141]F (RGB) - - FF [AE, 2D CNN]Y- SL C, R ID1, IC2,\nP3P, ST 25, 87 LOSO UNBC1,\nBioVid (A)289.1711ACC,\n0.8121PCC,\n85.6532ACC,\n40.4012ACC\n'21 [143]F (RGB) entropy texture\ndescriptors- - 2D CNN`- SL C ID O 8 k-fold other 0.92 PPV;\n'18 [147]F (RGB) - - - 2D CNN`- SL C P PS 14 k-fold UNBC 93.00 ACC\n'19 [149]F (RGB) - - - 2D CNN - SL C P PS 25, 20 k-fold UNBC &\nBioVid (A)\u02db56.75 ACC\n'17 [156]F (RGB) HOG, LBP - FF 2D CNN`SVM SL C P O 26 LOSO iCOPE 73.78 ACC\n'19 [160]F (RGB) - - - 2D CNN - SL C P O 31 LOSO NPAD1,\niCOPE296.981ACC;,\n89.802ACC\n'21 [165]F (RGB) - - - 2D CNN`- FL C P PS 25 LOSO UNBC 76.00 ACC;\n'21 [166]F (RGB) - - - 2D CNN`- SL C P O 25 hold-out UNBC 75.49 ACC\n'21 [167]F (RGB) - - - 2D CNN`SVR SL R IC P 25 LOSO UNBC 0.34 MSE\n'21 [168]F (RGB) - - - 2D R-CNN - SL C P O ? hold-out other 87.80 PPV\nY: The authors combined the deep models into a unified framework \u02db: The authors experimented with additional datasets combinations Non deep features: AUs: actions units HOG: histogram of oriented gradients Non\ndeep model: SVR: support vector regression Learning Method: FL: federated learning Metrics: RMSE: root mean squared error34 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [161]F (RGB) optical flow - FF 2D CNN`SVM,\nkNN, NBSL C P O 31 k-fold other 92.71 ACC,\n94.80 AUR\n'19 [163]F (RGB) - - - WGAN-GP - SL C P O 26 LOSO iCOPE 93.38 ACC\n'17 [169]F (RGB) - - - 2D CNN`- SL R IC PS 25 LOSO UNBC 0.99 MAE;\n'20 [170]F (RGB) - - - 2D CNN - SL C ID ST 87 hold-out BioVid (A) 36.60 ACC\n'20 [171]F (RGB) - - - 2D CNN - SL C P PS 25 hold-out UNBC 97.00 PPV;\n'21 [172]F (RGB) - - - 2D CNN - SL C ID P 28 LOSO:UNBC 90.30 ACC\n'19 [173]F (RGB) - - - 2D CNN - SL C P O 31 hold-out NPAD1,\niCOPE291.001ACC;,\n84.502ACC;\n'21 [174]F (RGB) - - - 2D CNN`- SL C P O 26, 30 hold-out iCOPE &\nUNIFESP89.90 ACC;\n'21 [175]F (RGB) - - - 2D CNN - SL C AUs-D P 10 hold-out Pain-ICU 77.00 ACC;3.4. UNIMODAL STUDIES 35\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach)\nPain assessment is particularly challenging due to its complex and dynamic nature. Rely-\ning on static, individual frames to assess pain fails to capture the phenomenon\u2019s temporal\nprogression and often leads to inaccurate estimations. Additionally, many studies highlight\nthe difficulties of applying deep learning techniques to small datasets, with one proposed\nsolution being the combination of deep learning and traditional feature extraction methods.\nEgede et al. [176] addressed this by extracting deep features from a pre-trained CNN, explic-\nitly targeting the eyes and mouth regions. Using a relevance vector regressor (RVR), they\ndemonstrated that combining deep and hand-crafted features led to optimal performance. De-\nspite the valuable insights the UNBC-McMaster database provides, its imbalanced sample\ndistribution\u2014particularly the limited number of frames showing pain\u2014poses a significant\nchallenge for deep learning models. In response, Egede and Valstar [177] devised a method\nbased on the observation that neighboring pain level classes share many common features.\nThis approach allowed them to avoid extracting all possible features for classes with fewer\nsamples, as certain features had already been utilized from other related classes. The study\nalso showed that combining deep and hand-crafted features improved performance. How-\never, in a later study [178], the authors applied a similar approach, using only deep-learned\nfeatures to address data imbalance, but could not replicate the same high-performance levels.\nTavakolian et al. [179] took a different approach, focusing on the detection of genuine\nversus acted pain through facial expressions, a technique with important applications in both\nmedical and forensic contexts. They developed a residual GAN (R-GAN) to capture subtle\nfacial changes and the dynamic nature of expressions, using a weighted spatio-temporal pool-\ning (WSP) method. In a subsequent study [180], the authors suggested that self-supervised\nlearning could reduce the time and effort needed for data labeling, as it does not require\ncomplete dataset annotation. They introduced a new similarity function for learning general-\nized representations with a Siamese network. They also employed statistical spatio-temporal\ndistillation (SSD) based on the Gaussian scale mixture (GSM) to improve computational effi-\nciency. This technique encodes spatiotemporal variations in facial videos into a single RGB\nimage, simplifying the model while maintaining effectiveness.\nOther studies also aim to capture the dynamic aspects of pain. For instance, [181] com-\nbined a random forest classifier with the pre-trained MobileNetV2 model [182], encoding\nvideos by selecting and merging three frames from different time points into a single image.\nOthman et al. [183] emphasized the importance of using diverse datasets\u2014including vary-\ning age, gender, pose, occlusion, and lighting conditions\u2014to improve model generalization.\nThey used multiple data combinations and a reduced version of MobileNetV2 , showing that\ncross-dataset training is essential for achieving better generalizability.36 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.3 Vision-based: Implicit Temporal Utilization\nSeveral studies have explored the application of 3D CNNs for pain assessment. Tavakolian\nand Hadid [184] developed a 3D CNN to capture dynamic facial representations from videos.\nThey noted that researchers often use fixed temporal kernel depths when employing 3D\nconvolution techniques, which limits the ability to capture short, mid, and long temporal\nranges simultaneously. To address this, they designed a model with parallel 3D convolutional\nlayers featuring variable temporal depths, allowing the capture of temporal dependencies\nfrom 32consecutive frames. Similarly, Wang and Sun [185] applied 3D convolutions based\non the architecture proposed in [186], consisting of 8convolutional layers with 3\u02c63\u02c63\nfilters. While they reported high performance, the authors acknowledged that extracting deep\nfeatures from small datasets posed a challenge for model generalization. In a related study,\nHuang et al. [187] developed a framework that integrated 3D, 2D, and 1D CNNs to extract\nspatio-temporal, spatial, and geometric features. For the 3D CNN component, they modified\nthe architecture from [188] by using discrete kernels of 1\u02c63\u02c63and3\u02c61\u02c61rather than the\ntraditional 3\u02c63\u02c63kernel. Other researchers have also proposed 3D deep CNNs with varying\ntemporal depths to capture short, mid, and long-range facial expression variations [189].\nRecognizing the difficulty and time consumption involved in training a deep 3D CNN from\nscratch, they introduced a cross-architecture knowledge transfer learning technique, utilizing\na pre-trained 2D CNN to assist in the training of the 3D CNN. In studies by Praveen et\nal.[190] and [191], the authors employed weakly-supervised domain adaptation, where the\nsource domain focused on human affective expressions and the target domain was explicitly\nrelated to pain expressions. Their framework featured an inflated 3D-CNN (I3D) [192],\nincorporating 3convolutional layers and 3inception modules [132] to capture both spatial\nand temporal information from video data.\nBargshady et al. [193] opted to use the HSV color space instead of RGB, arguing that it\nbetter reflects human visual perception for tasks such as skin pixel detection and multi-face\ndetection. They employed the pre-trained VGG-Face [157] for feature extraction, followed\nby a temporal convolutional network (TCN) using dilated causal convolutional operations to\nleverage temporal dependencies. Rezaei et al. [194] tackled the challenge of pain detection\nin people with dementia, a difficult task due to insufficient pain-related images or videos\nof elderly subjects in existing datasets. They developed a 10-layer 2D CNN that processed\npairs of pain and no-pain images, analyzing frame-to-frame changes and employing con-\ntrastive training methods [195]. The model demonstrated high performance in both healthy\nindividuals and people with dementia. In another study, Pandit and Schmitt [196] explored\nthe potential of using shallow 1D CNN architectures for real-time pain recognition. They ex-\ntracted facial action units from each frame using the OpenFace 2.03toolkit, with promising\n3https://github.com/TadasBaltrusaitis/OpenFace3.4. UNIMODAL STUDIES 37\nresults for pain detection in real-time settings.\n3.4.4 Vision-based: Explicit Temporal Utilization\nSeveral efforts have focused on addressing the limitations of static frames by developing\ndedicated temporal modules. Zhou et al. [197] tackled this issue using a regression frame-\nwork based on a 4-layer recurrent convolutional neural network (RCNN), each with a se-\nquence length of 3time steps. Rodriguez et al. [198] leveraged dynamic information by\ndesigning an LSTM model fed with feature vectors extracted from VGG-16 [122]. Simi-\nlarly, Bellantonio et al. [199] emphasized that facial expressions evolve, making it essential\nto analyze the spatio-temporal dimension of pain. They improved estimation performance\nusing a fine-tuned 16-layer CNN model [157], an LSTM processing 16frames as a time\nwindow, and super-resolution techniques. In another study, Bargshady et al. [200] com-\nbined the VGG-Face CNN [157] with a 3-layer LSTM to extract spatio-temporal features\nfrom grayscale images, applying zero-phase component analysis (ZCA). In [201], principal\ncomponent analysis (PCA) was used to reduce dimensionality. Mauricio et al. [202] also\nemployed VGG-Face but replaced LSTM with a 2-layer gated recurrent unit (GRU) to cap-\nture temporal dependencies. Thuseethan et al. [203] used a conventional 2D CNN and two\nRCNNs to extract temporal features from previous and subsequent frames, enhancing the\ntime dimension of expression analysis.\nA similar approach was followed by Bargshady et al. [204], who employed ensemble\nlearning with three distinct CNN-biLSTM modules, merging their outputs for the final pre-\ndiction. Salekin et al. [205] used a bilinear CNN (B-CNN) based on the VGG architecture\n[121], pre-trained on VGGFace24andImageNet5datasets, along with an LSTM to capture\ntemporal dependencies in image sequences. Kalischek et al. [206] explored deep domain\nadaptation for facial expression and pain detection, utilizing the self-ensembling approach\n[207] with a long-term recurrent convolutional network (LRCN). While they achieved state-\nof-the-art results for facial expression recognition, performance was lower for pain detection,\nlikely due to the subtle nature of pain-related expressions.\nDespite the availability of additional information in pain datasets, multi-task approaches\nremain limited. Martinez et al. [208] proposed a personalized multi-task learning method\nbased on individual physiological and behavioral pain responses. They extracted AAM fa-\ncial landmarks, processed them through a biLSTM to produce PSPI scores, and predicted the\nfinal V AS score. Erekat et al. [209] combined AlexNet [155] with 2 GRU layers to capture\ntemporal dependencies, using both self and observer-reported pain intensity as ground truth.\nVuet al. [210] developed a multi-task framework to estimate pain levels while reconstruct-\n4https://www.robots.ox.ac.uk/ \u02dcvgg/data/vgg_face\n5https://www.image-net.org38 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\ning heatmaps of action unit locations, improving model generalization with a CNN-LSTM\ncombination to capture micro facial movements.\nHuang et al. [211] noted that specific frames within a video sequence exhibit more pro-\nnounced pain expressions, requiring special handling. They developed a novel framework\nusing attention saliency maps with a VGG-16 model, GRUs and learned weights for each\nframe\u2019s contribution to pain intensity estimation. The study demonstrated that dynamic and\nsalient features can significantly improve performance. Similarly, Yu et al. [212] used VGG-\n11 (configuration A) and an LSTM to create an attention mechanism, predicting pain in-\ntensity from 16consecutive frames. Xu and Liu [213] adopted a ResNet-50 model with an\nattention mechanism to extract spatial features, followed by a transformer encoder to capture\ntemporal sequences, achieving promising results.\nIn other studies, Ragolta et al. [214] used extracted action units to train a 2-layer LSTM\npredicting pain on an 11-point scale, employing curriculum learning. Guo et al. [215] devel-\noped a convolutional LSTM (C-LSTM) to extract both spatial and temporal features from\nvideos, showing that temporal models outperform non-temporal models for pain estimation\naccuracy. Rasipuram et al. [216] utilized in-the-wild video data for pain detection, gener-\nating a 3D morphable model without relying on facial landmarks and combining it with an\nLSTM. Zhi and Wan [217] introduced sparse coding with LSTM (SLTM), using the iterative\nhard thresholding algorithm (ISTA) [218] to capture dynamic facial expressions. Although\nSLTM did not achieve high performance, it offers speed and efficiency for specific applica-\ntions. Finally, Thiam et al. [219] developed a method combining motion history and optical\nflow images with a 10-layer CNN and 2-layer biLSTM, showing that weighted score aggre-\ngation improves performance. Table 3.3 summarizes studies incorporating the modalities\u2019\ntemporal dimensions.3.4. UNIMODAL STUDIES 39Table 3.3: Vision-based studies with temporal utilization.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'17 [176] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVR SL R IC PS 25 LOSO UNBC 0.99 RMSE,\n0.67 PCC\n'17 [177] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVM SL R IC PS 25 LOSO UNBC 1.04 RMSE,\n0.64 PCC\n\u201918 [178] F (RGB) - - - NL 2D CNN - SL R IC PS 25 LOSO UNBC 1.20 RMSE,\n0.47 PCC\n'18 [184] F (RGB) - - - I 3D CNN - SL R IC PS 25 LOSO UNBC 0.53 MSE,\n0.84 PCC;\n'18 [185] F (RGB) HOG,\ngeometric\ndifference- DF I 3D CNN SVR SL R IC PS 25 LOSO UNBC 0.94 RMSE,\n0.67 PCC\n'20 [191] F (RGB) - - - I 3D CNN`- WSL R IC PS 24, ? LOSO UNBC\n& RECOLA0.64 MAE,\n0.82 PCC;\n'16 [197] F (RGB) - - FF E RCNN - SL R IC PS 25 LOSO UNBC 1.54 MSE,\n0.65 PCC\n'17 [198] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C, R P, IC1PS 25 LOSO UNBC 0.741MSE,\n0.781PCC;\n'17 [199] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 61.90 ACC\n'19 [200] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 75.20 ACC\n'20 [201] F (RGB) PCA - DF E [2D CNN`,\n1D CNN, biLSTM]Y- SL C ID PS 25 LOSO:UNBC 85.00 ACC;\n'19 [202] F (RGB) - - - E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 85.40 ACC,\n0.62 MSE;\n'19 [203] F (RGB) - - FF E [2D CNN, RCNN]Y- SL R IC PS 25 LOSO UNBC 1.29 MSE,\n0.73 PCC\n'17 [208] F (RGB) - - FF E biLSTM HCRF,\nFCSL C IC O,\nS25 hold-out UNBC 2.46 MAE;\nNon deep features: PCA: principal component analysis Temporal Exploitation: NL: non-machine learning method I: implicit method E: explicit method Deep models: RCNN: recurrent convolutional neural network\nLSTM: long short memory networks biLSTM: bidirectional neural network GRU: gated recurrent unit Non deep models: SVM: support vector machine RVM: relevance vector machine GPM: Gaussian process regression\nmodel HCRF: hidden conditional random fields FC: fully connected SVR: support vector regression Objective: I2: intensity in binary pairs Metrics: PCC: Pearson correlation coefficient40 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [114]F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN RVR SL R IC O 13 LOSO APN-DB 1.71 MAE;\n'19 [179]F (RGB) - - - NL R-GAN - UL C genuine\nvs posedPS,\nST25,\n34,\n87,\n87? UNBC\n& STOIC\n& BioVid (A)\n& BioVid (D)90.97 ACC\n'20 [180]F (RGB) - - FF NL 2D CNN`- SSL C IC P,\nST25\n87LOSO UNBC1,\nBioVid (A)2\u20180.781PCC;,\n71.022AUC;\n'21 [181]F (RGB) AUs\nintensity- H NL 2D CNN`RF SL C ID ST 127 k-fold X-ITE 25.00 ACC\n'19 [183]F (RGB) - - - NL 2D CNN - SL C P ST 87\n134k-fold BioVid (A)\n& X-ITE\u201867.90 ACC\n'20 [209]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC O,\nS25 k-fold UNBC 2.34 MAE\n'20 [211]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC PS 19 LOSO UNBC 0.21 MSE,\n0.89 PCC\n'19 [212]F (RGB) - - FF E [2D CNN, LSTM]Y- SL R IC PS 24 LOSO UNBC 1.22 MSE;,\n0.40 PCC;\n'20 [214]F (RGB) AUs\nintensity- - E LSTM - SL R IC O 36 hold-out EmoPain 2.12 RMSE,\n1.60 MAE;\n'20 [216]F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C P O ? k-fold UNBC 78.20 ACC;\n'20 [219]F (RGB) - - DF E [2D CNN, biLSTM,\nNN]Y- SL C P ST 87\n40LOSO BioVid (A)1,\nSenseEmotion269.251ACC,\n64.352ACC\n'20 [220]F (RGB) - - FF E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 0.84 ACC,\n0.69 PCC;\n\u2018: The authors provide experiments with cross-dataset settings Fusion: H: hybrid Non deep models: RF: random forest classifier3.4. UNIMODAL STUDIES 41Table 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [187]F (RGB) facial\nlandmarks- DF I [3D CNN`,\n2D CNN`,\n1D CNN, FC]Y- SL R IC PS 25 LOSO UNBC 0.76 MSE,\n0.82 PCC;\n'19 [189]F (RGB) - - - I [2D CNN`,\n3D CNN]Y- UL,\nSLC, R IC1, P2P,\nST25, 87 LOSO UNBC1,\nBioVid (A)20.9211PCC;,\n86.0222AUC\n'20 [190]F (RGB) - - - I 3D CNN`- WSL R IC PS 24,?,\n87, 18LOSO UNBC1\n& RECOLA\n& BioVid (A)2\u20180.741PCC,\n0.342PCC\n'20 [193]F (RGB) PCA - FF I [2D CNN`,\nTCN]Y- SL C ID P,\nST25, 20 LOSO:UNBC1,\nMIntPAIN292.441ACC;,\n89.002ACC;\n'20 [194]F (RGB) - - - I 2D CNN - SL C, R IC, P1P 95, 25 k-fold UofR & UNBC182.0011PCC;\n'20 [196]F (RGB) AUs\noccurrence- FF I 1D CNN - SL R IC P 24, 87 hold-\noutUNBC1,\nBioVid (A)0.801CCC\n'20 [204]F (RGB) PCA - DF E [2D CNN`, 1D\nCNN, biLSTM]Y- SL C ID PS,\nST25, 20 k-fold UNBC1,\nMIntPAIN286.001ACC;\n92.262ACC;\n'20 [205]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- SL R P, IC1O 45 LOSO NPAD 3.991MSE,\n1.552MAE\n'19 [206]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- UL C P ST 40 LOSO SenseEmotion 60.61 ACC\n'21 [210]F (RGB) - - - E [2D CNN`,\nLSTM]Y- SL R IC P 25, 27 LOSO UNBC1,\nDISFA\u20180.60`MSE,\n0.82`PCC;\n'21 [213]F (RGB) - - - E [2D CNN`,\nTransformer]Y- SL R IC P 25 LOSO UNBC 0.40 MSE,\n0.76 PCC;\n'21 [215]F (RGB) - - - E 2D C-LSTM - SL C ID S 29 hold-\noutother 69.58 F1\n'19 [217]F (RGB) - - FF E SLSTM - SL C P1, ID2ST 85 LOSO BioVid (A) 61.701ACC\n29.702ACC\n'21 [221]F (RGB) - - - I 3D CNN`- SL R IC S 25 k-fold UNBC 0.66 ICC;\nFusion: H: hybrid Deep models: TCN: temporal convolutional neural network C-LSTM: convolutional-LSTM SLTM: sparse long short memory network Learning Method: SSL: self-supervised learning Metrics: F1:\nF1 score CCC: concordance correlation coefficient42 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.5 Touch sensor-based\nTouch (contact) sensors provide a viable alternative for pain assessment, often outperforming\nvision-based methods. Table 3.4 highlights studies that utilized contact sensor data to evalu-\nate pain. Yu et al. [222] analyzed three categories of pain-no pain, moderate pain, and severe\npain\u2014using EEG signals. They extracted several bands from the biosignals, including al-\npha, beta, and gamma, and applied a convolutional module. The study found that combining\nthese bands yielded better results than evaluating them independently. Similarly, [223] used\nEEG potentials with an autoencoder to compress the raw data and applied a logistic regressor\nfor classification.\nOther researchers, such as Rojas et al. [224], utilized functional near-infrared spec-\ntroscopy (fNIRS) for pain detection. They developed three models\u2014multilayer perceptron\n(MLP), LSTM, and biLSTM\u2014with biLSTM demonstrating superior accuracy. Addition-\nally, [225] focused on PPG signals, extracting hand-crafted features from the time and fre-\nquency domains, which were then combined with a deep belief network (DBN) to achieve\nover65% accuracy in a 4-class pain assessment task. Hu et al. [226] used kinematic data\nto compare healthy individuals with those suffering from low back pain (LBP). Their ap-\nproach, which employed two stacked LSTM layers, reached over 97% accuracy in binary\nclassification using raw motion data. Lastly, Mamontov et al. [227] were the first to apply\nevolutionary algorithms in the design of an optimized recurrent neural network (RNN) for\npain estimation, achieving 91.94% accuracy using EDA signals.\n3.4.6 Audio-based\nA few studies have explored using audio information for pain detection and intensity esti-\nmation, as outlined in Table 3.5. These methods are especially relevant for neonates, where\nfrequent facial and body occlusions make analyzing cries a more effective approach for pain\ndetection. Chang and Li [228] concentrated on infant cries to differentiate between hunger,\npain, and sleepiness. They transformed the audio signals into 2D spectrograms using a fast\nFourier transform (FFT) and trained a 2D CNN for feature extraction. Similarly, [229] uti-\nlized spectrograms generated from recorded sounds, employing a model identical to that\nused in [160]. Thiam and Schwenker [230] focused on detecting adult pain by analyzing\nbreathing sounds. They leveraged deep-learned features from spectrograms with Mel-scaled\nshort-time Fourier transform, combined with various handcrafted cues. A CNN followed by\na biLSTM was used to capture spatial and temporal dependencies, integrating both low- and\nhigh-level features. In a different approach, Tsai et al. [231] examined pain events during\nemergency triage. They developed an LSTM autoencoder framework to extract temporal\nfeatures from verbal behavior, reporting encouraging results.3.4. UNIMODAL STUDIES 43Table 3.4: Touch sensor-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'20 [222]EEG - - FF I 1D TCN - S C ID S 32 k-fold other 97.30 ACC;\n'20 [223]EEG - - - I AE (TCN) LR UL, S C P S 29 LOSO other 74.60 ACC\n'21 [224]fNIRS - - - E biLSTM - SL C ID S 18 k-fold other 90.60 ACC;\n'19 [225]PPG - - - NL DBN SBM U, SL C P1, ID2S 100 k-fold other 86.791ACC,\n65.572ACC\n'18 [226]kinematatics - - FF E LSTM - SL C P LBP 44 LOSO other 97.20 ACC;\n'19 [227]EDA - - FF E [RNN, LSTM,\nGRU, NN]YSelfCGA,\nselfCGP,\nPSOPBSL C P ST 40 LOSO Sense-\nEmotion81.94 ACC\n'21 [232]EDA - - - I NN - SL C P1, I2 ST 87,\n55LOSO BioVid (A)1,\nPainMonit284.2211ACC;,\n86.5012ACC;\nModality: PPG: photoplethysmogram fNIRS: functional near-infrared spectroscopy EEG: electroencephalography EDA: electrodermal activity Deep models: DBN: Deep belief network RNN: recurrent neural network\nNon deep models: SBM: selective bagging model LR: Logistic Regression SelfCGA: Self-Configuring Genetic Algorithm SelfCGP: Self-Configuring Genetic Programming PSOPB: Particle Swarm Optimisation with\nparasitic behaviour GT: LBP: low back pain vs healthy population\nTable 3.5: Audio-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'16 [228]audio (cry) - - - - 2D CNN - SL C P O ? k-fold other 78.50 ACC\n'19 [229]audio (cry) - - - - 2D CNN - SL C P O 31 LOSO:NPAD 96.77 ACC;\n'19 [230]audio\n(breathing)MFCCs,\nRASTA-\nPLP,\nDTD- FF E [2D CNN,\nLSTM]YRFc SL C P ST 40 LOSO Sense-\nEmotion64.39 ACC\n'17 [231]audio\n(voice)prosodic-\nspectral\nfeatures,\nSF- FF E LSTM`SVM UL,\nSLC P1, ID2S 63 LOSO other 72.301UAR,\n54.202UAR\nNon deep features: MFCCs: Mel Frequency Cepstral Coefficients RASTA-PLT: Relative Spectral Perceptual Linear Predictive DTD: descriptors from temporal domain SF: statistical features44 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.5 Multimodal studies\nSince pain is a multidimensional phenomenon, combining multiple modalities in a multi-\nmodal system offers a promising approach. Heterogeneous information sources can com-\nplement one another, enhancing specificity and sensitivity. As reported in [106], when in-\ndividual modalities demonstrate good predictive performance, their fusion tends to yield\nimproved outcomes. Moreover, integrating cues from various channels may be helpful and\nnecessary, especially in clinical settings where specific modalities may become unavailable\n(for instance, if the patient turns and their face is occluded). The information channels can\noriginate from (1) the same hardware sensor but focus on different regions of interest, such\nas RGB facial images and RGB body images [233], (2) different hardware sensors but the\nsame region of interest, like RGB facial images and thermal facial images [110], or (3)\ndifferent hardware sensors and information sources, such as RGB facial images and ECG\nsignals [234]. Table 3.6 lists the studies utilizing multimodal approaches.\n3.5.1 Static Analysis\nA commonly used biosignal combination is those of EDA, EMG, and ECG, as these channels\nare found in all main pain reference databases. Thiam et al. [235] applied an early fusion\nmethod by merging these signals into a 2D representation and inputting it into a 9-layer 2D\nCNN. Their results showed a strong correlation between EDA and pain intensity, and com-\nbining all three modalities did not outperform using EDA alone. Al-Qerem et al. [236] used\nleast generative adversarial networks (LSGANs) to enhance EMG, EDA, and ECG samples,\nreporting a notable improvement in classification when using an SVM on the augmented\ndataset. Haque et al. [110] introduced the MIntPAIN dataset, which includes RGB, depth,\nand thermal videos for multi-class ( 5levels) pain recognition. They combined these three vi-\nsual modalities into a 5D matrix (RGB+D+T) and used it to train the pre-trained VGG-Face\nmodel [157], leading to better classification performance in their experiments.\n3.5.2 Temporal Utilization\nZhiet al. [237] proposed a multimodal stream-integrated neural network that leverages video\nand biosignal data. They combined raw facial video frames with optical flow images to cap-\nture spatio-temporal dependencies via 3D CNNs, integrating these with biosignal features\nextracted using LSTMs. The entire network was trained end-to-end, achieving superior re-\nsults compared to their unimodal methods. Beyond facial analysis, Salekin et al. [233]\nfocused on assessing neonatal pain through body movements in videos. After identifying\nrelevant body regions, video frames were fed into a pre-trained VGG-16 [121], connected to\nan LSTM to capture temporal dynamics. In a follow-up study, Salekin et al. [238] fused three3.5. MULTIMODAL STUDIES 45\nmodalities\u2014facial expressions, body movements, and crying sounds\u2013demonstrating that this\nmultimodal approach outperformed unimodal techniques. Similarly, Wang et al. [239] ex-\nplored combining EMG, EDA, and ECG biosignals with handcrafted and learned features\nfrom a biLSTM model. They applied the minimum relevance method (MRMR) to reduce\nthe number of features, resulting in notable outcomes.\nIn addition to EDA, EMG, and ECG, other biosignal combinations have been explored.\nZhao et al. [240] integrated PPG, EDA, and temperature signals, using 2D convolutions for\nspatial feature extraction and time windows for capturing temporal information. Yuan et\nal.[241] successfully estimated pain using whole-body MoCap sensors and EMG, utilizing\nLSTM layers with an attention mechanism in an autoencoder, which reduced training time\nby leveraging latent space representations of raw data. Similarly, Li et al. [242] employed\nMoCap and EMG as data sources and tested various LSTM configurations to predict pain\nintensity, achieving the best performance with a 3-layer vanilla LSTM combined with a 3-\nlayer fully connected network.46 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.6: Multimodal-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [110]F (RGB,\nthermal,\ndepth)- RF - - 2D CNN`- SL C ID S 20 k-fold MIntPAIN 36.55 ACC\n'19 [233]F, B (RGB) - FF - E [2D CNN`,\nLSTM]Y- SL C P O 31 LOSO other 92.48 ACC;\n'19 [234]F (RGB),\nECG, EDAbiosignals\u2019\nfeaturesmFF FF - 2D CNN`RFc SL C I2 S 85 k-fold BioVid (A) 74.00 ACC\n'19 [235]EDA,\nEMG,\nECG- RF - - 2D CNN - SL C P1I2,\nID2S 87,\n86LOSO BioVid (A)1\nBioVid (B)84.4011ACC;,\n36.5412ACC;\n'20 [236]EDA,\nEMG,\nECGBoruta\nfeaturesFF - - LSGAN SVM UL,\nSLC I2, ID1S 85 hold-\noutBioVid (A) 82.801ACC\n'21 [237]F (RGB),\nEDA,\nEMG, ECGoptical\nflowFF FF NL,\nE, I[3D CNN,\nLSTM]Y- SL C, R P1, I2,\nID2S 87,\n40k-fold:BioVid (A)1,\nMIntPain68.2011ACC;,\n28.1021ACC\n'21 [238]F, B (RGB),\nsound- DF - E [2D CNN`,\nLSTM]Y- SL C P O 45 LOSO NPAD 78.95 ACC;\n'20 [239]EDA,\nEMG,\nECGMRMR,\nbiosig-\nnals\u2019\nfeaturesRF\nFFE biLSTM NN SL C P1, I2 S 87 LOSO BioVid (A) 83.301ACC\n'20 [243]EDA,\nEMG,\nECG- FF - I [DDCAE,\nNN]Y- UL,\nSLC P1, I2 S 87 LOSO BioVid (A) 83.991ACC;\n'21 [244]EDA,\nEMG,\nECG, RSP- FF - I [DDCAE,\nNN]Y- UL,\nSL,\nSSLC, R P1, ID2,\nICS 87,\n40LOSO BioVid (A)1,\nSense-\nEmotion84.2511ACC;,\n35.4421ACC;\n'21 [245]EDA, ECG - FF - E 1D CNN,\nLSTM- UL C P1, I2 S 67 hold-\noutBioVid (A) 81.711ACC\n'20 [240]PPG, EDA,\ntemperature- RF - I 2D CNN - SL R\u02ddP1, ID2S 21 k-fold other 96.301ACC,\n95.232ACC\n'20 [241]MoCap,\nEMG- RF - E AE, LSTM - UL,\nSLC ID O 23 LOSO:EmoPain 52.60 ACC;\n'20 [242]MoCap,\nEMG- RF - E LSTM, NN - UL C ID O 30 hold-\noutEmoPain 80.00 ACC;\n'21 [246]MoCap,\nEMG- RF - E LSTM, NN - SL C ID O 30 LOSO:EmoPain 54.60 ACC;\nm: Not specifically described \u02dd: Ordinal Modality F: face region B: body region EMG: electromyography Non deep features: MRMR: Minimum Redundancy Maximum Relevance method Deep models: LSGAN:\nLeast Square Generative Adversarial Networks3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 47\n3.6 Summary of Automatic Pain Assessment Methods\nThis section presents an analysis of the reviewed studies, summarizing the main conclusions\non current methods for automatic pain assessment, their advantages, and corresponding lim-\nitations. Additionally, it offers recommendations for future research directions that could\nadvance the field of pain research from a computational perspective.\n3.6.1 Input\nFirst, we observe a clear imbalance between unimodal and multimodal approaches in pain\nassessment studies. More than 86% of the reported research focuses on unimodal methods,\neven though the databases often contain multiple information channels. Notably, contact\nsensor-based and audio-based approaches are underrepresented, with only seven and four\nstudies, respectively, compared to 84studies that utilize a vision-based approach.\nMultimodal approaches are even less explored, with only 15studies falling into this\ncategory, making it difficult to draw strong conclusions about the effectiveness of specific\nmodality combinations. However, there are indications that EDA sensor data is particularly\nvaluable compared to other biopotentials. Researchers have primarily focused on visual data,\nlikely due to the complexity of implementing multimodal frameworks or the impracticality\nof contact sensors in non-laboratory settings. Further exploration of diverse modality com-\nbinations is necessary to evaluate their potential for pain assessment fully\u2014additionally, 28\nstudies employed non-deep features to enhance deep-learned representations.\nFinally, we identified three primary strategies in examining the approaches that utilize\ntemporal information: non-machine learning-based, machine learning-based (implicit), and\nmachine learning-based (explicit). Non-machine learning-based methods, such as motion\nhistory images [219] or temporal distillation [180], rely on traditional computer vision tech-\nniques. These methods tend to be more straightforward but are generally less sophisti-\ncated. In contrast, machine learning-based approaches [190] [217] offer richer temporal\ninformation and the flexibility to adapt to specific requirements, such as emphasizing certain\nvideo frames. Among the studies reviewed, 55% employed temporal features, with explicit\nmethods\u2014most commonly LSTM models\u2014being the predominant choice. Given that many\nstudies report superior performance when temporal information is incorporated, compared\nto non-temporal methods, it is evident that further emphasis on temporal approaches is war-\nranted.\n3.6.2 Processing\nRegarding machine learning approaches, various models and techniques have been employed\nfor pain estimation. CNN models remain the most widely used, with more than 75% of stud-48 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nies utilizing 1D, 2D, or 3D filters, highlighting the central role of convolution operations\nin deep learning. Sequential models, such as RNNs, GRUs, LSTMs, and biLSTMs, follow\nclosely behind in popularity. Almost half of the studies used pre-trained models to achieve\ntheir desired performance. This suggests that existing pain databases may not be adequate\nfor training deep-learning models from scratch. Non-deep learning models have also been\nemployed in 26studies as auxiliary decision components, with SVMs and shallow neural net-\nworks being the most common choices. There seems to be significant potential for adopting\nnewer deep learning architectures, especially transformer-based models, which have demon-\nstrated state-of-the-art results in various AI research fields and are particularly suited for\nexploiting temporal modality information [247].\nThe predominant learning method used across studies is supervised learning. How-\never, 16papers explored or adopted alternative methods such as unsupervised learning [119,\n133, 179, 189, 206, 223, 225, 231, 236, 241, 243], self-supervised [180, 244], self-supervised\nlearning [180, 244], semi-supervised learning [119], weakly supervised learning [190, 191],\nand federated learning [165]. Given the limited availability of pain data resources, self-\nsupervised learning appears to be the most appropriate method for future research and should\nbe further embraced by the community.\nLastly, it is notable that most studies\u2014approximately 70%\u2014treat pain assessment as a\nclassification problem rather than a regression problem. However, we believe that regres-\nsion more closely reflects the continuous nature of pain and is better suited to capturing the\ncomplexity of pain sensation.\n3.6.3 Evaluation\nThe primary objectives of the reviewed studies were (i)to estimate pain intensity on a dis-\ncrete scale (multi-class classification), (ii)to measure pain intensity on a continuous scale,\nand(iii)to determine the presence or absence of pain (binary classification). Notably, 25\nstudies focused on pain detection rather than pain intensity estimation, which, from a clin-\nical standpoint, is less informative as it does not provide sufficient data for effective pain\nmanagement. From an engineering perspective, detecting the presence or absence of pain is\nalso a more straightforward and less demanding task.\nA small subset of studies took a different approach to pain estimation. For instance, one\nstudy [179] sought to differentiate genuine pain from acted pain. Another [231] explored\npain events in emergency triage settings rather than controlled laboratory environments,\nwhile [234] examined the feasibility of real-time pain detection on IoT devices. Addition-\nally, [142] and [143] aimed to address the issue of occluded faces in pain estimation. So-\nciodemographic and psychological factors were also considered, as seen in studies like [245],\nwhich explored gender differences, and [194], which focused on pain assessment in elderly3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 49\npatients with dementia. The limited exploration of pain estimation in real-world settings\nor unconventional contexts suggests that current approaches may not be fully applicable in\npractical environments like clinics and hospitals.\nVarious annotation types are used regarding ground truth, such as self-reported ratings,\nFACS, and observer scales. Temporal features are critical for accurately estimating pain\nintensity, making the temporal granularity of the ground truth equally important. Several\nstudies have questioned the objectivity of PSPI scores, as noted in [248], which highlights\nthat PSPI scores can be zero even when pain is present or that there may be no visible facial\nexpressions in low-intensity pain. Pain expressions not captured by the FACS system, such\nas raising eyebrows or opening the mouth, further challenge the use of PSPI [249]. Addi-\ntionally, PSPI does not account for pain-related head and body movements, which are par-\nticularly valuable in newborn assessments [250]. For these reasons, we recommend moving\naway from PSPI as ground truth in favor of self-reports and observer scales at the video-\nsegment level.\nAround 54% of the studies employed the leave-one-subject-out (LOSO) validation method,\nwhich is widely regarded as more objective and better for assessing the generalizability of\nmodels. However, LOSO can be less practical due to the increased model size and longer\ntraining times. When researchers use other validation methods, such as k-fold or hold-out,\nit is essential to ensure that consecutive, highly correlated frames from the same subject do\nnot skew the training and validation results, leading to flawed estimations. Moreover, when\nresearchers define their own validation or testing sets, comparing results across studies\u2014\nespecially between classification and regression models\u2014becomes nearly impossible. We\nbelieve standardized evaluation protocols should be developed for each publicly available\ndatabase for these reasons.\n3.6.4 Pain Databases for Evaluation\nThe availability of suitable public databases is arguably the most crucial factor in addressing\nthe challenge of automatic pain assessment. Several aspects must be considered in evaluating\nthese datasets, including the number of subjects and their characteristics, such as age, sex,\nhealth status, and race. Moreover, the ground truth must be objective and offer meaningful\ninsights into the subject\u2019s pain experience [154].\nFig. 3.1 illustrates the number of papers corresponding to the pain database utilized in\neach study. It is clear from this figure that the UNBC andBioVid databases were the most\ncommonly used public datasets. However, the UNBC dataset does not record the subjects\u2019\nages, despite age being a known factor in pain expression [35,66]. While the BioVid dataset\ndoes document age, the oldest participants are only 65years old, which is notable since pain\nand its management are critical issues among individuals aged 65and older [251]. Simi-50 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nlar limitations are found in other pain datasets, such as X-ITE [117], EmoPain [115], and\nSenseEmotion [116].\nIt is well known that aging causes skin changes, including texture, rigidity, and elastic-\nity alterations, which can impact facial emotion recognition tasks [78]. Additionally, race-\nrelated factors can lead to inaccurate pain assessments due to variations in how pain is ex-\npressed [252]. Notably, one study by Nerella et al. [175] reported lower performance when\ntheir model was tested on African American patients. Furthermore, only one study [194]\nwas found that specifically addressed pain estimation in elderly individuals with dementia.\nIn summary, developing objective, automated, and generalizable deep learning-based\npain assessment systems will only be possible if balanced and representative datasets are\navailable for training and external validation.\n3.6.5 Interpretation of Results\nRecent advancements in AI have shown state-of-the-art performance across nearly every\nscientific discipline, often surpassing human accuracy in specific diagnostic tasks [253].\nHowever, a significant drawback of AI solutions, particularly deep neural networks, is their\nlack of transparency, commonly called \u201cblack box AI\u201d. This term highlights how these\nmodels learn intricate functions that are opaque and frequently incomprehensible to hu-\nmans [254]. This opacity is a primary reason for the criticism directed toward deep learning\ntechniques [255]. Various techniques, such as visualizations and gradients-backpropagation\nfocusing on specific units, have been developed to offer insights into how these models func-\ntion. For further reading, refer to the comprehensive review on explanatory techniques in\ndeep learning [256].\nTable 3.7 outlines the different approaches used to interpret model decisions. Only a\nsmall fraction of the reviewed studies\u2014 20out of 110\u2014implemented methods to explain\nhow their models work and which features or elements they focus on. It is important to\nnote that interpretable machine learning can be broadly defined as the \u201cextraction of rele-\nvant knowledge from a machine-learning model concerning relationships either contained\nin data or learned by the model\u201d [257]. To summarize: (i)18% of the reviewed studies\nprovided an approach to enhance the interpretability of the model\u2019s decision, (ii)all of these\nmethods were applied to studies using facial images as the input modality, and (iii)around\nhalf of these studies were conducted by just three specific research groups. These findings\nsuggest that the issue of interpretability and explainability within deep learning remains un-\nderexplored, particularly in the context of automatically classifying pain severity levels.3.7. CHALLENGES AND FUTURE DIRECTIONS 51\nTable 3.7: Interpretation approaches.\nPaper Year Modality Method\n[124] 2021 F (RGB) visualization (saliency maps)\n[128] 2018 F (RGB) visualization (heat maps)\n[130] 2021 F (RGB) visualization (saliency map)\n[133] 2016 F (RGB) visualization (learned filters)\n[134] 2021 F (RGB) visualization (learned filters)\n[135] 2019 F (RGB) visualization (heat maps),\nvalues of learned weights\n[138] 2018 F (RGB) visualization (saliency maps)\n[141] 2021 F (RGB) visualization (attention maps)\n[142] 2021 F (RGB) visualization (saliency map)\n[143] 2021 F (RGB) visualization (activation maps)\n[153] 2020 F (RGB) visualization (pixels contributions)\n[177] 2017 F (RGB) visualization (average saliency map)\n[179] 2019 F (RGB) visualization\n(generated intermediate representation)\n[194] 2020 F (RGB) visualization (saliency maps)\n[196] 2020 F (RGB) weights per AU (contribution of AUs)\n[173] 2019 F (RGB) visualization (feature maps)\n[174] 2021 F (RGB) visualization (integrated gradients)\n[210] 2021 F (RGB) visualization (heatmaps)\n[211] 2020 F (RGB) visualization (attention maps),\nvalues of learned weights\n[212] 2019 F (RGB) visualization (attention maps)\n3.7 Challenges and Future Directions\nThis section discusses the existing challenges in automatic pain assessment and proposes\nfuture research directions to further progress in the field.\n3.7.1 Current Challenges in Automatic Pain Assessment & Future Research Direc-\ntions\nSeveral limitations exist in the current pain databases. Important demographic factors such\nas sex, gender, and age are often missing, and there is an apparent lack of racial diversity\namong subjects. For example, facial structures and emotional expressions vary across Cau-\ncasian, Asian, and African populations [258]. Moreover, social interactions, such as the\npresence of a partner during assessments, could influence pain manifestation and should\nbe included in future datasets [69]. Estimating the location of pain, particularly for infants\nor individuals with communication impairments, is another vital aspect of pain assessment52 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsystems, which current databases largely overlook. Future datasets should incorporate stim-\nuli targeting various body locations. Furthermore, the videos in existing visual databases\noften have low to medium resolution and frame rates, which are inadequate for capturing\nfacial micro-expressions. Audio data is also sparsely represented, though it holds potential\nas a valuable modality. From an audio perspective, integrating natural language processing\n(NLP) methods to extract linguistic features and create multimodal systems is a promising\ndirection, as shown in affective computing research [259]. Finally, specific validation proto-\ncols should be provided with present and future datasets to ensure objective and consistent\ncomparisons across studies.\nFrom an engineering perspective, several issues must be addressed to advance automatic\npain assessment. Developing multimodal approaches is essential for creating robust systems\nwith enhanced capabilities. Not only do multimodal methods demonstrate better perfor-\nmance than unimodal ones, but they are also crucial in real-world scenarios where a specific\nmodality may become unavailable. Additionally, it is essential to exploit each modality\u2019s\ntemporal aspects fully. We encourage using machine learning models or other techniques\nthat can accommodate the dynamic nature of pain. More work is needed to improve the accu-\nracy of multi-level and low-intensity pain estimation. Another area of research involves the\nrelationship between pain and other affective states, such as negative emotions, which often\ncoexist during painful events. Detecting these emotions could improve pain assessment. Ad-\ndressing challenges like occlusions or poor lighting conditions in vision-based systems also\nrequires attention. Researchers should explore these scenarios, even if current databases do\nnot account for them. Real-time application of pain assessment systems is another critical\nfactor, so future studies should measure throughput, such as the number of images processed\nper second during inference. Generalization is another crucial concern for AI systems, and\nevaluating trained models across different pain databases could be valuable. Finally, to facil-\nitate the clinical adoption of AI-based pain assessment systems, the models\u2019 decisions need\ngreater explainability. Developing or adopting methods that improve interpretability will\nenhance their clinical viability.Chapter 4\nDemographic Variables: Their Role and\nImpact\nContents\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.1", "Video Analysis with Vision Transformers": ". . . . . . . . . . . . . . . . . . 77\n5.2.1", "Video & Heart Rate Analysis with Transformer Architectures": "We introduce a proof of concept for an automatic pain assessment framework that integrates\nfacial video data captured by an RGB camera with heart rate signals. We build and extend\nour previous analysis in 5.2. Our main objectives include (1) evaluating the effectiveness and\nlimitations of video and heart rate data as standalone modalities in an unimodal setting, (2)\nexploring the efficacy of combining behavioral (video) and physiological (heart rate) markers\nto overcome challenges associated with their reliance on different sensing technologies and\ninformation representations, and (3) analyzing the performance and efficiency of recently\nintroduced transformer-based architectures.\n5.3.1", "Synthetic Data: The Role of Thermal Imaging": "103\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n7 General-Purpose Models 117\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n8 Conclusions, Perspectives and Future Work 157\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nBibliography 163\nAppendix 201\nAcronyms 209List of Figures\n2.1 The spinothalamic tract (STT) [43]. Pain, temperature, and some touch affer-\nents end in the posterior horn, where second-order fibers cross the midline\nto form the spinothalamic tract, ascending to the thalamus and projecting to\nvarious cortical areas. Along the way, collaterals connect to the reticular for-\nmation. Due to the rostral inclination of fibers in Lissauer\u2019s tract, cordotomy\nmust be performed several segments above the pain level for effective relief. 12\n2.2 Pain classification [48]: (A)Nociceptive pain , which results from detecting\npotentially harmful stimuli and serves a protective function. (B)Inflamma-\ntory pain is linked to tissue damage and immune cell infiltration, increas-\ning pain sensitivity during healing. (C)Pathological pain is a disease state\ncaused by either nervous system damage (neuropathic) or abnormal nervous\nsystem function (dysfunctional). . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 The number of studies utilizing these specific datasets. Note that various\nstudies used multiple datasets to conduct their experiments. . . . . . . . . . 25\n4.1 The PQRST waveform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure. . 55\n4.3 The signal preprocessing using the Pan-Tompkins algorithm. . . . . . . . . 57\n4.4 Results for the Gender Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Results for the Age Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.6 Results for the Gender-Age Scheme . . . . . . . . . . . . . . . . . . . . . . 64\n4.7 The proposed MTL network: The sizes of the extracted vectors for the net-\nwork are as follows: for the Pain classifier, n\u02c61, where nis the number of\npain estimation tasks ( e.g.,2for binary classification, 5for multi-class clas-\nsification); for the Age classifier, 36\u02c61, where 36represents the possible\nage values of the subjects; for the Gender classifier, 2\u02c61, corresponding to\nthe two possible gender categories ( i.e., males and females). . . . . . . . . 66\n4.8 Results for the proposed Schemes. . . . . . . . . . . . . . . . . . . . . . . 69\n4.9 Comparison of performances utilizing various neural networks approaches. 72\nxix5.1 The application of face alignment illustrates landmarks in 2D (left) and 3D\n(right) space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 An overview of our proposed transformer-based framework for automatic\npain assessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 The impact of the number of input frames on accuracy (left) and on runtime\nin milliseconds (right). Runtime calculated during inference on a NVIDIA\nRTX-3090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Relevance Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5 Outline of the proposed framework. . . . . . . . . . . . . . . . . . . . . . 86\n5.6 Comparison of mean accuracy and inference period for unimodal and multi-\nmodal strategies across NP versus P 4and MC tasks. The diagram adopts a\ndual-y-axis configuration\u2014accuracy measurements on the left and time met-\nrics on the right\u2014to outline the balance between performance efficacy and\ncomputational load, categorizing the methodologies along the x-axis. . . . . 98\n5.7 Regions highlighted in yellow and red denote areas of significant attention.\n(a) (1strow) Sequence of original frames. (2ndrow) Derived from the\nSpatial-Module after initial stage pretraining. (3rdrow) Derived from the\nSpatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module trained on the BioVid dataset. (b) (1strow) Derived from\ntheTemporal-Module incorporating video embeddings. (2ndrow) Derived\nfrom the Temporal-Module with heart rate embeddings. (3rdrow) Derived\nfrom the Temporal-Module using a combined embedding of video and heart\nrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.1 Illustration of the procedure for creating thermal images, featuring the archi-\ntecture of the Generator G(Encoder, mid-stage ResNet, Decoder), and the\nDiscriminator D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2 Representation of the proposed framework, illustrating its components and\ntheir main functions: (a)The Vision-MLP module, tasked with extracting\nfeature embeddings from video frames. (b)TheToken-Mixer , an important\nsub-module of Vision-MLP , generates the wave representation for the tokens.\n(c)The Channel-Mixer , a crucial sub-module within Vision-MLP .(d)The\nMLP, a core component of the Channel-Mixer .(e)The fusion procedure\nthat combines RGB and synthetic thermal embeddings, succeeded by the\nTransformer module, which conducts the final pain assessment. . . . . . . . 107\n6.3 Gradual blurring of RGB and synthetic thermal facial images: a series dis-\nplaying varying levels of Gaussian blur applied, with kernel sizes gradually\nincreased from k\u201c0(no blur) to k\u201c191(extensively blurred). . . . . . . 1136.4 Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain)\nclasses in RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191\n(heavily blurred). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1 PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer hav-\ning varying depths, showcasing how token resolution decreases at each stage;\n(b)Composition of the Token-Mixer module, featuring elements like depth-\nwise convolution (DWConv) and batch normalization; (c)Architecture of the\nFeed-Forward Network (FFN) within the Token-Mixer ;(d)The Cascaded\nAttention mechanism implemented across multiple heads, illustrating how\noutputs from preceding heads are incorporated to refine the self-attention\nprocess, culminating in the final output projection; (e)Configuration of the\nproposed multimodal pipeline, employing videos and fNIRS. The embed-\ndings from PainViT\u20131 are represented as waveform diagrams, which are\nmerged into a single diagram that illustrates both modalities before entering\nPainViT\u20132 for final pain evaluation. . . . . . . . . . . . . . . . . . . . . . . 151\n7.2 Waveform illustrations for various data types: (a)original fNIRS signal,\n(b)video embedding derived from PainViT\u20131 , and (c)fNIRS embedding\nobtained from PainViT\u20131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n7.3 Attention maps from the PainViT\u20132 . . . . . . . . . . . . . . . . . . . . . . 152\n7.4 Overview of primary models and their components outlined in this research:\n(a)PainFormer is structured hierarchically into four stages, incorporating\nSpectral andSelf-Attention Layers to extract embeddings from the inputs;\n(b)The Spectral Layer , a key element of PainFormer , uses FFT to ana-\nlyze frequency-specific data along with a learnable filter Kto highlight\ncritical frequencies; (c)The Self-Attention Layer , crucial for PainFormer ,\nenables parallel processing of features and their interconnections; (d)The\nEmbedding-Mixer , employing both cross and self-attention mechanisms, func-\ntions as the component for the final classification of embeddings in pain as-\nsessment; (e)TheVideo-Encoder , designed for compact and efficient encod-\ning, compresses video data into a reduced dimensional form; (f)TheMLP-1\nis part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)TheMLP-3 configuration is integrated into the Embedding-Mixer\nandVideo-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n7.5 Examples of different vision modalities in frame samples: (a)RGB frame,\n(b)synthetic thermal frame, and (c)depth estimation frame. . . . . . . . . 153\n7.6 Examples of different visual representations for biosignals: (a)waveform ,\n(b)spectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD . . 1547.7 An overview of the presented framework. PainFormer , the foundational\nmodel, excels in deriving high-quality embeddings from a diverse array of\nbehavioral and physiological modalities. The evaluation of RGB, thermal,\nand depth videos, alongside various representations of ECG, EMG, GSR,\nand fNIRS such as waveforms and spectrograms, underscores the rich infor-\nmation captured within these embeddings. Leveraging the embeddings from\nPainFormer facilitates the creation of various and diverse unimodal and mul-\ntimodal pipelines designed for the pain assessment task. Each pipeline can\nbe customized to suit the specific modalities involved, dataset characteristics,\nand the demands of the intended application or clinical setting. Our assess-\nments included the development and implementation of several pipelines\nin both unimodal and multimodal contexts, achieving leading-edge results\nacross various modalities and data representations. . . . . . . . . . . . . . 154\n7.8 Attention maps from the PainFormer :(a)(1strow) frames from RGB, ther-\nmal, and depth video modalities; (a)(2ndrow) corresponding attention maps;\n(b)(1strow) attention maps for ECG and EMG; (b)(2ndrow) attention maps\nfor EDA and fNIRS modalities. . . . . . . . . . . . . . . . . . . . . . . . . 155\n1 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n2 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n3 Additional attention maps from the PainViT\u20132 (refer to Section 7.2). . . . . 207List of Tables\n3.1 Most commonly utilized pain databases. . . . . . . . . . . . . . . . . . . . 24\n3.2 Vision-based studies with static analysis. . . . . . . . . . . . . . . . . . . . 32\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 33\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 34\n3.3 Vision-based studies with temporal utilization. . . . . . . . . . . . . . . . . 39\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 40\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 41\n3.4 Touch sensor-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Audio-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6 Multimodal-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.7 Interpretation approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1 Results for the Basic Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Results for the Gender Scheme (1). . . . . . . . . . . . . . . . . . . . . . . 60\n4.3 Results for the Age Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Results for the Gender-Age Scheme (Males) (1). . . . . . . . . . . . . . . . 62\n4.5 Results for the Gender-Age Scheme (Females) (1). . . . . . . . . . . . . . . 63\n4.6 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (1). 63\n4.7 Hyper-parameters used in our approach. . . . . . . . . . . . . . . . . . . . 65\n4.8 Results for the Basic Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . 68\n4.9 Results for the Gender Scheme (2). . . . . . . . . . . . . . . . . . . . . . . 68\n4.10 Results for the Age Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.11 Results for the Gender-Age Scheme (2). . . . . . . . . . . . . . . . . . . . 68\n4.12 Comparison of results adopting the feature augmentation approach. . . . . . 70\n4.13 Comparison of results adopting the MT-NN approach. . . . . . . . . . . . . 71\n4.14 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2). 72\n5.1 Training details for the automatic pain assessment. . . . . . . . . . . . . . 79\n5.2 Results on the pain estimation tasks. . . . . . . . . . . . . . . . . . . . . . 81\n5.3 Results for the pain estimation tasks using various numbers of input frames. 82\n5.4 Comparison of studies utilizing BioVid , RGB videos, and LOSO validation. 84\n5.5 Datasets utilized for the pre-training process of the framework. . . . . . . . 91\nxxiii5.6 Training details for the automatic pain assessment. . . . . . . . . . . . . . 92\n5.7 Results utilizing the video modality. . . . . . . . . . . . . . . . . . . . . . 93\n5.8 Results utilizing the heart rate modality. . . . . . . . . . . . . . . . . . . . 95\n5.9 Results utilizing the video &the heart rate modality. . . . . . . . . . . . . . 95\n5.10 Comparison of studies utilizing BioVid &LOSO validation, reported on ac-\ncuracy %. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.11 Module parameters and computational cost in FLOPS for the proposed frame-\nwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.1 Datasets utilized for the pretraining process of the framework. . . . . . . . 110\n6.2 Training specifications, and number of parameters and FLOPS for each mod-\nule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.3 Results utilizing the RGB video. . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Results utilizing the synthetic thermal video. . . . . . . . . . . . . . . . . . 112\n6.5 Results utilizing the RGB &the synthetic thermal video. . . . . . . . . . . . 113\n6.6 Results utilizing the fusion of RGB &synthetic thermal video. . . . . . . . 115\n6.7 Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.116\n6.8 Comparison with the MIntPAIN dataset. . . . . . . . . . . . . . . . . . . . 116\n7.1 Number of parameters and FLOPS for the components of the proposed Twins-\nPainViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.2 Datasets utilized for the pretraining process of the framework. . . . . . . . 123\n7.3 Training details for the automatic pain assessment. . . . . . . . . . . . . . 124\n7.4 Results utilizing the video modality & Addition method. . . . . . . . . . . . 125\n7.5 Results utilizing the video modality & Concatenation method. . . . . . . . . 125\n7.6 Results utilizing the HbR & Addition method. . . . . . . . . . . . . . . . . 126\n7.7 Results utilizing the HbR & Concatenation method. . . . . . . . . . . . . . 126\n7.8 Results utilizing the HbO & Addition method. . . . . . . . . . . . . . . . . 126\n7.9 Results utilizing the HbO & Concatenation method. . . . . . . . . . . . . . 127\n7.10 Results utilizing the HbR, HbO & Addition method. . . . . . . . . . . . . . 127\n7.11 Results utilizing the videos, HbO & Addition method. . . . . . . . . . . . . 127\n7.12 Results utilizing the videos, HbO & Single Diagram method. . . . . . . . . 128\n7.13 Comparison with the validation baseline provided by the AI4PAIN challenge\norganizers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.14 Number of parameters and FLOPS for the modules of the proposed framework.130\n7.15 Details of the PainFormer\u2019s architecture. . . . . . . . . . . . . . . . . . . . 132\n7.16 Datasets utilized for the multitask learning-based pretraining process of the\nframework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347.17 Training details of the proposed framework. . . . . . . . . . . . . . . . . . 135\n7.18 Results utilizing the video modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.19 Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.20 Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.21 Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.22 Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and\nF1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.23 Results on the validation set of AI4Pain dataset, multilevel classification task,\nreported on accuracy, recall and F1 score. . . . . . . . . . . . . . . . . . . 144\n7.24 Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.25 Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4\ntask and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.26 Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs.\nP4task and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.27 Comparison of studies on the testing set of AI4Pain dataset. . . . . . . . . . 148\n1 Results utilizing the video modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n2 Results utilizing the heart rate modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n3 Results utilizing the video &the heart rate modality reported on precision,\nrecall and F1 score (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . 202\n4 Results utilizing the RGB video modality, reported on recall and F1 score\n(refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n5 Results utilizing the synthetic thermal video modality, reported on recall and\nF1 score (refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . 203\n6 Results utilizing the fusion of RGB &synthetic thermal video modality, re-\nported on recall and F1 score (refer to Section 6.2). . . . . . . . . . . . . . 204\n7 Results of the proposed approaches, reported on macro-averaged precision,\nrecall and F1 score (refer to Section 7.2). . . . . . . . . . . . . . . . . . . . 204Chapter 1\nIntroduction\nContents\n1.1 Context and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Scope and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions \u2013 Peer-review Publications . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.1 Context and Motivation\nPain is a complex and deeply personal experience that is subjective by nature. Traditionally,\nit has been described in terms of its sensory dimension [1]. However, extensive research\nhas highlighted the importance of affective, cognitive and social aspects in shaping this ex-\nperience [2]. Studies have explored physiological, psychological, and socio-environmental\nfactors that contribute to the experience of pain. It is understood as a result of biological evo-\nlution and as influenced by psychological and social factors. As Ridell et al. [3] noted, \u201cPain\nis a synthesis\u2013a sum that is greater than its parts. \u201d The brain\u2019s ability to alter the perception\nof sensory inputs through the interplay of emotion, cognition, and social processes is signifi-\ncant. Although natural systems establish the initial biological framework for pain perception,\nthis structure is highly adaptable, particularly in humans. Throughout a person\u2019s life, both\nbiological developments and personal experiences significantly reshape this framework.\nA key question driving pain research across biological, psychological, and computational\nfields is why this topic of pain is meaningful and important. This question also forms the\nbasis for initiating this thesis, highlighting the broader relevance of studying pain. Williams\nand Kappesser [4] provide a compelling explanation, stating, \u201cWe care because we are wired\nto care: to attend to other people\u2019s expression of pain and to understand its meaning; to feel\ndistress in relation to their distress; and to be motivated to reduce their distress, and ours,\nif we are able to do so. \u201d This highlights the intrinsic human response to empathize and\n12 CHAPTER 1. INTRODUCTION\nalleviate pain, underlining the fundamental importance of this research area. Indeed, from a\nDarwinian perspective, pain serves a crucial role. The manifestation of pain in humans and\nthe reactions it elicits are examined through an evolutionary lens. Pain facilitates recovery by\npromoting responses to harmful stimuli and behaviors that demonstrate the adverse nature\nof painful experiences, common among animals. Specifically, the facial expression of pain,\nwhich communicates discomfort directly to those nearby, is universally recognized across\ndifferent ages, ethnicities, roles, and relationships. Evidence from healed major fractures\n[5, 6] suggests that injured members of hominid groups were not left to fend for themselves\nbut were supported through their recovery, indicating the fundamental importance of pain\nexpression in our evolutionary history.\nPain is a widespread health concern globally, affecting up to 30% of the adult popula-\ntion [7] and between 83% and93% of elderly adults in residential care [8]. The Global Bur-\nden of Disease (GBD) study identifies pain as the primary cause of years lived with disability\n(YLD) [9], with major contributors including chronic back pain, musculoskeletal disorders,\nand neck pain [10]. Pain impacts individuals and poses significant clinical, economic, and\nsocial challenges. In the United States, the economic and healthcare costs related to pain\ndue to reduced work productivity range from $560 to$635 billion annually, surpassing the\ncosts associated with heart disease, cancer, and diabetes combined [11]. In Europe, chronic\npain\u2019s direct healthcare costs and indirect socioeconomic impacts account for 3%to10% of\nthe GDP [12]. In Australia, the average annual cost for individuals among the 15.4%living\nwith chronic pain ranges from AU $22,588to AU $42,979, including non-financial costs [13].\nBeyond direct effects on health, pain contributes to a range of adverse outcomes, such as opi-\noid dependency, drug overuse, addiction, declining social relationships, and psychological\ndisorders [14]. In the last two decades, prescription opioid use has surged in the United\nStates, where overdose deaths have increased more than fourfold from 1999 to2016 [15].\nAdditionally, side effects from these opioids, like lethargy, depression, anxiety, and nausea,\nseverely impact workforce productivity and overall life quality [16].\nAccurate pain assessment is crucial for early diagnosis, disease progression monitoring,\nand treatment effectiveness evaluation, particularly in managing chronic pain [17]. This criti-\ncal role has resulted in pain being recognized as \u201cthe fifth vital sign\u201d in nursing literature [18].\nPain assessment is also fundamental in physiotherapy, where therapists apply external stim-\nuli and need to gauge the patient\u2019s pain levels accurately [19]. Objective evaluation of pain is\nessential to provide appropriate care, especially for vulnerable populations who may not be\nable to communicate their pain effectively, such as infants, young children, individuals with\nmental health issues, and the elderly. Various methods are used for pain assessment, with\nself-reporting\u2013where individuals describe their pain experiences\u2013considered the gold stan-\ndard [20]. Pain evaluation methods in clinical environments include quantifiable measures\nlike the Numeric Pain Rating Scale (NPRS), Visual Analogue Scale (V AS), and quantitative1.1. CONTEXT AND MOTIVATION 3\nsensory testing techniques such as the pressure pain detection threshold (PPDT) [21]. Behav-\nioral indicators are also crucial and include facial expressions ( e.g., grimacing, open mouth,\nlifted eyebrows), vocalizations (like crying, moaning, or screaming), and movements of the\nbody and head [22]. Physiological measures such as electrocardiography (ECG), electromyo-\ngraphy (EMG), galvanic skin responses (GSR), and respiration rates further contribute to\nunderstanding pain\u2019s physiological aspects [17]. Additionally, brain monitoring techniques\nlike near-infrared spectroscopy (fNIRS) have effectively detected changes in hemodynamic\nactivity associated with pain stimuli [23].\nCaregivers and family members often determine the presence or absence of pain in pa-\ntients by observing their behavioral or physiological responses [17]. However, accurately\nassessing pain poses a significant challenge for clinicians [24], especially with nonverbal\npatients such as the elderly, who may have reduced expressive abilities or may be reluctant\nto communicate pain [25]. Extensive research indicates that pain manifestations vary signif-\nicantly across different genders and ages, adding to the complexity of its assessment [26].\nFurther complicating the assessment process are the heightened workload and fatigue ex-\nperienced by nursing staff due to the demands of patient monitoring [27]. Technological\nsolutions are necessary for continuous patient monitoring. Nevertheless, concerns remain\nabout the objectivity and accuracy of these observations, as inadequately trained or biased\nobservers may struggle to assess pain [28] accurately. Even among trained observers, in-\nterpretations of behaviors can vary [22], and social and interpersonal dynamics can signif-\nicantly affect the pain assessment process, influencing both the evaluators\u2019 judgments and\nthe patients\u2019 expressions of pain [29]. Additionally, the presence of an observer can lead pa-\ntients to modify their behavior [30], and expressing pain through scales and measurements\ncan be challenging [31]. While self-reporting is used because pain is inherently subjective,\nrelying solely on a one-dimensional pain score fails to capture this complex phenomenon,\noften leading to inadequate pain management [32].\nGiven the challenges described above, scientific computing (SC) researchers have fo-\ncused on developing models and algorithms to enhance automatic pain recognition systems\nover the last two decades. Their goal is to accurately determine the presence and intensity\nof pain by analyzing physiological and behavioral indicators. Adopting deep learning and\nartificial intelligence (AI) techniques has expanded these automatic methods, designed to\ninterpret the complex and varied nature of pain [17]. Numerous studies have underscored\nthe effectiveness of automated systems that utilize behavioral or physiological modalities\nfor pain assessment [33]. Sario et al. [34] have shown the capability of these systems to\naccurately recognize pain through facial expressions, proving their utility in clinical envi-\nronments. Multimodal sensing has shown particular promise, offering enhanced accuracy\nin pain detection systems [22]. Furthermore, including temporal aspects in these modalities\nhas proven to significantly improve the accuracy of pain assessments [17].4 CHAPTER 1. INTRODUCTION\n1.2 Scope and Challenges\nAlthough considerable research has been conducted on automatic pain assessment, studies\nhave yet to explore factors like demographics and social aspects from a computational angle.\nFurthermore, despite the existence of deep learning-based methods, the approaches we ob-\nserve are often outdated and repeatedly recycled. For these reasons, we aimed to address two\nissues by (i)attempting to evaluate the social or demographic context, which significantly\nimpacts and influences pain sensation and perception, and (ii)introducing innovative deep\nlearning methods inspired by the latest developments in AI and generative AI literature. We\nbelieve these approaches can forge new paths in pain research, enhance the accuracy of rec-\nognizing this complex phenomenon, and, ultimately, be adopted in real-world scenarios to\nassist those in need. Additionally, (iii)recognizing the skepticism towards new technologies\namong clinicians and the general public, especially regarding the limited understanding of\nhow deep learning models function, we have devoted a portion of our research to interpret-\ning these models to offer some level of explanation and help the adoption process of them in\nclinical settings.\nNevertheless, this thesis initially faced challenges related to our objectives and goals as\nthe research progressed. The availability of pain datasets (to be discussed in the next chapter)\nis limited. Only a few datasets are available, and crucially, they are limited in size. This re-\nstriction poses a significant challenge for developing deep learning models, which typically\nrequire a large volume of data. In automatic pain assessment, researchers who develop deep\nlearning methods typically confront a decision: either train their models from scratch, which\ncan introduce performance limitations, or employ pre-trained models. These pre-trained\nmodels are generally trained on broadly available image datasets that include a variety of\nsubjects like animals and objects, or they rely on older architectures that were trained explic-\nitly on facial datasets. In this thesis, we addressed these issues by independently pre-training\nour deep-learning models using diverse datasets related explicitly to human facial images\nand biosignals. This strategy allowed us to design specific architectures to meet our unique\nneeds for each scenario, free from the constraints of relying on models developed and trained\nby others. Furthermore, we explored and evaluated several pre-training techniques to assess\ntheir effectiveness in pain assessment applications.\nRegarding, our objective to explore methods that utilize various modalities individually\nand in combination in a multimodal manner further constrains our dataset options. More-\nover, as previously outlined, our interest in the sociodemographic aspects of pain necessitates\ndatasets that include this information type, intensifying our challenges. For these reasons,\nthis thesis focuses specifically on examining the impact of age and gender on pain. In addi-\ntion, led us to utilize two pain datasets that most closely match the characteristics necessary\nfor our research, particularly in terms of demographic elements and multimodality.1.3. CONTRIBUTIONS \u2013 PEER-REVIEW PUBLICATIONS 5\n1.3 Contributions \u2013 Peer-review Publications\nThis section outlines the publications and projects produced during the Ph.D. research on\nautomatic pain assessment, where I was the first author.\n1.Automatic assessment of pain based on deep learning methods: A systematic re-\nview [17]\nThis systematic literature review (SLR) was conducted at the start of this Ph.D. re-\nsearch. This paper aims to explore the surge in recent years of deep learning algorithms\nadopted by researchers to encode the multidimensional nature of pain into meaning-\nful features. Specifically, this systematic review examines the models, methods, and\ndata types used to establish the foundation for deep learning-based automatic pain\nassessment systems. It identified relevant original studies from digital libraries such\nasScopus ,IEEE Xplore , and ACM Digital Library , following defined inclusion and\nexclusion criteria for studies published until December 2021 . The findings highlight\nthe critical role of multimodal approaches in automatic pain estimation, particularly\nin clinical environments, and emphasize the substantial gains observed with the inclu-\nsion of temporal exploitation of modalities. The review also recommends selecting\nhigh-performing deep learning architectures and methods, encouraging the adoption\nof robust evaluation protocols and interpretability techniques to deliver reliable and\nunderstandable outcomes. Additionally, it underscores the current limitations of exist-\ning pain databases in adequately supporting the development, validation, and practical\napplication of deep learning models as decision-support tools in real-world settings.\nFurthermore, we believe this paper is valuable not only for this Ph.D. project but also\nfor other practitioners and researchers in the field.\n2.Automatic Pain Intensity Estimation based on Electrocardiogram and Demographic\nFactors [35]\nThis study investigated the relationship between gender, age, and pain sensation and\ntheir effects on the automatic pain assessment process. By analyzing physiological\nsignals, particularly electrocardiography (ECG), we estimated pain intensity and ex-\namined the influence of these demographic factors. Utilizing the Pan-Tompkins algo-\nrithm for feature extraction and applying well-established classification methods, we\nexplored the correlation between gender, age, and pain manifestation.\n3.Multi-task Neural Networks for Pain Intensity Estimation Using Electrocardiogram\nand Demographic Factors [36]\nInspired by the previous study, this research further explored the influence of gender\nand age on pain perception. In this work, we analyze electrocardiography signals\nto uncover variations in pain perception across different demographic groups. We6 CHAPTER 1. INTRODUCTION\nleveraged these insights by developing a novel multi-task neural network for automatic\npain estimation, incorporating age and gender data for each individual. The study\ndemonstrated the advantages of this approach compared to other existing methods.\n4.A Full Transformer-based Framework for Automatic Pain Estimation using Videos\n[37]\nThis study introduced an innovative full transformer-based framework featuring a Trans-\nformer in Transformer (TNT) model combined with cross-attention and self-attention\nblocks. We achieved state-of-the-art performance using video data from the BioVid\ndatabase, demonstrating the model\u2019s effectiveness, efficiency, and strong generaliza-\ntion across primary pain estimation tasks.\n5.Multimodal automatic assessment of acute pain through facial videos and heart rate\nsignals utilizing transformer-based architectures [38]\nThis study presented a multimodal automatic acute pain assessment framework, inte-\ngrating video and heart rate signals. The framework consists of four key modules:\ntheSpatial Module , which extracts embeddings from videos; the Heart Rate Encoder ,\nwhich maps heart rate signals into a higher-dimensional space; the AugmNet , which\ngenerates learning-based augmentations in the latent space; and the Temporal Mod-\nule, which leverages the video and heart rate embeddings for the final assessment.\nThe Spatial Module undergoes a two-stage pre-training process: first, it learns uni-\nversal facial features through face recognition, followed by emotion recognition in a\nmultitask learning approach, enabling high-quality embeddings for pain assessment.\nExperiments with facial videos and heart rate data extracted from electrocardiograms\nin the BioVid database, alongside direct comparisons to 29studies, demonstrate state-\nof-the-art performance in unimodal and multimodal settings while maintaining high\nefficiency. In the multimodal setting, the framework achieved 82.74% accuracy for bi-\nnary pain classification and 39.77% for multi-level pain classification, using only 9.62\nmillion parameters across the entire framework.\n6.Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-\nMLP Architecture [39]\nThis paper introduced synthetic thermal videos generated by Generative Adversarial\nNetworks , which are integrated into the pain recognition process to assess their effec-\ntiveness. The framework employs a Vision-MLP andTransformer -based module, lever-\naging RBG and synthetic thermal videos in unimodal and multimodal settings. Exper-\niments conducted using facial videos from the BioVid database highlighted synthetic\nthermal videos\u2019 effectiveness and showcased their potential benefits in pain recognition\ntasks.1.4. THESIS OUTLINE 7\n7.Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for\nMultimodal Automatic Pain Assessment using Facial Videos and fNIRS [40]\nThis study was submitted to the First Multimodal Sensing Grand Challenge for Next-\nGen Pain Assessment (AI4PAIN) . The proposed multimodal framework leverages fa-\ncial videos and fNIRS, offering a modality-agnostic approach that eliminates the need\nfor domain-specific models. Utilizing a dual ViT configuration and waveform repre-\nsentations for both fNIRS and the extracted embeddings from the two modalities, the\nmethod demonstrates its effectiveness, achieving an accuracy of 46.76% in the multi-\nlevel pain assessment task.\n8.PainFormer: a Vision Foundation Model for Automatic Pain Assessment [41]1\nThis study introduces PainFormer , a vision foundation model built on multi-task learn-\ning principles and trained across 14distinct tasks and datasets comprising 10.9million\nsamples. As an embedding extractor for various input modalities, PainFormer provides\nfeature representations to the Embedding-Mixer , a transformer-based module respon-\nsible for conducting the final pain assessment. Extensive experimentation using both\nbehavioral modalities\u2013including RGB, synthetic thermal, and estimated depth videos\u2013\nand physiological modalities like ECG, EMG, GSR, and fNIRS revealed PainFormer \u2019s\nability to extract high-quality embeddings from diverse inputs. Tested on the BioVid\nandAI4Pain datasets and compared to more than 60existing methods, the framework\ndemonstrated state-of-the-art performance in unimodal and multimodal settings, posi-\ntioning itself as a step toward developing general-purpose models for automated pain\nevaluation.\n1.4 Thesis Outline\nThe dissertation is organized into the following chapters:\nChapter 2 introduces the foundational concepts of pain from biological, psychological, and\nclinical perspectives.\nChapter 3 reviews existing literature on automatic pain assessment using deep learning\nmethods and details the pain datasets used.\nChapter 4 outlines and proposes methods for evaluating demographic variables, their uti-\nlization, and their integration into an automatic pain assessment framework.\nChapter 5 discusses methods that utilize video and wearable device data, exploring the\ntrade-offs between efficiency and accuracy. It also proposes efficient, fast, effective models\nsuitable for real-world applications.\nChapter 6 explores synthetic data in pain assessment and introduces synthetic thermal im-\n1Under Review8 CHAPTER 1. INTRODUCTION\nagery techniques to enhance performance in automatic pain recognition.\nChapter 7 discusses general-purpose models, introduces a modality-agnostic framework,\nand presents the first foundation model used in automatic pain assessment.\nChapter 8 concludes the thesis with a final discussion, offering perspectives and ideas for\nfuture research in automatic pain assessment.Chapter 2\nClinical Pain Assessment\nContents\n2.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . . . . 20\n2.1 Chapter Overview\nThis chapter provides an anatomical and physiological overview of pain, focusing on the\nmechanisms responsible for generating, transmitting, processing, and interpreting pain sig-\nnals. It examines the various types of pain and explores the actions and expressions typically\nassociated with pain. Additionally, it reviews current pain assessment methods used in clin-\nical settings for adults, children, and newborns. The chapter also discusses developing and\nvalidating existing clinical pain assessment tools. This foundational knowledge is essen-\ntial for understanding the development and validation of computer-assisted pain assessment\n910 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nmethods discussed in later chapters. Finally, it highlights the challenges faced in clinical\npain assessment and underscores the need for automated pain assessment techniques.\n2.2 Biology of Pain\nPain, according to the International Association for the Study of Pain (IASP) [42], is \u201can\nunpleasant sensory and emotional experience associated with actual or potential tissue dam-\nage, or described in terms of such damage\u201d. Biologically, pain is an undesirable sensation\noriginating from the peripheral nervous system. Its fundamental function is to engage sen-\nsory neurons, notifying the organism of potential harm and playing a vital role in recognizing\nand responding to threats [43].\nThe transmission of a noxious stimulus from the periphery to the central nervous sys-\ntem involves a complex pathway through the spinal cord, resulting in the physical sensation\nof pain and a corresponding emotional response and memory. This process culminates in\nthe perception of pain. The initial stage of pain processing occurs when a stimulus at noci-\nceptive sensory fibers in the periphery is converted into an action potential. A nerve signal\nis generated if the stimulus is strong enough to surpass the action potential threshold [44].\nThis signal travels along the primary afferent fiber toward the central nervous system. As the\nstimulus intensity grows, more nerve fibers and areas of the nervous system are engaged [44].\nDue to their branching nature, primary afferent fibers typically relay information from sev-\neral pain receptors. These fibers and their receptors comprise a sensory unit, which gathers\ndata from a specific receptive field [44]. When receptive fields are larger and overlap with\nnearby fields, it becomes more challenging for the sensory system to locate the source of pain\naccurately. The primary afferent neuron is a pseudounipolar neuron that splits into a periph-\neral and central axon. The cell bodies of these neurons are located in the peripheral nervous\nsystem, within the posterior or cranial root ganglia. The peripheral axon extends to the skin,\nmuscles, tendons, or joints, branching into terminal fibers that connect with somatosensory\nreceptors. In contrast, the central axon leads to the central nervous system [45].\nPeripheral somatosensory fibers are categorized into three main groups. The first group\nincludes A\u00b4\u03b1,A\u00b4\u03b2,A\u00b4\u03b3fibers, large, myelinated fibers that rapidly conduct sig-\nnals [46]. These fibers involve touch and proprioception but are not associated with pain\nperception. The second group consists of A\u00b4\u03b4fibers, which are smaller and slower con-\nducting. Certain A\u00b4\u03b4fibers play a key role in pain sensation, with some responding only\nto intense mechanical stimuli and others reacting to noxious and non-noxious heat. The\nthird group comprises Cfibers, which are small, unmyelinated, and conduct signals very\nslowly. Most Cfibers are polymodal for pain perception, responding to various noxious\nmechanical, thermal, and chemical stimuli. These fibers are mainly linked to burning pain\nsensations [43]. The sensation of pain, known as nociception, is primarily facilitated by2.3. CLASSIFICATION AND CHARACTERISTICS OF PAIN 11\nvarious intracellular and extracellular molecular messengers. When activated by a specific\nstimulus, nociceptors relay information through glutamate, an excitatory neurotransmitter.\nAdditionally, inflammatory mediators are released at the injury site, further stimulating no-\nciceptor activation by releasing chemicals such as neurotransmitters ( e.g., serotonin), lipids\n(e.g., prostaglandins), peptides ( e.g., bradykinin), and neurotrophins ( e.g., nerve growth fac-\ntor) [46]. There are ascending tracts responsible for transmitting sensory information from\nthe periphery to the central nervous system. Fibers that convey two-point discrimination, tac-\ntile information, pressure, vibration, and proprioception ascend via the dorsal column of the\nspinal cord, forming the gracile and cuneate fasciculi. Fibers transmitting pain, temperature,\nand crude touch from somatic and visceral structures travel through the lateral spinothalamic\ntract. The anterior spinothalamic tract also transmits pain, temperature, and touch informa-\ntion to the brainstem and diencephalon (Figure 2.1) [47].\n2.3 Classification and Characteristics of Pain\nAccording to neurobiologist Clifford Woolf [48], pain can be classified into three categories\nbased on its function and characteristics: nociceptive ,inflammatory , and pathological pain.\nThese classes and their respective functions are illustrated in Figure 2.2.\nNociceptive pain (refer to Figure. 2.2(A)), arising from tissue damage, is a high-threshold\npain that activates only in response to intense stimuli [49], serving as a vital warning signal\nto the body. The neurobiological system responsible for nociceptive pain evolved from the\nability of even the most primitive nervous systems to detect impending or actual tissue dam-\nage caused by external stimuli. Its protective role requires immediate attention and action,\nachieved through the withdrawal reflex it initiates, the unpleasant sensation it produces, and\nthe emotional distress it triggers. Nociceptive pain demands avoidance in the present mo-\nment, and when activated, it overrides most other neural processes [48].\nInflammatory pain (refer to Figure. 2.2(B)) is also protective and adaptive, increasing\nsensory sensitivity following tissue damage to aid healing by discouraging movement and\ncontact with the injured area. This heightened sensitivity, or tenderness, helps prevent further\nharm and supports recovery, as seen after surgical wounds or inflamed joints where normally\nnon-painful stimuli now cause pain. It is triggered by immune system activation in response\nto tissue injury or infection. Despite its adaptive role, this pain often needs to be alleviated in\npatients with persistent inflammation, such as in rheumatoid arthritis or severe injuries [48].\nPathological pain (Figure. 2.2(C)) is maladaptive, arising from abnormal nervous sys-\ntem functioning and not serving a protective role. Unlike nociceptive and inflammatory pain,\npathological pain is a disease state of the nervous system itself. It may occur following\nnerve damage (neuropathic pain) or in conditions without apparent damage or inflammation\n(dysfunction l pain). Examples of dysfunctional pain include fibromyalgia, irritable bowel12 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFIGURE\n 2: Spinothalamic tract.\nPain, temperature, and some touch and pressure afferents end in the posterior horn. Second- or\nhigher-order fibers cross the midline, form the spinothalamic tract, and ascend to the ventral\nposterolateral (VPL) nucleus of the thalamus (and also to other thalamic nuclei not shown).\nThalamic cells then project to the somatosensory cortex of the postcentral gyrus, to the insula,\nand to other cortical areas (also not shown). Along their course through the brainstem,\nspinothalamic fibers give off many collaterals to the reticular formation (RF). The inset to the left\nshows the lamination of fibers in the posterior columns and the spinothalamic tract in a leg-\nlower trunk-upper trunk-arm sequence. The inset to the right shows the longitudinal formation\nof the spinothalamic tract. Primary afferents ascend several segments in Lissauer\u02bcs tract before\nall their branches terminate; fibers crossing to join the spinothalamic tract do so with a rostral\ninclination. As a result, a cordotomy incision at any given level would spare most of the\ninformation entering the contralateral side of the spinal cord at that level, and to be effective,\nthe incision must be made several segments rostral to the highest dermatomal level of pain.\n2017 Khalid et al. Cureus 9(10): e1754. DOI 10.7759/cureus.1754\n5\n of \n14\nFigure 2.1: The spinothalamic tract (STT) [43]. Pain, temperature, and some touch afferents\nend in the posterior horn, where second-order fibers cross the midline to form the\nspinothalamic tract, ascending to the thalamus and projecting to various cortical\nareas. Along the way, collaterals connect to the reticular formation. Due to the ros-\ntral inclination of fibers in Lissauer\u2019s tract, cordotomy must be performed several\nsegments above the pain level for effective relief.\nsyndrome, tension headaches, and temporomandibular joint disease, where significant pain\nexists without an apparent noxious stimulus or peripheral pathology. Pathological pain, a\nlow-threshold pain primarily driven by amplified sensory signals in the central nervous sys-\ntem, is the clinical pain syndrome with the greatest unmet need. To analogize, while nocicep-\ntive pain acts as a fire alarm for intense heat, and inflammatory pain reacts to warm tempera-\ntures, pathological pain is a false alarm triggered by a system malfunction. Thus, treatment\nmust specifically target the underlying mechanisms causing each type of pain [48].\nPain from a time-duration perspective can be categorized by duration into acute and2.4. PAIN INDICATORS 13\nchronic , with chronic pain persisting or recurring for more than three months [50]. Acute\npain is typically related to identifiable physiological damage from injury, surgery, illness,\ntrauma, or medical procedures and generally subsides once the underlying cause is resolved.\nHowever, if untreated, it may develop into chronic pain. Acute pain is further classified\nintoprocedural pain, caused by medical interventions such as muscular injections [51], and\npostoperative pain, which occurs after surgery and is a significant concern for both patients\nand healthcare providers. Effective management is crucial to aid recovery and prevent the\ntransition to chronic pain [52]. Chronic pain manifests in various forms, including chronic-\nrecurrent pain, like migraine headaches, and chronic-continuous pain, such as persistent low\nback pain [53].\n2.4 Pain Indicators\nPain can manifest in numerous ways and is often shaped by individual characteristics and\nenvironmental influences. Various human expressions, actions, and bodily responses have\nbeen linked to pain, serving both communicative and coping purposes. These pain indicators\nare generally categorized into three primary groups: (i)behavioral, (ii)physiological, and\n(iii)biochemical. While these indicators are universally present, certain expressions are more\nprominent in specific groups. For instance, crying is a common pain response across all age\ngroups but is more frequently observed in younger infants. This may be due to contextual\nfactors\u2014such as culture, social status, age, and ego\u2014influencing how pain is expressed\nover time. Adults, for example, may suppress crying in favor of other vocalizations, such as\ngroans and moans, as crying could be perceived as inappropriate in certain contexts. These\nmediating factors are often considered when interpreting pain indicators. The following\nsections will delve into each of these three categories [51].\n2.4.1 Behavioral Indicators\nBehavioral indicators such as facial expressions ( e.g., grimacing, open mouth, raised eye-\nbrows), vocalizations ( e.g., crying, moaning, screaming), and various bodily movements\n(e.g., changes in posture, signs of tension) are vital markers used in assessing pain [22].\nFacial expressions and limb movements in response to acute pain are typically rapid and\ninvoluntary. Facial reactions include brow bulging, eye squeezing, nasolabial furrow forma-\ntion [54], grimacing, clenched teeth, jaw-dropping, and tightened lips [55]. Body movements\nassociated with pain include bracing (gripping an object or the affected area during move-\nment), rubbing (massaging the painful area), restlessness (constant shifting of position) [55],\nand knee flexion [56]. Non-verbal vocalizations such as groaning, moaning, sighing, crying,\nand gasping [57] also indicate pain. Verbal expressions like \u201couch\u201d ,\u201cstop\u201d ,\u201cthat hurts\u201d ,14 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFigure 2.2: Pain classification [48]: (A)Nociceptive pain , which results from detecting po-\ntentially harmful stimuli and serves a protective function. (B)Inflammatory pain is\nlinked to tissue damage and immune cell infiltration, increasing pain sensitivity dur-\ning healing. (C)Pathological pain is a disease state caused by either nervous sys-\ntem damage (neuropathic) or abnormal nervous system function (dysfunctional).\n\u201cthat is enough\u201d , and even cursing [55] also serve as pain indicators. Interestingly, swearing\nhas been found to significantly alleviate pain, although its effect diminishes with frequent\nuse over a short period [58, 59].2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 15\n2.4.2 Physiological Indicators\nVital signs can reflect the state of the central nervous system, and since pain is mediated\nthrough this system, trends in vital signs can provide insights into pain levels. Clinical stud-\nies [60, 61] have examined physiological changes in response to pain and established em-\npirical solid evidence linking pain to vital sign alterations. However, as vital signs can also\nchange due to other non-pain-related pathological conditions, it is recommended that they be\nassessed alongside behavioral pain indicators for accuracy. Physiological pain responses are\nconsidered more reliable than behavioral signals, as they cannot be consciously controlled\nor altered. Physiological measurements such as electrocardiography (ECG), electromyogra-\nphy (EMG), galvanic skin responses (GSR), and respiration rate provide critical insights into\nthe body\u2019s reaction to pain [17]. In addition, brain monitoring techniques like near-infrared\nspectroscopy (fNIRS) have demonstrated the ability to detect pain-related hemodynamic\nchanges [23]. At the same time, functional magnetic resonance imaging (fMRI) has been\nexplored for assessing pain in both normal and pathological conditions [62].\n2.4.3 Biochemical Indicators\nCompared to other pain indicators, biochemical changes are the most precise and sensitive\nreactions to pain. However, their routine use in pain assessment is restricted due to the\ninvasive nature of measurement techniques [63]. These biochemical responses are most\nevident during surgical procedures with limited anesthesia, leading to increased levels of\nendorphins, norepinephrine, cortisol, growth hormones, renin, glucagon, aldosterone, and\ncatecholamines, along with a decrease in insulin levels [60].\n2.5 Sociodemographic and Psychological Variables\nIn1965 , Melzack and Wall [64] introduced the \u201cGate Control Theory\u201d , which interprets pain\nfrom two perspectives. The first involves the mechanisms of nociceptive signal transmission\nand modulation, while the second emphasizes pain as a psychophysiological phenomenon\narising from the interaction between physiological and psychological factors [53]. Observa-\ntions, empirical research, and theoretical models increasingly suggest that a comprehensive\nunderstanding of pain requires a biopsychological approach. It is also becoming apparent\nthat, although pain is often regarded as private and subjective, it is also fundamentally a so-\ncial experience [53]. Pain is not solely explained by biomedical components ( e.g., muscle\ndamage) but also involves psychological ( e.g., cognitive, affective) and social factors ( e.g.,\nfriends, family, health professionals), leading to what is known as a biopsychosocial sensa-\ntion [65]. Numerous factors contribute to how painful experiences are expressed and per-\nceived, varying wildly due to social and personal biases. These factors prompted Williams16 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nand Craig [2] to define pain as \u201ca distressing experience associated with actual or potential\ntissue damage with sensory, emotional, cognitive, and social components. \u201d\n2.5.1 Sex and Gender\nSeveral studies have explored the relationship between gender and pain expressiveness, as\nwell as variations in pain reporting. Research indicates that women generally exhibit a lower\npain threshold compared to men. A meta-analysis by Boerner et al. [66] on gender differ-\nences in children and adolescents found that girls over the age of 12reported higher pain\nintensity in response to cold-induced pain than boys. Furthermore, multiple studies suggest\nthat women tend to describe a greater degree of pain compared to men. In addition to bi-\nological differences, psychological aspects linked to gender also play a role. For instance,\nindividuals with a masculine identity may be less inclined to express or report their pain or\nseek assistance [67].\nMoreover, the manifestation of pain is not only influenced by the individual\u2019s gender but\nalso by dyadic interactions between people of different sexes. Levine and Desimone [68]\nconducted one of the initial studies on this phenomenon, showing that male participants in\na cold pressure experiment reported lower pain intensity when a female experimenter was\npresent. Similarly, McClelland and McCubbin [69] found that female participants expressed\nand reported higher pain levels when accompanied by a female friend. This dynamic also\nextends to patient-healthcare provider interactions. In studying health records, Vigil and\nAlcock [70] discovered that when the pain intensity was reported as high, the patients ( i.e.,\nmen and women) were examined by a female doctor or nurse. Additionally, studies exam-\nining gender differences among physicians in pain treatment options revealed that female\npatients were more likely to receive prescriptions for more potent drugs, such as analgesics,\nand female physicians were more likely to prescribe medications. Extensive research has\nalso shown that both lay observers and healthcare professionals tend to estimate higher pain\nlevels for female patients compared to male patients [71]. Hooper et al. [72] further noted\nthat clinicians communicate more effectively with female patients, often displaying greater\nempathy. Gender roles, beliefs, and expectations play a significant role in understanding\nhow social factors influence the differences in pain perception and experience between men\nand women [73].\n2.5.2 Age\nAge plays a crucial role in pain assessment and management. At the same time, there are\nsignificant challenges, limitations, and biases related to the patient\u2019s age group. Two of the\nmost vulnerable groups, albeit for different reasons, are the elderly and infants.\nPain recognition and interpretation among the elderly, particularly by caregivers, often2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 17\npresent unique challenges. Older adults frequently exhibit stoicism and reluctance to ex-\npress their pain, while healthcare providers struggle to accurately assess the patient\u2019s pain,\nleading to inappropriate pain management decisions [74]. McPherson et al. [75] noted that\ndespite caregivers\u2019 accommodating and empathetic relationships with elderly patients, con-\nflicts still arise. Older patients may resist acknowledging their weaknesses and accepting\nhelp, which can cause them to conceal their pain. The situation becomes even more complex\nwhen dealing with dementia, a disorder encompassing a range of conditions ( e.g., Parkin-\nson\u2019s, Alzheimer\u2019s, Vascular dementia), characterized by abnormal brain changes that im-\npair cognitive and linguistic abilities. A person with dementia may find it challenging to\ncommunicate their pain verbally. However, non-verbal pain expressions remain intact even\nin moderate dementia, although such reactions can be exaggerated [76]. However, aggres-\nsive behavior and disturbances in dementia patients, often caused by pain, are frequently\nmisinterpreted as psychiatric symptoms, leading to improper medication that can have life-\nthreatening consequences [77]. Caregivers of dementia patients face additional challenges,\nnot only related to pain management but also in addressing dementia\u2019s impact on language\nand memory. Particularly in the later stages of dementia, patients encounter severe pain\ncommunication difficulties due to cognitive decline, necessitating that caregivers recognize\nbehavioral and contextual indicators of pain [74]. Age is also known to cause changes in\nskin characteristics, such as texture, rigidity, and elasticity, which impact the performance of\nemotional face recognition tasks [78].\nInfants represent another vulnerable age group where pain assessment requires special-\nized attention, particularly when they experience painful events. The first challenge is obvi-\nously their limited reporting ability to express their pain through language. Although crying\nmight appear to signal pain, this is an oversimplified and unreliable method, as crying can in-\ndicate a variety of situations, such as discomfort, hunger, or pain. Accurately discerning the\ntype of cry is only one part of the challenge; assessing pain in infants is far more complex and\ninfluenced by numerous factors, including the interpersonal relationships within their envi-\nronment. Riddell and Racine [79] found that through various distressing experiences, infants\ncan learn that specific signaling behaviors can prompt their caregiver\u2019s proximity. This at-\ntachment dynamic suggests that, to some extent, infants may consciously utilize pain-related\nbehaviors to elicit responses from their caregivers. Similarly, the context affects older chil-\ndren as well; for example, self-reports of pain tend to be significantly lower when a parent is\npresent compared to when the child is alone [80].\n2.5.3 Psychological Factors\nMultiple studies have revealed that several psychological factors are consistently linked with\npain-related behavior, including depression, pain-related fear, and catastrophizing. Research18 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nfocusing on the impact of depression and anxiety on pain-related behavior has been con-\nducted mainly on patient populations. These studies have shown that depressed individ-\nuals exhibit more pronounced protective and communicative behaviors compared to non-\ndepressed patients [81]. Similarly, numerous studies suggest that patients with higher levels\nof anxiety demonstrate more pain-related behaviors than those with lower anxiety levels [82].\nDespite the frequent coexistence of pain with psychological conditions, research indicates\nthat these patients often experience underestimation of their pain. For instance, De Ruddere\net al. [83] found that patients dealing with psychological stressors such as anxiety, depres-\nsion, and daily life challenges are often perceived by physiotherapists as experiencing less\nsevere pain, illustrating the influence of psychosocial factors on the patient\u2019s pain experience.\n2.5.4 Race and Culture\nPain expression is generally understood across ethnicities and cultures, though differences\nexist in how it is conveyed [4]. However, cultural variations and the nuances of facial ex-\npressions related to emotion are complex and necessitate deeper study. Additionally, racial\nand cultural biases significantly influence pain assessment, judgment, and interpretation.\nExtensive research highlights the impact of a patient\u2019s race as a sociodemographic factor\non observer responses. The most examined topic relates to the different responses toward\nCaucasian versus non-Caucasian individuals, particularly African Americans, who are more\nlikely to have their pain underestimated and undertreated by healthcare providers [84].\nEthnocultural factors are crucial in shaping how individuals perceive and express pain.\nFor example, Western cultures often emphasize conservative expressions and self-control,\nleading to restrained responses in personal pain experiences and in perceiving others\u2019 pain\n[3]. Differences also arise in coping mechanisms; African Americans, for instance, are more\nprone to catastrophizing pain events compared to European Americans [85]. Furthermore,\nevidence shows racial biases in pain treatment across various racial groups, with certain\ngroups being more sensitive to pain but receiving lower-quality treatment [86]. For exam-\nple, Cleeland et al. [87] found that minority cancer patients, mainly Black and Hispanic\nindividuals, were more likely to experience inadequate analgesia compared to non-minority\npatients.\n2.5.5 Observer\u2019s Impact on Pain\nThe variability in pain management stems from the interplay of various elements, including\nsociocultural, biomedical, and psychosocial factors, especially in cases of chronic pain [88].\nWhen it comes to the observer responsible for assessing a patient\u2019s pain, several characteris-\ntics directly influence the objectivity of their evaluation. The first and perhaps most critical\nfactor is the observer\u2019s experience level. One would expect that more experience leads to2.6. IMPACT OF INADEQUATE PAIN MANAGEMENT 19\nbetter and more accurate assessments, but studies show that even experienced healthcare\nproviders consistently underestimate pain, much like laypersons [28]. The greater the expe-\nrience, the more pronounced the underestimation tends to be. This may be due to desensitiza-\ntion caused by repeated exposure to pain events, as seen in the differences between internists\nand surgeons in their evaluation of postoperative pain, with surgeons often encountering se-\nvere pain more regularly [89]. Another significant factor is the observer\u2019s knowledge and\nbeliefs about pain. For example, [83] found that laypersons and healthcare professionals\nwithout physical signs of pain might view the patient\u2019s complaints less seriously. Proper\ntraining is also essential for adequate pain assessment, which is why the Department of\nHealth and Human Services (DHHS) initiated a strategic program to improve healthcare\nproviders\u2019 education and knowledge regarding pain management, following evidence of in-\nadequate training in the field [90].\n2.6 Impact of Inadequate Pain Management\nThe experience of pain, particularly persistent pain, can have detrimental effects on the indi-\nvidual and their surrounding environment. Thoughts about severe pain often lead to grief and\nfear, causing individuals to perceive pain as a threat and feel incapable of managing it. This\ncan prompt avoidance behaviors aimed at escaping perceived harm [91]. Studies have shown\nthat children with a catastrophizing mindset about pain struggle with daily activities, while\nadolescents with chronic pain tend to have fewer friends and may miss out on social and\nentertainment opportunities, putting them at greater risk of victimization [92]. These adoles-\ncents often feel isolated and lonely compared to their healthy peers, and they may experience\nanxiety in social interactions [93]. Parental reactions to their children\u2019s pain can further com-\nplicate the situation, as parents with catastrophic tendencies tend to engage in overprotective\nbehaviors that hinder the child\u2019s functioning and psychosocial development [94]. Addition-\nally, the family\u2019s overall dynamic is affected, with the patient\u2019s sadness, sleep disorders, and\nchanges in leisure activities impacting the household [74].\nOn a biological level, pain, particularly when experienced early and severely, can alter\nthe brain and nervous system. These early pain experiences can disrupt neurobiological\ndevelopment and affect how pain is processed later in life [95]. A growing body of research\nlinks chronic pain to changes in the medial prefrontal cortex, a region crucial to emotional\nprocessing. Chronic pain is associated with structural and biochemical alterations in this\nbrain area, suggesting that these changes play a role in the pathophysiology of chronic pain\n[96].20 CHAPTER 2. CLINICAL PAIN ASSESSMENT\n2.7 Pain Measurement Scales and Metrics\nIn clinical settings, self-reporting remains the gold standard for assessing pain, allowing in-\ndividuals to describe their pain\u2019s intensity and location. Various self-report scales have been\ndeveloped for different age groups, such as the visual analog scale (V AS) [97] and the verbal\nrating scale (VRS) [98]. Additionally, observation-based scales, where a third party eval-\nuates the pain\u2019s severity, include tools like the Prkachin and Solomon pain intensity scale\n(PSPI) [99] and the neonatal/infant pain scale (NIPS) [100]. However, some studies suggest\nthat patients may exaggerate their pain severity to prompt more aggressive treatment inter-\nventions [101], raising concerns about the accuracy of self-reported symptoms. Therefore,\nobjective pain measurement remains clinically crucial.Chapter 3\nAutomatic Pain Assessment\u2013A Literature\nReview\nContents\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database . . . . 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 49\n2122 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research Di-\nrections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.1 Chapter Overview\nThis chapter corresponds to the publication [17], a systematic literature review (SLR) con-\nducted at the start of this Ph.D. research. This review facilitated an understanding of au-\ntomatic pain assessment methods, particularly those based on deep learning, and the tech-\nniques and strategies employed. It enabled the identification and proposal of new approaches\nthat could enhance the effectiveness of pain recognition.\nAdditionally, it allowed for identifying gaps in the literature from other reviews con-\nducted on this specific research topic. Every existing systematic review on pain assessment\nwas identified and assessed, revealing several insights. The first review on automatic pain\nassessment, published by Prkachin in 2009 [102], did not cover papers on deep learning,\nas the practical implementations of deep architectures only began around 2012. Zamzmi et\nal.[103] focused their review exclusively on infants, omitting deep learning methods. In\n2018, Chen et al. [104] reviewed automated pain detection methods using the Facial Ac-\ntion Coding System (FACS), noting only three publications that employed deep learning\ntechniques. In 2019, Hassan et al. [105] included only seven papers that used deep learn-\ning methods in their review. Similarly, Werner et al. [106], also in 2019, discussed pain\nassessment without restrictions on modalities or age groups, finding fewer than ten papers\nthat reported on deep learning methods. In 2020, Al-Eidan et al. [107] published the first\nsystematic literature review titled \u201cDeep-Learning-Based Models for Pain Recognition: A\nSystematic Review\u201d, which included fifteen papers but was critiqued for having significant\nlimitations and incorrect information. It was noted that some papers analyzed might not be\nrelevant, and there was confusion between \u201cneural networks\u201d and \u201cdeep learning\u201d. For in-\nstance, while study [105] mentioned using neural network approaches, they did not provide\nevidence of using deep learning methods. Moreover, in the study [104], the authors devel-\noped a neural network with only two layers combined with handcrafted features, which does\nnot qualify as a deep learning method. Additionally, studies [103, 107] focused on detect-\ning protective movement behaviors in chronic pain patients, which deviates from the central\ntopic of automatic pain assessment. Several reviews and SLRs on automatic pain assessment\nhave been published, but none exclusively or adequately focus on deep learning methods.\nThis SLR aims to bridge this gap by thoroughly reviewing deep learning techniques used for\nautomatic pain assessment.3.2. MODALITIES AND HARDWARE FOR AUTOMATIC PAIN ASSESSMENT 23\n3.2 Modalities and Hardware for Automatic Pain Assessment\nCreating an automatic pain assessment system hinges on capturing the necessary input data\nthrough various information channels, referred to as modalities. These modalities are cat-\negorized into behavioral and physiological types. A system utilizing only one modality is\ntermed unimodal, whereas a multimodal system incorporates multiple modalities.\nKey behavioral modalities encompass facial expressions, body movements, gestures, and\nauditory signals. Researchers use a range of optical and light sensors to record images or\nvideo sequences of facial and body movements. Commonly, researchers employ color RGB\ncameras, but depth and thermal sensors are also used to enhance visual data. Motion capture\nsensors are also employed to track movements, and microphones are frequently employed\nto capture sound. On the physiological front, modalities often involve biosignals that detect\nelectrical activities from various tissues and organs. Techniques such as electrocardiogra-\nphy (ECG), electromyography (EMG), electrodermal activity (EDA), photoplethysmogra-\nphy (PPG), blood oxygen saturation (SpO2), near-infrared spectroscopy (NIRS), respiration\nrate, and skin temperature are commonly used to gauge pain. Multiple sensors can mea-\nsure several modalities simultaneously \u2014 for instance, strain sensors and cameras can track\nrespiration rates.\nBesides the sensors that gather input data, the computational hardware is crucial. Deep\nlearning-based systems operate in two phases: training and inference. The training phase is\nparticularly resource-intensive, necessitating a graphics processing unit (GPU). The trained\nmodel makes predictions on new data during inference, typically processed on a central\nprocessing unit (CPU). The choice of hardware depends on various factors, especially in\nreal-time scenarios where low latency is crucial, compared to offline settings where data\nprocessing can be deferred. Additionally, characteristics of the model, such as floating point\noperations per second (FLOPS) and total computational operations, are significant consider-\nations.\n3.3 Pain Databases\nAccess to data is crucial for evaluating methods and algorithms in automatic pain assess-\nment. However, only a few databases have explicitly been developed for automatic pain\nrecognition based on human behavioral and physiological changes. Unlike the extensive\ndata found in most facial expression databases, publicly accessible pain datasets often offer\nlimited samples and suffer from significant class imbalance. This primarily stems from the\nethical concerns associated with collecting pain data. Table 3.1 lists the principal databases\nreviewed in the studies. Figure 3.1 shows how frequently each database was used. Most\nresearch utilized publicly available datasets, with some studies exploring multiple datasets.24 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nFew studies used private datasets, mainly those aimed at detecting pain in neonates. The\nUNBC-McMaster Shoulder Pain Archive Database [108] is the most utilized, followed by\nTheBioVid Heat Pain Database [109]. The former contains 200facial videos of 25individ-\nuals with shoulder pain. At the same time, the latter includes facial videos and biopotentials\nof90healthy participants subjected to experimentally induced heat pain at four intensity\nlevels. The following subsections provide a brief description of some of these datasets.\nTable 3.1: Most commonly utilized pain databases.\nDatabase Modality Population Annotation\nGranularityAnnotation Labels\nUNBC-McMaster\nShoulder PainA[108]RGB video of face 25 adults with shoulder painFrame level\nSequence levelFACS\nV AS, OPI\nBioVidA[109] RGB video of face, EDA, ECG,\nEMG87 healthy adults Sequence level stimulus\n(calibrated per person)\nMIntPAINA[110] RGB-Depth-Thermal video of\nface20 healthy adults Sequence level stimulus\n(calibrated per person),\nV AS\niCOPEA[111] RGB photographs of face 26 healthy neonates Frame level pain, cry, rest, air puff,\nfriction\niCOPEvidA[112] Grayscale video of face 49 neonates Sequence level pain, no pain\nNPAD-IA[113] RGB video of face & body, HR,\nSpO2, BP, NIRS36 healthy neonates & 9 neonates\nwith tissue injured by surgerySequence level NIPS, N-PASS\nAPN-dbA[114] RGB video of face 112 healthy neonates Sequence level NFLAPS, NIPS, NFCS\nEmoPainN[115] video, audio, EMG, MoCap 22 adults with chronic pack pain &\n28 healthy adultsSequence level self-report, naive OPI\nSenseEmotionN\n[116]video of face, audio, EDA, ECG,\nEMG, RSP45 healthy adults Sequence level stimulus\n(calibrated per person)\nX-ITEN[117] RGB-Thermal video of face,\nRGB-Depth video of body, au-\ndio, EDA, ECG, EMG134 healthy adults Sequence level stimulus\n(calibrated per person)\nA: Publicly available by request, complete or part of the dataset N: Not yet available Modality: HR: heart rate SpO2: oxygen saturation rate BP:\nblood pressure NIRS: near-infrared spectroscopy MoCap: motion capture RSP: respiration rate EDA: electrodermal activity ECG: electrocardiogram EMG:\nelectromyogram Annotation Labels: FACS: Facial Action Coding System V AS: visual analogue scale OPI: observer pain intensity NIPS: neonatal infant\nscale N-PASS: neonatal pain, agitation and sedation scale NFLAPS: neonatal face and limb acute pain scale NFCS: neonatal facial coding system\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database\nTheUNBC McMaster Shoulder Pain Database [108] comprises 200video sequences show-\ning the facial expressions of 25subjects undergoing motion tests, including arm abduction\nand external and internal rotations. The data collection utilized both active and passive ap-\nproaches: in the active mode, subjects moved their affected arms to their bearable limit,\nwhile in the passive mode, a physiotherapist moved the subjects\u2019 arms. Each video sequence\ncontains about 60to700frames, totaling 48,398, with 82.71% of frames scoring a pain\nrating of zero, indicating a significant imbalance in the data. All frames are FACS-coded\nfor pain-related action units (AUs)\u2014AU4, AU6, AU7, AU9, AU10, AU12, AU20, AU25,\nAU26, AU27, and AU43\u2014with each AU coded for intensity from A to E, 0, or5, except\nfor AU43 (closed eyes), which is coded as either present or absent. Pain scores are assigned\nusing the PSPI metric based on the intensity of the AUs present. Additionally, the database3.3. PAIN DATABASES 25\n0815233038455360\nUNBC-McMasterBioVidEmoPainSenseEmotionX-ITEMIntPAINiCOPEiCOPEvidNPAD-IAPN-dbother\nTable 1Category AUNBC-McMaster59BioVid21EmoPain7SenseEmotion5X-ITE2MIntPAIN4iCOPE3iCOPEvid1NPAD-I5APN-db1other21\n1\nFigure 3.1: The number of studies utilizing these specific datasets. Note that various studies\nused multiple datasets to conduct their experiments.\nincludes 66facial landmarks per frame, determined by an active appearance model. Pain as-\nsessments also include self-reports using two Likert scales with 15options each and a visual\nanalog scale (V AS) from 1(no pain) to 10(extreme pain). One scale measures the sen-\nsory intensity from \u201cextremely weak\u201d to \u201cextremely intense\u201d , while the other assesses the\naffective-motivation aspect of pain from \u201cbearable\u201d to \u201cextremely excruciating\u201d. Indepen-\ndent observer pain intensity (OPI) ratings use a 6-point scale from 0(no pain) to 5(intense\npain). The UNBC database is currently the most extensively utilized dataset for automatic\npain recognition among publicly available resources.\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database\nTheBioVid dataset [109] is a prominent resource in pain research, comprising facial videos,\nelectrocardiograms, electromyograms, and galvanic skin response data from eighty-seven\npn=87qhealthy participants ( 44males and 43females, aged 20to65). The pain was in-\nduced using a thermode on the participants\u2019 right arm, with pain and tolerance thresholds\nestablished before data collection. These thresholds defined the range of pain from No Pain\n(NP) to Very Severe Pain (P 4), encompassing five levels of pain intensity. The temperatures\nfor the pain inductions ranged from P 1to P 4and did not exceed 50.5\u02ddC. Each participant\nunderwent 20inductions at each of four pain levels, with each induction lasting 4sfollowed\nby a recovery period of 8to12s. In addition, 20baseline measurements were taken at 32\u02ddC\n(NP), totaling 100stimulations per participant, randomly administered. Data processing\nsegmented these into 5.5sdurations starting 1safter the target temperature was reached, re-26 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsulting in 8,700samples across the five pain intensity classes, equally distributed among all\nmodalities for each participant. Video recordings were made at a frame rate of 25FPS, and\nbiosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\n3.3.3 The EmoPain Database\nTheEmoPain [115] dataset encompasses various pain indicators, including body movements,\naudio, biosignals, and postural and facial expressions. It features video and audio recordings\nof22patients ( 7male, 15female) exhibiting natural pain expressions while engaging in\nphysiotherapy-like exercises. These exercises, performed at regular and challenging levels,\ninclude a sitting-standing sequence, balancing on one leg for five minutes, and reaching for-\nward while standing. The video signals are captured in high resolution ( 1024\u02c61024 pixels)\nusing eight cameras positioned at various angles, enhanced by specialized lighting condi-\ntions. Audio is recorded with two microphones: an AKG C-1000S MKIII placed near the\ncameras and an AKG HC 577 L worn by the patients, both operating at a 48kHz sampling\nrate with bit Pulse Code Modulation. Body movements and postures are tracked using a mo-\ntion capture suit with 18sensors distributed across the body. Biosignals are monitored with\nfour sEMG sensors attached to the trapezius and lumbar para-spinal muscles. Additionally,\nthe dataset provides continuous frame-wise pain ratings for facial expressions by eight naive\nannotators and binary frame-wise annotations for protective behaviors by four experts, along\nwith coordinates from 26body nodes. Six annotated protective behaviors include stiffness,\nbracing, hesitation, limping, rubbing, and abrupt actions. Audio and EMG signals from the\neight activities per subject also contribute to multimodal pain recognition. Like the UNBC\ndatabase, EmoPain faces significant challenges due to data sparsity and imbalance\u2014only\n11.4%of frames show facial expressions of pain, and 8.6%show protective behaviors. This\nscarcity complicates pain recognition research, necessitating the development of methods\nthat efficiently utilize limited data to achieve optimal performance.\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database\nTheX-ITE [117] dataset is one of the largest pain datasets but is not publicly available. It\ninvolved 134healthy adults ( 67men and 67women) aged between 18and50. The aver-\nage age was 31.4years (SD = 9.7), with men averaging 33.4years (SD = 9.3) and women\n32.9years (SD = 10.2). Participants had no chronic pain, depression, psychiatric disorders,\nneurological conditions, headache syndromes, or cardiovascular disease, nor had they taken\npain medication or painkillers before the experiment. Pain stimuli were stimulated using the\nMedoc PATHWAY Model ATS for heat pain on the forearm and the Digitimer DS7A for elec-\ntrical pain on the index and middle fingers. Both modalities featured phasic stimuli (short,\n5seconds) and tonic stimuli (long, 60seconds), each in three intensities. After calibration,3.4. UNIMODAL STUDIES 27\nparticipants underwent a 90-minute stimulation phase where phasic stimuli were repeated\n30times in a randomized sequence with 8-12-second pauses. The tonic stimuli were applied\nonce per intensity, totaling six per participant, each followed by a five-minute pause. The\nhighest intensity tonic stimuli for heat and electrical pain were induced at the experiment\u2019s\nending, with the other stimuli randomly interspersed during the phasic period. Simultane-\nous to the pain stimulation, various sensors collected multimodal pain response data: frontal\nand side view RGB videos for facial expression and head pose analysis, audio for paralin-\nguistic response analysis, electrocardiogram (ECG) to monitor heart rate variability, surface\nelectromyography (EMG) to assess muscle activity in the trapezius, corrugator supercilii,\nand zygomaticus major, electrodermal activity (EDA) to measure sweating, video for body\nmovement analysis, and thermal video for facial temperature changes.\n3.3.5 The AI4Pain Database\nThe AI4Pain Grand Challenge 2024 [118] dataset is a recent contribution to the pain re-\nsearch field, tailored for sophisticated pain recognition tasks using fNIRS and facial video\ndata. This dataset involves sixty-five volunteers pn=65q, including 23females, with ages\nranging from 17to52years (mean age of 29.06years and a standard deviation of 8.28years).\nAlthough it captures physiological signals such as photoplethysmography (PPG), electroder-\nmal activity (EDA), and respiration (RESP), these signals are not publicly available yet. The\ndataset is segmented into three parts: training ( 41volunteers), validation ( 12volunteers),\nand testing ( 12volunteers). The experimental setup includes fNIRS data recorded with an\nArtinis device, measuring changes in oxygenated and deoxygenated haemoglobin concentra-\ntions across 24channels targeting the prefrontal cortex. The optodes configuration includes\n10sources and 8detectors spaced 30mm apart, using near-infrared light at 760nm and 840\nnm, sampled at 50Hz. Additionally, facial movements are captured by a Logitech Stream-\nCam at30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low Pain ,\nandHigh Pain . It features 65instances of No Pain (each lasting 60s),780instances of Low\nPain (each lasting 10s), and 780instances of High Pain (each lasting 10s). The No Pain\ninstances, recorded during baseline, serve as control data. The Low Pain instances reflect\nmild pain responses, and the High Pain instances capture significant pain, both derived from\na pain tolerance test and reflected in the corresponding neurological and behavioral data\nrecorded.\n3.4 Unimodal studies\nThis section presents the studies that utilized only one information channel to estimate the\nsubject\u2019s pain condition.28 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.1 Vision-based: Static Analysis\nThe first publicly available pain database that significantly contributed to the development\nof automatic pain assessment methods was the UNBC-McMaster Shoulder Pain Database .\nNumerous studies have employed this dataset. Pedersen [119] implemented the first deep\nlearning approach in 2015 to address the pain assessment problem, utilizing a 4-layer contrac-\ntive autoencoder. He combined the encoded representations with a support vector machine\n(SVM), achieving high performance in frame-level pain detection. A significant advance-\nment in vision-based pain recognition methods was the EmoPain challenge in 2020, which\nbecame the first international competition to compare machine learning methods for chronic\npain assessment. Egede et al. [120] presented the EMOPAIN 2020 Challenge , utilizing a\ndataset composed of features extracted via both handcrafted methods and deep-learned mod-\nels. They utilized facial landmarks, histogram of oriented gradients (HOG), and deep vectors\nfrom VGG-16 [121] and ResNet-50 [122], both pre-trained on the Aff-Wild dataset1. The au-\nthors report that combining hand-engineered features with deep learning cues led to the best\nperformance. Similarly, Yang et al. [123] extracted both low- and high-level features from lo-\ncal descriptors and the pre-trained VGG-16 CNN, combining them through weighted coeffi-\ncients. Semwal and Londhe [124] demonstrated that fusing deep-learned features with facial\nlandmarks is beneficial for multi-class pain estimation. Lakshminarayan et al. [125] com-\nbined deep-learned features with handcrafted ones\u2014namely features from VGG-16 [121]\nandResNet-50 [122], HOG, action unit occurrence and intensity, facial landmarks, and head\npose\u2014through a fully connected network. Their study found that combining VGG-16 with\nhandcrafted features lowered regression error, whereas [126] achieved maximal performance\nusing only VGG-16 features with a fully connected network.\nConversely, Semwal and Londhe [127] noted the limitations of traditional handcrafted\nfeature engineering and the computational expense of deep neural networks. As a solution,\nthey proposed a relatively shallow 4-layer CNN, which reduces computational costs due to\nfewer parameters while achieving performance comparable to deeper models. A different\napproach came from [128], where the authors focused on representing facial expressions\nas compact binary codes for pain intensity classification. Feature extraction was conducted\nusing a pre-trained model [129], with a fully connected network used to generate the binary\ncodes.\nSeveral studies utilized CNN ensemble designs with varying architectures to exploit fea-\nture diversity. Semwal and Londhe [130] combined predictions from three compact CNNs\u2014\nVGG-16 ,M-MobileNet [131], and GoogleNet [132]\u2014using the average ensemble rule, re-\nsulting in improved classification performance. Kharghanian et al. [133] developed a con-\nvolutional deep belief network (CDBN) using unsupervised feature learning. An SVM used\n1https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge3.4. UNIMODAL STUDIES 29\nthe extracted features to differentiate between two states for binary pain classification ( i.e.,\npain vs. no pain). Later, [134] added two layers to the CDBN, though the results were not\ndirectly comparable due to differing evaluation methods.\nSeveral papers suggest that because pain is predominantly expressed in specific facial\nregions, focusing on these areas rather than the whole face could improve model accuracy by\nreducing noise. Huang et al. [135] initially identified the left eye, right eye, nose, and mouth\nas key regions and utilized a multi-stream CNN for feature extraction, assigning learned\nweights to enhance attention on these regions. Xin et al. [136] employed a 9-layer CNN\nwith an attention mechanism to assign different weights to face regions, resulting in more\naccurate attention face maps and boosting prediction accuracy by up to 19%. Cui and Huang\n[137] introduced a multi-scale regional attention network (MSRAN), which uses multiple\ncropping regions from video frames. The framework includes self-attention and relation-\nattention modules to highlight pain-relevant regions and explore interrelationships. Li et\nal.[138] extended this concept by integrating contrastive and multi-task training through an\nautoencoder, building on the work of [139].\nOne challenge in pain intensity estimation is that individual facial features, such as face\nshape, can introduce significant variability in how pain is expressed. This makes it difficult\nto distinguish between adjacent intensity levels. To address this, Peng et al. [140] examined\nfacial shape information and developed a deep multi-task network to account for the rela-\ntionship between pain recognition and shape, which improved pain estimation performance.\nSimilarly, Xin et al. [141] proposed a novel multi-task framework that combines a CNN\nfeature learning module with an autoencoder attention component, also estimating subject\nidentity, as individual differences in pain manifestation are key. Their experiments achieved\nstate-of-the-art results on publicly available datasets.\nMost studies report results obtained from controlled laboratory settings, which typically\nfeature proper lighting, minimal head pose variability, and no occlusions. However, such\nconditions do not represent typical hospital environments. Semwal and Londhe [142] ad-\ndressed this by focusing on pain assessment in uncontrolled settings, developing a shallow\nCNN with three convolutional layers that performed comparably to deeper pre-trained mod-\nels. In a subsequent study [143], they introduced a more complex framework comprising\nthree modules that leveraged high-level spatial descriptors with both local and global geomet-\nric cues, achieving results comparable to models like GoogleNet [144] and VGG [121]. Lee\nand Wang [145] explored pain assessment in intensive care unit (ICU) settings, where par-\ntially occluded faces frequently complicate facial analysis. They developed a 4-layer CNN\ncombined with an extreme learning machine (ELM) for final estimation. Virrey and Cae-\nsarendra [146] used CNNs to classify sections of frames where pain was triggered, peaked,\nand subsided. Nugroho et al. [147] tackled pain detection in smart home-care settings, par-\nticularly for elderly patients, using relatively low-power mobile devices. They modified the30 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nOpenFace2library, based on pre-trained FaceNets [148], and showed that transfer learning\ncould enable real-time binary classification ( pain vs.no pain ), even on low-powered hard-\nware.\nResearchers like Dai et al. [149] and Menchetti et al. [150] have noted that most models,\nwhether deep or shallow, are trained on dataset-specific features rather than actual pain-\nrelated features. Moreover, most studies employ validation methods using the same dataset,\nwhile cross-dataset performance is rarely addressed, limiting real-world applicability. To\ntackle these issues, Dai et al. [149] combined pain and emotion detection datasets to develop\na real-time pain assessment system with better generalization capabilities. They emphasized\nthe importance of cross-corpus evaluation, real-time testing, and the need for well-balanced,\necologically valid pain datasets [151].\nSeveral studies have explored combining pain scales to improve prediction objectivity\nand reliability. Liu et al. [152] developed a two-stage personalized model trained using active\nappearance model (AAM) facial landmarks and multi-task learning, with visual analog scale\n(V AS) and observed pain index (OPI) as ground truth. Xu et al. [153] similarly reduced\nmean square error (MSE) by incorporating various pain scales with the VGG-Face model.\nHowever, Casti et al. [154] pointed out the limitations of original ground truth data due to\nsubjectivity and annotation inconsistencies. To address this, they re-annotated their dataset\nwith judgments from multiple experts, using multidimensional scaling to map frames to\nillumination-invariant 3D space, which they then fed into a pre-trained AlexNet [155].\nCelona and Manoni [156] investigated neonatal facial expressions to detect pain, achiev-\ning the highest accuracy when utilizing two pre-trained models: VGG-Face [157] and mapped\nLBP+CNN (MBPCNN) [158]. Similarly, Lu and Hao [159] found that pre-trained models\nwere crucial for small datasets like neonates, as training from scratch led to overfitting. They\nachieved optimal classification performance by fine-tuning the entire VGG-16 model [122].\nHowever, Zamzmi et al. [160] argue that most face recognition methods are tailored for\nadults and thus less applicable to infants. They developed a lightweight 2D CNN trained\nend-to-end and achieved high pain detection accuracy, but external validation on a different\nneonatal dataset revealed challenges with generalizability. In 2019, Brahnam et al. [112]\nintroduced the iCOPEvid neonatal video dataset, a significant contribution since the only\npublicly available neonatal pain dataset [111] previously contained only static images. Their\nexperiments showed that local descriptors based on the bag-of-features (BoF) approach out-\nperformed deep learning models like VGG-Face andResNet . Combining handcrafted and\ndeep-learned features offered only a marginal improvement in performance. In contrast, Za-\nmzmi et al. [161] found that the most effective approach for binary classification (pain vs.\nno pain) was the fusion of high-level features from VGG [162] and optical flow strains, with\n2http://cmusatyalab.github.io/openface3.4. UNIMODAL STUDIES 31\nnaive Bayes serving as the classifier. Celona and Brahnam [163] applied a Wasserstein gen-\nerative adversarial network with gradient penalty (WGAN-GP) [164], demonstrating that\ntraining set augmentation with synthetic samples improved classification performance. Ta-\nble 3.2 summarizes the vision-based studies focusing exclusively on the spatial dimension.32 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [112]F (RGB) texture\ndescriptors- FF 2D CNN`SVM SL C P O 49 k-fold iCOPEvid 79.80 AUC\n'15 [119]F (RGB) - - - AE SVM SeSL,\nSLC P PS 25 LOSO UNBC 86.10 ACC,\n96.50 AUC\n'20 [120]F (RGB) - - FF 2D CNN`NN SL R IC O 36 hold-out EmoPain 0.91 MAE;\n'18 [123]F (RGB) HOG,\nstatistics- FF 2D CNN`SVR SL R IC PS 25 LOSO UNBC 1.44 MSE;\n'21 [130]F (RGB) - - DF 2D CNN`- SL C ID PS 25 k-fold UNBC 93.87 ACC;\n'16 [133]F (RGB) - - - CDBN SVM UL C P PS 25 LOSO:UNBC 87.20 ACC;\n'21 [134]F (RGB) - - - CDBN SVM SL C P PS 25 LOSO UNBC 93.16 AUC\n'19 [135]F (RGB) - - FF 2D CNN - SL C ID1, IC PS 25 LOSO UNBC 88.191ACC\n'20 [136]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 51.10 ACC;\n'20 [140]F (RGB) - - FF 2D CNN`- SL R ID S 25 ? UNBC 79.94 ACC;\n'21 [142]F (RGB) - - - 2D CNN - SL C ID O 8 k-fold other 97.48 ACC;\n'19 [145]F (RGB) - - - 2D CNN ELM SL R IC PS 25 k-fold UNBC\u201a1.22 MSE;\n'19 [146]F (RGB) - - - 2D CNN - SL C TR, CL, DI PS 25 k-fold UNBC 60.00 ACC\n'19 [150]F (RGB) - - - 2D CNN`- SL C AUs-D PS 25, 43 k-fold UNBC & CK+1,\nWilkie97.701ACC;\n'17 [152]F (RGB) statistics - - NN GPM WSL R IC O, S 25 k-fold UNBC 2.18 MAE\n'20 [153]F (RGB) statistics - FF 2D CNN`NN SL R IC S 25 k-fold UNBC 1.95 MAE;\n'19 [154]F (RGB) LBP, MDS - - 2D CNN`- SL C ID O 25 hold-out UNBC 80.00 ACC\n'18 [159]F (RGB) - - - 2D CNN`- SL C ID O ? hold-out other 78.30 ACC\n`: Pre-trained model -:Not exist &: in Dataset indicates the utilization of cross-database training/validation ?: Not found :: The authors provide additional experiments with other validation methods \u201a: The authors\nutilized occluded facial images ;: The authors provide additional metrics Modality: F: face region Non deep features: LBP: local binary pattern MDS: multidimensional scaling Fusion: M: fusion of modalities E:\nfusion of deep learned features or hand-crafted features Deep models: AE: autoencoder RCNN: recurrent convolutional neural network CDBN: convolutional deep belief network CNN: convolutional neural network\nNN: neural network WGAN-GP: Wasserstein generative adversarial model with gradient penalty Non deep model: SVM: support vector machine GPM: Gaussian process regression model kNN: k-nearest neighbors\nNB: naive Bayes ELM: extreme learning machine Learning Method: SL: supervised learning SeSL: semi-supervised learning UL: unsupervised learning WSL: weakly supervised learning Classific./Regres.: C:\nclassification R: regression Objective: P: presence of pain ID: intensity in discrete scale IC: intensity in continuous scale TR: trigger CL: climax DI: diminishing AUs-D: Action Units detection GT: ground truth\nPS: Prkachin and Solomon S: self-report O: observer rating ST: stimulus Validation Method: LOSO: leave one subject out Metrics: AUC: Area Under the ROC Curve ACC: accuracy PPV: precision MSE: mean\nsquared error MAE: mean absolute error3.4. UNIMODAL STUDIES 33Table 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [124]F (RGB) facial landmarks - FF 2D CNN NN SL C, R ID, IC1P 25 LOSO:UNBC 0.171MSE;\n'20 [125]F (RGB) HOG, head pose,\nAUs intensity/\noccurrence, facial\nlandmarksFF - 2D CNN`NN SL R IC O 36 hold-out EmoPain 5.48 RMSE;\n'20 [126]F (RGB) - - - 2D CNN`NN SL R IC O 36 hold-out EmoPain 1.49 RMSE;\n'18 [127]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 92.00 ACC;\n'18 [128]F (RGB) statistics, distance\nmetrics- FF 2D CNN`- SL C, R ID, IC PS 25 LOSO UNBC 0.81 PCC,\n0.69 MSE\n'21 [137]F (RGB) - - FF 2D CNN`- SL C, R ID, IC P 25 LOSO UNBC 91.13 ACC,\n0.78 PCC,\n0.46 MSE\n'18 [138]F (RGB) - - - AE`- SL R IC PS 25 k-fold UNBC 0.33 MAE;\n'21 [141]F (RGB) - - FF [AE, 2D CNN]Y- SL C, R ID1, IC2,\nP3P, ST 25, 87 LOSO UNBC1,\nBioVid (A)289.1711ACC,\n0.8121PCC,\n85.6532ACC,\n40.4012ACC\n'21 [143]F (RGB) entropy texture\ndescriptors- - 2D CNN`- SL C ID O 8 k-fold other 0.92 PPV;\n'18 [147]F (RGB) - - - 2D CNN`- SL C P PS 14 k-fold UNBC 93.00 ACC\n'19 [149]F (RGB) - - - 2D CNN - SL C P PS 25, 20 k-fold UNBC &\nBioVid (A)\u02db56.75 ACC\n'17 [156]F (RGB) HOG, LBP - FF 2D CNN`SVM SL C P O 26 LOSO iCOPE 73.78 ACC\n'19 [160]F (RGB) - - - 2D CNN - SL C P O 31 LOSO NPAD1,\niCOPE296.981ACC;,\n89.802ACC\n'21 [165]F (RGB) - - - 2D CNN`- FL C P PS 25 LOSO UNBC 76.00 ACC;\n'21 [166]F (RGB) - - - 2D CNN`- SL C P O 25 hold-out UNBC 75.49 ACC\n'21 [167]F (RGB) - - - 2D CNN`SVR SL R IC P 25 LOSO UNBC 0.34 MSE\n'21 [168]F (RGB) - - - 2D R-CNN - SL C P O ? hold-out other 87.80 PPV\nY: The authors combined the deep models into a unified framework \u02db: The authors experimented with additional datasets combinations Non deep features: AUs: actions units HOG: histogram of oriented gradients Non\ndeep model: SVR: support vector regression Learning Method: FL: federated learning Metrics: RMSE: root mean squared error34 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [161]F (RGB) optical flow - FF 2D CNN`SVM,\nkNN, NBSL C P O 31 k-fold other 92.71 ACC,\n94.80 AUR\n'19 [163]F (RGB) - - - WGAN-GP - SL C P O 26 LOSO iCOPE 93.38 ACC\n'17 [169]F (RGB) - - - 2D CNN`- SL R IC PS 25 LOSO UNBC 0.99 MAE;\n'20 [170]F (RGB) - - - 2D CNN - SL C ID ST 87 hold-out BioVid (A) 36.60 ACC\n'20 [171]F (RGB) - - - 2D CNN - SL C P PS 25 hold-out UNBC 97.00 PPV;\n'21 [172]F (RGB) - - - 2D CNN - SL C ID P 28 LOSO:UNBC 90.30 ACC\n'19 [173]F (RGB) - - - 2D CNN - SL C P O 31 hold-out NPAD1,\niCOPE291.001ACC;,\n84.502ACC;\n'21 [174]F (RGB) - - - 2D CNN`- SL C P O 26, 30 hold-out iCOPE &\nUNIFESP89.90 ACC;\n'21 [175]F (RGB) - - - 2D CNN - SL C AUs-D P 10 hold-out Pain-ICU 77.00 ACC;3.4. UNIMODAL STUDIES 35\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach)\nPain assessment is particularly challenging due to its complex and dynamic nature. Rely-\ning on static, individual frames to assess pain fails to capture the phenomenon\u2019s temporal\nprogression and often leads to inaccurate estimations. Additionally, many studies highlight\nthe difficulties of applying deep learning techniques to small datasets, with one proposed\nsolution being the combination of deep learning and traditional feature extraction methods.\nEgede et al. [176] addressed this by extracting deep features from a pre-trained CNN, explic-\nitly targeting the eyes and mouth regions. Using a relevance vector regressor (RVR), they\ndemonstrated that combining deep and hand-crafted features led to optimal performance. De-\nspite the valuable insights the UNBC-McMaster database provides, its imbalanced sample\ndistribution\u2014particularly the limited number of frames showing pain\u2014poses a significant\nchallenge for deep learning models. In response, Egede and Valstar [177] devised a method\nbased on the observation that neighboring pain level classes share many common features.\nThis approach allowed them to avoid extracting all possible features for classes with fewer\nsamples, as certain features had already been utilized from other related classes. The study\nalso showed that combining deep and hand-crafted features improved performance. How-\never, in a later study [178], the authors applied a similar approach, using only deep-learned\nfeatures to address data imbalance, but could not replicate the same high-performance levels.\nTavakolian et al. [179] took a different approach, focusing on the detection of genuine\nversus acted pain through facial expressions, a technique with important applications in both\nmedical and forensic contexts. They developed a residual GAN (R-GAN) to capture subtle\nfacial changes and the dynamic nature of expressions, using a weighted spatio-temporal pool-\ning (WSP) method. In a subsequent study [180], the authors suggested that self-supervised\nlearning could reduce the time and effort needed for data labeling, as it does not require\ncomplete dataset annotation. They introduced a new similarity function for learning general-\nized representations with a Siamese network. They also employed statistical spatio-temporal\ndistillation (SSD) based on the Gaussian scale mixture (GSM) to improve computational effi-\nciency. This technique encodes spatiotemporal variations in facial videos into a single RGB\nimage, simplifying the model while maintaining effectiveness.\nOther studies also aim to capture the dynamic aspects of pain. For instance, [181] com-\nbined a random forest classifier with the pre-trained MobileNetV2 model [182], encoding\nvideos by selecting and merging three frames from different time points into a single image.\nOthman et al. [183] emphasized the importance of using diverse datasets\u2014including vary-\ning age, gender, pose, occlusion, and lighting conditions\u2014to improve model generalization.\nThey used multiple data combinations and a reduced version of MobileNetV2 , showing that\ncross-dataset training is essential for achieving better generalizability.36 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.3 Vision-based: Implicit Temporal Utilization\nSeveral studies have explored the application of 3D CNNs for pain assessment. Tavakolian\nand Hadid [184] developed a 3D CNN to capture dynamic facial representations from videos.\nThey noted that researchers often use fixed temporal kernel depths when employing 3D\nconvolution techniques, which limits the ability to capture short, mid, and long temporal\nranges simultaneously. To address this, they designed a model with parallel 3D convolutional\nlayers featuring variable temporal depths, allowing the capture of temporal dependencies\nfrom 32consecutive frames. Similarly, Wang and Sun [185] applied 3D convolutions based\non the architecture proposed in [186], consisting of 8convolutional layers with 3\u02c63\u02c63\nfilters. While they reported high performance, the authors acknowledged that extracting deep\nfeatures from small datasets posed a challenge for model generalization. In a related study,\nHuang et al. [187] developed a framework that integrated 3D, 2D, and 1D CNNs to extract\nspatio-temporal, spatial, and geometric features. For the 3D CNN component, they modified\nthe architecture from [188] by using discrete kernels of 1\u02c63\u02c63and3\u02c61\u02c61rather than the\ntraditional 3\u02c63\u02c63kernel. Other researchers have also proposed 3D deep CNNs with varying\ntemporal depths to capture short, mid, and long-range facial expression variations [189].\nRecognizing the difficulty and time consumption involved in training a deep 3D CNN from\nscratch, they introduced a cross-architecture knowledge transfer learning technique, utilizing\na pre-trained 2D CNN to assist in the training of the 3D CNN. In studies by Praveen et\nal.[190] and [191], the authors employed weakly-supervised domain adaptation, where the\nsource domain focused on human affective expressions and the target domain was explicitly\nrelated to pain expressions. Their framework featured an inflated 3D-CNN (I3D) [192],\nincorporating 3convolutional layers and 3inception modules [132] to capture both spatial\nand temporal information from video data.\nBargshady et al. [193] opted to use the HSV color space instead of RGB, arguing that it\nbetter reflects human visual perception for tasks such as skin pixel detection and multi-face\ndetection. They employed the pre-trained VGG-Face [157] for feature extraction, followed\nby a temporal convolutional network (TCN) using dilated causal convolutional operations to\nleverage temporal dependencies. Rezaei et al. [194] tackled the challenge of pain detection\nin people with dementia, a difficult task due to insufficient pain-related images or videos\nof elderly subjects in existing datasets. They developed a 10-layer 2D CNN that processed\npairs of pain and no-pain images, analyzing frame-to-frame changes and employing con-\ntrastive training methods [195]. The model demonstrated high performance in both healthy\nindividuals and people with dementia. In another study, Pandit and Schmitt [196] explored\nthe potential of using shallow 1D CNN architectures for real-time pain recognition. They ex-\ntracted facial action units from each frame using the OpenFace 2.03toolkit, with promising\n3https://github.com/TadasBaltrusaitis/OpenFace3.4. UNIMODAL STUDIES 37\nresults for pain detection in real-time settings.\n3.4.4 Vision-based: Explicit Temporal Utilization\nSeveral efforts have focused on addressing the limitations of static frames by developing\ndedicated temporal modules. Zhou et al. [197] tackled this issue using a regression frame-\nwork based on a 4-layer recurrent convolutional neural network (RCNN), each with a se-\nquence length of 3time steps. Rodriguez et al. [198] leveraged dynamic information by\ndesigning an LSTM model fed with feature vectors extracted from VGG-16 [122]. Simi-\nlarly, Bellantonio et al. [199] emphasized that facial expressions evolve, making it essential\nto analyze the spatio-temporal dimension of pain. They improved estimation performance\nusing a fine-tuned 16-layer CNN model [157], an LSTM processing 16frames as a time\nwindow, and super-resolution techniques. In another study, Bargshady et al. [200] com-\nbined the VGG-Face CNN [157] with a 3-layer LSTM to extract spatio-temporal features\nfrom grayscale images, applying zero-phase component analysis (ZCA). In [201], principal\ncomponent analysis (PCA) was used to reduce dimensionality. Mauricio et al. [202] also\nemployed VGG-Face but replaced LSTM with a 2-layer gated recurrent unit (GRU) to cap-\nture temporal dependencies. Thuseethan et al. [203] used a conventional 2D CNN and two\nRCNNs to extract temporal features from previous and subsequent frames, enhancing the\ntime dimension of expression analysis.\nA similar approach was followed by Bargshady et al. [204], who employed ensemble\nlearning with three distinct CNN-biLSTM modules, merging their outputs for the final pre-\ndiction. Salekin et al. [205] used a bilinear CNN (B-CNN) based on the VGG architecture\n[121], pre-trained on VGGFace24andImageNet5datasets, along with an LSTM to capture\ntemporal dependencies in image sequences. Kalischek et al. [206] explored deep domain\nadaptation for facial expression and pain detection, utilizing the self-ensembling approach\n[207] with a long-term recurrent convolutional network (LRCN). While they achieved state-\nof-the-art results for facial expression recognition, performance was lower for pain detection,\nlikely due to the subtle nature of pain-related expressions.\nDespite the availability of additional information in pain datasets, multi-task approaches\nremain limited. Martinez et al. [208] proposed a personalized multi-task learning method\nbased on individual physiological and behavioral pain responses. They extracted AAM fa-\ncial landmarks, processed them through a biLSTM to produce PSPI scores, and predicted the\nfinal V AS score. Erekat et al. [209] combined AlexNet [155] with 2 GRU layers to capture\ntemporal dependencies, using both self and observer-reported pain intensity as ground truth.\nVuet al. [210] developed a multi-task framework to estimate pain levels while reconstruct-\n4https://www.robots.ox.ac.uk/ \u02dcvgg/data/vgg_face\n5https://www.image-net.org38 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\ning heatmaps of action unit locations, improving model generalization with a CNN-LSTM\ncombination to capture micro facial movements.\nHuang et al. [211] noted that specific frames within a video sequence exhibit more pro-\nnounced pain expressions, requiring special handling. They developed a novel framework\nusing attention saliency maps with a VGG-16 model, GRUs and learned weights for each\nframe\u2019s contribution to pain intensity estimation. The study demonstrated that dynamic and\nsalient features can significantly improve performance. Similarly, Yu et al. [212] used VGG-\n11 (configuration A) and an LSTM to create an attention mechanism, predicting pain in-\ntensity from 16consecutive frames. Xu and Liu [213] adopted a ResNet-50 model with an\nattention mechanism to extract spatial features, followed by a transformer encoder to capture\ntemporal sequences, achieving promising results.\nIn other studies, Ragolta et al. [214] used extracted action units to train a 2-layer LSTM\npredicting pain on an 11-point scale, employing curriculum learning. Guo et al. [215] devel-\noped a convolutional LSTM (C-LSTM) to extract both spatial and temporal features from\nvideos, showing that temporal models outperform non-temporal models for pain estimation\naccuracy. Rasipuram et al. [216] utilized in-the-wild video data for pain detection, gener-\nating a 3D morphable model without relying on facial landmarks and combining it with an\nLSTM. Zhi and Wan [217] introduced sparse coding with LSTM (SLTM), using the iterative\nhard thresholding algorithm (ISTA) [218] to capture dynamic facial expressions. Although\nSLTM did not achieve high performance, it offers speed and efficiency for specific applica-\ntions. Finally, Thiam et al. [219] developed a method combining motion history and optical\nflow images with a 10-layer CNN and 2-layer biLSTM, showing that weighted score aggre-\ngation improves performance. Table 3.3 summarizes studies incorporating the modalities\u2019\ntemporal dimensions.3.4. UNIMODAL STUDIES 39Table 3.3: Vision-based studies with temporal utilization.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'17 [176] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVR SL R IC PS 25 LOSO UNBC 0.99 RMSE,\n0.67 PCC\n'17 [177] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVM SL R IC PS 25 LOSO UNBC 1.04 RMSE,\n0.64 PCC\n\u201918 [178] F (RGB) - - - NL 2D CNN - SL R IC PS 25 LOSO UNBC 1.20 RMSE,\n0.47 PCC\n'18 [184] F (RGB) - - - I 3D CNN - SL R IC PS 25 LOSO UNBC 0.53 MSE,\n0.84 PCC;\n'18 [185] F (RGB) HOG,\ngeometric\ndifference- DF I 3D CNN SVR SL R IC PS 25 LOSO UNBC 0.94 RMSE,\n0.67 PCC\n'20 [191] F (RGB) - - - I 3D CNN`- WSL R IC PS 24, ? LOSO UNBC\n& RECOLA0.64 MAE,\n0.82 PCC;\n'16 [197] F (RGB) - - FF E RCNN - SL R IC PS 25 LOSO UNBC 1.54 MSE,\n0.65 PCC\n'17 [198] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C, R P, IC1PS 25 LOSO UNBC 0.741MSE,\n0.781PCC;\n'17 [199] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 61.90 ACC\n'19 [200] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 75.20 ACC\n'20 [201] F (RGB) PCA - DF E [2D CNN`,\n1D CNN, biLSTM]Y- SL C ID PS 25 LOSO:UNBC 85.00 ACC;\n'19 [202] F (RGB) - - - E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 85.40 ACC,\n0.62 MSE;\n'19 [203] F (RGB) - - FF E [2D CNN, RCNN]Y- SL R IC PS 25 LOSO UNBC 1.29 MSE,\n0.73 PCC\n'17 [208] F (RGB) - - FF E biLSTM HCRF,\nFCSL C IC O,\nS25 hold-out UNBC 2.46 MAE;\nNon deep features: PCA: principal component analysis Temporal Exploitation: NL: non-machine learning method I: implicit method E: explicit method Deep models: RCNN: recurrent convolutional neural network\nLSTM: long short memory networks biLSTM: bidirectional neural network GRU: gated recurrent unit Non deep models: SVM: support vector machine RVM: relevance vector machine GPM: Gaussian process regression\nmodel HCRF: hidden conditional random fields FC: fully connected SVR: support vector regression Objective: I2: intensity in binary pairs Metrics: PCC: Pearson correlation coefficient40 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [114]F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN RVR SL R IC O 13 LOSO APN-DB 1.71 MAE;\n'19 [179]F (RGB) - - - NL R-GAN - UL C genuine\nvs posedPS,\nST25,\n34,\n87,\n87? UNBC\n& STOIC\n& BioVid (A)\n& BioVid (D)90.97 ACC\n'20 [180]F (RGB) - - FF NL 2D CNN`- SSL C IC P,\nST25\n87LOSO UNBC1,\nBioVid (A)2\u20180.781PCC;,\n71.022AUC;\n'21 [181]F (RGB) AUs\nintensity- H NL 2D CNN`RF SL C ID ST 127 k-fold X-ITE 25.00 ACC\n'19 [183]F (RGB) - - - NL 2D CNN - SL C P ST 87\n134k-fold BioVid (A)\n& X-ITE\u201867.90 ACC\n'20 [209]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC O,\nS25 k-fold UNBC 2.34 MAE\n'20 [211]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC PS 19 LOSO UNBC 0.21 MSE,\n0.89 PCC\n'19 [212]F (RGB) - - FF E [2D CNN, LSTM]Y- SL R IC PS 24 LOSO UNBC 1.22 MSE;,\n0.40 PCC;\n'20 [214]F (RGB) AUs\nintensity- - E LSTM - SL R IC O 36 hold-out EmoPain 2.12 RMSE,\n1.60 MAE;\n'20 [216]F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C P O ? k-fold UNBC 78.20 ACC;\n'20 [219]F (RGB) - - DF E [2D CNN, biLSTM,\nNN]Y- SL C P ST 87\n40LOSO BioVid (A)1,\nSenseEmotion269.251ACC,\n64.352ACC\n'20 [220]F (RGB) - - FF E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 0.84 ACC,\n0.69 PCC;\n\u2018: The authors provide experiments with cross-dataset settings Fusion: H: hybrid Non deep models: RF: random forest classifier3.4. UNIMODAL STUDIES 41Table 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [187]F (RGB) facial\nlandmarks- DF I [3D CNN`,\n2D CNN`,\n1D CNN, FC]Y- SL R IC PS 25 LOSO UNBC 0.76 MSE,\n0.82 PCC;\n'19 [189]F (RGB) - - - I [2D CNN`,\n3D CNN]Y- UL,\nSLC, R IC1, P2P,\nST25, 87 LOSO UNBC1,\nBioVid (A)20.9211PCC;,\n86.0222AUC\n'20 [190]F (RGB) - - - I 3D CNN`- WSL R IC PS 24,?,\n87, 18LOSO UNBC1\n& RECOLA\n& BioVid (A)2\u20180.741PCC,\n0.342PCC\n'20 [193]F (RGB) PCA - FF I [2D CNN`,\nTCN]Y- SL C ID P,\nST25, 20 LOSO:UNBC1,\nMIntPAIN292.441ACC;,\n89.002ACC;\n'20 [194]F (RGB) - - - I 2D CNN - SL C, R IC, P1P 95, 25 k-fold UofR & UNBC182.0011PCC;\n'20 [196]F (RGB) AUs\noccurrence- FF I 1D CNN - SL R IC P 24, 87 hold-\noutUNBC1,\nBioVid (A)0.801CCC\n'20 [204]F (RGB) PCA - DF E [2D CNN`, 1D\nCNN, biLSTM]Y- SL C ID PS,\nST25, 20 k-fold UNBC1,\nMIntPAIN286.001ACC;\n92.262ACC;\n'20 [205]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- SL R P, IC1O 45 LOSO NPAD 3.991MSE,\n1.552MAE\n'19 [206]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- UL C P ST 40 LOSO SenseEmotion 60.61 ACC\n'21 [210]F (RGB) - - - E [2D CNN`,\nLSTM]Y- SL R IC P 25, 27 LOSO UNBC1,\nDISFA\u20180.60`MSE,\n0.82`PCC;\n'21 [213]F (RGB) - - - E [2D CNN`,\nTransformer]Y- SL R IC P 25 LOSO UNBC 0.40 MSE,\n0.76 PCC;\n'21 [215]F (RGB) - - - E 2D C-LSTM - SL C ID S 29 hold-\noutother 69.58 F1\n'19 [217]F (RGB) - - FF E SLSTM - SL C P1, ID2ST 85 LOSO BioVid (A) 61.701ACC\n29.702ACC\n'21 [221]F (RGB) - - - I 3D CNN`- SL R IC S 25 k-fold UNBC 0.66 ICC;\nFusion: H: hybrid Deep models: TCN: temporal convolutional neural network C-LSTM: convolutional-LSTM SLTM: sparse long short memory network Learning Method: SSL: self-supervised learning Metrics: F1:\nF1 score CCC: concordance correlation coefficient42 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.5 Touch sensor-based\nTouch (contact) sensors provide a viable alternative for pain assessment, often outperforming\nvision-based methods. Table 3.4 highlights studies that utilized contact sensor data to evalu-\nate pain. Yu et al. [222] analyzed three categories of pain-no pain, moderate pain, and severe\npain\u2014using EEG signals. They extracted several bands from the biosignals, including al-\npha, beta, and gamma, and applied a convolutional module. The study found that combining\nthese bands yielded better results than evaluating them independently. Similarly, [223] used\nEEG potentials with an autoencoder to compress the raw data and applied a logistic regressor\nfor classification.\nOther researchers, such as Rojas et al. [224], utilized functional near-infrared spec-\ntroscopy (fNIRS) for pain detection. They developed three models\u2014multilayer perceptron\n(MLP), LSTM, and biLSTM\u2014with biLSTM demonstrating superior accuracy. Addition-\nally, [225] focused on PPG signals, extracting hand-crafted features from the time and fre-\nquency domains, which were then combined with a deep belief network (DBN) to achieve\nover65% accuracy in a 4-class pain assessment task. Hu et al. [226] used kinematic data\nto compare healthy individuals with those suffering from low back pain (LBP). Their ap-\nproach, which employed two stacked LSTM layers, reached over 97% accuracy in binary\nclassification using raw motion data. Lastly, Mamontov et al. [227] were the first to apply\nevolutionary algorithms in the design of an optimized recurrent neural network (RNN) for\npain estimation, achieving 91.94% accuracy using EDA signals.\n3.4.6 Audio-based\nA few studies have explored using audio information for pain detection and intensity esti-\nmation, as outlined in Table 3.5. These methods are especially relevant for neonates, where\nfrequent facial and body occlusions make analyzing cries a more effective approach for pain\ndetection. Chang and Li [228] concentrated on infant cries to differentiate between hunger,\npain, and sleepiness. They transformed the audio signals into 2D spectrograms using a fast\nFourier transform (FFT) and trained a 2D CNN for feature extraction. Similarly, [229] uti-\nlized spectrograms generated from recorded sounds, employing a model identical to that\nused in [160]. Thiam and Schwenker [230] focused on detecting adult pain by analyzing\nbreathing sounds. They leveraged deep-learned features from spectrograms with Mel-scaled\nshort-time Fourier transform, combined with various handcrafted cues. A CNN followed by\na biLSTM was used to capture spatial and temporal dependencies, integrating both low- and\nhigh-level features. In a different approach, Tsai et al. [231] examined pain events during\nemergency triage. They developed an LSTM autoencoder framework to extract temporal\nfeatures from verbal behavior, reporting encouraging results.3.4. UNIMODAL STUDIES 43Table 3.4: Touch sensor-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'20 [222]EEG - - FF I 1D TCN - S C ID S 32 k-fold other 97.30 ACC;\n'20 [223]EEG - - - I AE (TCN) LR UL, S C P S 29 LOSO other 74.60 ACC\n'21 [224]fNIRS - - - E biLSTM - SL C ID S 18 k-fold other 90.60 ACC;\n'19 [225]PPG - - - NL DBN SBM U, SL C P1, ID2S 100 k-fold other 86.791ACC,\n65.572ACC\n'18 [226]kinematatics - - FF E LSTM - SL C P LBP 44 LOSO other 97.20 ACC;\n'19 [227]EDA - - FF E [RNN, LSTM,\nGRU, NN]YSelfCGA,\nselfCGP,\nPSOPBSL C P ST 40 LOSO Sense-\nEmotion81.94 ACC\n'21 [232]EDA - - - I NN - SL C P1, I2 ST 87,\n55LOSO BioVid (A)1,\nPainMonit284.2211ACC;,\n86.5012ACC;\nModality: PPG: photoplethysmogram fNIRS: functional near-infrared spectroscopy EEG: electroencephalography EDA: electrodermal activity Deep models: DBN: Deep belief network RNN: recurrent neural network\nNon deep models: SBM: selective bagging model LR: Logistic Regression SelfCGA: Self-Configuring Genetic Algorithm SelfCGP: Self-Configuring Genetic Programming PSOPB: Particle Swarm Optimisation with\nparasitic behaviour GT: LBP: low back pain vs healthy population\nTable 3.5: Audio-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'16 [228]audio (cry) - - - - 2D CNN - SL C P O ? k-fold other 78.50 ACC\n'19 [229]audio (cry) - - - - 2D CNN - SL C P O 31 LOSO:NPAD 96.77 ACC;\n'19 [230]audio\n(breathing)MFCCs,\nRASTA-\nPLP,\nDTD- FF E [2D CNN,\nLSTM]YRFc SL C P ST 40 LOSO Sense-\nEmotion64.39 ACC\n'17 [231]audio\n(voice)prosodic-\nspectral\nfeatures,\nSF- FF E LSTM`SVM UL,\nSLC P1, ID2S 63 LOSO other 72.301UAR,\n54.202UAR\nNon deep features: MFCCs: Mel Frequency Cepstral Coefficients RASTA-PLT: Relative Spectral Perceptual Linear Predictive DTD: descriptors from temporal domain SF: statistical features44 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.5 Multimodal studies\nSince pain is a multidimensional phenomenon, combining multiple modalities in a multi-\nmodal system offers a promising approach. Heterogeneous information sources can com-\nplement one another, enhancing specificity and sensitivity. As reported in [106], when in-\ndividual modalities demonstrate good predictive performance, their fusion tends to yield\nimproved outcomes. Moreover, integrating cues from various channels may be helpful and\nnecessary, especially in clinical settings where specific modalities may become unavailable\n(for instance, if the patient turns and their face is occluded). The information channels can\noriginate from (1) the same hardware sensor but focus on different regions of interest, such\nas RGB facial images and RGB body images [233], (2) different hardware sensors but the\nsame region of interest, like RGB facial images and thermal facial images [110], or (3)\ndifferent hardware sensors and information sources, such as RGB facial images and ECG\nsignals [234]. Table 3.6 lists the studies utilizing multimodal approaches.\n3.5.1 Static Analysis\nA commonly used biosignal combination is those of EDA, EMG, and ECG, as these channels\nare found in all main pain reference databases. Thiam et al. [235] applied an early fusion\nmethod by merging these signals into a 2D representation and inputting it into a 9-layer 2D\nCNN. Their results showed a strong correlation between EDA and pain intensity, and com-\nbining all three modalities did not outperform using EDA alone. Al-Qerem et al. [236] used\nleast generative adversarial networks (LSGANs) to enhance EMG, EDA, and ECG samples,\nreporting a notable improvement in classification when using an SVM on the augmented\ndataset. Haque et al. [110] introduced the MIntPAIN dataset, which includes RGB, depth,\nand thermal videos for multi-class ( 5levels) pain recognition. They combined these three vi-\nsual modalities into a 5D matrix (RGB+D+T) and used it to train the pre-trained VGG-Face\nmodel [157], leading to better classification performance in their experiments.\n3.5.2 Temporal Utilization\nZhiet al. [237] proposed a multimodal stream-integrated neural network that leverages video\nand biosignal data. They combined raw facial video frames with optical flow images to cap-\nture spatio-temporal dependencies via 3D CNNs, integrating these with biosignal features\nextracted using LSTMs. The entire network was trained end-to-end, achieving superior re-\nsults compared to their unimodal methods. Beyond facial analysis, Salekin et al. [233]\nfocused on assessing neonatal pain through body movements in videos. After identifying\nrelevant body regions, video frames were fed into a pre-trained VGG-16 [121], connected to\nan LSTM to capture temporal dynamics. In a follow-up study, Salekin et al. [238] fused three3.5. MULTIMODAL STUDIES 45\nmodalities\u2014facial expressions, body movements, and crying sounds\u2013demonstrating that this\nmultimodal approach outperformed unimodal techniques. Similarly, Wang et al. [239] ex-\nplored combining EMG, EDA, and ECG biosignals with handcrafted and learned features\nfrom a biLSTM model. They applied the minimum relevance method (MRMR) to reduce\nthe number of features, resulting in notable outcomes.\nIn addition to EDA, EMG, and ECG, other biosignal combinations have been explored.\nZhao et al. [240] integrated PPG, EDA, and temperature signals, using 2D convolutions for\nspatial feature extraction and time windows for capturing temporal information. Yuan et\nal.[241] successfully estimated pain using whole-body MoCap sensors and EMG, utilizing\nLSTM layers with an attention mechanism in an autoencoder, which reduced training time\nby leveraging latent space representations of raw data. Similarly, Li et al. [242] employed\nMoCap and EMG as data sources and tested various LSTM configurations to predict pain\nintensity, achieving the best performance with a 3-layer vanilla LSTM combined with a 3-\nlayer fully connected network.46 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.6: Multimodal-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [110]F (RGB,\nthermal,\ndepth)- RF - - 2D CNN`- SL C ID S 20 k-fold MIntPAIN 36.55 ACC\n'19 [233]F, B (RGB) - FF - E [2D CNN`,\nLSTM]Y- SL C P O 31 LOSO other 92.48 ACC;\n'19 [234]F (RGB),\nECG, EDAbiosignals\u2019\nfeaturesmFF FF - 2D CNN`RFc SL C I2 S 85 k-fold BioVid (A) 74.00 ACC\n'19 [235]EDA,\nEMG,\nECG- RF - - 2D CNN - SL C P1I2,\nID2S 87,\n86LOSO BioVid (A)1\nBioVid (B)84.4011ACC;,\n36.5412ACC;\n'20 [236]EDA,\nEMG,\nECGBoruta\nfeaturesFF - - LSGAN SVM UL,\nSLC I2, ID1S 85 hold-\noutBioVid (A) 82.801ACC\n'21 [237]F (RGB),\nEDA,\nEMG, ECGoptical\nflowFF FF NL,\nE, I[3D CNN,\nLSTM]Y- SL C, R P1, I2,\nID2S 87,\n40k-fold:BioVid (A)1,\nMIntPain68.2011ACC;,\n28.1021ACC\n'21 [238]F, B (RGB),\nsound- DF - E [2D CNN`,\nLSTM]Y- SL C P O 45 LOSO NPAD 78.95 ACC;\n'20 [239]EDA,\nEMG,\nECGMRMR,\nbiosig-\nnals\u2019\nfeaturesRF\nFFE biLSTM NN SL C P1, I2 S 87 LOSO BioVid (A) 83.301ACC\n'20 [243]EDA,\nEMG,\nECG- FF - I [DDCAE,\nNN]Y- UL,\nSLC P1, I2 S 87 LOSO BioVid (A) 83.991ACC;\n'21 [244]EDA,\nEMG,\nECG, RSP- FF - I [DDCAE,\nNN]Y- UL,\nSL,\nSSLC, R P1, ID2,\nICS 87,\n40LOSO BioVid (A)1,\nSense-\nEmotion84.2511ACC;,\n35.4421ACC;\n'21 [245]EDA, ECG - FF - E 1D CNN,\nLSTM- UL C P1, I2 S 67 hold-\noutBioVid (A) 81.711ACC\n'20 [240]PPG, EDA,\ntemperature- RF - I 2D CNN - SL R\u02ddP1, ID2S 21 k-fold other 96.301ACC,\n95.232ACC\n'20 [241]MoCap,\nEMG- RF - E AE, LSTM - UL,\nSLC ID O 23 LOSO:EmoPain 52.60 ACC;\n'20 [242]MoCap,\nEMG- RF - E LSTM, NN - UL C ID O 30 hold-\noutEmoPain 80.00 ACC;\n'21 [246]MoCap,\nEMG- RF - E LSTM, NN - SL C ID O 30 LOSO:EmoPain 54.60 ACC;\nm: Not specifically described \u02dd: Ordinal Modality F: face region B: body region EMG: electromyography Non deep features: MRMR: Minimum Redundancy Maximum Relevance method Deep models: LSGAN:\nLeast Square Generative Adversarial Networks3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 47\n3.6 Summary of Automatic Pain Assessment Methods\nThis section presents an analysis of the reviewed studies, summarizing the main conclusions\non current methods for automatic pain assessment, their advantages, and corresponding lim-\nitations. Additionally, it offers recommendations for future research directions that could\nadvance the field of pain research from a computational perspective.\n3.6.1 Input\nFirst, we observe a clear imbalance between unimodal and multimodal approaches in pain\nassessment studies. More than 86% of the reported research focuses on unimodal methods,\neven though the databases often contain multiple information channels. Notably, contact\nsensor-based and audio-based approaches are underrepresented, with only seven and four\nstudies, respectively, compared to 84studies that utilize a vision-based approach.\nMultimodal approaches are even less explored, with only 15studies falling into this\ncategory, making it difficult to draw strong conclusions about the effectiveness of specific\nmodality combinations. However, there are indications that EDA sensor data is particularly\nvaluable compared to other biopotentials. Researchers have primarily focused on visual data,\nlikely due to the complexity of implementing multimodal frameworks or the impracticality\nof contact sensors in non-laboratory settings. Further exploration of diverse modality com-\nbinations is necessary to evaluate their potential for pain assessment fully\u2014additionally, 28\nstudies employed non-deep features to enhance deep-learned representations.\nFinally, we identified three primary strategies in examining the approaches that utilize\ntemporal information: non-machine learning-based, machine learning-based (implicit), and\nmachine learning-based (explicit). Non-machine learning-based methods, such as motion\nhistory images [219] or temporal distillation [180], rely on traditional computer vision tech-\nniques. These methods tend to be more straightforward but are generally less sophisti-\ncated. In contrast, machine learning-based approaches [190] [217] offer richer temporal\ninformation and the flexibility to adapt to specific requirements, such as emphasizing certain\nvideo frames. Among the studies reviewed, 55% employed temporal features, with explicit\nmethods\u2014most commonly LSTM models\u2014being the predominant choice. Given that many\nstudies report superior performance when temporal information is incorporated, compared\nto non-temporal methods, it is evident that further emphasis on temporal approaches is war-\nranted.\n3.6.2 Processing\nRegarding machine learning approaches, various models and techniques have been employed\nfor pain estimation. CNN models remain the most widely used, with more than 75% of stud-48 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nies utilizing 1D, 2D, or 3D filters, highlighting the central role of convolution operations\nin deep learning. Sequential models, such as RNNs, GRUs, LSTMs, and biLSTMs, follow\nclosely behind in popularity. Almost half of the studies used pre-trained models to achieve\ntheir desired performance. This suggests that existing pain databases may not be adequate\nfor training deep-learning models from scratch. Non-deep learning models have also been\nemployed in 26studies as auxiliary decision components, with SVMs and shallow neural net-\nworks being the most common choices. There seems to be significant potential for adopting\nnewer deep learning architectures, especially transformer-based models, which have demon-\nstrated state-of-the-art results in various AI research fields and are particularly suited for\nexploiting temporal modality information [247].\nThe predominant learning method used across studies is supervised learning. How-\never, 16papers explored or adopted alternative methods such as unsupervised learning [119,\n133, 179, 189, 206, 223, 225, 231, 236, 241, 243], self-supervised [180, 244], self-supervised\nlearning [180, 244], semi-supervised learning [119], weakly supervised learning [190, 191],\nand federated learning [165]. Given the limited availability of pain data resources, self-\nsupervised learning appears to be the most appropriate method for future research and should\nbe further embraced by the community.\nLastly, it is notable that most studies\u2014approximately 70%\u2014treat pain assessment as a\nclassification problem rather than a regression problem. However, we believe that regres-\nsion more closely reflects the continuous nature of pain and is better suited to capturing the\ncomplexity of pain sensation.\n3.6.3 Evaluation\nThe primary objectives of the reviewed studies were (i)to estimate pain intensity on a dis-\ncrete scale (multi-class classification), (ii)to measure pain intensity on a continuous scale,\nand(iii)to determine the presence or absence of pain (binary classification). Notably, 25\nstudies focused on pain detection rather than pain intensity estimation, which, from a clin-\nical standpoint, is less informative as it does not provide sufficient data for effective pain\nmanagement. From an engineering perspective, detecting the presence or absence of pain is\nalso a more straightforward and less demanding task.\nA small subset of studies took a different approach to pain estimation. For instance, one\nstudy [179] sought to differentiate genuine pain from acted pain. Another [231] explored\npain events in emergency triage settings rather than controlled laboratory environments,\nwhile [234] examined the feasibility of real-time pain detection on IoT devices. Addition-\nally, [142] and [143] aimed to address the issue of occluded faces in pain estimation. So-\nciodemographic and psychological factors were also considered, as seen in studies like [245],\nwhich explored gender differences, and [194], which focused on pain assessment in elderly3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 49\npatients with dementia. The limited exploration of pain estimation in real-world settings\nor unconventional contexts suggests that current approaches may not be fully applicable in\npractical environments like clinics and hospitals.\nVarious annotation types are used regarding ground truth, such as self-reported ratings,\nFACS, and observer scales. Temporal features are critical for accurately estimating pain\nintensity, making the temporal granularity of the ground truth equally important. Several\nstudies have questioned the objectivity of PSPI scores, as noted in [248], which highlights\nthat PSPI scores can be zero even when pain is present or that there may be no visible facial\nexpressions in low-intensity pain. Pain expressions not captured by the FACS system, such\nas raising eyebrows or opening the mouth, further challenge the use of PSPI [249]. Addi-\ntionally, PSPI does not account for pain-related head and body movements, which are par-\nticularly valuable in newborn assessments [250]. For these reasons, we recommend moving\naway from PSPI as ground truth in favor of self-reports and observer scales at the video-\nsegment level.\nAround 54% of the studies employed the leave-one-subject-out (LOSO) validation method,\nwhich is widely regarded as more objective and better for assessing the generalizability of\nmodels. However, LOSO can be less practical due to the increased model size and longer\ntraining times. When researchers use other validation methods, such as k-fold or hold-out,\nit is essential to ensure that consecutive, highly correlated frames from the same subject do\nnot skew the training and validation results, leading to flawed estimations. Moreover, when\nresearchers define their own validation or testing sets, comparing results across studies\u2014\nespecially between classification and regression models\u2014becomes nearly impossible. We\nbelieve standardized evaluation protocols should be developed for each publicly available\ndatabase for these reasons.\n3.6.4 Pain Databases for Evaluation\nThe availability of suitable public databases is arguably the most crucial factor in addressing\nthe challenge of automatic pain assessment. Several aspects must be considered in evaluating\nthese datasets, including the number of subjects and their characteristics, such as age, sex,\nhealth status, and race. Moreover, the ground truth must be objective and offer meaningful\ninsights into the subject\u2019s pain experience [154].\nFig. 3.1 illustrates the number of papers corresponding to the pain database utilized in\neach study. It is clear from this figure that the UNBC andBioVid databases were the most\ncommonly used public datasets. However, the UNBC dataset does not record the subjects\u2019\nages, despite age being a known factor in pain expression [35,66]. While the BioVid dataset\ndoes document age, the oldest participants are only 65years old, which is notable since pain\nand its management are critical issues among individuals aged 65and older [251]. Simi-50 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nlar limitations are found in other pain datasets, such as X-ITE [117], EmoPain [115], and\nSenseEmotion [116].\nIt is well known that aging causes skin changes, including texture, rigidity, and elastic-\nity alterations, which can impact facial emotion recognition tasks [78]. Additionally, race-\nrelated factors can lead to inaccurate pain assessments due to variations in how pain is ex-\npressed [252]. Notably, one study by Nerella et al. [175] reported lower performance when\ntheir model was tested on African American patients. Furthermore, only one study [194]\nwas found that specifically addressed pain estimation in elderly individuals with dementia.\nIn summary, developing objective, automated, and generalizable deep learning-based\npain assessment systems will only be possible if balanced and representative datasets are\navailable for training and external validation.\n3.6.5 Interpretation of Results\nRecent advancements in AI have shown state-of-the-art performance across nearly every\nscientific discipline, often surpassing human accuracy in specific diagnostic tasks [253].\nHowever, a significant drawback of AI solutions, particularly deep neural networks, is their\nlack of transparency, commonly called \u201cblack box AI\u201d. This term highlights how these\nmodels learn intricate functions that are opaque and frequently incomprehensible to hu-\nmans [254]. This opacity is a primary reason for the criticism directed toward deep learning\ntechniques [255]. Various techniques, such as visualizations and gradients-backpropagation\nfocusing on specific units, have been developed to offer insights into how these models func-\ntion. For further reading, refer to the comprehensive review on explanatory techniques in\ndeep learning [256].\nTable 3.7 outlines the different approaches used to interpret model decisions. Only a\nsmall fraction of the reviewed studies\u2014 20out of 110\u2014implemented methods to explain\nhow their models work and which features or elements they focus on. It is important to\nnote that interpretable machine learning can be broadly defined as the \u201cextraction of rele-\nvant knowledge from a machine-learning model concerning relationships either contained\nin data or learned by the model\u201d [257]. To summarize: (i)18% of the reviewed studies\nprovided an approach to enhance the interpretability of the model\u2019s decision, (ii)all of these\nmethods were applied to studies using facial images as the input modality, and (iii)around\nhalf of these studies were conducted by just three specific research groups. These findings\nsuggest that the issue of interpretability and explainability within deep learning remains un-\nderexplored, particularly in the context of automatically classifying pain severity levels.3.7. CHALLENGES AND FUTURE DIRECTIONS 51\nTable 3.7: Interpretation approaches.\nPaper Year Modality Method\n[124] 2021 F (RGB) visualization (saliency maps)\n[128] 2018 F (RGB) visualization (heat maps)\n[130] 2021 F (RGB) visualization (saliency map)\n[133] 2016 F (RGB) visualization (learned filters)\n[134] 2021 F (RGB) visualization (learned filters)\n[135] 2019 F (RGB) visualization (heat maps),\nvalues of learned weights\n[138] 2018 F (RGB) visualization (saliency maps)\n[141] 2021 F (RGB) visualization (attention maps)\n[142] 2021 F (RGB) visualization (saliency map)\n[143] 2021 F (RGB) visualization (activation maps)\n[153] 2020 F (RGB) visualization (pixels contributions)\n[177] 2017 F (RGB) visualization (average saliency map)\n[179] 2019 F (RGB) visualization\n(generated intermediate representation)\n[194] 2020 F (RGB) visualization (saliency maps)\n[196] 2020 F (RGB) weights per AU (contribution of AUs)\n[173] 2019 F (RGB) visualization (feature maps)\n[174] 2021 F (RGB) visualization (integrated gradients)\n[210] 2021 F (RGB) visualization (heatmaps)\n[211] 2020 F (RGB) visualization (attention maps),\nvalues of learned weights\n[212] 2019 F (RGB) visualization (attention maps)\n3.7 Challenges and Future Directions\nThis section discusses the existing challenges in automatic pain assessment and proposes\nfuture research directions to further progress in the field.\n3.7.1 Current Challenges in Automatic Pain Assessment & Future Research Direc-\ntions\nSeveral limitations exist in the current pain databases. Important demographic factors such\nas sex, gender, and age are often missing, and there is an apparent lack of racial diversity\namong subjects. For example, facial structures and emotional expressions vary across Cau-\ncasian, Asian, and African populations [258]. Moreover, social interactions, such as the\npresence of a partner during assessments, could influence pain manifestation and should\nbe included in future datasets [69]. Estimating the location of pain, particularly for infants\nor individuals with communication impairments, is another vital aspect of pain assessment52 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsystems, which current databases largely overlook. Future datasets should incorporate stim-\nuli targeting various body locations. Furthermore, the videos in existing visual databases\noften have low to medium resolution and frame rates, which are inadequate for capturing\nfacial micro-expressions. Audio data is also sparsely represented, though it holds potential\nas a valuable modality. From an audio perspective, integrating natural language processing\n(NLP) methods to extract linguistic features and create multimodal systems is a promising\ndirection, as shown in affective computing research [259]. Finally, specific validation proto-\ncols should be provided with present and future datasets to ensure objective and consistent\ncomparisons across studies.\nFrom an engineering perspective, several issues must be addressed to advance automatic\npain assessment. Developing multimodal approaches is essential for creating robust systems\nwith enhanced capabilities. Not only do multimodal methods demonstrate better perfor-\nmance than unimodal ones, but they are also crucial in real-world scenarios where a specific\nmodality may become unavailable. Additionally, it is essential to exploit each modality\u2019s\ntemporal aspects fully. We encourage using machine learning models or other techniques\nthat can accommodate the dynamic nature of pain. More work is needed to improve the accu-\nracy of multi-level and low-intensity pain estimation. Another area of research involves the\nrelationship between pain and other affective states, such as negative emotions, which often\ncoexist during painful events. Detecting these emotions could improve pain assessment. Ad-\ndressing challenges like occlusions or poor lighting conditions in vision-based systems also\nrequires attention. Researchers should explore these scenarios, even if current databases do\nnot account for them. Real-time application of pain assessment systems is another critical\nfactor, so future studies should measure throughput, such as the number of images processed\nper second during inference. Generalization is another crucial concern for AI systems, and\nevaluating trained models across different pain databases could be valuable. Finally, to facil-\nitate the clinical adoption of AI-based pain assessment systems, the models\u2019 decisions need\ngreater explainability. Developing or adopting methods that improve interpretability will\nenhance their clinical viability.Chapter 4\nDemographic Variables: Their Role and\nImpact\nContents\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.1", "Synthetic Thermal Videos using Generative Adversarial Networks": ". . . . . 104\n6.2.1", "Combination of RGB and Synthetic Thermal Videos": ". . . . . . . . . . . . 105\n6.3.1", "General-Purpose Models": "117\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1197.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n8 Conclusions, Perspectives and Future Work 157\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\nBibliography 163\nAppendix 201\nAcronyms 209List of Figures\n2.1 The spinothalamic tract (STT) [43]. Pain, temperature, and some touch affer-\nents end in the posterior horn, where second-order fibers cross the midline\nto form the spinothalamic tract, ascending to the thalamus and projecting to\nvarious cortical areas. Along the way, collaterals connect to the reticular for-\nmation. Due to the rostral inclination of fibers in Lissauer\u2019s tract, cordotomy\nmust be performed several segments above the pain level for effective relief. 12\n2.2 Pain classification [48]: (A)Nociceptive pain , which results from detecting\npotentially harmful stimuli and serves a protective function. (B)Inflamma-\ntory pain is linked to tissue damage and immune cell infiltration, increas-\ning pain sensitivity during healing. (C)Pathological pain is a disease state\ncaused by either nervous system damage (neuropathic) or abnormal nervous\nsystem function (dysfunctional). . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 The number of studies utilizing these specific datasets. Note that various\nstudies used multiple datasets to conduct their experiments. . . . . . . . . . 25\n4.1 The PQRST waveform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure. . 55\n4.3 The signal preprocessing using the Pan-Tompkins algorithm. . . . . . . . . 57\n4.4 Results for the Gender Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Results for the Age Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.6 Results for the Gender-Age Scheme . . . . . . . . . . . . . . . . . . . . . . 64\n4.7 The proposed MTL network: The sizes of the extracted vectors for the net-\nwork are as follows: for the Pain classifier, n\u02c61, where nis the number of\npain estimation tasks ( e.g.,2for binary classification, 5for multi-class clas-\nsification); for the Age classifier, 36\u02c61, where 36represents the possible\nage values of the subjects; for the Gender classifier, 2\u02c61, corresponding to\nthe two possible gender categories ( i.e., males and females). . . . . . . . . 66\n4.8 Results for the proposed Schemes. . . . . . . . . . . . . . . . . . . . . . . 69\n4.9 Comparison of performances utilizing various neural networks approaches. 72\nxix5.1 The application of face alignment illustrates landmarks in 2D (left) and 3D\n(right) space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 An overview of our proposed transformer-based framework for automatic\npain assessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 The impact of the number of input frames on accuracy (left) and on runtime\nin milliseconds (right). Runtime calculated during inference on a NVIDIA\nRTX-3090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Relevance Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5 Outline of the proposed framework. . . . . . . . . . . . . . . . . . . . . . 86\n5.6 Comparison of mean accuracy and inference period for unimodal and multi-\nmodal strategies across NP versus P 4and MC tasks. The diagram adopts a\ndual-y-axis configuration\u2014accuracy measurements on the left and time met-\nrics on the right\u2014to outline the balance between performance efficacy and\ncomputational load, categorizing the methodologies along the x-axis. . . . . 98\n5.7 Regions highlighted in yellow and red denote areas of significant attention.\n(a) (1strow) Sequence of original frames. (2ndrow) Derived from the\nSpatial-Module after initial stage pretraining. (3rdrow) Derived from the\nSpatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module trained on the BioVid dataset. (b) (1strow) Derived from\ntheTemporal-Module incorporating video embeddings. (2ndrow) Derived\nfrom the Temporal-Module with heart rate embeddings. (3rdrow) Derived\nfrom the Temporal-Module using a combined embedding of video and heart\nrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.1 Illustration of the procedure for creating thermal images, featuring the archi-\ntecture of the Generator G(Encoder, mid-stage ResNet, Decoder), and the\nDiscriminator D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2 Representation of the proposed framework, illustrating its components and\ntheir main functions: (a)The Vision-MLP module, tasked with extracting\nfeature embeddings from video frames. (b)TheToken-Mixer , an important\nsub-module of Vision-MLP , generates the wave representation for the tokens.\n(c)The Channel-Mixer , a crucial sub-module within Vision-MLP .(d)The\nMLP, a core component of the Channel-Mixer .(e)The fusion procedure\nthat combines RGB and synthetic thermal embeddings, succeeded by the\nTransformer module, which conducts the final pain assessment. . . . . . . . 107\n6.3 Gradual blurring of RGB and synthetic thermal facial images: a series dis-\nplaying varying levels of Gaussian blur applied, with kernel sizes gradually\nincreased from k\u201c0(no blur) to k\u201c191(extensively blurred). . . . . . . 1136.4 Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain)\nclasses in RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191\n(heavily blurred). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1 PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer hav-\ning varying depths, showcasing how token resolution decreases at each stage;\n(b)Composition of the Token-Mixer module, featuring elements like depth-\nwise convolution (DWConv) and batch normalization; (c)Architecture of the\nFeed-Forward Network (FFN) within the Token-Mixer ;(d)The Cascaded\nAttention mechanism implemented across multiple heads, illustrating how\noutputs from preceding heads are incorporated to refine the self-attention\nprocess, culminating in the final output projection; (e)Configuration of the\nproposed multimodal pipeline, employing videos and fNIRS. The embed-\ndings from PainViT\u20131 are represented as waveform diagrams, which are\nmerged into a single diagram that illustrates both modalities before entering\nPainViT\u20132 for final pain evaluation. . . . . . . . . . . . . . . . . . . . . . . 151\n7.2 Waveform illustrations for various data types: (a)original fNIRS signal,\n(b)video embedding derived from PainViT\u20131 , and (c)fNIRS embedding\nobtained from PainViT\u20131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n7.3 Attention maps from the PainViT\u20132 . . . . . . . . . . . . . . . . . . . . . . 152\n7.4 Overview of primary models and their components outlined in this research:\n(a)PainFormer is structured hierarchically into four stages, incorporating\nSpectral andSelf-Attention Layers to extract embeddings from the inputs;\n(b)The Spectral Layer , a key element of PainFormer , uses FFT to ana-\nlyze frequency-specific data along with a learnable filter Kto highlight\ncritical frequencies; (c)The Self-Attention Layer , crucial for PainFormer ,\nenables parallel processing of features and their interconnections; (d)The\nEmbedding-Mixer , employing both cross and self-attention mechanisms, func-\ntions as the component for the final classification of embeddings in pain as-\nsessment; (e)TheVideo-Encoder , designed for compact and efficient encod-\ning, compresses video data into a reduced dimensional form; (f)TheMLP-1\nis part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)TheMLP-3 configuration is integrated into the Embedding-Mixer\nandVideo-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n7.5 Examples of different vision modalities in frame samples: (a)RGB frame,\n(b)synthetic thermal frame, and (c)depth estimation frame. . . . . . . . . 153\n7.6 Examples of different visual representations for biosignals: (a)waveform ,\n(b)spectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD . . 1547.7 An overview of the presented framework. PainFormer , the foundational\nmodel, excels in deriving high-quality embeddings from a diverse array of\nbehavioral and physiological modalities. The evaluation of RGB, thermal,\nand depth videos, alongside various representations of ECG, EMG, GSR,\nand fNIRS such as waveforms and spectrograms, underscores the rich infor-\nmation captured within these embeddings. Leveraging the embeddings from\nPainFormer facilitates the creation of various and diverse unimodal and mul-\ntimodal pipelines designed for the pain assessment task. Each pipeline can\nbe customized to suit the specific modalities involved, dataset characteristics,\nand the demands of the intended application or clinical setting. Our assess-\nments included the development and implementation of several pipelines\nin both unimodal and multimodal contexts, achieving leading-edge results\nacross various modalities and data representations. . . . . . . . . . . . . . 154\n7.8 Attention maps from the PainFormer :(a)(1strow) frames from RGB, ther-\nmal, and depth video modalities; (a)(2ndrow) corresponding attention maps;\n(b)(1strow) attention maps for ECG and EMG; (b)(2ndrow) attention maps\nfor EDA and fNIRS modalities. . . . . . . . . . . . . . . . . . . . . . . . . 155\n1 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n2 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n3 Additional attention maps from the PainViT\u20132 (refer to Section 7.2). . . . . 207List of Tables\n3.1 Most commonly utilized pain databases. . . . . . . . . . . . . . . . . . . . 24\n3.2 Vision-based studies with static analysis. . . . . . . . . . . . . . . . . . . . 32\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 33\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 34\n3.3 Vision-based studies with temporal utilization. . . . . . . . . . . . . . . . . 39\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 40\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 41\n3.4 Touch sensor-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Audio-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6 Multimodal-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.7 Interpretation approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1 Results for the Basic Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Results for the Gender Scheme (1). . . . . . . . . . . . . . . . . . . . . . . 60\n4.3 Results for the Age Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Results for the Gender-Age Scheme (Males) (1). . . . . . . . . . . . . . . . 62\n4.5 Results for the Gender-Age Scheme (Females) (1). . . . . . . . . . . . . . . 63\n4.6 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (1). 63\n4.7 Hyper-parameters used in our approach. . . . . . . . . . . . . . . . . . . . 65\n4.8 Results for the Basic Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . 68\n4.9 Results for the Gender Scheme (2). . . . . . . . . . . . . . . . . . . . . . . 68\n4.10 Results for the Age Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.11 Results for the Gender-Age Scheme (2). . . . . . . . . . . . . . . . . . . . 68\n4.12 Comparison of results adopting the feature augmentation approach. . . . . . 70\n4.13 Comparison of results adopting the MT-NN approach. . . . . . . . . . . . . 71\n4.14 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2). 72\n5.1 Training details for the automatic pain assessment. . . . . . . . . . . . . . 79\n5.2 Results on the pain estimation tasks. . . . . . . . . . . . . . . . . . . . . . 81\n5.3 Results for the pain estimation tasks using various numbers of input frames. 82\n5.4 Comparison of studies utilizing BioVid , RGB videos, and LOSO validation. 84\n5.5 Datasets utilized for the pre-training process of the framework. . . . . . . . 91\nxxiii5.6 Training details for the automatic pain assessment. . . . . . . . . . . . . . 92\n5.7 Results utilizing the video modality. . . . . . . . . . . . . . . . . . . . . . 93\n5.8 Results utilizing the heart rate modality. . . . . . . . . . . . . . . . . . . . 95\n5.9 Results utilizing the video &the heart rate modality. . . . . . . . . . . . . . 95\n5.10 Comparison of studies utilizing BioVid &LOSO validation, reported on ac-\ncuracy %. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.11 Module parameters and computational cost in FLOPS for the proposed frame-\nwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.1 Datasets utilized for the pretraining process of the framework. . . . . . . . 110\n6.2 Training specifications, and number of parameters and FLOPS for each mod-\nule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.3 Results utilizing the RGB video. . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Results utilizing the synthetic thermal video. . . . . . . . . . . . . . . . . . 112\n6.5 Results utilizing the RGB &the synthetic thermal video. . . . . . . . . . . . 113\n6.6 Results utilizing the fusion of RGB &synthetic thermal video. . . . . . . . 115\n6.7 Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.116\n6.8 Comparison with the MIntPAIN dataset. . . . . . . . . . . . . . . . . . . . 116\n7.1 Number of parameters and FLOPS for the components of the proposed Twins-\nPainViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.2 Datasets utilized for the pretraining process of the framework. . . . . . . . 123\n7.3 Training details for the automatic pain assessment. . . . . . . . . . . . . . 124\n7.4 Results utilizing the video modality & Addition method. . . . . . . . . . . . 125\n7.5 Results utilizing the video modality & Concatenation method. . . . . . . . . 125\n7.6 Results utilizing the HbR & Addition method. . . . . . . . . . . . . . . . . 126\n7.7 Results utilizing the HbR & Concatenation method. . . . . . . . . . . . . . 126\n7.8 Results utilizing the HbO & Addition method. . . . . . . . . . . . . . . . . 126\n7.9 Results utilizing the HbO & Concatenation method. . . . . . . . . . . . . . 127\n7.10 Results utilizing the HbR, HbO & Addition method. . . . . . . . . . . . . . 127\n7.11 Results utilizing the videos, HbO & Addition method. . . . . . . . . . . . . 127\n7.12 Results utilizing the videos, HbO & Single Diagram method. . . . . . . . . 128\n7.13 Comparison with the validation baseline provided by the AI4PAIN challenge\norganizers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.14 Number of parameters and FLOPS for the modules of the proposed framework.130\n7.15 Details of the PainFormer\u2019s architecture. . . . . . . . . . . . . . . . . . . . 132\n7.16 Datasets utilized for the multitask learning-based pretraining process of the\nframework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347.17 Training details of the proposed framework. . . . . . . . . . . . . . . . . . 135\n7.18 Results utilizing the video modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.19 Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.20 Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.21 Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.22 Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and\nF1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.23 Results on the validation set of AI4Pain dataset, multilevel classification task,\nreported on accuracy, recall and F1 score. . . . . . . . . . . . . . . . . . . 144\n7.24 Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.25 Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4\ntask and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.26 Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs.\nP4task and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.27 Comparison of studies on the testing set of AI4Pain dataset. . . . . . . . . . 148\n1 Results utilizing the video modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n2 Results utilizing the heart rate modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n3 Results utilizing the video &the heart rate modality reported on precision,\nrecall and F1 score (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . 202\n4 Results utilizing the RGB video modality, reported on recall and F1 score\n(refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n5 Results utilizing the synthetic thermal video modality, reported on recall and\nF1 score (refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . 203\n6 Results utilizing the fusion of RGB &synthetic thermal video modality, re-\nported on recall and F1 score (refer to Section 6.2). . . . . . . . . . . . . . 204\n7 Results of the proposed approaches, reported on macro-averaged precision,\nrecall and F1 score (refer to Section 7.2). . . . . . . . . . . . . . . . . . . . 204Chapter 1\nIntroduction\nContents\n1.1 Context and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Scope and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions \u2013 Peer-review Publications . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.1 Context and Motivation\nPain is a complex and deeply personal experience that is subjective by nature. Traditionally,\nit has been described in terms of its sensory dimension [1]. However, extensive research\nhas highlighted the importance of affective, cognitive and social aspects in shaping this ex-\nperience [2]. Studies have explored physiological, psychological, and socio-environmental\nfactors that contribute to the experience of pain. It is understood as a result of biological evo-\nlution and as influenced by psychological and social factors. As Ridell et al. [3] noted, \u201cPain\nis a synthesis\u2013a sum that is greater than its parts. \u201d The brain\u2019s ability to alter the perception\nof sensory inputs through the interplay of emotion, cognition, and social processes is signifi-\ncant. Although natural systems establish the initial biological framework for pain perception,\nthis structure is highly adaptable, particularly in humans. Throughout a person\u2019s life, both\nbiological developments and personal experiences significantly reshape this framework.\nA key question driving pain research across biological, psychological, and computational\nfields is why this topic of pain is meaningful and important. This question also forms the\nbasis for initiating this thesis, highlighting the broader relevance of studying pain. Williams\nand Kappesser [4] provide a compelling explanation, stating, \u201cWe care because we are wired\nto care: to attend to other people\u2019s expression of pain and to understand its meaning; to feel\ndistress in relation to their distress; and to be motivated to reduce their distress, and ours,\nif we are able to do so. \u201d This highlights the intrinsic human response to empathize and\n12 CHAPTER 1. INTRODUCTION\nalleviate pain, underlining the fundamental importance of this research area. Indeed, from a\nDarwinian perspective, pain serves a crucial role. The manifestation of pain in humans and\nthe reactions it elicits are examined through an evolutionary lens. Pain facilitates recovery by\npromoting responses to harmful stimuli and behaviors that demonstrate the adverse nature\nof painful experiences, common among animals. Specifically, the facial expression of pain,\nwhich communicates discomfort directly to those nearby, is universally recognized across\ndifferent ages, ethnicities, roles, and relationships. Evidence from healed major fractures\n[5, 6] suggests that injured members of hominid groups were not left to fend for themselves\nbut were supported through their recovery, indicating the fundamental importance of pain\nexpression in our evolutionary history.\nPain is a widespread health concern globally, affecting up to 30% of the adult popula-\ntion [7] and between 83% and93% of elderly adults in residential care [8]. The Global Bur-\nden of Disease (GBD) study identifies pain as the primary cause of years lived with disability\n(YLD) [9], with major contributors including chronic back pain, musculoskeletal disorders,\nand neck pain [10]. Pain impacts individuals and poses significant clinical, economic, and\nsocial challenges. In the United States, the economic and healthcare costs related to pain\ndue to reduced work productivity range from $560 to$635 billion annually, surpassing the\ncosts associated with heart disease, cancer, and diabetes combined [11]. In Europe, chronic\npain\u2019s direct healthcare costs and indirect socioeconomic impacts account for 3%to10% of\nthe GDP [12]. In Australia, the average annual cost for individuals among the 15.4%living\nwith chronic pain ranges from AU $22,588to AU $42,979, including non-financial costs [13].\nBeyond direct effects on health, pain contributes to a range of adverse outcomes, such as opi-\noid dependency, drug overuse, addiction, declining social relationships, and psychological\ndisorders [14]. In the last two decades, prescription opioid use has surged in the United\nStates, where overdose deaths have increased more than fourfold from 1999 to2016 [15].\nAdditionally, side effects from these opioids, like lethargy, depression, anxiety, and nausea,\nseverely impact workforce productivity and overall life quality [16].\nAccurate pain assessment is crucial for early diagnosis, disease progression monitoring,\nand treatment effectiveness evaluation, particularly in managing chronic pain [17]. This criti-\ncal role has resulted in pain being recognized as \u201cthe fifth vital sign\u201d in nursing literature [18].\nPain assessment is also fundamental in physiotherapy, where therapists apply external stim-\nuli and need to gauge the patient\u2019s pain levels accurately [19]. Objective evaluation of pain is\nessential to provide appropriate care, especially for vulnerable populations who may not be\nable to communicate their pain effectively, such as infants, young children, individuals with\nmental health issues, and the elderly. Various methods are used for pain assessment, with\nself-reporting\u2013where individuals describe their pain experiences\u2013considered the gold stan-\ndard [20]. Pain evaluation methods in clinical environments include quantifiable measures\nlike the Numeric Pain Rating Scale (NPRS), Visual Analogue Scale (V AS), and quantitative1.1. CONTEXT AND MOTIVATION 3\nsensory testing techniques such as the pressure pain detection threshold (PPDT) [21]. Behav-\nioral indicators are also crucial and include facial expressions ( e.g., grimacing, open mouth,\nlifted eyebrows), vocalizations (like crying, moaning, or screaming), and movements of the\nbody and head [22]. Physiological measures such as electrocardiography (ECG), electromyo-\ngraphy (EMG), galvanic skin responses (GSR), and respiration rates further contribute to\nunderstanding pain\u2019s physiological aspects [17]. Additionally, brain monitoring techniques\nlike near-infrared spectroscopy (fNIRS) have effectively detected changes in hemodynamic\nactivity associated with pain stimuli [23].\nCaregivers and family members often determine the presence or absence of pain in pa-\ntients by observing their behavioral or physiological responses [17]. However, accurately\nassessing pain poses a significant challenge for clinicians [24], especially with nonverbal\npatients such as the elderly, who may have reduced expressive abilities or may be reluctant\nto communicate pain [25]. Extensive research indicates that pain manifestations vary signif-\nicantly across different genders and ages, adding to the complexity of its assessment [26].\nFurther complicating the assessment process are the heightened workload and fatigue ex-\nperienced by nursing staff due to the demands of patient monitoring [27]. Technological\nsolutions are necessary for continuous patient monitoring. Nevertheless, concerns remain\nabout the objectivity and accuracy of these observations, as inadequately trained or biased\nobservers may struggle to assess pain [28] accurately. Even among trained observers, in-\nterpretations of behaviors can vary [22], and social and interpersonal dynamics can signif-\nicantly affect the pain assessment process, influencing both the evaluators\u2019 judgments and\nthe patients\u2019 expressions of pain [29]. Additionally, the presence of an observer can lead pa-\ntients to modify their behavior [30], and expressing pain through scales and measurements\ncan be challenging [31]. While self-reporting is used because pain is inherently subjective,\nrelying solely on a one-dimensional pain score fails to capture this complex phenomenon,\noften leading to inadequate pain management [32].\nGiven the challenges described above, scientific computing (SC) researchers have fo-\ncused on developing models and algorithms to enhance automatic pain recognition systems\nover the last two decades. Their goal is to accurately determine the presence and intensity\nof pain by analyzing physiological and behavioral indicators. Adopting deep learning and\nartificial intelligence (AI) techniques has expanded these automatic methods, designed to\ninterpret the complex and varied nature of pain [17]. Numerous studies have underscored\nthe effectiveness of automated systems that utilize behavioral or physiological modalities\nfor pain assessment [33]. Sario et al. [34] have shown the capability of these systems to\naccurately recognize pain through facial expressions, proving their utility in clinical envi-\nronments. Multimodal sensing has shown particular promise, offering enhanced accuracy\nin pain detection systems [22]. Furthermore, including temporal aspects in these modalities\nhas proven to significantly improve the accuracy of pain assessments [17].4 CHAPTER 1. INTRODUCTION\n1.2 Scope and Challenges\nAlthough considerable research has been conducted on automatic pain assessment, studies\nhave yet to explore factors like demographics and social aspects from a computational angle.\nFurthermore, despite the existence of deep learning-based methods, the approaches we ob-\nserve are often outdated and repeatedly recycled. For these reasons, we aimed to address two\nissues by (i)attempting to evaluate the social or demographic context, which significantly\nimpacts and influences pain sensation and perception, and (ii)introducing innovative deep\nlearning methods inspired by the latest developments in AI and generative AI literature. We\nbelieve these approaches can forge new paths in pain research, enhance the accuracy of rec-\nognizing this complex phenomenon, and, ultimately, be adopted in real-world scenarios to\nassist those in need. Additionally, (iii)recognizing the skepticism towards new technologies\namong clinicians and the general public, especially regarding the limited understanding of\nhow deep learning models function, we have devoted a portion of our research to interpret-\ning these models to offer some level of explanation and help the adoption process of them in\nclinical settings.\nNevertheless, this thesis initially faced challenges related to our objectives and goals as\nthe research progressed. The availability of pain datasets (to be discussed in the next chapter)\nis limited. Only a few datasets are available, and crucially, they are limited in size. This re-\nstriction poses a significant challenge for developing deep learning models, which typically\nrequire a large volume of data. In automatic pain assessment, researchers who develop deep\nlearning methods typically confront a decision: either train their models from scratch, which\ncan introduce performance limitations, or employ pre-trained models. These pre-trained\nmodels are generally trained on broadly available image datasets that include a variety of\nsubjects like animals and objects, or they rely on older architectures that were trained explic-\nitly on facial datasets. In this thesis, we addressed these issues by independently pre-training\nour deep-learning models using diverse datasets related explicitly to human facial images\nand biosignals. This strategy allowed us to design specific architectures to meet our unique\nneeds for each scenario, free from the constraints of relying on models developed and trained\nby others. Furthermore, we explored and evaluated several pre-training techniques to assess\ntheir effectiveness in pain assessment applications.\nRegarding, our objective to explore methods that utilize various modalities individually\nand in combination in a multimodal manner further constrains our dataset options. More-\nover, as previously outlined, our interest in the sociodemographic aspects of pain necessitates\ndatasets that include this information type, intensifying our challenges. For these reasons,\nthis thesis focuses specifically on examining the impact of age and gender on pain. In addi-\ntion, led us to utilize two pain datasets that most closely match the characteristics necessary\nfor our research, particularly in terms of demographic elements and multimodality.1.3. CONTRIBUTIONS \u2013 PEER-REVIEW PUBLICATIONS 5\n1.3 Contributions \u2013 Peer-review Publications\nThis section outlines the publications and projects produced during the Ph.D. research on\nautomatic pain assessment, where I was the first author.\n1.Automatic assessment of pain based on deep learning methods: A systematic re-\nview [17]\nThis systematic literature review (SLR) was conducted at the start of this Ph.D. re-\nsearch. This paper aims to explore the surge in recent years of deep learning algorithms\nadopted by researchers to encode the multidimensional nature of pain into meaning-\nful features. Specifically, this systematic review examines the models, methods, and\ndata types used to establish the foundation for deep learning-based automatic pain\nassessment systems. It identified relevant original studies from digital libraries such\nasScopus ,IEEE Xplore , and ACM Digital Library , following defined inclusion and\nexclusion criteria for studies published until December 2021 . The findings highlight\nthe critical role of multimodal approaches in automatic pain estimation, particularly\nin clinical environments, and emphasize the substantial gains observed with the inclu-\nsion of temporal exploitation of modalities. The review also recommends selecting\nhigh-performing deep learning architectures and methods, encouraging the adoption\nof robust evaluation protocols and interpretability techniques to deliver reliable and\nunderstandable outcomes. Additionally, it underscores the current limitations of exist-\ning pain databases in adequately supporting the development, validation, and practical\napplication of deep learning models as decision-support tools in real-world settings.\nFurthermore, we believe this paper is valuable not only for this Ph.D. project but also\nfor other practitioners and researchers in the field.\n2.Automatic Pain Intensity Estimation based on Electrocardiogram and Demographic\nFactors [35]\nThis study investigated the relationship between gender, age, and pain sensation and\ntheir effects on the automatic pain assessment process. By analyzing physiological\nsignals, particularly electrocardiography (ECG), we estimated pain intensity and ex-\namined the influence of these demographic factors. Utilizing the Pan-Tompkins algo-\nrithm for feature extraction and applying well-established classification methods, we\nexplored the correlation between gender, age, and pain manifestation.\n3.Multi-task Neural Networks for Pain Intensity Estimation Using Electrocardiogram\nand Demographic Factors [36]\nInspired by the previous study, this research further explored the influence of gender\nand age on pain perception. In this work, we analyze electrocardiography signals\nto uncover variations in pain perception across different demographic groups. We6 CHAPTER 1. INTRODUCTION\nleveraged these insights by developing a novel multi-task neural network for automatic\npain estimation, incorporating age and gender data for each individual. The study\ndemonstrated the advantages of this approach compared to other existing methods.\n4.A Full Transformer-based Framework for Automatic Pain Estimation using Videos\n[37]\nThis study introduced an innovative full transformer-based framework featuring a Trans-\nformer in Transformer (TNT) model combined with cross-attention and self-attention\nblocks. We achieved state-of-the-art performance using video data from the BioVid\ndatabase, demonstrating the model\u2019s effectiveness, efficiency, and strong generaliza-\ntion across primary pain estimation tasks.\n5.Multimodal automatic assessment of acute pain through facial videos and heart rate\nsignals utilizing transformer-based architectures [38]\nThis study presented a multimodal automatic acute pain assessment framework, inte-\ngrating video and heart rate signals. The framework consists of four key modules:\ntheSpatial Module , which extracts embeddings from videos; the Heart Rate Encoder ,\nwhich maps heart rate signals into a higher-dimensional space; the AugmNet , which\ngenerates learning-based augmentations in the latent space; and the Temporal Mod-\nule, which leverages the video and heart rate embeddings for the final assessment.\nThe Spatial Module undergoes a two-stage pre-training process: first, it learns uni-\nversal facial features through face recognition, followed by emotion recognition in a\nmultitask learning approach, enabling high-quality embeddings for pain assessment.\nExperiments with facial videos and heart rate data extracted from electrocardiograms\nin the BioVid database, alongside direct comparisons to 29studies, demonstrate state-\nof-the-art performance in unimodal and multimodal settings while maintaining high\nefficiency. In the multimodal setting, the framework achieved 82.74% accuracy for bi-\nnary pain classification and 39.77% for multi-level pain classification, using only 9.62\nmillion parameters across the entire framework.\n6.Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-\nMLP Architecture [39]\nThis paper introduced synthetic thermal videos generated by Generative Adversarial\nNetworks , which are integrated into the pain recognition process to assess their effec-\ntiveness. The framework employs a Vision-MLP andTransformer -based module, lever-\naging RBG and synthetic thermal videos in unimodal and multimodal settings. Exper-\niments conducted using facial videos from the BioVid database highlighted synthetic\nthermal videos\u2019 effectiveness and showcased their potential benefits in pain recognition\ntasks.1.4. THESIS OUTLINE 7\n7.Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for\nMultimodal Automatic Pain Assessment using Facial Videos and fNIRS [40]\nThis study was submitted to the First Multimodal Sensing Grand Challenge for Next-\nGen Pain Assessment (AI4PAIN) . The proposed multimodal framework leverages fa-\ncial videos and fNIRS, offering a modality-agnostic approach that eliminates the need\nfor domain-specific models. Utilizing a dual ViT configuration and waveform repre-\nsentations for both fNIRS and the extracted embeddings from the two modalities, the\nmethod demonstrates its effectiveness, achieving an accuracy of 46.76% in the multi-\nlevel pain assessment task.\n8.PainFormer: a Vision Foundation Model for Automatic Pain Assessment [41]1\nThis study introduces PainFormer , a vision foundation model built on multi-task learn-\ning principles and trained across 14distinct tasks and datasets comprising 10.9million\nsamples. As an embedding extractor for various input modalities, PainFormer provides\nfeature representations to the Embedding-Mixer , a transformer-based module respon-\nsible for conducting the final pain assessment. Extensive experimentation using both\nbehavioral modalities\u2013including RGB, synthetic thermal, and estimated depth videos\u2013\nand physiological modalities like ECG, EMG, GSR, and fNIRS revealed PainFormer \u2019s\nability to extract high-quality embeddings from diverse inputs. Tested on the BioVid\nandAI4Pain datasets and compared to more than 60existing methods, the framework\ndemonstrated state-of-the-art performance in unimodal and multimodal settings, posi-\ntioning itself as a step toward developing general-purpose models for automated pain\nevaluation.\n1.4 Thesis Outline\nThe dissertation is organized into the following chapters:\nChapter 2 introduces the foundational concepts of pain from biological, psychological, and\nclinical perspectives.\nChapter 3 reviews existing literature on automatic pain assessment using deep learning\nmethods and details the pain datasets used.\nChapter 4 outlines and proposes methods for evaluating demographic variables, their uti-\nlization, and their integration into an automatic pain assessment framework.\nChapter 5 discusses methods that utilize video and wearable device data, exploring the\ntrade-offs between efficiency and accuracy. It also proposes efficient, fast, effective models\nsuitable for real-world applications.\nChapter 6 explores synthetic data in pain assessment and introduces synthetic thermal im-\n1Under Review8 CHAPTER 1. INTRODUCTION\nagery techniques to enhance performance in automatic pain recognition.\nChapter 7 discusses general-purpose models, introduces a modality-agnostic framework,\nand presents the first foundation model used in automatic pain assessment.\nChapter 8 concludes the thesis with a final discussion, offering perspectives and ideas for\nfuture research in automatic pain assessment.Chapter 2\nClinical Pain Assessment\nContents\n2.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . . . . 20\n2.1 Chapter Overview\nThis chapter provides an anatomical and physiological overview of pain, focusing on the\nmechanisms responsible for generating, transmitting, processing, and interpreting pain sig-\nnals. It examines the various types of pain and explores the actions and expressions typically\nassociated with pain. Additionally, it reviews current pain assessment methods used in clin-\nical settings for adults, children, and newborns. The chapter also discusses developing and\nvalidating existing clinical pain assessment tools. This foundational knowledge is essen-\ntial for understanding the development and validation of computer-assisted pain assessment\n910 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nmethods discussed in later chapters. Finally, it highlights the challenges faced in clinical\npain assessment and underscores the need for automated pain assessment techniques.\n2.2 Biology of Pain\nPain, according to the International Association for the Study of Pain (IASP) [42], is \u201can\nunpleasant sensory and emotional experience associated with actual or potential tissue dam-\nage, or described in terms of such damage\u201d. Biologically, pain is an undesirable sensation\noriginating from the peripheral nervous system. Its fundamental function is to engage sen-\nsory neurons, notifying the organism of potential harm and playing a vital role in recognizing\nand responding to threats [43].\nThe transmission of a noxious stimulus from the periphery to the central nervous sys-\ntem involves a complex pathway through the spinal cord, resulting in the physical sensation\nof pain and a corresponding emotional response and memory. This process culminates in\nthe perception of pain. The initial stage of pain processing occurs when a stimulus at noci-\nceptive sensory fibers in the periphery is converted into an action potential. A nerve signal\nis generated if the stimulus is strong enough to surpass the action potential threshold [44].\nThis signal travels along the primary afferent fiber toward the central nervous system. As the\nstimulus intensity grows, more nerve fibers and areas of the nervous system are engaged [44].\nDue to their branching nature, primary afferent fibers typically relay information from sev-\neral pain receptors. These fibers and their receptors comprise a sensory unit, which gathers\ndata from a specific receptive field [44]. When receptive fields are larger and overlap with\nnearby fields, it becomes more challenging for the sensory system to locate the source of pain\naccurately. The primary afferent neuron is a pseudounipolar neuron that splits into a periph-\neral and central axon. The cell bodies of these neurons are located in the peripheral nervous\nsystem, within the posterior or cranial root ganglia. The peripheral axon extends to the skin,\nmuscles, tendons, or joints, branching into terminal fibers that connect with somatosensory\nreceptors. In contrast, the central axon leads to the central nervous system [45].\nPeripheral somatosensory fibers are categorized into three main groups. The first group\nincludes A\u00b4\u03b1,A\u00b4\u03b2,A\u00b4\u03b3fibers, large, myelinated fibers that rapidly conduct sig-\nnals [46]. These fibers involve touch and proprioception but are not associated with pain\nperception. The second group consists of A\u00b4\u03b4fibers, which are smaller and slower con-\nducting. Certain A\u00b4\u03b4fibers play a key role in pain sensation, with some responding only\nto intense mechanical stimuli and others reacting to noxious and non-noxious heat. The\nthird group comprises Cfibers, which are small, unmyelinated, and conduct signals very\nslowly. Most Cfibers are polymodal for pain perception, responding to various noxious\nmechanical, thermal, and chemical stimuli. These fibers are mainly linked to burning pain\nsensations [43]. The sensation of pain, known as nociception, is primarily facilitated by2.3. CLASSIFICATION AND CHARACTERISTICS OF PAIN 11\nvarious intracellular and extracellular molecular messengers. When activated by a specific\nstimulus, nociceptors relay information through glutamate, an excitatory neurotransmitter.\nAdditionally, inflammatory mediators are released at the injury site, further stimulating no-\nciceptor activation by releasing chemicals such as neurotransmitters ( e.g., serotonin), lipids\n(e.g., prostaglandins), peptides ( e.g., bradykinin), and neurotrophins ( e.g., nerve growth fac-\ntor) [46]. There are ascending tracts responsible for transmitting sensory information from\nthe periphery to the central nervous system. Fibers that convey two-point discrimination, tac-\ntile information, pressure, vibration, and proprioception ascend via the dorsal column of the\nspinal cord, forming the gracile and cuneate fasciculi. Fibers transmitting pain, temperature,\nand crude touch from somatic and visceral structures travel through the lateral spinothalamic\ntract. The anterior spinothalamic tract also transmits pain, temperature, and touch informa-\ntion to the brainstem and diencephalon (Figure 2.1) [47].\n2.3 Classification and Characteristics of Pain\nAccording to neurobiologist Clifford Woolf [48], pain can be classified into three categories\nbased on its function and characteristics: nociceptive ,inflammatory , and pathological pain.\nThese classes and their respective functions are illustrated in Figure 2.2.\nNociceptive pain (refer to Figure. 2.2(A)), arising from tissue damage, is a high-threshold\npain that activates only in response to intense stimuli [49], serving as a vital warning signal\nto the body. The neurobiological system responsible for nociceptive pain evolved from the\nability of even the most primitive nervous systems to detect impending or actual tissue dam-\nage caused by external stimuli. Its protective role requires immediate attention and action,\nachieved through the withdrawal reflex it initiates, the unpleasant sensation it produces, and\nthe emotional distress it triggers. Nociceptive pain demands avoidance in the present mo-\nment, and when activated, it overrides most other neural processes [48].\nInflammatory pain (refer to Figure. 2.2(B)) is also protective and adaptive, increasing\nsensory sensitivity following tissue damage to aid healing by discouraging movement and\ncontact with the injured area. This heightened sensitivity, or tenderness, helps prevent further\nharm and supports recovery, as seen after surgical wounds or inflamed joints where normally\nnon-painful stimuli now cause pain. It is triggered by immune system activation in response\nto tissue injury or infection. Despite its adaptive role, this pain often needs to be alleviated in\npatients with persistent inflammation, such as in rheumatoid arthritis or severe injuries [48].\nPathological pain (Figure. 2.2(C)) is maladaptive, arising from abnormal nervous sys-\ntem functioning and not serving a protective role. Unlike nociceptive and inflammatory pain,\npathological pain is a disease state of the nervous system itself. It may occur following\nnerve damage (neuropathic pain) or in conditions without apparent damage or inflammation\n(dysfunction l pain). Examples of dysfunctional pain include fibromyalgia, irritable bowel12 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFIGURE\n 2: Spinothalamic tract.\nPain, temperature, and some touch and pressure afferents end in the posterior horn. Second- or\nhigher-order fibers cross the midline, form the spinothalamic tract, and ascend to the ventral\nposterolateral (VPL) nucleus of the thalamus (and also to other thalamic nuclei not shown).\nThalamic cells then project to the somatosensory cortex of the postcentral gyrus, to the insula,\nand to other cortical areas (also not shown). Along their course through the brainstem,\nspinothalamic fibers give off many collaterals to the reticular formation (RF). The inset to the left\nshows the lamination of fibers in the posterior columns and the spinothalamic tract in a leg-\nlower trunk-upper trunk-arm sequence. The inset to the right shows the longitudinal formation\nof the spinothalamic tract. Primary afferents ascend several segments in Lissauer\u02bcs tract before\nall their branches terminate; fibers crossing to join the spinothalamic tract do so with a rostral\ninclination. As a result, a cordotomy incision at any given level would spare most of the\ninformation entering the contralateral side of the spinal cord at that level, and to be effective,\nthe incision must be made several segments rostral to the highest dermatomal level of pain.\n2017 Khalid et al. Cureus 9(10): e1754. DOI 10.7759/cureus.1754\n5\n of \n14\nFigure 2.1: The spinothalamic tract (STT) [43]. Pain, temperature, and some touch afferents\nend in the posterior horn, where second-order fibers cross the midline to form the\nspinothalamic tract, ascending to the thalamus and projecting to various cortical\nareas. Along the way, collaterals connect to the reticular formation. Due to the ros-\ntral inclination of fibers in Lissauer\u2019s tract, cordotomy must be performed several\nsegments above the pain level for effective relief.\nsyndrome, tension headaches, and temporomandibular joint disease, where significant pain\nexists without an apparent noxious stimulus or peripheral pathology. Pathological pain, a\nlow-threshold pain primarily driven by amplified sensory signals in the central nervous sys-\ntem, is the clinical pain syndrome with the greatest unmet need. To analogize, while nocicep-\ntive pain acts as a fire alarm for intense heat, and inflammatory pain reacts to warm tempera-\ntures, pathological pain is a false alarm triggered by a system malfunction. Thus, treatment\nmust specifically target the underlying mechanisms causing each type of pain [48].\nPain from a time-duration perspective can be categorized by duration into acute and2.4. PAIN INDICATORS 13\nchronic , with chronic pain persisting or recurring for more than three months [50]. Acute\npain is typically related to identifiable physiological damage from injury, surgery, illness,\ntrauma, or medical procedures and generally subsides once the underlying cause is resolved.\nHowever, if untreated, it may develop into chronic pain. Acute pain is further classified\nintoprocedural pain, caused by medical interventions such as muscular injections [51], and\npostoperative pain, which occurs after surgery and is a significant concern for both patients\nand healthcare providers. Effective management is crucial to aid recovery and prevent the\ntransition to chronic pain [52]. Chronic pain manifests in various forms, including chronic-\nrecurrent pain, like migraine headaches, and chronic-continuous pain, such as persistent low\nback pain [53].\n2.4 Pain Indicators\nPain can manifest in numerous ways and is often shaped by individual characteristics and\nenvironmental influences. Various human expressions, actions, and bodily responses have\nbeen linked to pain, serving both communicative and coping purposes. These pain indicators\nare generally categorized into three primary groups: (i)behavioral, (ii)physiological, and\n(iii)biochemical. While these indicators are universally present, certain expressions are more\nprominent in specific groups. For instance, crying is a common pain response across all age\ngroups but is more frequently observed in younger infants. This may be due to contextual\nfactors\u2014such as culture, social status, age, and ego\u2014influencing how pain is expressed\nover time. Adults, for example, may suppress crying in favor of other vocalizations, such as\ngroans and moans, as crying could be perceived as inappropriate in certain contexts. These\nmediating factors are often considered when interpreting pain indicators. The following\nsections will delve into each of these three categories [51].\n2.4.1 Behavioral Indicators\nBehavioral indicators such as facial expressions ( e.g., grimacing, open mouth, raised eye-\nbrows), vocalizations ( e.g., crying, moaning, screaming), and various bodily movements\n(e.g., changes in posture, signs of tension) are vital markers used in assessing pain [22].\nFacial expressions and limb movements in response to acute pain are typically rapid and\ninvoluntary. Facial reactions include brow bulging, eye squeezing, nasolabial furrow forma-\ntion [54], grimacing, clenched teeth, jaw-dropping, and tightened lips [55]. Body movements\nassociated with pain include bracing (gripping an object or the affected area during move-\nment), rubbing (massaging the painful area), restlessness (constant shifting of position) [55],\nand knee flexion [56]. Non-verbal vocalizations such as groaning, moaning, sighing, crying,\nand gasping [57] also indicate pain. Verbal expressions like \u201couch\u201d ,\u201cstop\u201d ,\u201cthat hurts\u201d ,14 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFigure 2.2: Pain classification [48]: (A)Nociceptive pain , which results from detecting po-\ntentially harmful stimuli and serves a protective function. (B)Inflammatory pain is\nlinked to tissue damage and immune cell infiltration, increasing pain sensitivity dur-\ning healing. (C)Pathological pain is a disease state caused by either nervous sys-\ntem damage (neuropathic) or abnormal nervous system function (dysfunctional).\n\u201cthat is enough\u201d , and even cursing [55] also serve as pain indicators. Interestingly, swearing\nhas been found to significantly alleviate pain, although its effect diminishes with frequent\nuse over a short period [58, 59].2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 15\n2.4.2 Physiological Indicators\nVital signs can reflect the state of the central nervous system, and since pain is mediated\nthrough this system, trends in vital signs can provide insights into pain levels. Clinical stud-\nies [60, 61] have examined physiological changes in response to pain and established em-\npirical solid evidence linking pain to vital sign alterations. However, as vital signs can also\nchange due to other non-pain-related pathological conditions, it is recommended that they be\nassessed alongside behavioral pain indicators for accuracy. Physiological pain responses are\nconsidered more reliable than behavioral signals, as they cannot be consciously controlled\nor altered. Physiological measurements such as electrocardiography (ECG), electromyogra-\nphy (EMG), galvanic skin responses (GSR), and respiration rate provide critical insights into\nthe body\u2019s reaction to pain [17]. In addition, brain monitoring techniques like near-infrared\nspectroscopy (fNIRS) have demonstrated the ability to detect pain-related hemodynamic\nchanges [23]. At the same time, functional magnetic resonance imaging (fMRI) has been\nexplored for assessing pain in both normal and pathological conditions [62].\n2.4.3 Biochemical Indicators\nCompared to other pain indicators, biochemical changes are the most precise and sensitive\nreactions to pain. However, their routine use in pain assessment is restricted due to the\ninvasive nature of measurement techniques [63]. These biochemical responses are most\nevident during surgical procedures with limited anesthesia, leading to increased levels of\nendorphins, norepinephrine, cortisol, growth hormones, renin, glucagon, aldosterone, and\ncatecholamines, along with a decrease in insulin levels [60].\n2.5 Sociodemographic and Psychological Variables\nIn1965 , Melzack and Wall [64] introduced the \u201cGate Control Theory\u201d , which interprets pain\nfrom two perspectives. The first involves the mechanisms of nociceptive signal transmission\nand modulation, while the second emphasizes pain as a psychophysiological phenomenon\narising from the interaction between physiological and psychological factors [53]. Observa-\ntions, empirical research, and theoretical models increasingly suggest that a comprehensive\nunderstanding of pain requires a biopsychological approach. It is also becoming apparent\nthat, although pain is often regarded as private and subjective, it is also fundamentally a so-\ncial experience [53]. Pain is not solely explained by biomedical components ( e.g., muscle\ndamage) but also involves psychological ( e.g., cognitive, affective) and social factors ( e.g.,\nfriends, family, health professionals), leading to what is known as a biopsychosocial sensa-\ntion [65]. Numerous factors contribute to how painful experiences are expressed and per-\nceived, varying wildly due to social and personal biases. These factors prompted Williams16 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nand Craig [2] to define pain as \u201ca distressing experience associated with actual or potential\ntissue damage with sensory, emotional, cognitive, and social components. \u201d\n2.5.1 Sex and Gender\nSeveral studies have explored the relationship between gender and pain expressiveness, as\nwell as variations in pain reporting. Research indicates that women generally exhibit a lower\npain threshold compared to men. A meta-analysis by Boerner et al. [66] on gender differ-\nences in children and adolescents found that girls over the age of 12reported higher pain\nintensity in response to cold-induced pain than boys. Furthermore, multiple studies suggest\nthat women tend to describe a greater degree of pain compared to men. In addition to bi-\nological differences, psychological aspects linked to gender also play a role. For instance,\nindividuals with a masculine identity may be less inclined to express or report their pain or\nseek assistance [67].\nMoreover, the manifestation of pain is not only influenced by the individual\u2019s gender but\nalso by dyadic interactions between people of different sexes. Levine and Desimone [68]\nconducted one of the initial studies on this phenomenon, showing that male participants in\na cold pressure experiment reported lower pain intensity when a female experimenter was\npresent. Similarly, McClelland and McCubbin [69] found that female participants expressed\nand reported higher pain levels when accompanied by a female friend. This dynamic also\nextends to patient-healthcare provider interactions. In studying health records, Vigil and\nAlcock [70] discovered that when the pain intensity was reported as high, the patients ( i.e.,\nmen and women) were examined by a female doctor or nurse. Additionally, studies exam-\nining gender differences among physicians in pain treatment options revealed that female\npatients were more likely to receive prescriptions for more potent drugs, such as analgesics,\nand female physicians were more likely to prescribe medications. Extensive research has\nalso shown that both lay observers and healthcare professionals tend to estimate higher pain\nlevels for female patients compared to male patients [71]. Hooper et al. [72] further noted\nthat clinicians communicate more effectively with female patients, often displaying greater\nempathy. Gender roles, beliefs, and expectations play a significant role in understanding\nhow social factors influence the differences in pain perception and experience between men\nand women [73].\n2.5.2 Age\nAge plays a crucial role in pain assessment and management. At the same time, there are\nsignificant challenges, limitations, and biases related to the patient\u2019s age group. Two of the\nmost vulnerable groups, albeit for different reasons, are the elderly and infants.\nPain recognition and interpretation among the elderly, particularly by caregivers, often2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 17\npresent unique challenges. Older adults frequently exhibit stoicism and reluctance to ex-\npress their pain, while healthcare providers struggle to accurately assess the patient\u2019s pain,\nleading to inappropriate pain management decisions [74]. McPherson et al. [75] noted that\ndespite caregivers\u2019 accommodating and empathetic relationships with elderly patients, con-\nflicts still arise. Older patients may resist acknowledging their weaknesses and accepting\nhelp, which can cause them to conceal their pain. The situation becomes even more complex\nwhen dealing with dementia, a disorder encompassing a range of conditions ( e.g., Parkin-\nson\u2019s, Alzheimer\u2019s, Vascular dementia), characterized by abnormal brain changes that im-\npair cognitive and linguistic abilities. A person with dementia may find it challenging to\ncommunicate their pain verbally. However, non-verbal pain expressions remain intact even\nin moderate dementia, although such reactions can be exaggerated [76]. However, aggres-\nsive behavior and disturbances in dementia patients, often caused by pain, are frequently\nmisinterpreted as psychiatric symptoms, leading to improper medication that can have life-\nthreatening consequences [77]. Caregivers of dementia patients face additional challenges,\nnot only related to pain management but also in addressing dementia\u2019s impact on language\nand memory. Particularly in the later stages of dementia, patients encounter severe pain\ncommunication difficulties due to cognitive decline, necessitating that caregivers recognize\nbehavioral and contextual indicators of pain [74]. Age is also known to cause changes in\nskin characteristics, such as texture, rigidity, and elasticity, which impact the performance of\nemotional face recognition tasks [78].\nInfants represent another vulnerable age group where pain assessment requires special-\nized attention, particularly when they experience painful events. The first challenge is obvi-\nously their limited reporting ability to express their pain through language. Although crying\nmight appear to signal pain, this is an oversimplified and unreliable method, as crying can in-\ndicate a variety of situations, such as discomfort, hunger, or pain. Accurately discerning the\ntype of cry is only one part of the challenge; assessing pain in infants is far more complex and\ninfluenced by numerous factors, including the interpersonal relationships within their envi-\nronment. Riddell and Racine [79] found that through various distressing experiences, infants\ncan learn that specific signaling behaviors can prompt their caregiver\u2019s proximity. This at-\ntachment dynamic suggests that, to some extent, infants may consciously utilize pain-related\nbehaviors to elicit responses from their caregivers. Similarly, the context affects older chil-\ndren as well; for example, self-reports of pain tend to be significantly lower when a parent is\npresent compared to when the child is alone [80].\n2.5.3 Psychological Factors\nMultiple studies have revealed that several psychological factors are consistently linked with\npain-related behavior, including depression, pain-related fear, and catastrophizing. Research18 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nfocusing on the impact of depression and anxiety on pain-related behavior has been con-\nducted mainly on patient populations. These studies have shown that depressed individ-\nuals exhibit more pronounced protective and communicative behaviors compared to non-\ndepressed patients [81]. Similarly, numerous studies suggest that patients with higher levels\nof anxiety demonstrate more pain-related behaviors than those with lower anxiety levels [82].\nDespite the frequent coexistence of pain with psychological conditions, research indicates\nthat these patients often experience underestimation of their pain. For instance, De Ruddere\net al. [83] found that patients dealing with psychological stressors such as anxiety, depres-\nsion, and daily life challenges are often perceived by physiotherapists as experiencing less\nsevere pain, illustrating the influence of psychosocial factors on the patient\u2019s pain experience.\n2.5.4 Race and Culture\nPain expression is generally understood across ethnicities and cultures, though differences\nexist in how it is conveyed [4]. However, cultural variations and the nuances of facial ex-\npressions related to emotion are complex and necessitate deeper study. Additionally, racial\nand cultural biases significantly influence pain assessment, judgment, and interpretation.\nExtensive research highlights the impact of a patient\u2019s race as a sociodemographic factor\non observer responses. The most examined topic relates to the different responses toward\nCaucasian versus non-Caucasian individuals, particularly African Americans, who are more\nlikely to have their pain underestimated and undertreated by healthcare providers [84].\nEthnocultural factors are crucial in shaping how individuals perceive and express pain.\nFor example, Western cultures often emphasize conservative expressions and self-control,\nleading to restrained responses in personal pain experiences and in perceiving others\u2019 pain\n[3]. Differences also arise in coping mechanisms; African Americans, for instance, are more\nprone to catastrophizing pain events compared to European Americans [85]. Furthermore,\nevidence shows racial biases in pain treatment across various racial groups, with certain\ngroups being more sensitive to pain but receiving lower-quality treatment [86]. For exam-\nple, Cleeland et al. [87] found that minority cancer patients, mainly Black and Hispanic\nindividuals, were more likely to experience inadequate analgesia compared to non-minority\npatients.\n2.5.5 Observer\u2019s Impact on Pain\nThe variability in pain management stems from the interplay of various elements, including\nsociocultural, biomedical, and psychosocial factors, especially in cases of chronic pain [88].\nWhen it comes to the observer responsible for assessing a patient\u2019s pain, several characteris-\ntics directly influence the objectivity of their evaluation. The first and perhaps most critical\nfactor is the observer\u2019s experience level. One would expect that more experience leads to2.6. IMPACT OF INADEQUATE PAIN MANAGEMENT 19\nbetter and more accurate assessments, but studies show that even experienced healthcare\nproviders consistently underestimate pain, much like laypersons [28]. The greater the expe-\nrience, the more pronounced the underestimation tends to be. This may be due to desensitiza-\ntion caused by repeated exposure to pain events, as seen in the differences between internists\nand surgeons in their evaluation of postoperative pain, with surgeons often encountering se-\nvere pain more regularly [89]. Another significant factor is the observer\u2019s knowledge and\nbeliefs about pain. For example, [83] found that laypersons and healthcare professionals\nwithout physical signs of pain might view the patient\u2019s complaints less seriously. Proper\ntraining is also essential for adequate pain assessment, which is why the Department of\nHealth and Human Services (DHHS) initiated a strategic program to improve healthcare\nproviders\u2019 education and knowledge regarding pain management, following evidence of in-\nadequate training in the field [90].\n2.6 Impact of Inadequate Pain Management\nThe experience of pain, particularly persistent pain, can have detrimental effects on the indi-\nvidual and their surrounding environment. Thoughts about severe pain often lead to grief and\nfear, causing individuals to perceive pain as a threat and feel incapable of managing it. This\ncan prompt avoidance behaviors aimed at escaping perceived harm [91]. Studies have shown\nthat children with a catastrophizing mindset about pain struggle with daily activities, while\nadolescents with chronic pain tend to have fewer friends and may miss out on social and\nentertainment opportunities, putting them at greater risk of victimization [92]. These adoles-\ncents often feel isolated and lonely compared to their healthy peers, and they may experience\nanxiety in social interactions [93]. Parental reactions to their children\u2019s pain can further com-\nplicate the situation, as parents with catastrophic tendencies tend to engage in overprotective\nbehaviors that hinder the child\u2019s functioning and psychosocial development [94]. Addition-\nally, the family\u2019s overall dynamic is affected, with the patient\u2019s sadness, sleep disorders, and\nchanges in leisure activities impacting the household [74].\nOn a biological level, pain, particularly when experienced early and severely, can alter\nthe brain and nervous system. These early pain experiences can disrupt neurobiological\ndevelopment and affect how pain is processed later in life [95]. A growing body of research\nlinks chronic pain to changes in the medial prefrontal cortex, a region crucial to emotional\nprocessing. Chronic pain is associated with structural and biochemical alterations in this\nbrain area, suggesting that these changes play a role in the pathophysiology of chronic pain\n[96].20 CHAPTER 2. CLINICAL PAIN ASSESSMENT\n2.7 Pain Measurement Scales and Metrics\nIn clinical settings, self-reporting remains the gold standard for assessing pain, allowing in-\ndividuals to describe their pain\u2019s intensity and location. Various self-report scales have been\ndeveloped for different age groups, such as the visual analog scale (V AS) [97] and the verbal\nrating scale (VRS) [98]. Additionally, observation-based scales, where a third party eval-\nuates the pain\u2019s severity, include tools like the Prkachin and Solomon pain intensity scale\n(PSPI) [99] and the neonatal/infant pain scale (NIPS) [100]. However, some studies suggest\nthat patients may exaggerate their pain severity to prompt more aggressive treatment inter-\nventions [101], raising concerns about the accuracy of self-reported symptoms. Therefore,\nobjective pain measurement remains clinically crucial.Chapter 3\nAutomatic Pain Assessment\u2013A Literature\nReview\nContents\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database . . . . 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 49\n2122 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research Di-\nrections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.1 Chapter Overview\nThis chapter corresponds to the publication [17], a systematic literature review (SLR) con-\nducted at the start of this Ph.D. research. This review facilitated an understanding of au-\ntomatic pain assessment methods, particularly those based on deep learning, and the tech-\nniques and strategies employed. It enabled the identification and proposal of new approaches\nthat could enhance the effectiveness of pain recognition.\nAdditionally, it allowed for identifying gaps in the literature from other reviews con-\nducted on this specific research topic. Every existing systematic review on pain assessment\nwas identified and assessed, revealing several insights. The first review on automatic pain\nassessment, published by Prkachin in 2009 [102], did not cover papers on deep learning,\nas the practical implementations of deep architectures only began around 2012. Zamzmi et\nal.[103] focused their review exclusively on infants, omitting deep learning methods. In\n2018, Chen et al. [104] reviewed automated pain detection methods using the Facial Ac-\ntion Coding System (FACS), noting only three publications that employed deep learning\ntechniques. In 2019, Hassan et al. [105] included only seven papers that used deep learn-\ning methods in their review. Similarly, Werner et al. [106], also in 2019, discussed pain\nassessment without restrictions on modalities or age groups, finding fewer than ten papers\nthat reported on deep learning methods. In 2020, Al-Eidan et al. [107] published the first\nsystematic literature review titled \u201cDeep-Learning-Based Models for Pain Recognition: A\nSystematic Review\u201d, which included fifteen papers but was critiqued for having significant\nlimitations and incorrect information. It was noted that some papers analyzed might not be\nrelevant, and there was confusion between \u201cneural networks\u201d and \u201cdeep learning\u201d. For in-\nstance, while study [105] mentioned using neural network approaches, they did not provide\nevidence of using deep learning methods. Moreover, in the study [104], the authors devel-\noped a neural network with only two layers combined with handcrafted features, which does\nnot qualify as a deep learning method. Additionally, studies [103, 107] focused on detect-\ning protective movement behaviors in chronic pain patients, which deviates from the central\ntopic of automatic pain assessment. Several reviews and SLRs on automatic pain assessment\nhave been published, but none exclusively or adequately focus on deep learning methods.\nThis SLR aims to bridge this gap by thoroughly reviewing deep learning techniques used for\nautomatic pain assessment.3.2. MODALITIES AND HARDWARE FOR AUTOMATIC PAIN ASSESSMENT 23\n3.2 Modalities and Hardware for Automatic Pain Assessment\nCreating an automatic pain assessment system hinges on capturing the necessary input data\nthrough various information channels, referred to as modalities. These modalities are cat-\negorized into behavioral and physiological types. A system utilizing only one modality is\ntermed unimodal, whereas a multimodal system incorporates multiple modalities.\nKey behavioral modalities encompass facial expressions, body movements, gestures, and\nauditory signals. Researchers use a range of optical and light sensors to record images or\nvideo sequences of facial and body movements. Commonly, researchers employ color RGB\ncameras, but depth and thermal sensors are also used to enhance visual data. Motion capture\nsensors are also employed to track movements, and microphones are frequently employed\nto capture sound. On the physiological front, modalities often involve biosignals that detect\nelectrical activities from various tissues and organs. Techniques such as electrocardiogra-\nphy (ECG), electromyography (EMG), electrodermal activity (EDA), photoplethysmogra-\nphy (PPG), blood oxygen saturation (SpO2), near-infrared spectroscopy (NIRS), respiration\nrate, and skin temperature are commonly used to gauge pain. Multiple sensors can mea-\nsure several modalities simultaneously \u2014 for instance, strain sensors and cameras can track\nrespiration rates.\nBesides the sensors that gather input data, the computational hardware is crucial. Deep\nlearning-based systems operate in two phases: training and inference. The training phase is\nparticularly resource-intensive, necessitating a graphics processing unit (GPU). The trained\nmodel makes predictions on new data during inference, typically processed on a central\nprocessing unit (CPU). The choice of hardware depends on various factors, especially in\nreal-time scenarios where low latency is crucial, compared to offline settings where data\nprocessing can be deferred. Additionally, characteristics of the model, such as floating point\noperations per second (FLOPS) and total computational operations, are significant consider-\nations.\n3.3 Pain Databases\nAccess to data is crucial for evaluating methods and algorithms in automatic pain assess-\nment. However, only a few databases have explicitly been developed for automatic pain\nrecognition based on human behavioral and physiological changes. Unlike the extensive\ndata found in most facial expression databases, publicly accessible pain datasets often offer\nlimited samples and suffer from significant class imbalance. This primarily stems from the\nethical concerns associated with collecting pain data. Table 3.1 lists the principal databases\nreviewed in the studies. Figure 3.1 shows how frequently each database was used. Most\nresearch utilized publicly available datasets, with some studies exploring multiple datasets.24 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nFew studies used private datasets, mainly those aimed at detecting pain in neonates. The\nUNBC-McMaster Shoulder Pain Archive Database [108] is the most utilized, followed by\nTheBioVid Heat Pain Database [109]. The former contains 200facial videos of 25individ-\nuals with shoulder pain. At the same time, the latter includes facial videos and biopotentials\nof90healthy participants subjected to experimentally induced heat pain at four intensity\nlevels. The following subsections provide a brief description of some of these datasets.\nTable 3.1: Most commonly utilized pain databases.\nDatabase Modality Population Annotation\nGranularityAnnotation Labels\nUNBC-McMaster\nShoulder PainA[108]RGB video of face 25 adults with shoulder painFrame level\nSequence levelFACS\nV AS, OPI\nBioVidA[109] RGB video of face, EDA, ECG,\nEMG87 healthy adults Sequence level stimulus\n(calibrated per person)\nMIntPAINA[110] RGB-Depth-Thermal video of\nface20 healthy adults Sequence level stimulus\n(calibrated per person),\nV AS\niCOPEA[111] RGB photographs of face 26 healthy neonates Frame level pain, cry, rest, air puff,\nfriction\niCOPEvidA[112] Grayscale video of face 49 neonates Sequence level pain, no pain\nNPAD-IA[113] RGB video of face & body, HR,\nSpO2, BP, NIRS36 healthy neonates & 9 neonates\nwith tissue injured by surgerySequence level NIPS, N-PASS\nAPN-dbA[114] RGB video of face 112 healthy neonates Sequence level NFLAPS, NIPS, NFCS\nEmoPainN[115] video, audio, EMG, MoCap 22 adults with chronic pack pain &\n28 healthy adultsSequence level self-report, naive OPI\nSenseEmotionN\n[116]video of face, audio, EDA, ECG,\nEMG, RSP45 healthy adults Sequence level stimulus\n(calibrated per person)\nX-ITEN[117] RGB-Thermal video of face,\nRGB-Depth video of body, au-\ndio, EDA, ECG, EMG134 healthy adults Sequence level stimulus\n(calibrated per person)\nA: Publicly available by request, complete or part of the dataset N: Not yet available Modality: HR: heart rate SpO2: oxygen saturation rate BP:\nblood pressure NIRS: near-infrared spectroscopy MoCap: motion capture RSP: respiration rate EDA: electrodermal activity ECG: electrocardiogram EMG:\nelectromyogram Annotation Labels: FACS: Facial Action Coding System V AS: visual analogue scale OPI: observer pain intensity NIPS: neonatal infant\nscale N-PASS: neonatal pain, agitation and sedation scale NFLAPS: neonatal face and limb acute pain scale NFCS: neonatal facial coding system\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database\nTheUNBC McMaster Shoulder Pain Database [108] comprises 200video sequences show-\ning the facial expressions of 25subjects undergoing motion tests, including arm abduction\nand external and internal rotations. The data collection utilized both active and passive ap-\nproaches: in the active mode, subjects moved their affected arms to their bearable limit,\nwhile in the passive mode, a physiotherapist moved the subjects\u2019 arms. Each video sequence\ncontains about 60to700frames, totaling 48,398, with 82.71% of frames scoring a pain\nrating of zero, indicating a significant imbalance in the data. All frames are FACS-coded\nfor pain-related action units (AUs)\u2014AU4, AU6, AU7, AU9, AU10, AU12, AU20, AU25,\nAU26, AU27, and AU43\u2014with each AU coded for intensity from A to E, 0, or5, except\nfor AU43 (closed eyes), which is coded as either present or absent. Pain scores are assigned\nusing the PSPI metric based on the intensity of the AUs present. Additionally, the database3.3. PAIN DATABASES 25\n0815233038455360\nUNBC-McMasterBioVidEmoPainSenseEmotionX-ITEMIntPAINiCOPEiCOPEvidNPAD-IAPN-dbother\nTable 1Category AUNBC-McMaster59BioVid21EmoPain7SenseEmotion5X-ITE2MIntPAIN4iCOPE3iCOPEvid1NPAD-I5APN-db1other21\n1\nFigure 3.1: The number of studies utilizing these specific datasets. Note that various studies\nused multiple datasets to conduct their experiments.\nincludes 66facial landmarks per frame, determined by an active appearance model. Pain as-\nsessments also include self-reports using two Likert scales with 15options each and a visual\nanalog scale (V AS) from 1(no pain) to 10(extreme pain). One scale measures the sen-\nsory intensity from \u201cextremely weak\u201d to \u201cextremely intense\u201d , while the other assesses the\naffective-motivation aspect of pain from \u201cbearable\u201d to \u201cextremely excruciating\u201d. Indepen-\ndent observer pain intensity (OPI) ratings use a 6-point scale from 0(no pain) to 5(intense\npain). The UNBC database is currently the most extensively utilized dataset for automatic\npain recognition among publicly available resources.\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database\nTheBioVid dataset [109] is a prominent resource in pain research, comprising facial videos,\nelectrocardiograms, electromyograms, and galvanic skin response data from eighty-seven\npn=87qhealthy participants ( 44males and 43females, aged 20to65). The pain was in-\nduced using a thermode on the participants\u2019 right arm, with pain and tolerance thresholds\nestablished before data collection. These thresholds defined the range of pain from No Pain\n(NP) to Very Severe Pain (P 4), encompassing five levels of pain intensity. The temperatures\nfor the pain inductions ranged from P 1to P 4and did not exceed 50.5\u02ddC. Each participant\nunderwent 20inductions at each of four pain levels, with each induction lasting 4sfollowed\nby a recovery period of 8to12s. In addition, 20baseline measurements were taken at 32\u02ddC\n(NP), totaling 100stimulations per participant, randomly administered. Data processing\nsegmented these into 5.5sdurations starting 1safter the target temperature was reached, re-26 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsulting in 8,700samples across the five pain intensity classes, equally distributed among all\nmodalities for each participant. Video recordings were made at a frame rate of 25FPS, and\nbiosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\n3.3.3 The EmoPain Database\nTheEmoPain [115] dataset encompasses various pain indicators, including body movements,\naudio, biosignals, and postural and facial expressions. It features video and audio recordings\nof22patients ( 7male, 15female) exhibiting natural pain expressions while engaging in\nphysiotherapy-like exercises. These exercises, performed at regular and challenging levels,\ninclude a sitting-standing sequence, balancing on one leg for five minutes, and reaching for-\nward while standing. The video signals are captured in high resolution ( 1024\u02c61024 pixels)\nusing eight cameras positioned at various angles, enhanced by specialized lighting condi-\ntions. Audio is recorded with two microphones: an AKG C-1000S MKIII placed near the\ncameras and an AKG HC 577 L worn by the patients, both operating at a 48kHz sampling\nrate with bit Pulse Code Modulation. Body movements and postures are tracked using a mo-\ntion capture suit with 18sensors distributed across the body. Biosignals are monitored with\nfour sEMG sensors attached to the trapezius and lumbar para-spinal muscles. Additionally,\nthe dataset provides continuous frame-wise pain ratings for facial expressions by eight naive\nannotators and binary frame-wise annotations for protective behaviors by four experts, along\nwith coordinates from 26body nodes. Six annotated protective behaviors include stiffness,\nbracing, hesitation, limping, rubbing, and abrupt actions. Audio and EMG signals from the\neight activities per subject also contribute to multimodal pain recognition. Like the UNBC\ndatabase, EmoPain faces significant challenges due to data sparsity and imbalance\u2014only\n11.4%of frames show facial expressions of pain, and 8.6%show protective behaviors. This\nscarcity complicates pain recognition research, necessitating the development of methods\nthat efficiently utilize limited data to achieve optimal performance.\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database\nTheX-ITE [117] dataset is one of the largest pain datasets but is not publicly available. It\ninvolved 134healthy adults ( 67men and 67women) aged between 18and50. The aver-\nage age was 31.4years (SD = 9.7), with men averaging 33.4years (SD = 9.3) and women\n32.9years (SD = 10.2). Participants had no chronic pain, depression, psychiatric disorders,\nneurological conditions, headache syndromes, or cardiovascular disease, nor had they taken\npain medication or painkillers before the experiment. Pain stimuli were stimulated using the\nMedoc PATHWAY Model ATS for heat pain on the forearm and the Digitimer DS7A for elec-\ntrical pain on the index and middle fingers. Both modalities featured phasic stimuli (short,\n5seconds) and tonic stimuli (long, 60seconds), each in three intensities. After calibration,3.4. UNIMODAL STUDIES 27\nparticipants underwent a 90-minute stimulation phase where phasic stimuli were repeated\n30times in a randomized sequence with 8-12-second pauses. The tonic stimuli were applied\nonce per intensity, totaling six per participant, each followed by a five-minute pause. The\nhighest intensity tonic stimuli for heat and electrical pain were induced at the experiment\u2019s\nending, with the other stimuli randomly interspersed during the phasic period. Simultane-\nous to the pain stimulation, various sensors collected multimodal pain response data: frontal\nand side view RGB videos for facial expression and head pose analysis, audio for paralin-\nguistic response analysis, electrocardiogram (ECG) to monitor heart rate variability, surface\nelectromyography (EMG) to assess muscle activity in the trapezius, corrugator supercilii,\nand zygomaticus major, electrodermal activity (EDA) to measure sweating, video for body\nmovement analysis, and thermal video for facial temperature changes.\n3.3.5 The AI4Pain Database\nThe AI4Pain Grand Challenge 2024 [118] dataset is a recent contribution to the pain re-\nsearch field, tailored for sophisticated pain recognition tasks using fNIRS and facial video\ndata. This dataset involves sixty-five volunteers pn=65q, including 23females, with ages\nranging from 17to52years (mean age of 29.06years and a standard deviation of 8.28years).\nAlthough it captures physiological signals such as photoplethysmography (PPG), electroder-\nmal activity (EDA), and respiration (RESP), these signals are not publicly available yet. The\ndataset is segmented into three parts: training ( 41volunteers), validation ( 12volunteers),\nand testing ( 12volunteers). The experimental setup includes fNIRS data recorded with an\nArtinis device, measuring changes in oxygenated and deoxygenated haemoglobin concentra-\ntions across 24channels targeting the prefrontal cortex. The optodes configuration includes\n10sources and 8detectors spaced 30mm apart, using near-infrared light at 760nm and 840\nnm, sampled at 50Hz. Additionally, facial movements are captured by a Logitech Stream-\nCam at30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low Pain ,\nandHigh Pain . It features 65instances of No Pain (each lasting 60s),780instances of Low\nPain (each lasting 10s), and 780instances of High Pain (each lasting 10s). The No Pain\ninstances, recorded during baseline, serve as control data. The Low Pain instances reflect\nmild pain responses, and the High Pain instances capture significant pain, both derived from\na pain tolerance test and reflected in the corresponding neurological and behavioral data\nrecorded.\n3.4 Unimodal studies\nThis section presents the studies that utilized only one information channel to estimate the\nsubject\u2019s pain condition.28 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.1 Vision-based: Static Analysis\nThe first publicly available pain database that significantly contributed to the development\nof automatic pain assessment methods was the UNBC-McMaster Shoulder Pain Database .\nNumerous studies have employed this dataset. Pedersen [119] implemented the first deep\nlearning approach in 2015 to address the pain assessment problem, utilizing a 4-layer contrac-\ntive autoencoder. He combined the encoded representations with a support vector machine\n(SVM), achieving high performance in frame-level pain detection. A significant advance-\nment in vision-based pain recognition methods was the EmoPain challenge in 2020, which\nbecame the first international competition to compare machine learning methods for chronic\npain assessment. Egede et al. [120] presented the EMOPAIN 2020 Challenge , utilizing a\ndataset composed of features extracted via both handcrafted methods and deep-learned mod-\nels. They utilized facial landmarks, histogram of oriented gradients (HOG), and deep vectors\nfrom VGG-16 [121] and ResNet-50 [122], both pre-trained on the Aff-Wild dataset1. The au-\nthors report that combining hand-engineered features with deep learning cues led to the best\nperformance. Similarly, Yang et al. [123] extracted both low- and high-level features from lo-\ncal descriptors and the pre-trained VGG-16 CNN, combining them through weighted coeffi-\ncients. Semwal and Londhe [124] demonstrated that fusing deep-learned features with facial\nlandmarks is beneficial for multi-class pain estimation. Lakshminarayan et al. [125] com-\nbined deep-learned features with handcrafted ones\u2014namely features from VGG-16 [121]\nandResNet-50 [122], HOG, action unit occurrence and intensity, facial landmarks, and head\npose\u2014through a fully connected network. Their study found that combining VGG-16 with\nhandcrafted features lowered regression error, whereas [126] achieved maximal performance\nusing only VGG-16 features with a fully connected network.\nConversely, Semwal and Londhe [127] noted the limitations of traditional handcrafted\nfeature engineering and the computational expense of deep neural networks. As a solution,\nthey proposed a relatively shallow 4-layer CNN, which reduces computational costs due to\nfewer parameters while achieving performance comparable to deeper models. A different\napproach came from [128], where the authors focused on representing facial expressions\nas compact binary codes for pain intensity classification. Feature extraction was conducted\nusing a pre-trained model [129], with a fully connected network used to generate the binary\ncodes.\nSeveral studies utilized CNN ensemble designs with varying architectures to exploit fea-\nture diversity. Semwal and Londhe [130] combined predictions from three compact CNNs\u2014\nVGG-16 ,M-MobileNet [131], and GoogleNet [132]\u2014using the average ensemble rule, re-\nsulting in improved classification performance. Kharghanian et al. [133] developed a con-\nvolutional deep belief network (CDBN) using unsupervised feature learning. An SVM used\n1https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge3.4. UNIMODAL STUDIES 29\nthe extracted features to differentiate between two states for binary pain classification ( i.e.,\npain vs. no pain). Later, [134] added two layers to the CDBN, though the results were not\ndirectly comparable due to differing evaluation methods.\nSeveral papers suggest that because pain is predominantly expressed in specific facial\nregions, focusing on these areas rather than the whole face could improve model accuracy by\nreducing noise. Huang et al. [135] initially identified the left eye, right eye, nose, and mouth\nas key regions and utilized a multi-stream CNN for feature extraction, assigning learned\nweights to enhance attention on these regions. Xin et al. [136] employed a 9-layer CNN\nwith an attention mechanism to assign different weights to face regions, resulting in more\naccurate attention face maps and boosting prediction accuracy by up to 19%. Cui and Huang\n[137] introduced a multi-scale regional attention network (MSRAN), which uses multiple\ncropping regions from video frames. The framework includes self-attention and relation-\nattention modules to highlight pain-relevant regions and explore interrelationships. Li et\nal.[138] extended this concept by integrating contrastive and multi-task training through an\nautoencoder, building on the work of [139].\nOne challenge in pain intensity estimation is that individual facial features, such as face\nshape, can introduce significant variability in how pain is expressed. This makes it difficult\nto distinguish between adjacent intensity levels. To address this, Peng et al. [140] examined\nfacial shape information and developed a deep multi-task network to account for the rela-\ntionship between pain recognition and shape, which improved pain estimation performance.\nSimilarly, Xin et al. [141] proposed a novel multi-task framework that combines a CNN\nfeature learning module with an autoencoder attention component, also estimating subject\nidentity, as individual differences in pain manifestation are key. Their experiments achieved\nstate-of-the-art results on publicly available datasets.\nMost studies report results obtained from controlled laboratory settings, which typically\nfeature proper lighting, minimal head pose variability, and no occlusions. However, such\nconditions do not represent typical hospital environments. Semwal and Londhe [142] ad-\ndressed this by focusing on pain assessment in uncontrolled settings, developing a shallow\nCNN with three convolutional layers that performed comparably to deeper pre-trained mod-\nels. In a subsequent study [143], they introduced a more complex framework comprising\nthree modules that leveraged high-level spatial descriptors with both local and global geomet-\nric cues, achieving results comparable to models like GoogleNet [144] and VGG [121]. Lee\nand Wang [145] explored pain assessment in intensive care unit (ICU) settings, where par-\ntially occluded faces frequently complicate facial analysis. They developed a 4-layer CNN\ncombined with an extreme learning machine (ELM) for final estimation. Virrey and Cae-\nsarendra [146] used CNNs to classify sections of frames where pain was triggered, peaked,\nand subsided. Nugroho et al. [147] tackled pain detection in smart home-care settings, par-\nticularly for elderly patients, using relatively low-power mobile devices. They modified the30 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nOpenFace2library, based on pre-trained FaceNets [148], and showed that transfer learning\ncould enable real-time binary classification ( pain vs.no pain ), even on low-powered hard-\nware.\nResearchers like Dai et al. [149] and Menchetti et al. [150] have noted that most models,\nwhether deep or shallow, are trained on dataset-specific features rather than actual pain-\nrelated features. Moreover, most studies employ validation methods using the same dataset,\nwhile cross-dataset performance is rarely addressed, limiting real-world applicability. To\ntackle these issues, Dai et al. [149] combined pain and emotion detection datasets to develop\na real-time pain assessment system with better generalization capabilities. They emphasized\nthe importance of cross-corpus evaluation, real-time testing, and the need for well-balanced,\necologically valid pain datasets [151].\nSeveral studies have explored combining pain scales to improve prediction objectivity\nand reliability. Liu et al. [152] developed a two-stage personalized model trained using active\nappearance model (AAM) facial landmarks and multi-task learning, with visual analog scale\n(V AS) and observed pain index (OPI) as ground truth. Xu et al. [153] similarly reduced\nmean square error (MSE) by incorporating various pain scales with the VGG-Face model.\nHowever, Casti et al. [154] pointed out the limitations of original ground truth data due to\nsubjectivity and annotation inconsistencies. To address this, they re-annotated their dataset\nwith judgments from multiple experts, using multidimensional scaling to map frames to\nillumination-invariant 3D space, which they then fed into a pre-trained AlexNet [155].\nCelona and Manoni [156] investigated neonatal facial expressions to detect pain, achiev-\ning the highest accuracy when utilizing two pre-trained models: VGG-Face [157] and mapped\nLBP+CNN (MBPCNN) [158]. Similarly, Lu and Hao [159] found that pre-trained models\nwere crucial for small datasets like neonates, as training from scratch led to overfitting. They\nachieved optimal classification performance by fine-tuning the entire VGG-16 model [122].\nHowever, Zamzmi et al. [160] argue that most face recognition methods are tailored for\nadults and thus less applicable to infants. They developed a lightweight 2D CNN trained\nend-to-end and achieved high pain detection accuracy, but external validation on a different\nneonatal dataset revealed challenges with generalizability. In 2019, Brahnam et al. [112]\nintroduced the iCOPEvid neonatal video dataset, a significant contribution since the only\npublicly available neonatal pain dataset [111] previously contained only static images. Their\nexperiments showed that local descriptors based on the bag-of-features (BoF) approach out-\nperformed deep learning models like VGG-Face andResNet . Combining handcrafted and\ndeep-learned features offered only a marginal improvement in performance. In contrast, Za-\nmzmi et al. [161] found that the most effective approach for binary classification (pain vs.\nno pain) was the fusion of high-level features from VGG [162] and optical flow strains, with\n2http://cmusatyalab.github.io/openface3.4. UNIMODAL STUDIES 31\nnaive Bayes serving as the classifier. Celona and Brahnam [163] applied a Wasserstein gen-\nerative adversarial network with gradient penalty (WGAN-GP) [164], demonstrating that\ntraining set augmentation with synthetic samples improved classification performance. Ta-\nble 3.2 summarizes the vision-based studies focusing exclusively on the spatial dimension.32 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [112]F (RGB) texture\ndescriptors- FF 2D CNN`SVM SL C P O 49 k-fold iCOPEvid 79.80 AUC\n'15 [119]F (RGB) - - - AE SVM SeSL,\nSLC P PS 25 LOSO UNBC 86.10 ACC,\n96.50 AUC\n'20 [120]F (RGB) - - FF 2D CNN`NN SL R IC O 36 hold-out EmoPain 0.91 MAE;\n'18 [123]F (RGB) HOG,\nstatistics- FF 2D CNN`SVR SL R IC PS 25 LOSO UNBC 1.44 MSE;\n'21 [130]F (RGB) - - DF 2D CNN`- SL C ID PS 25 k-fold UNBC 93.87 ACC;\n'16 [133]F (RGB) - - - CDBN SVM UL C P PS 25 LOSO:UNBC 87.20 ACC;\n'21 [134]F (RGB) - - - CDBN SVM SL C P PS 25 LOSO UNBC 93.16 AUC\n'19 [135]F (RGB) - - FF 2D CNN - SL C ID1, IC PS 25 LOSO UNBC 88.191ACC\n'20 [136]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 51.10 ACC;\n'20 [140]F (RGB) - - FF 2D CNN`- SL R ID S 25 ? UNBC 79.94 ACC;\n'21 [142]F (RGB) - - - 2D CNN - SL C ID O 8 k-fold other 97.48 ACC;\n'19 [145]F (RGB) - - - 2D CNN ELM SL R IC PS 25 k-fold UNBC\u201a1.22 MSE;\n'19 [146]F (RGB) - - - 2D CNN - SL C TR, CL, DI PS 25 k-fold UNBC 60.00 ACC\n'19 [150]F (RGB) - - - 2D CNN`- SL C AUs-D PS 25, 43 k-fold UNBC & CK+1,\nWilkie97.701ACC;\n'17 [152]F (RGB) statistics - - NN GPM WSL R IC O, S 25 k-fold UNBC 2.18 MAE\n'20 [153]F (RGB) statistics - FF 2D CNN`NN SL R IC S 25 k-fold UNBC 1.95 MAE;\n'19 [154]F (RGB) LBP, MDS - - 2D CNN`- SL C ID O 25 hold-out UNBC 80.00 ACC\n'18 [159]F (RGB) - - - 2D CNN`- SL C ID O ? hold-out other 78.30 ACC\n`: Pre-trained model -:Not exist &: in Dataset indicates the utilization of cross-database training/validation ?: Not found :: The authors provide additional experiments with other validation methods \u201a: The authors\nutilized occluded facial images ;: The authors provide additional metrics Modality: F: face region Non deep features: LBP: local binary pattern MDS: multidimensional scaling Fusion: M: fusion of modalities E:\nfusion of deep learned features or hand-crafted features Deep models: AE: autoencoder RCNN: recurrent convolutional neural network CDBN: convolutional deep belief network CNN: convolutional neural network\nNN: neural network WGAN-GP: Wasserstein generative adversarial model with gradient penalty Non deep model: SVM: support vector machine GPM: Gaussian process regression model kNN: k-nearest neighbors\nNB: naive Bayes ELM: extreme learning machine Learning Method: SL: supervised learning SeSL: semi-supervised learning UL: unsupervised learning WSL: weakly supervised learning Classific./Regres.: C:\nclassification R: regression Objective: P: presence of pain ID: intensity in discrete scale IC: intensity in continuous scale TR: trigger CL: climax DI: diminishing AUs-D: Action Units detection GT: ground truth\nPS: Prkachin and Solomon S: self-report O: observer rating ST: stimulus Validation Method: LOSO: leave one subject out Metrics: AUC: Area Under the ROC Curve ACC: accuracy PPV: precision MSE: mean\nsquared error MAE: mean absolute error3.4. UNIMODAL STUDIES 33Table 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [124]F (RGB) facial landmarks - FF 2D CNN NN SL C, R ID, IC1P 25 LOSO:UNBC 0.171MSE;\n'20 [125]F (RGB) HOG, head pose,\nAUs intensity/\noccurrence, facial\nlandmarksFF - 2D CNN`NN SL R IC O 36 hold-out EmoPain 5.48 RMSE;\n'20 [126]F (RGB) - - - 2D CNN`NN SL R IC O 36 hold-out EmoPain 1.49 RMSE;\n'18 [127]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 92.00 ACC;\n'18 [128]F (RGB) statistics, distance\nmetrics- FF 2D CNN`- SL C, R ID, IC PS 25 LOSO UNBC 0.81 PCC,\n0.69 MSE\n'21 [137]F (RGB) - - FF 2D CNN`- SL C, R ID, IC P 25 LOSO UNBC 91.13 ACC,\n0.78 PCC,\n0.46 MSE\n'18 [138]F (RGB) - - - AE`- SL R IC PS 25 k-fold UNBC 0.33 MAE;\n'21 [141]F (RGB) - - FF [AE, 2D CNN]Y- SL C, R ID1, IC2,\nP3P, ST 25, 87 LOSO UNBC1,\nBioVid (A)289.1711ACC,\n0.8121PCC,\n85.6532ACC,\n40.4012ACC\n'21 [143]F (RGB) entropy texture\ndescriptors- - 2D CNN`- SL C ID O 8 k-fold other 0.92 PPV;\n'18 [147]F (RGB) - - - 2D CNN`- SL C P PS 14 k-fold UNBC 93.00 ACC\n'19 [149]F (RGB) - - - 2D CNN - SL C P PS 25, 20 k-fold UNBC &\nBioVid (A)\u02db56.75 ACC\n'17 [156]F (RGB) HOG, LBP - FF 2D CNN`SVM SL C P O 26 LOSO iCOPE 73.78 ACC\n'19 [160]F (RGB) - - - 2D CNN - SL C P O 31 LOSO NPAD1,\niCOPE296.981ACC;,\n89.802ACC\n'21 [165]F (RGB) - - - 2D CNN`- FL C P PS 25 LOSO UNBC 76.00 ACC;\n'21 [166]F (RGB) - - - 2D CNN`- SL C P O 25 hold-out UNBC 75.49 ACC\n'21 [167]F (RGB) - - - 2D CNN`SVR SL R IC P 25 LOSO UNBC 0.34 MSE\n'21 [168]F (RGB) - - - 2D R-CNN - SL C P O ? hold-out other 87.80 PPV\nY: The authors combined the deep models into a unified framework \u02db: The authors experimented with additional datasets combinations Non deep features: AUs: actions units HOG: histogram of oriented gradients Non\ndeep model: SVR: support vector regression Learning Method: FL: federated learning Metrics: RMSE: root mean squared error34 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [161]F (RGB) optical flow - FF 2D CNN`SVM,\nkNN, NBSL C P O 31 k-fold other 92.71 ACC,\n94.80 AUR\n'19 [163]F (RGB) - - - WGAN-GP - SL C P O 26 LOSO iCOPE 93.38 ACC\n'17 [169]F (RGB) - - - 2D CNN`- SL R IC PS 25 LOSO UNBC 0.99 MAE;\n'20 [170]F (RGB) - - - 2D CNN - SL C ID ST 87 hold-out BioVid (A) 36.60 ACC\n'20 [171]F (RGB) - - - 2D CNN - SL C P PS 25 hold-out UNBC 97.00 PPV;\n'21 [172]F (RGB) - - - 2D CNN - SL C ID P 28 LOSO:UNBC 90.30 ACC\n'19 [173]F (RGB) - - - 2D CNN - SL C P O 31 hold-out NPAD1,\niCOPE291.001ACC;,\n84.502ACC;\n'21 [174]F (RGB) - - - 2D CNN`- SL C P O 26, 30 hold-out iCOPE &\nUNIFESP89.90 ACC;\n'21 [175]F (RGB) - - - 2D CNN - SL C AUs-D P 10 hold-out Pain-ICU 77.00 ACC;3.4. UNIMODAL STUDIES 35\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach)\nPain assessment is particularly challenging due to its complex and dynamic nature. Rely-\ning on static, individual frames to assess pain fails to capture the phenomenon\u2019s temporal\nprogression and often leads to inaccurate estimations. Additionally, many studies highlight\nthe difficulties of applying deep learning techniques to small datasets, with one proposed\nsolution being the combination of deep learning and traditional feature extraction methods.\nEgede et al. [176] addressed this by extracting deep features from a pre-trained CNN, explic-\nitly targeting the eyes and mouth regions. Using a relevance vector regressor (RVR), they\ndemonstrated that combining deep and hand-crafted features led to optimal performance. De-\nspite the valuable insights the UNBC-McMaster database provides, its imbalanced sample\ndistribution\u2014particularly the limited number of frames showing pain\u2014poses a significant\nchallenge for deep learning models. In response, Egede and Valstar [177] devised a method\nbased on the observation that neighboring pain level classes share many common features.\nThis approach allowed them to avoid extracting all possible features for classes with fewer\nsamples, as certain features had already been utilized from other related classes. The study\nalso showed that combining deep and hand-crafted features improved performance. How-\never, in a later study [178], the authors applied a similar approach, using only deep-learned\nfeatures to address data imbalance, but could not replicate the same high-performance levels.\nTavakolian et al. [179] took a different approach, focusing on the detection of genuine\nversus acted pain through facial expressions, a technique with important applications in both\nmedical and forensic contexts. They developed a residual GAN (R-GAN) to capture subtle\nfacial changes and the dynamic nature of expressions, using a weighted spatio-temporal pool-\ning (WSP) method. In a subsequent study [180], the authors suggested that self-supervised\nlearning could reduce the time and effort needed for data labeling, as it does not require\ncomplete dataset annotation. They introduced a new similarity function for learning general-\nized representations with a Siamese network. They also employed statistical spatio-temporal\ndistillation (SSD) based on the Gaussian scale mixture (GSM) to improve computational effi-\nciency. This technique encodes spatiotemporal variations in facial videos into a single RGB\nimage, simplifying the model while maintaining effectiveness.\nOther studies also aim to capture the dynamic aspects of pain. For instance, [181] com-\nbined a random forest classifier with the pre-trained MobileNetV2 model [182], encoding\nvideos by selecting and merging three frames from different time points into a single image.\nOthman et al. [183] emphasized the importance of using diverse datasets\u2014including vary-\ning age, gender, pose, occlusion, and lighting conditions\u2014to improve model generalization.\nThey used multiple data combinations and a reduced version of MobileNetV2 , showing that\ncross-dataset training is essential for achieving better generalizability.36 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.3 Vision-based: Implicit Temporal Utilization\nSeveral studies have explored the application of 3D CNNs for pain assessment. Tavakolian\nand Hadid [184] developed a 3D CNN to capture dynamic facial representations from videos.\nThey noted that researchers often use fixed temporal kernel depths when employing 3D\nconvolution techniques, which limits the ability to capture short, mid, and long temporal\nranges simultaneously. To address this, they designed a model with parallel 3D convolutional\nlayers featuring variable temporal depths, allowing the capture of temporal dependencies\nfrom 32consecutive frames. Similarly, Wang and Sun [185] applied 3D convolutions based\non the architecture proposed in [186], consisting of 8convolutional layers with 3\u02c63\u02c63\nfilters. While they reported high performance, the authors acknowledged that extracting deep\nfeatures from small datasets posed a challenge for model generalization. In a related study,\nHuang et al. [187] developed a framework that integrated 3D, 2D, and 1D CNNs to extract\nspatio-temporal, spatial, and geometric features. For the 3D CNN component, they modified\nthe architecture from [188] by using discrete kernels of 1\u02c63\u02c63and3\u02c61\u02c61rather than the\ntraditional 3\u02c63\u02c63kernel. Other researchers have also proposed 3D deep CNNs with varying\ntemporal depths to capture short, mid, and long-range facial expression variations [189].\nRecognizing the difficulty and time consumption involved in training a deep 3D CNN from\nscratch, they introduced a cross-architecture knowledge transfer learning technique, utilizing\na pre-trained 2D CNN to assist in the training of the 3D CNN. In studies by Praveen et\nal.[190] and [191], the authors employed weakly-supervised domain adaptation, where the\nsource domain focused on human affective expressions and the target domain was explicitly\nrelated to pain expressions. Their framework featured an inflated 3D-CNN (I3D) [192],\nincorporating 3convolutional layers and 3inception modules [132] to capture both spatial\nand temporal information from video data.\nBargshady et al. [193] opted to use the HSV color space instead of RGB, arguing that it\nbetter reflects human visual perception for tasks such as skin pixel detection and multi-face\ndetection. They employed the pre-trained VGG-Face [157] for feature extraction, followed\nby a temporal convolutional network (TCN) using dilated causal convolutional operations to\nleverage temporal dependencies. Rezaei et al. [194] tackled the challenge of pain detection\nin people with dementia, a difficult task due to insufficient pain-related images or videos\nof elderly subjects in existing datasets. They developed a 10-layer 2D CNN that processed\npairs of pain and no-pain images, analyzing frame-to-frame changes and employing con-\ntrastive training methods [195]. The model demonstrated high performance in both healthy\nindividuals and people with dementia. In another study, Pandit and Schmitt [196] explored\nthe potential of using shallow 1D CNN architectures for real-time pain recognition. They ex-\ntracted facial action units from each frame using the OpenFace 2.03toolkit, with promising\n3https://github.com/TadasBaltrusaitis/OpenFace3.4. UNIMODAL STUDIES 37\nresults for pain detection in real-time settings.\n3.4.4 Vision-based: Explicit Temporal Utilization\nSeveral efforts have focused on addressing the limitations of static frames by developing\ndedicated temporal modules. Zhou et al. [197] tackled this issue using a regression frame-\nwork based on a 4-layer recurrent convolutional neural network (RCNN), each with a se-\nquence length of 3time steps. Rodriguez et al. [198] leveraged dynamic information by\ndesigning an LSTM model fed with feature vectors extracted from VGG-16 [122]. Simi-\nlarly, Bellantonio et al. [199] emphasized that facial expressions evolve, making it essential\nto analyze the spatio-temporal dimension of pain. They improved estimation performance\nusing a fine-tuned 16-layer CNN model [157], an LSTM processing 16frames as a time\nwindow, and super-resolution techniques. In another study, Bargshady et al. [200] com-\nbined the VGG-Face CNN [157] with a 3-layer LSTM to extract spatio-temporal features\nfrom grayscale images, applying zero-phase component analysis (ZCA). In [201], principal\ncomponent analysis (PCA) was used to reduce dimensionality. Mauricio et al. [202] also\nemployed VGG-Face but replaced LSTM with a 2-layer gated recurrent unit (GRU) to cap-\nture temporal dependencies. Thuseethan et al. [203] used a conventional 2D CNN and two\nRCNNs to extract temporal features from previous and subsequent frames, enhancing the\ntime dimension of expression analysis.\nA similar approach was followed by Bargshady et al. [204], who employed ensemble\nlearning with three distinct CNN-biLSTM modules, merging their outputs for the final pre-\ndiction. Salekin et al. [205] used a bilinear CNN (B-CNN) based on the VGG architecture\n[121], pre-trained on VGGFace24andImageNet5datasets, along with an LSTM to capture\ntemporal dependencies in image sequences. Kalischek et al. [206] explored deep domain\nadaptation for facial expression and pain detection, utilizing the self-ensembling approach\n[207] with a long-term recurrent convolutional network (LRCN). While they achieved state-\nof-the-art results for facial expression recognition, performance was lower for pain detection,\nlikely due to the subtle nature of pain-related expressions.\nDespite the availability of additional information in pain datasets, multi-task approaches\nremain limited. Martinez et al. [208] proposed a personalized multi-task learning method\nbased on individual physiological and behavioral pain responses. They extracted AAM fa-\ncial landmarks, processed them through a biLSTM to produce PSPI scores, and predicted the\nfinal V AS score. Erekat et al. [209] combined AlexNet [155] with 2 GRU layers to capture\ntemporal dependencies, using both self and observer-reported pain intensity as ground truth.\nVuet al. [210] developed a multi-task framework to estimate pain levels while reconstruct-\n4https://www.robots.ox.ac.uk/ \u02dcvgg/data/vgg_face\n5https://www.image-net.org38 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\ning heatmaps of action unit locations, improving model generalization with a CNN-LSTM\ncombination to capture micro facial movements.\nHuang et al. [211] noted that specific frames within a video sequence exhibit more pro-\nnounced pain expressions, requiring special handling. They developed a novel framework\nusing attention saliency maps with a VGG-16 model, GRUs and learned weights for each\nframe\u2019s contribution to pain intensity estimation. The study demonstrated that dynamic and\nsalient features can significantly improve performance. Similarly, Yu et al. [212] used VGG-\n11 (configuration A) and an LSTM to create an attention mechanism, predicting pain in-\ntensity from 16consecutive frames. Xu and Liu [213] adopted a ResNet-50 model with an\nattention mechanism to extract spatial features, followed by a transformer encoder to capture\ntemporal sequences, achieving promising results.\nIn other studies, Ragolta et al. [214] used extracted action units to train a 2-layer LSTM\npredicting pain on an 11-point scale, employing curriculum learning. Guo et al. [215] devel-\noped a convolutional LSTM (C-LSTM) to extract both spatial and temporal features from\nvideos, showing that temporal models outperform non-temporal models for pain estimation\naccuracy. Rasipuram et al. [216] utilized in-the-wild video data for pain detection, gener-\nating a 3D morphable model without relying on facial landmarks and combining it with an\nLSTM. Zhi and Wan [217] introduced sparse coding with LSTM (SLTM), using the iterative\nhard thresholding algorithm (ISTA) [218] to capture dynamic facial expressions. Although\nSLTM did not achieve high performance, it offers speed and efficiency for specific applica-\ntions. Finally, Thiam et al. [219] developed a method combining motion history and optical\nflow images with a 10-layer CNN and 2-layer biLSTM, showing that weighted score aggre-\ngation improves performance. Table 3.3 summarizes studies incorporating the modalities\u2019\ntemporal dimensions.3.4. UNIMODAL STUDIES 39Table 3.3: Vision-based studies with temporal utilization.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'17 [176] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVR SL R IC PS 25 LOSO UNBC 0.99 RMSE,\n0.67 PCC\n'17 [177] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVM SL R IC PS 25 LOSO UNBC 1.04 RMSE,\n0.64 PCC\n\u201918 [178] F (RGB) - - - NL 2D CNN - SL R IC PS 25 LOSO UNBC 1.20 RMSE,\n0.47 PCC\n'18 [184] F (RGB) - - - I 3D CNN - SL R IC PS 25 LOSO UNBC 0.53 MSE,\n0.84 PCC;\n'18 [185] F (RGB) HOG,\ngeometric\ndifference- DF I 3D CNN SVR SL R IC PS 25 LOSO UNBC 0.94 RMSE,\n0.67 PCC\n'20 [191] F (RGB) - - - I 3D CNN`- WSL R IC PS 24, ? LOSO UNBC\n& RECOLA0.64 MAE,\n0.82 PCC;\n'16 [197] F (RGB) - - FF E RCNN - SL R IC PS 25 LOSO UNBC 1.54 MSE,\n0.65 PCC\n'17 [198] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C, R P, IC1PS 25 LOSO UNBC 0.741MSE,\n0.781PCC;\n'17 [199] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 61.90 ACC\n'19 [200] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 75.20 ACC\n'20 [201] F (RGB) PCA - DF E [2D CNN`,\n1D CNN, biLSTM]Y- SL C ID PS 25 LOSO:UNBC 85.00 ACC;\n'19 [202] F (RGB) - - - E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 85.40 ACC,\n0.62 MSE;\n'19 [203] F (RGB) - - FF E [2D CNN, RCNN]Y- SL R IC PS 25 LOSO UNBC 1.29 MSE,\n0.73 PCC\n'17 [208] F (RGB) - - FF E biLSTM HCRF,\nFCSL C IC O,\nS25 hold-out UNBC 2.46 MAE;\nNon deep features: PCA: principal component analysis Temporal Exploitation: NL: non-machine learning method I: implicit method E: explicit method Deep models: RCNN: recurrent convolutional neural network\nLSTM: long short memory networks biLSTM: bidirectional neural network GRU: gated recurrent unit Non deep models: SVM: support vector machine RVM: relevance vector machine GPM: Gaussian process regression\nmodel HCRF: hidden conditional random fields FC: fully connected SVR: support vector regression Objective: I2: intensity in binary pairs Metrics: PCC: Pearson correlation coefficient40 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [114]F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN RVR SL R IC O 13 LOSO APN-DB 1.71 MAE;\n'19 [179]F (RGB) - - - NL R-GAN - UL C genuine\nvs posedPS,\nST25,\n34,\n87,\n87? UNBC\n& STOIC\n& BioVid (A)\n& BioVid (D)90.97 ACC\n'20 [180]F (RGB) - - FF NL 2D CNN`- SSL C IC P,\nST25\n87LOSO UNBC1,\nBioVid (A)2\u20180.781PCC;,\n71.022AUC;\n'21 [181]F (RGB) AUs\nintensity- H NL 2D CNN`RF SL C ID ST 127 k-fold X-ITE 25.00 ACC\n'19 [183]F (RGB) - - - NL 2D CNN - SL C P ST 87\n134k-fold BioVid (A)\n& X-ITE\u201867.90 ACC\n'20 [209]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC O,\nS25 k-fold UNBC 2.34 MAE\n'20 [211]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC PS 19 LOSO UNBC 0.21 MSE,\n0.89 PCC\n'19 [212]F (RGB) - - FF E [2D CNN, LSTM]Y- SL R IC PS 24 LOSO UNBC 1.22 MSE;,\n0.40 PCC;\n'20 [214]F (RGB) AUs\nintensity- - E LSTM - SL R IC O 36 hold-out EmoPain 2.12 RMSE,\n1.60 MAE;\n'20 [216]F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C P O ? k-fold UNBC 78.20 ACC;\n'20 [219]F (RGB) - - DF E [2D CNN, biLSTM,\nNN]Y- SL C P ST 87\n40LOSO BioVid (A)1,\nSenseEmotion269.251ACC,\n64.352ACC\n'20 [220]F (RGB) - - FF E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 0.84 ACC,\n0.69 PCC;\n\u2018: The authors provide experiments with cross-dataset settings Fusion: H: hybrid Non deep models: RF: random forest classifier3.4. UNIMODAL STUDIES 41Table 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [187]F (RGB) facial\nlandmarks- DF I [3D CNN`,\n2D CNN`,\n1D CNN, FC]Y- SL R IC PS 25 LOSO UNBC 0.76 MSE,\n0.82 PCC;\n'19 [189]F (RGB) - - - I [2D CNN`,\n3D CNN]Y- UL,\nSLC, R IC1, P2P,\nST25, 87 LOSO UNBC1,\nBioVid (A)20.9211PCC;,\n86.0222AUC\n'20 [190]F (RGB) - - - I 3D CNN`- WSL R IC PS 24,?,\n87, 18LOSO UNBC1\n& RECOLA\n& BioVid (A)2\u20180.741PCC,\n0.342PCC\n'20 [193]F (RGB) PCA - FF I [2D CNN`,\nTCN]Y- SL C ID P,\nST25, 20 LOSO:UNBC1,\nMIntPAIN292.441ACC;,\n89.002ACC;\n'20 [194]F (RGB) - - - I 2D CNN - SL C, R IC, P1P 95, 25 k-fold UofR & UNBC182.0011PCC;\n'20 [196]F (RGB) AUs\noccurrence- FF I 1D CNN - SL R IC P 24, 87 hold-\noutUNBC1,\nBioVid (A)0.801CCC\n'20 [204]F (RGB) PCA - DF E [2D CNN`, 1D\nCNN, biLSTM]Y- SL C ID PS,\nST25, 20 k-fold UNBC1,\nMIntPAIN286.001ACC;\n92.262ACC;\n'20 [205]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- SL R P, IC1O 45 LOSO NPAD 3.991MSE,\n1.552MAE\n'19 [206]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- UL C P ST 40 LOSO SenseEmotion 60.61 ACC\n'21 [210]F (RGB) - - - E [2D CNN`,\nLSTM]Y- SL R IC P 25, 27 LOSO UNBC1,\nDISFA\u20180.60`MSE,\n0.82`PCC;\n'21 [213]F (RGB) - - - E [2D CNN`,\nTransformer]Y- SL R IC P 25 LOSO UNBC 0.40 MSE,\n0.76 PCC;\n'21 [215]F (RGB) - - - E 2D C-LSTM - SL C ID S 29 hold-\noutother 69.58 F1\n'19 [217]F (RGB) - - FF E SLSTM - SL C P1, ID2ST 85 LOSO BioVid (A) 61.701ACC\n29.702ACC\n'21 [221]F (RGB) - - - I 3D CNN`- SL R IC S 25 k-fold UNBC 0.66 ICC;\nFusion: H: hybrid Deep models: TCN: temporal convolutional neural network C-LSTM: convolutional-LSTM SLTM: sparse long short memory network Learning Method: SSL: self-supervised learning Metrics: F1:\nF1 score CCC: concordance correlation coefficient42 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.5 Touch sensor-based\nTouch (contact) sensors provide a viable alternative for pain assessment, often outperforming\nvision-based methods. Table 3.4 highlights studies that utilized contact sensor data to evalu-\nate pain. Yu et al. [222] analyzed three categories of pain-no pain, moderate pain, and severe\npain\u2014using EEG signals. They extracted several bands from the biosignals, including al-\npha, beta, and gamma, and applied a convolutional module. The study found that combining\nthese bands yielded better results than evaluating them independently. Similarly, [223] used\nEEG potentials with an autoencoder to compress the raw data and applied a logistic regressor\nfor classification.\nOther researchers, such as Rojas et al. [224], utilized functional near-infrared spec-\ntroscopy (fNIRS) for pain detection. They developed three models\u2014multilayer perceptron\n(MLP), LSTM, and biLSTM\u2014with biLSTM demonstrating superior accuracy. Addition-\nally, [225] focused on PPG signals, extracting hand-crafted features from the time and fre-\nquency domains, which were then combined with a deep belief network (DBN) to achieve\nover65% accuracy in a 4-class pain assessment task. Hu et al. [226] used kinematic data\nto compare healthy individuals with those suffering from low back pain (LBP). Their ap-\nproach, which employed two stacked LSTM layers, reached over 97% accuracy in binary\nclassification using raw motion data. Lastly, Mamontov et al. [227] were the first to apply\nevolutionary algorithms in the design of an optimized recurrent neural network (RNN) for\npain estimation, achieving 91.94% accuracy using EDA signals.\n3.4.6 Audio-based\nA few studies have explored using audio information for pain detection and intensity esti-\nmation, as outlined in Table 3.5. These methods are especially relevant for neonates, where\nfrequent facial and body occlusions make analyzing cries a more effective approach for pain\ndetection. Chang and Li [228] concentrated on infant cries to differentiate between hunger,\npain, and sleepiness. They transformed the audio signals into 2D spectrograms using a fast\nFourier transform (FFT) and trained a 2D CNN for feature extraction. Similarly, [229] uti-\nlized spectrograms generated from recorded sounds, employing a model identical to that\nused in [160]. Thiam and Schwenker [230] focused on detecting adult pain by analyzing\nbreathing sounds. They leveraged deep-learned features from spectrograms with Mel-scaled\nshort-time Fourier transform, combined with various handcrafted cues. A CNN followed by\na biLSTM was used to capture spatial and temporal dependencies, integrating both low- and\nhigh-level features. In a different approach, Tsai et al. [231] examined pain events during\nemergency triage. They developed an LSTM autoencoder framework to extract temporal\nfeatures from verbal behavior, reporting encouraging results.3.4. UNIMODAL STUDIES 43Table 3.4: Touch sensor-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'20 [222]EEG - - FF I 1D TCN - S C ID S 32 k-fold other 97.30 ACC;\n'20 [223]EEG - - - I AE (TCN) LR UL, S C P S 29 LOSO other 74.60 ACC\n'21 [224]fNIRS - - - E biLSTM - SL C ID S 18 k-fold other 90.60 ACC;\n'19 [225]PPG - - - NL DBN SBM U, SL C P1, ID2S 100 k-fold other 86.791ACC,\n65.572ACC\n'18 [226]kinematatics - - FF E LSTM - SL C P LBP 44 LOSO other 97.20 ACC;\n'19 [227]EDA - - FF E [RNN, LSTM,\nGRU, NN]YSelfCGA,\nselfCGP,\nPSOPBSL C P ST 40 LOSO Sense-\nEmotion81.94 ACC\n'21 [232]EDA - - - I NN - SL C P1, I2 ST 87,\n55LOSO BioVid (A)1,\nPainMonit284.2211ACC;,\n86.5012ACC;\nModality: PPG: photoplethysmogram fNIRS: functional near-infrared spectroscopy EEG: electroencephalography EDA: electrodermal activity Deep models: DBN: Deep belief network RNN: recurrent neural network\nNon deep models: SBM: selective bagging model LR: Logistic Regression SelfCGA: Self-Configuring Genetic Algorithm SelfCGP: Self-Configuring Genetic Programming PSOPB: Particle Swarm Optimisation with\nparasitic behaviour GT: LBP: low back pain vs healthy population\nTable 3.5: Audio-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'16 [228]audio (cry) - - - - 2D CNN - SL C P O ? k-fold other 78.50 ACC\n'19 [229]audio (cry) - - - - 2D CNN - SL C P O 31 LOSO:NPAD 96.77 ACC;\n'19 [230]audio\n(breathing)MFCCs,\nRASTA-\nPLP,\nDTD- FF E [2D CNN,\nLSTM]YRFc SL C P ST 40 LOSO Sense-\nEmotion64.39 ACC\n'17 [231]audio\n(voice)prosodic-\nspectral\nfeatures,\nSF- FF E LSTM`SVM UL,\nSLC P1, ID2S 63 LOSO other 72.301UAR,\n54.202UAR\nNon deep features: MFCCs: Mel Frequency Cepstral Coefficients RASTA-PLT: Relative Spectral Perceptual Linear Predictive DTD: descriptors from temporal domain SF: statistical features44 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.5 Multimodal studies\nSince pain is a multidimensional phenomenon, combining multiple modalities in a multi-\nmodal system offers a promising approach. Heterogeneous information sources can com-\nplement one another, enhancing specificity and sensitivity. As reported in [106], when in-\ndividual modalities demonstrate good predictive performance, their fusion tends to yield\nimproved outcomes. Moreover, integrating cues from various channels may be helpful and\nnecessary, especially in clinical settings where specific modalities may become unavailable\n(for instance, if the patient turns and their face is occluded). The information channels can\noriginate from (1) the same hardware sensor but focus on different regions of interest, such\nas RGB facial images and RGB body images [233], (2) different hardware sensors but the\nsame region of interest, like RGB facial images and thermal facial images [110], or (3)\ndifferent hardware sensors and information sources, such as RGB facial images and ECG\nsignals [234]. Table 3.6 lists the studies utilizing multimodal approaches.\n3.5.1 Static Analysis\nA commonly used biosignal combination is those of EDA, EMG, and ECG, as these channels\nare found in all main pain reference databases. Thiam et al. [235] applied an early fusion\nmethod by merging these signals into a 2D representation and inputting it into a 9-layer 2D\nCNN. Their results showed a strong correlation between EDA and pain intensity, and com-\nbining all three modalities did not outperform using EDA alone. Al-Qerem et al. [236] used\nleast generative adversarial networks (LSGANs) to enhance EMG, EDA, and ECG samples,\nreporting a notable improvement in classification when using an SVM on the augmented\ndataset. Haque et al. [110] introduced the MIntPAIN dataset, which includes RGB, depth,\nand thermal videos for multi-class ( 5levels) pain recognition. They combined these three vi-\nsual modalities into a 5D matrix (RGB+D+T) and used it to train the pre-trained VGG-Face\nmodel [157], leading to better classification performance in their experiments.\n3.5.2 Temporal Utilization\nZhiet al. [237] proposed a multimodal stream-integrated neural network that leverages video\nand biosignal data. They combined raw facial video frames with optical flow images to cap-\nture spatio-temporal dependencies via 3D CNNs, integrating these with biosignal features\nextracted using LSTMs. The entire network was trained end-to-end, achieving superior re-\nsults compared to their unimodal methods. Beyond facial analysis, Salekin et al. [233]\nfocused on assessing neonatal pain through body movements in videos. After identifying\nrelevant body regions, video frames were fed into a pre-trained VGG-16 [121], connected to\nan LSTM to capture temporal dynamics. In a follow-up study, Salekin et al. [238] fused three3.5. MULTIMODAL STUDIES 45\nmodalities\u2014facial expressions, body movements, and crying sounds\u2013demonstrating that this\nmultimodal approach outperformed unimodal techniques. Similarly, Wang et al. [239] ex-\nplored combining EMG, EDA, and ECG biosignals with handcrafted and learned features\nfrom a biLSTM model. They applied the minimum relevance method (MRMR) to reduce\nthe number of features, resulting in notable outcomes.\nIn addition to EDA, EMG, and ECG, other biosignal combinations have been explored.\nZhao et al. [240] integrated PPG, EDA, and temperature signals, using 2D convolutions for\nspatial feature extraction and time windows for capturing temporal information. Yuan et\nal.[241] successfully estimated pain using whole-body MoCap sensors and EMG, utilizing\nLSTM layers with an attention mechanism in an autoencoder, which reduced training time\nby leveraging latent space representations of raw data. Similarly, Li et al. [242] employed\nMoCap and EMG as data sources and tested various LSTM configurations to predict pain\nintensity, achieving the best performance with a 3-layer vanilla LSTM combined with a 3-\nlayer fully connected network.46 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.6: Multimodal-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [110]F (RGB,\nthermal,\ndepth)- RF - - 2D CNN`- SL C ID S 20 k-fold MIntPAIN 36.55 ACC\n'19 [233]F, B (RGB) - FF - E [2D CNN`,\nLSTM]Y- SL C P O 31 LOSO other 92.48 ACC;\n'19 [234]F (RGB),\nECG, EDAbiosignals\u2019\nfeaturesmFF FF - 2D CNN`RFc SL C I2 S 85 k-fold BioVid (A) 74.00 ACC\n'19 [235]EDA,\nEMG,\nECG- RF - - 2D CNN - SL C P1I2,\nID2S 87,\n86LOSO BioVid (A)1\nBioVid (B)84.4011ACC;,\n36.5412ACC;\n'20 [236]EDA,\nEMG,\nECGBoruta\nfeaturesFF - - LSGAN SVM UL,\nSLC I2, ID1S 85 hold-\noutBioVid (A) 82.801ACC\n'21 [237]F (RGB),\nEDA,\nEMG, ECGoptical\nflowFF FF NL,\nE, I[3D CNN,\nLSTM]Y- SL C, R P1, I2,\nID2S 87,\n40k-fold:BioVid (A)1,\nMIntPain68.2011ACC;,\n28.1021ACC\n'21 [238]F, B (RGB),\nsound- DF - E [2D CNN`,\nLSTM]Y- SL C P O 45 LOSO NPAD 78.95 ACC;\n'20 [239]EDA,\nEMG,\nECGMRMR,\nbiosig-\nnals\u2019\nfeaturesRF\nFFE biLSTM NN SL C P1, I2 S 87 LOSO BioVid (A) 83.301ACC\n'20 [243]EDA,\nEMG,\nECG- FF - I [DDCAE,\nNN]Y- UL,\nSLC P1, I2 S 87 LOSO BioVid (A) 83.991ACC;\n'21 [244]EDA,\nEMG,\nECG, RSP- FF - I [DDCAE,\nNN]Y- UL,\nSL,\nSSLC, R P1, ID2,\nICS 87,\n40LOSO BioVid (A)1,\nSense-\nEmotion84.2511ACC;,\n35.4421ACC;\n'21 [245]EDA, ECG - FF - E 1D CNN,\nLSTM- UL C P1, I2 S 67 hold-\noutBioVid (A) 81.711ACC\n'20 [240]PPG, EDA,\ntemperature- RF - I 2D CNN - SL R\u02ddP1, ID2S 21 k-fold other 96.301ACC,\n95.232ACC\n'20 [241]MoCap,\nEMG- RF - E AE, LSTM - UL,\nSLC ID O 23 LOSO:EmoPain 52.60 ACC;\n'20 [242]MoCap,\nEMG- RF - E LSTM, NN - UL C ID O 30 hold-\noutEmoPain 80.00 ACC;\n'21 [246]MoCap,\nEMG- RF - E LSTM, NN - SL C ID O 30 LOSO:EmoPain 54.60 ACC;\nm: Not specifically described \u02dd: Ordinal Modality F: face region B: body region EMG: electromyography Non deep features: MRMR: Minimum Redundancy Maximum Relevance method Deep models: LSGAN:\nLeast Square Generative Adversarial Networks3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 47\n3.6 Summary of Automatic Pain Assessment Methods\nThis section presents an analysis of the reviewed studies, summarizing the main conclusions\non current methods for automatic pain assessment, their advantages, and corresponding lim-\nitations. Additionally, it offers recommendations for future research directions that could\nadvance the field of pain research from a computational perspective.\n3.6.1 Input\nFirst, we observe a clear imbalance between unimodal and multimodal approaches in pain\nassessment studies. More than 86% of the reported research focuses on unimodal methods,\neven though the databases often contain multiple information channels. Notably, contact\nsensor-based and audio-based approaches are underrepresented, with only seven and four\nstudies, respectively, compared to 84studies that utilize a vision-based approach.\nMultimodal approaches are even less explored, with only 15studies falling into this\ncategory, making it difficult to draw strong conclusions about the effectiveness of specific\nmodality combinations. However, there are indications that EDA sensor data is particularly\nvaluable compared to other biopotentials. Researchers have primarily focused on visual data,\nlikely due to the complexity of implementing multimodal frameworks or the impracticality\nof contact sensors in non-laboratory settings. Further exploration of diverse modality com-\nbinations is necessary to evaluate their potential for pain assessment fully\u2014additionally, 28\nstudies employed non-deep features to enhance deep-learned representations.\nFinally, we identified three primary strategies in examining the approaches that utilize\ntemporal information: non-machine learning-based, machine learning-based (implicit), and\nmachine learning-based (explicit). Non-machine learning-based methods, such as motion\nhistory images [219] or temporal distillation [180], rely on traditional computer vision tech-\nniques. These methods tend to be more straightforward but are generally less sophisti-\ncated. In contrast, machine learning-based approaches [190] [217] offer richer temporal\ninformation and the flexibility to adapt to specific requirements, such as emphasizing certain\nvideo frames. Among the studies reviewed, 55% employed temporal features, with explicit\nmethods\u2014most commonly LSTM models\u2014being the predominant choice. Given that many\nstudies report superior performance when temporal information is incorporated, compared\nto non-temporal methods, it is evident that further emphasis on temporal approaches is war-\nranted.\n3.6.2 Processing\nRegarding machine learning approaches, various models and techniques have been employed\nfor pain estimation. CNN models remain the most widely used, with more than 75% of stud-48 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nies utilizing 1D, 2D, or 3D filters, highlighting the central role of convolution operations\nin deep learning. Sequential models, such as RNNs, GRUs, LSTMs, and biLSTMs, follow\nclosely behind in popularity. Almost half of the studies used pre-trained models to achieve\ntheir desired performance. This suggests that existing pain databases may not be adequate\nfor training deep-learning models from scratch. Non-deep learning models have also been\nemployed in 26studies as auxiliary decision components, with SVMs and shallow neural net-\nworks being the most common choices. There seems to be significant potential for adopting\nnewer deep learning architectures, especially transformer-based models, which have demon-\nstrated state-of-the-art results in various AI research fields and are particularly suited for\nexploiting temporal modality information [247].\nThe predominant learning method used across studies is supervised learning. How-\never, 16papers explored or adopted alternative methods such as unsupervised learning [119,\n133, 179, 189, 206, 223, 225, 231, 236, 241, 243], self-supervised [180, 244], self-supervised\nlearning [180, 244], semi-supervised learning [119], weakly supervised learning [190, 191],\nand federated learning [165]. Given the limited availability of pain data resources, self-\nsupervised learning appears to be the most appropriate method for future research and should\nbe further embraced by the community.\nLastly, it is notable that most studies\u2014approximately 70%\u2014treat pain assessment as a\nclassification problem rather than a regression problem. However, we believe that regres-\nsion more closely reflects the continuous nature of pain and is better suited to capturing the\ncomplexity of pain sensation.\n3.6.3 Evaluation\nThe primary objectives of the reviewed studies were (i)to estimate pain intensity on a dis-\ncrete scale (multi-class classification), (ii)to measure pain intensity on a continuous scale,\nand(iii)to determine the presence or absence of pain (binary classification). Notably, 25\nstudies focused on pain detection rather than pain intensity estimation, which, from a clin-\nical standpoint, is less informative as it does not provide sufficient data for effective pain\nmanagement. From an engineering perspective, detecting the presence or absence of pain is\nalso a more straightforward and less demanding task.\nA small subset of studies took a different approach to pain estimation. For instance, one\nstudy [179] sought to differentiate genuine pain from acted pain. Another [231] explored\npain events in emergency triage settings rather than controlled laboratory environments,\nwhile [234] examined the feasibility of real-time pain detection on IoT devices. Addition-\nally, [142] and [143] aimed to address the issue of occluded faces in pain estimation. So-\nciodemographic and psychological factors were also considered, as seen in studies like [245],\nwhich explored gender differences, and [194], which focused on pain assessment in elderly3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 49\npatients with dementia. The limited exploration of pain estimation in real-world settings\nor unconventional contexts suggests that current approaches may not be fully applicable in\npractical environments like clinics and hospitals.\nVarious annotation types are used regarding ground truth, such as self-reported ratings,\nFACS, and observer scales. Temporal features are critical for accurately estimating pain\nintensity, making the temporal granularity of the ground truth equally important. Several\nstudies have questioned the objectivity of PSPI scores, as noted in [248], which highlights\nthat PSPI scores can be zero even when pain is present or that there may be no visible facial\nexpressions in low-intensity pain. Pain expressions not captured by the FACS system, such\nas raising eyebrows or opening the mouth, further challenge the use of PSPI [249]. Addi-\ntionally, PSPI does not account for pain-related head and body movements, which are par-\nticularly valuable in newborn assessments [250]. For these reasons, we recommend moving\naway from PSPI as ground truth in favor of self-reports and observer scales at the video-\nsegment level.\nAround 54% of the studies employed the leave-one-subject-out (LOSO) validation method,\nwhich is widely regarded as more objective and better for assessing the generalizability of\nmodels. However, LOSO can be less practical due to the increased model size and longer\ntraining times. When researchers use other validation methods, such as k-fold or hold-out,\nit is essential to ensure that consecutive, highly correlated frames from the same subject do\nnot skew the training and validation results, leading to flawed estimations. Moreover, when\nresearchers define their own validation or testing sets, comparing results across studies\u2014\nespecially between classification and regression models\u2014becomes nearly impossible. We\nbelieve standardized evaluation protocols should be developed for each publicly available\ndatabase for these reasons.\n3.6.4 Pain Databases for Evaluation\nThe availability of suitable public databases is arguably the most crucial factor in addressing\nthe challenge of automatic pain assessment. Several aspects must be considered in evaluating\nthese datasets, including the number of subjects and their characteristics, such as age, sex,\nhealth status, and race. Moreover, the ground truth must be objective and offer meaningful\ninsights into the subject\u2019s pain experience [154].\nFig. 3.1 illustrates the number of papers corresponding to the pain database utilized in\neach study. It is clear from this figure that the UNBC andBioVid databases were the most\ncommonly used public datasets. However, the UNBC dataset does not record the subjects\u2019\nages, despite age being a known factor in pain expression [35,66]. While the BioVid dataset\ndoes document age, the oldest participants are only 65years old, which is notable since pain\nand its management are critical issues among individuals aged 65and older [251]. Simi-50 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nlar limitations are found in other pain datasets, such as X-ITE [117], EmoPain [115], and\nSenseEmotion [116].\nIt is well known that aging causes skin changes, including texture, rigidity, and elastic-\nity alterations, which can impact facial emotion recognition tasks [78]. Additionally, race-\nrelated factors can lead to inaccurate pain assessments due to variations in how pain is ex-\npressed [252]. Notably, one study by Nerella et al. [175] reported lower performance when\ntheir model was tested on African American patients. Furthermore, only one study [194]\nwas found that specifically addressed pain estimation in elderly individuals with dementia.\nIn summary, developing objective, automated, and generalizable deep learning-based\npain assessment systems will only be possible if balanced and representative datasets are\navailable for training and external validation.\n3.6.5 Interpretation of Results\nRecent advancements in AI have shown state-of-the-art performance across nearly every\nscientific discipline, often surpassing human accuracy in specific diagnostic tasks [253].\nHowever, a significant drawback of AI solutions, particularly deep neural networks, is their\nlack of transparency, commonly called \u201cblack box AI\u201d. This term highlights how these\nmodels learn intricate functions that are opaque and frequently incomprehensible to hu-\nmans [254]. This opacity is a primary reason for the criticism directed toward deep learning\ntechniques [255]. Various techniques, such as visualizations and gradients-backpropagation\nfocusing on specific units, have been developed to offer insights into how these models func-\ntion. For further reading, refer to the comprehensive review on explanatory techniques in\ndeep learning [256].\nTable 3.7 outlines the different approaches used to interpret model decisions. Only a\nsmall fraction of the reviewed studies\u2014 20out of 110\u2014implemented methods to explain\nhow their models work and which features or elements they focus on. It is important to\nnote that interpretable machine learning can be broadly defined as the \u201cextraction of rele-\nvant knowledge from a machine-learning model concerning relationships either contained\nin data or learned by the model\u201d [257]. To summarize: (i)18% of the reviewed studies\nprovided an approach to enhance the interpretability of the model\u2019s decision, (ii)all of these\nmethods were applied to studies using facial images as the input modality, and (iii)around\nhalf of these studies were conducted by just three specific research groups. These findings\nsuggest that the issue of interpretability and explainability within deep learning remains un-\nderexplored, particularly in the context of automatically classifying pain severity levels.3.7. CHALLENGES AND FUTURE DIRECTIONS 51\nTable 3.7: Interpretation approaches.\nPaper Year Modality Method\n[124] 2021 F (RGB) visualization (saliency maps)\n[128] 2018 F (RGB) visualization (heat maps)\n[130] 2021 F (RGB) visualization (saliency map)\n[133] 2016 F (RGB) visualization (learned filters)\n[134] 2021 F (RGB) visualization (learned filters)\n[135] 2019 F (RGB) visualization (heat maps),\nvalues of learned weights\n[138] 2018 F (RGB) visualization (saliency maps)\n[141] 2021 F (RGB) visualization (attention maps)\n[142] 2021 F (RGB) visualization (saliency map)\n[143] 2021 F (RGB) visualization (activation maps)\n[153] 2020 F (RGB) visualization (pixels contributions)\n[177] 2017 F (RGB) visualization (average saliency map)\n[179] 2019 F (RGB) visualization\n(generated intermediate representation)\n[194] 2020 F (RGB) visualization (saliency maps)\n[196] 2020 F (RGB) weights per AU (contribution of AUs)\n[173] 2019 F (RGB) visualization (feature maps)\n[174] 2021 F (RGB) visualization (integrated gradients)\n[210] 2021 F (RGB) visualization (heatmaps)\n[211] 2020 F (RGB) visualization (attention maps),\nvalues of learned weights\n[212] 2019 F (RGB) visualization (attention maps)\n3.7 Challenges and Future Directions\nThis section discusses the existing challenges in automatic pain assessment and proposes\nfuture research directions to further progress in the field.\n3.7.1 Current Challenges in Automatic Pain Assessment & Future Research Direc-\ntions\nSeveral limitations exist in the current pain databases. Important demographic factors such\nas sex, gender, and age are often missing, and there is an apparent lack of racial diversity\namong subjects. For example, facial structures and emotional expressions vary across Cau-\ncasian, Asian, and African populations [258]. Moreover, social interactions, such as the\npresence of a partner during assessments, could influence pain manifestation and should\nbe included in future datasets [69]. Estimating the location of pain, particularly for infants\nor individuals with communication impairments, is another vital aspect of pain assessment52 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsystems, which current databases largely overlook. Future datasets should incorporate stim-\nuli targeting various body locations. Furthermore, the videos in existing visual databases\noften have low to medium resolution and frame rates, which are inadequate for capturing\nfacial micro-expressions. Audio data is also sparsely represented, though it holds potential\nas a valuable modality. From an audio perspective, integrating natural language processing\n(NLP) methods to extract linguistic features and create multimodal systems is a promising\ndirection, as shown in affective computing research [259]. Finally, specific validation proto-\ncols should be provided with present and future datasets to ensure objective and consistent\ncomparisons across studies.\nFrom an engineering perspective, several issues must be addressed to advance automatic\npain assessment. Developing multimodal approaches is essential for creating robust systems\nwith enhanced capabilities. Not only do multimodal methods demonstrate better perfor-\nmance than unimodal ones, but they are also crucial in real-world scenarios where a specific\nmodality may become unavailable. Additionally, it is essential to exploit each modality\u2019s\ntemporal aspects fully. We encourage using machine learning models or other techniques\nthat can accommodate the dynamic nature of pain. More work is needed to improve the accu-\nracy of multi-level and low-intensity pain estimation. Another area of research involves the\nrelationship between pain and other affective states, such as negative emotions, which often\ncoexist during painful events. Detecting these emotions could improve pain assessment. Ad-\ndressing challenges like occlusions or poor lighting conditions in vision-based systems also\nrequires attention. Researchers should explore these scenarios, even if current databases do\nnot account for them. Real-time application of pain assessment systems is another critical\nfactor, so future studies should measure throughput, such as the number of images processed\nper second during inference. Generalization is another crucial concern for AI systems, and\nevaluating trained models across different pain databases could be valuable. Finally, to facil-\nitate the clinical adoption of AI-based pain assessment systems, the models\u2019 decisions need\ngreater explainability. Developing or adopting methods that improve interpretability will\nenhance their clinical viability.Chapter 4\nDemographic Variables: Their Role and\nImpact\nContents\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.1", "A Modality-Agnostic Pipeline for Automatic Pain Assessment": ". . . . . . . 119\n7.2.1", "Experimental Evaluation & Results": "We employ the dataset provided by the challenge organizers [118,333], which includes facial\nvideos and fNIRS data from 65participants. The dataset is partitioned into 41 training, 12\nvalidation, and 12 testing subjects, all recorded at the Human-Machine Interface Laboratory,\nUniversity of Canberra, Australia. Electrodes for transcutaneous electrical nerve stimula-\ntion, used as pain stimuli, were placed on the right hand\u2019s inner forearm and back. The study\nmeasures both pain threshold\u2014the lowest stimulus intensity perceived as painful\u2014and pain\ntolerance\u2014the maximum intensity of pain a participant can tolerate. For the fNIRS mea-\nsurements, 24channels were utilized for both HbO and HbR, and each video in the dataset\ncontains 30frames. This paper focuses on the results from the validation segment of the\ndataset, which are structured into a multi-level classification setting for No Pain ,Low Pain ,\nandHigh Pain . Table 7.3 details the training framework for the automatic pain assessment. It\nshould be noted that while numerous experiments were conducted across different modalities\nand their combinations, only the most successful outcomes are discussed in the subsequent\nsections and detailed in the corresponding tables.\nFacial Videos\nFor facial videos, two embedding fusion methods were implemented: the Addition tech-\nnique, which aggregates 30embeddings into a single vector of dimension d\u201c500, and the\nConcatenation approach, which merges the embeddings into a larger vector of d\u201c15,000.\nWith the Addition method, the initial accuracy reached 41.90% under basic augmentation\nand regularization settings. Enhancing the augmentation intensity and adjusting MaskOut\nimproved the accuracy incrementally, achieving a peak of 44.91%. Adjusting DropOut and\nother augmentation parameters refined the performance to 43.52%. These findings are de-\ntailed in Table 7.4. Employing the Concatenation method with initial uniform augmentation\nprobabilities resulted in a starting accuracy of 40.28%. Strategic increases in MaskOut and\nmaintaining other augmentations at moderate levels led to a gradual accuracy improvement,\nculminating in a high of 43.75% when all augmentations were maximized except MaskOut ,\npaired with high regularization settings. The results of this approach are outlined in Table\n7.5.7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 125\nTable 7.4: Results utilizing the video modality & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.1 0.1 0.1 0.1 |3 0.1 0.5 41.90\n0.5 0.5 0.5 0.7 |3 0.0 0.5 44.91\n0.5 0.5 0.5 0.7 |10 0.0 0.5 42.13\n0.5 0.5 0.5 0.7 |3 0.0 0.6 42.36\n0.9 0.9 0.9 0.7 |3 0.3 0.7 43.52\nRand :RandAugment Trivial : TrivialAugment LS:Label Smoothing MS: multiclass pain\nassessment. For Augmentation & Regularization the first number represents the probability\nof application, while in MaskOut the number followed |indicates the number of square\nsections applied.\nTable 7.5: Results utilizing the video modality & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.3 0.3 0.3 0.3 |3 0.1 0.5 40.28\n0.5 0.5 0.5 0.8 |5 0.0 0.5 41.44\n0.9 0.9 0.9 0.7 |3 0.2 0.7 42.13\n0.9 0.9 0.9 0.7 |1 0.4 0.5 41.90\n0.9 0.9 0.9 0.6 |3 0.4 0.5 43.75\nfNIRS\nSimilar to facial videos, the Addition andConcatenation methods were also applied to the\nfNIRS channels, excluding two faulty ones from the original 24. For the HbR with the\nAddition method, the initial accuracy was 39.35%, set with uniform probabilities of 0.5for\nAugMix ,Rand , and Trivial , and MaskOut adjusted to 0.6|5. Modifying MaskOut to0.7|3and\nincreasing LSslightly reduced accuracy, while subsequent adjustments in LSandDropOut\nimproved it to 41.20% (refer to Table 7.6). In the HbR with the Concatenation method, start-\ning with MaskOut at0.7|3led to an accuracy of 40.97%. Escalating all augmentations to\n0.9, while keeping MaskOut at0.7|3, achieved a peak accuracy of 42.13% (refer to Table\n7.7). For the HbO with the Addition method, accuracies started at 43.06% with uniform aug-\nmentation probabilities of 0.3andMaskOut at0.3|3. Elevating MaskOut to0.7|3with minor\nadjustments in LSandDropOut maintained similar accuracies, while optimizing MaskOut\nto0.8|3enhanced performance to 44.68% (refer to Table 7.8). The HbO with the Concate-\nnation method began with an accuracy of 42.13% under an augmentation probability of\n0.1. A balanced augmentation setup at 0.9andMaskOut at0.7|3lifted the peak accuracy\nto44.44%, demonstrating the effectiveness of increased overall augmentation coupled with\nhigh regularization. Further adjustments slightly reduced accuracy, highlighting the critical126 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.6: Results utilizing the HbR & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.6 |5 0.0 0.5 39.35\n0.5 0.5 0.5 0.7 |3 0.4 0.5 38.89\n0.9 0.9 0.9 0.7 |3 0.1 0.9 40.05\n0.9 0.9 0.9 0.7 |5 0.4 0.5 41.20\n0.5 0.5 0.5 0.7 |3 0.0 0.4 40.51\nTable 7.7: Results utilizing the HbR & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.0 0.0 0.7 0.7 |3 0.0 0.5 40.97\n0.5 0.5 0.5 0.7 |1 0.0 0.5 41.44\n0.9 0.9 0.9 0.7 |3 0.1 0.8 42.13\n0.9 0.9 0.9 0.7 |3 0.4 0.5 41.20\n0.5 0.5 0.5 0.7 |3 0.0 0.3 39.81\nTable 7.8: Results utilizing the HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.3 0.3 0.3 0.3 |3 0.1 0.5 43.06\n0.5 0.5 0.5 0.7 |3 0.2 0.5 42.82\n0.9 0.9 0.9 0.7 |3 0.4 0.8 43.29\n0.9 0.9 0.9 0.7 |9 0.4 0.5 44.44\n0.9 0.9 0.9 0.8 |3 0.4 0.5 44.68\nnature of optimal augmentation settings (refer to Table 7.9). Generally, enhanced perfor-\nmance is observed with HbO compared to HbR, as noted in other studies [334], attributed\nto its superior signal-to-noise ratio. Combining HbR and HbO using the Addition method\ninitially showed an accuracy of 42.82% with all augmentations at zero except for MaskOut\nat0.7|3. Increasing AugMix ,Rand , and Trivial to0.5while raising MaskOut to0.7|7slightly\nimproved accuracy to 43.29%. Adjustments to MaskOut back to 0.7|3with a slight increase\ninLSled to a minor reduction in accuracy to 42.59%. However, further increasing all aug-\nmentations to 0.9andLSto0.3while maintaining MaskOut at0.7|3maximized accuracy\nto43.75%. A reduction in DropOut to0.1in the final setup slightly reduced accuracy to\n43.06%, emphasizing the importance of optimizing regularization alongside augmentation\nstrategies for achieving optimal results (refer to Table 7.10).7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 127\nTable 7.9: Results utilizing the HbO & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.1 0.1 0.1 0.1 |3 0.1 0.5 42.13\n0.5 0.5 0.5 0.0 |0 0.0 0.5 43.98\n0.5 0.5 0.5 0.7 |1 0.0 0.5 42.36\n0.9 0.9 0.9 0.7 |3 0.4 0.9 44.44\n0.5 0.5 0.5 0.7 |3 0.0 0.8 43.52\nTable 7.10: Results utilizing the HbR, HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.0 0.0 0.7 0.7 |3 0.0 0.5 42.82\n0.5 0.5 0.5 0.7 |7 0.0 0.5 43.29\n0.5 0.5 0.5 0.7 |3 0.1 0.5 42.59\n0.9 0.9 0.9 0.7 |3 0.3 0.9 43.75\n0.5 0.5 0.5 0.7 |3 0.0 0.1 43.06\nTable 7.11: Results utilizing the videos, HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.4 |5 0.0 0.5 42.36\n0.5 0.5 0.5 0.7 |9 0.0 0.5 41.67\n0.9 0.9 09. 0.7 |3 0.1 0.6 42.59\n0.9 0.9 0.9 0.7 |3 0.3 0.9 43.06\n0.9 0.9 0.9 0.7 |5 0.4 0.5 43.75\nFusion\nThis section explores the fusion of facial videos and fNIRS, explicitly utilizing HbO due\nto its demonstrated superior performance over HbR. Two fusion methods were employed:\ntheAddition method, which aggregates embeddings from video frames and fNIRS channels\ninto a unified vector, and the Single-Diagram method, where aggregated embeddings from\nboth modalities are visualized simultaneously in a single image. For the Addition method,\ninitial configurations with moderate augmentation levels ( 0.5forAugMix ,Rand ,Trivial ) and\nMaskOut at0.4|5achieved an accuracy of 42.36%. Increasing the augmentation levels to 0.9\nand adjusting the regularization parameters ( LSup to 0.4andDropOut up to 0.9) enhanced\nthe accuracy, peaking at 43.75% (refer to Table 7.11). For the Single Diagram method, accu-\nracy improvements were noted, as shown in Table 7.12. Starting with lower MaskOut levels128 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.12: Results utilizing the videos, HbO & Single Diagram method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.3 |5 0.0 0.5 45.83\n0.9 0.9 0.9 0.7 |3 0.1 0.6 46.76\n0.9 0.9 0.9 0.7 |3 0.3 0.6 46.53\n0.9 0.9 0.9 0.9 |3 0.4 0.5 45.83\n0.5 0.5 0.5 0.7 |3 0.0 0.7 45.14\nTable 7.13: Comparison with the validation baseline provided by the AI4PAIN challenge organizers.\nApproachModality\nVideo fNIRS Fusion\nBaseline 40.00 43.20 40.20\nOur 44.91 44.68 46.76\nat0.3|5and standard augmentation probabilities ( 0.5), the accuracy was 45.83%. Utilizing\naugmentation probabilities to 0.9andMaskOut adjustments to 0.7|3significantly improved\nperformance, achieving a high of 46.76%.\nInterpretation & Comparison\nIn the framework\u2019s analysis, attention maps from the last layer of PainViT\u20132 were generated,\nillustrating the processed unified image that integrates both the video and HbO embedding\nwaveforms. This layer consists of 500neurons, each specifically engaging with different in-\nput aspects. Figure 7.3 displays four examples demonstrating how specific neurons predom-\ninantly focus on the video embedding waveform. In contrast, others concentrate on the HbO\nwaveform, and some attend to both, highlighting various details. Table 7.13 compares the\nproposed pipeline and the baseline results set by the challenge organizers. The video-based\napproach utilizing the Addition method exceeded the baseline by 4.91%. Implementing the\nHbO with the Addition method showed a minor improvement of 1.48%. However, the fu-\nsion of modalities through the Single Diagram method achieved a more substantial gain of\n6.56%.\n7.2.3 Discussion\nThis chapter contributes to the First Multimodal Sensing Grand Challenge for Next-Gen\nPain Assessment (AI4PAIN) , utilizing facial videos and fNIRS in a modality-agnostic frame-\nwork. The Twins-PainViT , a dual Vision Transformer configuration, was pre-trained across7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 129\nmultiple datasets using a multi-task learning approach. A key feature of our approach is the\nwaveform representation applied to the original fNIRS data and the learned embeddings, al-\nlowing for their integration into a single image diagram. This method effectively eliminates\nthe need for domain-specific models for each modality. Our experiments demonstrated high\nperformance in both unimodal and multimodal configurations, outperforming the established\nbaselines. The analysis of PainViT\u20132 through attention maps further revealed that specific\nneurons specifically target different modalities or distinct aspects within them, suggesting a\ncomprehensive analytical approach. Future research should continue to explore multimodal\nstrategies, as they have shown superior efficacy in real-world pain assessment settings. De-\nveloping interpretative methods is crucial for integrating these advanced frameworks into\nclinical practice.\n7.3 A Foundation Model for Automatic Pain Assessment\nWe introduce PainFormer , a multi-task learning vision foundation model tailored for auto-\nmatic pain assessment. This initiative is the first to develop and deploy a foundation model\nfor pain recognition, inspired by the frameworks discussed in [320]. Our method involves\ntraining across various datasets and tasks, leveraging large-scale corpora to enhance repre-\nsentation learning for pain assessment applications. This research makes three key contri-\nbutions: (1) it introduces a foundation model capable of extracting robust embeddings from\ndiverse modalities, (2) it incorporates synthetic thermal and estimated depth videos as inno-\nvative modalities, and (3) it evaluates the performance of these modalities in both unimodal\nand multimodal settings.\n7.3.1 Methodology\nThis section outlines the structure and components of the suggested framework. It covers\nthe foundation model\u2019s pretraining based on multi-task learning, the methods for augmen-\ntation, and the training configurations for pretraining and pain evaluation tasks. It also de-\nscribes how synthetic thermal and depth videos are generated and details the visualization\ntechniques for biosignal modalities employed in this research.\nFramework Architecture\nThe framework integrates three models: the PainFormer , a foundation model that extracts\nembeddings from input data; the Embedding-Mixer , which applies these embeddings, either\nsingly or in combination, to classify pain; and the Video-Encoder , which reduces video data\ninto a lower-dimensional latent space for use in the multimodal approaches that are explained\nlater. This framework operates in two separate stages: initially extracting embeddings and130 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.14: Number of parameters and FLOPS for the modules of the proposed framework.\nModule Params\n(Millions)FLOPS\n(Giga)\nPainFormer 19.60 5.82\nEmbedding-Mixer 9.85 2.94\nVideo-Encoder 3.37 0.86\nTotal 32.82 9.62\nthen deploying them according to the demands of specific modality pipelines. Table 7.14\ndetails the parameters and computational costs for each module\u2019s floating-point operations\n(FLOPS).\nPainFormer: Vision Transformers (ViT) have become increasingly widespread for various\nimage-processing tasks, demonstrating the effectiveness of their self-attention mechanisms.\nMoreover, developing Vision Multilayer Perceptron (Vision-MLP) models that use spectral\nmixing techniques\u2014substituting self-attention layers with Fourier transformation layers\u2014\nillustrates that simpler structures with fewer inductive biases can achieve similar outcomes.\nOur strategy incorporates two key concepts: hierarchical Vision Transformers (ViT) [326],\nwhich use multiple embedding extraction stages to boost performance and scalability, and\nthe Fourier transform\u2019s efficient token information mixing as shown in [335]. PainFormer\nintegrates spectral layers using the Fast Fourier Transform (FFT) with self-attention layers.\nBoth spectral and self-attention layers are initially applied, whereas later stages rely solely\non self-attention. The architecture of PainFormer is illustrated in Fig. 1(a). Each 2D input\nimage Iis segmented into nnon-overlapping patches, each patch PRn\u02c6h\u02c6w\u02c63, where hand\nware the patch resolution set at 16\u02c616, and 3represents the RGB channels. Each patch\nis linearly projected into a dimension d\u201c768, followed by positional encoding. Applying\nDiscrete Fourier Transform (DFT) to a 1D sequence of Nelements, xrns, ranging from 0to\nN\u00b41, yields:\nXrks\u201cN\u00b41\u00ff\nn\u201c0xrns\u00a8e\u00b4i2\u03c0k\nNn:\u201cN\u00b41\u00ff\nn\u201c0xrns\u00a8Wkn\nN, (7.11)\nwhere iis the imaginary unit, and WNis defined as e\u00b4i2\u03c0\nN. The sequence can be transformed\nback to the time domain by applying the inverse Discrete Fourier Transform (IDFT):\nxrns\u201c1\nNN\u00b41\u00ff\nk\u201c0Xrks\u00a8ei2\u03c0k\nNn, (7.12)7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 131\nwhere xrnsrepresents the original sequence. Furthermore, for two-dimensional inputs, xrm, ns,\nwith0\u010fm\u010fM\u00b41and0\u010fn\u010fN\u00b41, the formula extends to:\nXru, vs\u201cM\u00b41\u00ff\nm\u201c0N\u00b41\u00ff\nn\u201c0xrm, ns\u00a8e\u00b4i2\u03c0`um\nM`vn\nN\u02d8\n, (7.13)\nwhere Xru, vsis the frequency-domain representation of the input xrm, ns.\nSpectral Layer: For the tokens xfrom image I, a 2D FFT is applied across the spatial\ndimensions to transform xinto the frequency domain:\nX\u201cFrxsPCh\u02c6w\u02c6d. (7.14)\nAfter applying the FFT to extract the various frequency components of the image, we employ\na learnable filter, KPCh\u02c6w\u02c6dacts as a gate to regulate the significance of each frequency\ncomponent. This spectrum modulation allows for the identification and learning of features\nsuch as lines and edges. Specifically:\n\u02dcX\u201cKdX, (7.15)\nwhereddefines the element-wise multiplication. Afterward, the inverse Fast Fourier Trans-\nform (IFFT) is applied, which converts the spectral space back into the physical space:\nx\u00d0F\u00b41r\u02dcXs, (7.16)\nwhere the physical space is referred to as the spatial domain in this case. The final component\nof a spectrum layer is an MLP module, which enables efficient channel mixing communica-\ntion:\n\u03a6pxq\u201cW2\u00a8GELUpDWConvpW1\u00a8x`b1qq`b2, (7.17)\nwhere DWConv denotes a depthwise convolution layer. In addition, layer normalization is\nemployed before and after the FFT and IFFT processes, refer to Fig. 1(b).\nSelf-Attention Layer: In this layer, the standard self-attention mechanism characteristic\nof transformers is utilized. For a given token sequence X, the attention mechanism is defined\nas:\nAttpXq:\u201csoftmax\u02c6XW qpXW kqT\n?\nd\u02d9\nXW v, (7.18)\nwhere Att maps RN\u02c6dtoRN\u02c6d, with Nrepresenting hw. The matrices Wq,Wk, and Wvin\nRd\u02c6dcorrespond to the query, key, and value weights, respectively. Layer normalization is132 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.15: Details of the PainFormer\u2019s architecture.\nStage # Spectral\nLayers# Self-Attention\nLayers# Self-Attention\nHeadsDimension\nd\n1 2 1 2 64\n2 2 2 4 128\n3 \u2013 12 10 320\n4 \u2013 3 16 160\nd: token dimensions\napplied both before and after the attention mechanism, mirroring the approach used in the\nspectral layer. Additionally, the MLP component within this layer is expressed as:\n\u03a6pxq\u201cW2\u00a8GELUpW1\u00a8x`b1q`b2. (7.19)\nFig. 1(c) illustrates the design of this layer.\nStages: A stage-based architecture was developed to create a hierarchical representa-\ntion. PainFormer is organized into four stages, each followed by a single-layer 2D CNN that\ndownsamples the resolution by a factor of 2to reduce the token dimensions. Additionally,\neach stage incorporates a distinct combination of spectral and self-attention layers, with vary-\ning numbers of heads in the self-attention layers and different dimensions for the extracted\ntokens. Table 7.15 presents the relevant details.\nEmbedding-Mixer: The model is built on a transformer architecture, leveraging cross- and\nself-attention mechanisms. Echoing insights from prior research [277], it employs an asym-\nmetrical attention scheme using cross-attention with fewer latent variables to reduce compu-\ntational demands and boost efficiency. Cross-attention operation parallels self-attention, as\ndetailed in Eq. (7.18). Nonetheless, the dimensions for Wq,Wk, and Wvadjust to n\u02c6d\nfrom d\u02c6d, with n\u0103dandnspecifically set at 256. The Embedding-Mixer includes 2\nlayers, each equipped with 1cross-attention and 2self-attention modules. The head counts\nfor cross- and self-attention are 1and8, respectively. The final classification task utilizes an\noutput embedding of length 512, as shown in Fig. 1(d).\nVideo-Encoder: The design of this specific module mirrors the Embedding-Mixer . How-\never, it is simplified for efficiency by including only a 1layer that contains a single cross-\nattention module with a 1head. The number of latent variables, n, is maintained at 256,\nand the output embedding length is set at 40. This module functions exclusively within a7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 133\nparticular framework as part of one of the multimodal strategies, integrating video and GSR\nembeddings. The module\u2019s architecture is depicted in Fig. 1(e).\nSynthetic Thermal & Depth Videos\nThis section incorporates thermal and depth vision modalities alandGB videos into our pain\nassessment frameworks. For the thermal modality, we utilize thermal videos from our ear-\nlier research [39], described in Chapter 6, which introduced an image-to-image translation\n(I2I) method using a conditional generative adversarial network (cGAN). This network was\ndesigned and trained to map the data distribution from the RGB to the thermal domain, fa-\ncilitating the creation of synthetic thermal images from new RGB videos. For the depth\nvideos, we employ the \u201cDepth Anything\u201d technique [336], which is a pioneering model for\nmonocular depth estimation (MDE) that uses a vision transformer-based encoder-decoder\narchitecture with semi-supervised learning. Fig. 7.5 displays a frame sample from the RGB,\nsynthetic thermal, and depth modalities.\nBiosignal Visualization\nGiven that the core model in this research is vision-based, it necessitates using 2D represen-\ntations for physiological modalities. We explore four distinct visualizations: (1) waveform\ndiagrams, which outline the signal\u2019s progression over time, showcasing its amplitude, fre-\nquency, and phase characteristics; (2) spectrogram-angle , which displays the phase angles\nassociated with different frequencies; (3) spectrogram-phase , which reveals phase details\nand incorporates unwrapping to rectify discontinuities; and (4) spectrogram-PSD , which de-\nlineates the power spectral density, indicating the distribution of power across frequencies\nover time. Fig. 7.6 provides an example of each visualization type.\nFoundation Training\nPainFormer , our proposed foundation model, serves as an embedding extractor as previously\noutlined. It has undergone extensive training on 14datasets, which include a total of 10.9\nmillion samples; for further details, see Table 7.16. The training datasets cover a variety\nof human-centric data, ranging from facial recognition datasets such as VGGFace2 [280]\nandDigiFace-1M [313] to datasets aimed at recognizing basic and compound emotions,\nlike AffectNet [287] and RAF-DB [289]. Furthermore, datasets based on biosignals like\nEEG, EMG, and ECG have been incorporated. Regarding training methodology, PainFormer\nemploys a multi-task learning strategy, with each dataset representing a separate supervised\nlearning task. Architecturally, the model adheres to its initial design as specified in 7.3.1\nbut now includes additional task-specific auxiliary classifiers. These classifiers comprise\na single-layer, fully connected network with an ELU (Exponential Linear Unit) activation134 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.16: Datasets utilized for the multitask learning-based pretraining process of the framework.\nDataset # samples # classes Modality\nVGGFace2 [280] 3.31M 9,131 Facial Images\nSpeakingFaces RGB [312]\u00150.76M 142 Facial Images\nSpeakingFaces Thermal [312]\u00150.76M 142 Facial Images\nDigiFace-1M [313] 0.72M 10,000 Facial Images\nDigiFace-1M [313] 0.50M 100,000 Facial Images\nAffectNet [287] 0.40M 8 Facial Images\nSFace [337] 1.84M 10,341 Facial Images\nCACIA-WebFace [129] 0.50M 10,575 Facial Images\nRAF-DB basic [289] 15,000 7 Facial Images\nRAF-DB compound [289] 4,000 11 Facial Images\nCompound FEE-DB [288] 6,000 26 Facial Images\nEEG-BST-SZ [328]\u00121.50M 2 EEG signals\nSilent-EMG [329]\u00120.19M 8 EMG signals\nECG HBC Dataset [291]\u00120.45M 5 ECG signals\nTotal: 14 datasets\u2013tasks 10.9M\nEEG: electroencephalogram EMG: electromyography ECG \u0015: The datasets were also used for the I2I process\ndescribed in 7.3.1, in addition to the training of the PainFormer \u0012: The samples were transformed into spectrograms\nbefore being employed.\nfunction. The objective during training is to simultaneously learn from all 14datasets/tasks.\nThe following equation formalizes this approach:\nLtotal\u201c14\u00ff\ni\u201c1rewiLSi`wis, (7.20)\nwhere LSiindicates the loss linked to each dataset/task, and wiare the adaptive weights that\naim to minimize the overall loss Ltotal, encompassing all individual task losses. The model\nwas trained using this methodology over 200epochs.\nAugmentation & Regularization Methods\nVarious augmentation and regularization strategies were applied during the pre-training of\nPainFormer and in the downstream pain assessment tasks. For foundational training, Triv-\nialAugment [282] and AugMix [281] were used. A customized augmentation method that\nmodifies brightness, contrast, and saturation and involves image cropping was also imple-\nmented. The pre-training regime included adding random noise sourced from a Gaussian\ndistribution. Moreover, a technique was devised to obscure random square portions of the\ninput images. Regularization during pre-training was achieved using DropPath [338] and\nLabel Smoothing [331]. Within the pain assessment framework, two specific augmentation7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 135\nTable 7.17: Training details of the proposed framework.\nTask Optimizer LR LR decay Weight\ndecayWarmup\nepochsCooldown\nepochsBatch size\nMTL AdamW 2e-5 cosine 0.1 5 10 126\u0010\nPain AdamW 2e-5 cosine 0.1 10 10 32\nMTL: multi-task learning for pre-training the foundation model Pain: pain assessment task LR: learning rate \u0010: batch\nsize is proportionally distributed across the 14 tasks\nmethods were integrated. The first, termed Basic , involves polarity inversion and the addition\nof noise, which alters the original input embeddings by reversing data elements\u2019 polarity and\nadding random noise from a Gaussian distribution, thereby inducing variability. The second\nmethod, Masking , implements zero-valued masks on the embeddings, effectively nullify-\ning sections of the vectors. These masks are randomly sized and placed, obscuring 10%\nto20% of the embedding\u2019s total dimensions. For further regularization, techniques such as\nDropOut [332] and Label Smoothing [331] were employed. Additional specifics on the two\ntraining methodologies are detailed in Table 7.17.\nDataset Details\nTo evaluate the effectiveness and resilience of our proposed framework, we performed tests\non two specific pain datasets, BioVid [109] and AI4Pain [118]. These datasets offer a varied\nand solid foundation for validating the performance of our model in pain assessment tasks.\nBioVid Heat Pain Database: This dataset is recognized and well-established within the do-\nmain of pain research. It encompasses facial videos, electrocardiograms, electromyograms,\nand galvanic skin response measurements from eighty-seven pn=87qhealthy participants ( 44\nmales and 43females, aged between 20and65). The experiment involved applying a ther-\nmode to the participants\u2019 right arm to induce pain. Before data collection, each participant\u2019s\npain and tolerance thresholds were determined, defining the minimum and maximum levels\nof pain experienced. This setup included two additional intermediate levels, culminating in\nfive distinct pain intensities: No Pain (NP), Mild Pain (P 1), Moderate Pain (P 2), Severe Pain\n(P3), and Very Severe Pain (P 4). The temperature for inducing these pain levels ranged from\nP1to P 4but did not exceed 50.5\u02ddC. Participants underwent 20inductions at each of the four\nspecified intensity levels (P 1to P 4), with each stimulus lasting 4s, followed by a recovery\ninterval of 8to12s. Additionally, 20baseline measurements at 32\u02ddC (NP) were conducted,\nresulting in 100total stimulations per participant, administered randomly. After reaching\nthe target temperature for each induction, the data was subsequently segmented into 5.5s\nintervals starting at 1s. This segmentation generated 8,700samples, each 5.5slong, evenly\ndistributed across the five pain intensity classes for each modality, encompassing all 87sub-136 CHAPTER 7. GENERAL-PURPOSE MODELS\njects. The video recordings were captured at a frame rate of 25frames per second (FPS),\nwhile the biosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\nAI4Pain Dataset: The AI4Pain Grand Challenge 2024 dataset is a recent addition tailored\nfor advanced pain recognition tasks using fNIRS and facial video data. Sixty-five pn=65q\nvolunteers participated, including 23females, with ages ranging from 17to52years. The\ndataset additionally includes physiological signals like photoplethysmography (PPG), elec-\ntrodermal activity (EDA), and respiration (RESP), though these are not currently publicly\navailable. The dataset is segmented into three parts: training ( 41volunteers), validation\n(12volunteers), and testing ( 12volunteers). The data collection setup for this dataset in-\nvolves comprehensive fNIRS and video recording to capture both brain activity and facial\nexpressions. The fNIRS recordings were conducted using an Artinis device (Artinis Medical\nSystems, Gelderland, the Netherlands), measuring fluctuations in oxygenated haemoglobin\n(HBO2) and deoxygenated haemoglobin (HHB) concentrations (in \u00b5mol/L). This fNIRS sys-\ntem uses 24channels to cover the prefrontal cortex with optodes ( 10sources and 8detectors)\nplaced 30mm apart. It emits near-infrared light at wavelengths of 760nm and 840nm and\nsamples at a rate of 50Hz. The video data is recorded using a Logitech StreamCam at a\nframe rate of 30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low\nPain, and High Pain . It includes 65instances of No Pain (lasting 60s each), 780instances\nofLow Pain (lasting 10s each), and 780instances of High Pain (also lasting 10s each). The\nNo Pain instances represent baseline data. In contrast, Low Pain andHigh Pain are derived\nfrom the pain tolerance tests, capturing subtle and significant changes in neurological and\nbehavioral responses via fNIRS and video data.\n7.3.2 Experimental Evaluation & Results\nThis research devised various testing scenarios, including unimodal and multimodal settings,\nto assess the effectiveness of the proposed foundational model. The aim is to utilize a variety\nof behavioral and physiological modalities to ascertain the capability of PainFormer to gener-\nate and provide high-quality embeddings for pain assessment. The experimental framework\nutilizes a comprehensive set of modalities, encompassing RGB, synthetic thermal imaging,\ndepth videos, and physiological measurements such as ECG, EMG, GSR, and fNIRS, with\nwaveform and spectrogram representations. Additionally, specific pipelines were tailored to\nsingle modalities or their combinations based on each integration need. This adaptability\nis a cornerstone of our approach, as different pipelines might be required depending on the\nspecific demands, data availability, or intended application. We aim to offer robust feature\nrepresentations for any given input modality and excel in performance across all modalities\nand testing scenarios. Figure 7.7 displays a high-level view of the proposed framework. Note7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 137\nthat all images, including video frames and biosignal visual representations, are standardized\nto a resolution of 224\u02c6224pixels.\nThis research employed Part A of the BioVid dataset, focusing on pain assessment in\nbinary terms, differentiating between No Pain (NP) and Very Severe Pain (P4). Validation\nwas conducted using the leave-one-subject-out (LOSO) cross-validation technique. In the\ncase of AI4Pain , a multilevel classification scheme was applied, categorizing pain into three\nlevels: No Pain ,Low Pain , and High Pain . The challenge organizers\u2019 hold-out method\n(training, validation, testing) was utilized for validation. For both datasets, the evaluation\nmetrics included accuracy, recall (sensitivity), and F1 score. It is also important to mention\nthat all experiments used a deterministic approach, ensuring no influence from random ini-\ntializations. This practice assures that any differences in performance observed are strictly\nattributable to specific optimization parameters, modalities, and other controlled variations\nrather than any random factors.\nBioVid\nNumerous experiments were performed using the BioVid dataset. Beyond the original RGB\nvideos, synthetic thermal and depth videos were developed to provide additional visual con-\ntexts, as detailed in 7.3.1. As specified in 7.3.1, four distinct representations of ECGs, EMGs,\nand GSRs were assessed for biosignals. Combinations of these representations were also ex-\nplored.\nVideo: For behavioral modalities within the BioVid dataset, PainFormer generates an em-\nbedding of dimension d\u201c160for each video frame. These embeddings are concatenated to\ncreate a comprehensive representation of each video:\nVD\u201crd1}d2}\u00a8\u00a8\u00a8} dms, DPRN1, (7.21)\nwhere mrepresents the number of frames per video, and N1is the dimensionality of the total\nembedding, computed as m\u02c6d\u00d1138\u02c6160\u201c22,080. This unified embedding is fed\ninto the Embedding-Mixer for final pain assessment. Starting with a training duration of 200\nepochs and using augmentation only on RGB videos, an accuracy of 71.83% and a recall\nof74.52% were recorded. Thermal and depth videos achieved accuracies of 69.83% and\n69.00%, respectively. When the training was extended to 300epochs with intensified aug-\nmentations and incorporating Label Smoothing for regularization, RGB accuracy improved\nto72.50%, although recall decreased slightly by 0.46%. Thermal modality performance de-\ncreased overall, highlighting its sensitivity to augmentations and regularization techniques.\nConversely, depth modalities responded well to these changes, showing improved metrics\nwith an accuracy of 70.08%, a recall of 71.27%, and an F1 score of 69.63%. In the final138 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.18: Results utilizing the video modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1RGB200 0.5 0.5 |10-20| 0.0 0.0 71.83 74.52 70.29\n300 0.7 0.7 |15-20| 0.1 0.0 72.50 74.06 70.93\n600 0.5 0.5 |15-20| 0.1 0.5 76.29 77.56 75.56Thermal200 0.5 0.5 |10-20| 0.0 0.0 69.83 71.51 69.17\n300 0.7 0.7 |15-20| 0.1 0.0 68.83 69.77 68.41\n600 0.5 0.5 |15-20| 0.1 0.5 71.55 72.83 71.12Depth200 0.5 0.5 |10-20| 0.0 0.0 69.00 69.44 67.94\n300 0.7 0.7 |15-20| 0.1 0.0 70.08 71.27 69.63\n600 0.5 0.5 |15-20| 0.1 0.5 71.67 72.84 71.26\nLS: Label Smoothing For Augmentation and Regularization, the number denotes the probability of application,\nwhile in Masking , the number in | |indicates the size of the mask applied.\nexperimental phase, training was extended to 600epochs, employing lighter augmentations\nat a0.7probability, alongside 0.1Label Smoothing and0.5DropOut . This regimen resulted\nin the highest performance for RGB videos, achieving an accuracy of 76.29% and a recall\nof77.56%. The F1 score also significantly increased, rising over 5%to75.56%. Similar pat-\nterns were observed for the thermal and depth videos in this final experimental setup, albeit\nwith minor improvements. Accuracy for thermal videos was 71.55% and for depth videos\n71.67%, with recall rates closely matching at 72.83% and72.84%, respectively. These find-\nings demonstrate consistent enhancement across all visual modalities with refined training\nparameters and extended training durations. Table 7.18 consolidates these experimental out-\ncomes, indicating that the RGB modality consistently surpasses others, while the thermal\nand depth modalities show comparable performance levels. Moreover, although thermal and\ndepth enhancements are modest, they suggest a plateau in potential performance increases.\nECG: The training configuration used for the video data was similarly applied to the ECG\nsignals. As previously indicated, four visual representations were utilized. Each represen-\ntation corresponds to an image dimension of 224\u02c6224pixels, from which embeddings of\ndimensionality d\u201c160are extracted and then inputted into the Embedding-Mixer . Starting\nwith200epochs and employing minimal augmentation without regularization, the waveform\nrepresentation reached an accuracy of 69.58%, with recall and F1 scores of 72.67% and\n68.10%, respectively. The spectrogram-angle had lower performance in all metrics, achiev-\ning an accuracy of 65.58%. Meanwhile, the spectrogram-phase showed better accuracy,7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 139\nTable 7.19: Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 69.58 72.67 68.10\n300 0.7 0.7 |15-20| 0.1 0.0 71.08 72.74 70.22\n600 0.5 0.5 |15-20| 0.1 0.5 73.36 74.75 72.52Angle200 0.5 0.5 |10-20| 0.0 0.0 65.58 66.68 64.89\n300 0.7 0.7 |15-20| 0.1 0.0 66.33 68.22 65.22\n600 0.5 0.5 |15-20| 0.1 0.5 68.25 71.24 66.99Phase200 0.5 0.5 |10-20| 0.0 0.0 70.08 71.54 69.40\n300 0.7 0.7 |15-20| 0.1 0.0 72.33 73.73 71.69\n600 0.5 0.5 |15-20| 0.1 0.5 72.70 74.19 72.14PSD200 0.5 0.5 |10-20| 0.0 0.0 71.08 73.13 70.19\n300 0.7 0.7 |15-20| 0.1 0.0 71.50 73.14 70.18\n600 0.5 0.5 |15-20| 0.1 0.5 75.49 77.15 74.90\nsurpassing the prior two by 0.5%and4.5%, respectively. The spectrogram-PSD achieved\nthe highest results, recording 71.08% accuracy, 73.13% recall, and 70.19% F1 score. Fur-\nther improvements were seen in the 300-epoch configuration across all visual representations\nand metrics. In the ultimate experimental setup extending to 600epochs, enhancements were\nnoted universally, but the spectrogram-PSD showed the most considerable gains, nearly 4%,\nachieving 75.49% accuracy, 77.15% recall, and 74.90% F1 score. This indicates that integrat-\ning amplitude and frequency information, as the PSD representation provides, is particularly\neffective and valuable for analyzing ECG signals. Table 7.19 documents the outcomes for\nthe ECG modality.\nEMG: For EMG signals, the initial training configuration of 200 epochs demonstrated\ncomparable accuracy across the waveform ,spectrogram-phase , and spectrogram-PSD rep-\nresentations, recording scores of 68.75%,68.33%, and 69.25% respectively. However, the\nspectrogram-angle representation underperformed with an accuracy of 66.42%, mirroring\nits lower performance in the ECG modality. In subsequent training sessions with increased\nepochs and enhanced augmentation and regularization, the spectrogram-angle representa-\ntion showed a notable decline in performance across all metrics. Despite some marginal\nimprovements in the 300-epoch configuration, it still trailed behind its initial results, posting\nan accuracy of 65.32%, with recall and F1 scores of 68.15% and63.17%, respectively. This\npattern suggests that the angle representation, which lacks phase unwrapping, is less effec-140 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.20: Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 68.75 70.55 67.93\n300 0.7 0.7 |15-20| 0.1 0.0 69.83 72.52 68.68\n600 0.5 0.5 |15-20| 0.1 0.5 72.07 73.64 71.48Angle200 0.5 0.5 |10-20| 0.0 0.0 66.42 68.57 65.26\n300 0.7 0.7 |15-20| 0.1 0.0 63.92 66.33 62.67\n600 0.5 0.5 |15-20| 0.1 0.5 65.32 68.15 63.77Phase200 0.5 0.5 |10-20| 0.0 0.0 68.33 69.75 67.68\n300 0.7 0.7 |15-20| 0.1 0.0 68.58 70.00 67.97\n600 0.5 0.5 |15-20| 0.1 0.5 69.37 71.17 68.66PSD200 0.5 0.5 |10-20| 0.0 0.0 69.25 70.38 68.84\n300 0.7 0.7 |15-20| 0.1 0.0 69.67 71.06 69.12\n600 0.5 0.5 |15-20| 0.1 0.5 72.10 72.82 71.82\ntive for pain assessment tasks in EMG signals. Conversely, the other visual representations\ndemonstrated consistent improvements in each training configuration. The spectrogram-\nPSD achieved the highest accuracy at 72.10% and an F1 score of 71.82%. The waveform\nrepresentation obtained the highest recall at 73.64%. These results are presented in Table\n7.20 for the EMG modality.\nGSR: For the GSR modality, distinct performance variations among the four representa-\ntions are evident. The waveform -based representations significantly outshine the others,\nstarting with an initial accuracy of 87.75% in the 200-epoch configuration, which is over\n14% higher than other metrics. With extended training sessions, a modest improvement is\nnoted across all representations, indicating that the GSR modality might have reached its\nmaximum potential performance. Among the spectrograms, the spectrogram-phase proves\nto be the most informative, culminating in final accuracy, recall, and F1 scores of 76.41%,\n77.23%, and 76.47%, respectively. The waveform representation emerges as the most effec-\ntive, achieving the highest metrics with an accuracy of 88.99%, recall of 89.55%, and an\nF1 score of 88.88%. The distinct performance of these representations can be related to the\ninherent characteristics of the GSR signal. As depicted in Fig. 7.7, GSR typically presented\nas a smooth curve with gradual slopes, indicative of slow and steady changes in skin conduc-\ntivity due to variations in sweat gland activity triggered by stress or arousal. In comparison,\nEMG signals are marked by sharp spikes and erratic fluctuations, reflecting rapid electrical\nactivities from skeletal muscle contractions. On the other hand, ECG signals display distinct7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 141\nTable 7.21: Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 87.75 88.68 87.56\n300 0.7 0.7 |15-20| 0.1 0.0 88.50 89.16 88.34\n600 0.5 0.5 |15-20| 0.1 0.5 88.99 89.55 88.88Angle200 0.5 0.5 |10-20| 0.0 0.0 73.67 75.00 73.26\n300 0.7 0.7 |15-20| 0.1 0.0 73.08 74.60 72.66\n600 0.5 0.5 |15-20| 0.1 0.5 73.24 75.02 72.83Phase200 0.5 0.5 |10-20| 0.0 0.0 75.17 76.13 74.79\n300 0.7 0.7 |15-20| 0.1 0.0 75.92 76.60 75.57\n600 0.5 0.5 |15-20| 0.1 0.5 76.41 77.23 76.47PSD200 0.5 0.5 |10-20| 0.0 0.0 72.83 73.91 72.34\n300 0.7 0.7 |15-20| 0.1 0.0 73.08 73.96 72.68\n600 0.5 0.5 |15-20| 0.1 0.5 73.96 74.81 73.50\ncyclical patterns, including the P and T waves and the QRS complex. These observations\nimply that the simpler patterns in GSR are not as well suited for spectral and frequency do-\nmain analyses, which are more effectively captured by spectrograms. However, waveform\nrepresentations excel in capturing critical physiological data from GSR signals, outperform-\ning all other modalities and visual representations due to their ability to effectively represent\nthe essential dynamics of GSR activity. The results for the GSR modality are summarized in\nTable 7.21.\nFusion: Various fusion techniques were tested to evaluate whether combining different rep-\nresentations or modalities could enhance performance. In this research, using inputs from\nthe same sensor type, such as RGB with depth-estimation videos or ECG waveforms with\nECG spectrogram-PSD , was considered an unimodal fusion approach. On the other hand,\ncombining inputs from different sensor types, like GSR with EMG, was treated as a multi-\nmodal fusion. Three primary methods of fusion were explored: feature fusion and decision\nfusion. Feature fusion includes strategies such as addition, where embeddings from various\ninputs are summed before progressing to the following processing stage, and concatenation,\nwhich aligns them along the y-axis . Decision fusion, meanwhile, involves processing each\nembedding through the Embedding-Mixer , which then aggregates the predictions from each\ninput to generate a final decision. All related experiments were conducted under the previ-\nously detailed 600-epoch training configuration, with results compiled in Table 7.22.\nIn video modality fusion, we assessed combinations of RGB with thermal, RGB with142 CHAPTER 7. GENERAL-PURPOSE MODELS\ndepth, and thermal with depth, plus a three-input amalgamation of RGB, thermal, and depth.\nThe RGB and thermal blend underperformed compared to RGB alone, with the best perfor-\nmance ( 75.66% accuracy) achieved through decision fusion. The RGB and depth combina-\ntion similarly yielded optimal results through decision fusion, achieving 75.53% accuracy\nbut falling short of RGB-only performance. Notably, merging thermal and depth videos im-\nproved upon using depth alone, particularly via decision fusion, which attained a 73.02%\naccuracy rate. The combination of RGB, thermal, and depth inputs was the sole group that\noutperformed the standalone RGB setup, with decision fusion delivering the highest metrics:\n76.55% accuracy, 77.91% recall, and 76.11% F1 score, indicating marginal improvements\nacross all measures. Decision fusion consistently outperformed the addition method in all\nvideo-based experiments.\nFor biosignals, experiments focused on ECG and EMG using the waveform and the rep-\nresentations of spectrogram-PSD . No fusion experiments were conducted for GSR due to\nthe waveform\u2019s dominance in performance. For ECG, all fusion methods were less effective\nthan the spectrogram-PSD alone, except for the addition method, which slightly improved\nrecall by 0.21%. EMG results were enhanced by all fusion techniques, with concatenation\nproving to be the most beneficial, leading to increases in accuracy, recall, and F1 score by\n0.74%,0.36%, and 0.64%, respectively.\nPhysiological and behavioral modalities were integrated into our multimodal setup, com-\nbining GSR signals with RGB, synthetic thermal, and estimated depth videos. The GSR\u2019s\nwaveform representation and video features, described in 7.21, were merged into a unified\nvector of dimension 22,080. This vector was then processed through the Video-Encoder into\na smaller space of 40. The resulting combined vector of 160`40\u201c200dimensions was\nformed by concatenating the GSR and video embeddings, represented as:\nMh\u201cGd}Enc\u201c\npVRGB\nD`VThermal\nD`VDepth\nDq\u2030\n, hPRN2, (7.22)\nwhere Gdenotes the GSR embedding and Mthe fused vector with N2equal to 200. This\napproach, visualized in Fig. 7.7 (bottom right), achieved the highest performance in the\nstudy, with accuracy, recall, and F1 scores of 89.08%,89.88%, and 88.87%, respectively.\nThis method slightly surpassed the performance of GSR used independently, especially in\naccuracy and recall.\nAI4Pain\nIn the AI4Pain dataset, experiments were conducted utilizing both unimodal and multimodal\napproaches. The original RGB videos were employed for the behavioral modality, while\nwaveforms from the fNIRS\u2019s HBO2 channels were used for the physiological modality. It\nis important to note that out of the 24available HBO2 channels, 2were excluded due to7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 143\nTable 7.22: Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and F1 score.\nModality Input FusionMetric\nAcc Rec F1\nVideoRGB, ThermalAdd 75.09\n-1.2076.97\n-0.5973.98\n-1.58\nDF 75.66\n-0.6377.23\n-0.3375.08\n-0.48\nRGB, DepthAdd 74.93\n-1.3676.41\n-1.1573.38\n-2.18\nDF 75.53\n-0.7677.18\n-0.3875.00\n-0.56\nThermal, DepthAdd 71.44\n-0.2373.15\n+0.3170.73\n-0.50\nDF 73.02\n+1.3574.46\n+1.6272.59\n+1.33\nRGB, Thermal, DepthAdd 76.26\n-0.0377.70\n+0.1475.78\n+0.22\nDF 76.55\n+0.2677.91\n+0.3576.11\n+0.55\nECG Wave, PSDAdd 75.43\n-0.0677.36\n+0.2174.75\n-0.15\nConcat 74.74\n-0.7576.77\n-0.3874.00\n-0.90\nEMG Wave, PSDAdd 72.79\n+0.6974.15\n+0.5172.28\n+0.46\nConcat 72.84\n+0.7474.00\n+0.3672.46\n+0.64\nVideo, GSRRGB, Thermal, Depth,\nWaveAdd &\nConcat89.08\n+0.0989.88\n+0.3388.87\n-0.01\n\u02da: All experiments follow the augmentation and regularization settings for the 600 epoch con-\nfiguration outlined in the unimodal experiments. + and - indicate an increase or decrease in\nperformance, respectively, compared to the best unimodal input approach. DF: Decision Fusion\nAdd: Addition Concat: Concatenation\nmalfunctions. Table 7.23 presents the corresponding results.\nVideo: Similar to 7.3.2, an embedding of dimension d\u201c160is extracted for every frame\nin the AI4Pain dataset. However, in this instance, the extracted embeddings are aggregated\ninto a unified vector:\nVd\u201crd1`d2`\u00a8\u00a8\u00a8` dms, dPRN3, (7.23)\nwhere mrepresents the number of frames in a video, and N3is the dimensionality of the\nunified embedding, set at 160. After processing the embedding through the Embedding-\nMixer and employing the same 600-epoch training configuration as used in prior experi-\nments, this setup achieved an accuracy of 49.77%, with recall and F1 scores of 50.11% and\n49.77%, respectively. Increasing the DropOut rate to 0.3improved the accuracy and F1\nscores to 51.39% and51.31%. Further elevating the DropOut rate to 0.8enhanced the recall\nto52.74%.144 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.23: Results on the validation set of AI4Pain dataset, multilevel classification task, re-\nported on accuracy, recall and F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Video600 0.5 0.5 |15-20| 0.1 0.5 49.77 50.11 49.77\n600 0.5 0.5 |15-20| 0.1 0.3 51.39 51.50 51.31\n600 0.5 0.5 |15-20| 0.1 0.8 48.38 52.74 46.69fNIRS600 0.5 0.5 |15-20| 0.1 0.5 43.06 42.80 42.07\n600 0.5 0.5 |15-20| 0.1 0.3 44.44 45.55 43.74\n600 0.4 0.4 |15-20| 0.1 0.1 43.06 44.18 42.44Fusion600 0.5 0.5 |15-20| 0.1 0.5 50.00 51.01 48.54\n600 0.1 0.1 |15-20| 0.1 0.8 50.23 50.25 50.24\n600 0.4 0.4 |15-20| 0.1 0.6 51.85 51.87 51.35\nFusion: the Addition method of the modalities applied\nfNIRS: For the fNIRS modality, embeddings were aggregated across the 22HBO2 chan-\nnels to produce a feature representation of Od\u201c160. The 600-epoch training setup initially\nyielded 43.06% accuracy, 42.80% recall, and 42.07% F1 score. By increasing the DropOut\nrate to 0.3, peak performance metrics of 44.44% accuracy, 45.55% recall, and 43.74% F1\nscore were achieved.\nFusion: For the fusion of video and fNIRS data, the following aggregation approach was\nutilized:\nFd\u201cVd`Od, dPRN3, (7.24)\nwhere Fdrepresents the combined feature representation. Starting with the same 600-epoch\ntraining configuration, the initial results were 50.00% accuracy, 51.01% recall, and 48.54%\nF1 score. Increasing the DropOut rate to 0.8slightly improved the accuracy and F1 score\nby0.23% and1.7%, respectively, though recall decreased by 0.75%. The optimal DropOut\nsetting of 0.6achieved peak performances of 51.85% accuracy, 51.57% recall, and 51.35%\nF1 score.\n7.3.3", "A Foundation Model for Automatic Pain Assessment": ". . . . . . . . . . . . 129\n7.3.1", "Comparison with existing methods": ". . . . . . . . . . . . . . . . . . 144\n7.3.4", "Interpretation": "of Results . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research\nDirections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4 Demographic Variables: Their Role and Impact 53\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 544.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4", "Conclusions, Perspectives and Future Work": "157\n8.1", "Summary of Thesis Achievements": ". . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1", "Reporting on Deep Learning in Automatic Pain Assessment": ". . . . 157\n8.1.2", "Insights from Gender and Age Analysis": ". . . . . . . . . . . . . . . 158\n8.1.3", "Pain Assessment with Compact, High-Performance Models": ". . . . 158\n8.1.4", "Synthetic Data for Improved Pain Assessment and Privacy": ". . . . . 158\n8.1.5", "Universal Modeling for Automatic Pain Assessment": ". . . . . . . . 159\n8.1.6", "Explainable Deep Learning": ". . . . . . . . . . . . . . . . . . . . . . 159\n8.2", "Perspectives for Automatic Pain Assessment Methods": ". . . . . . . . . . . . 160\n8.3", "Future Work": "157\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160", "Bibliography": "163", "Appendix": "201", "Acronyms": "209List of Figures\n2.1 The spinothalamic tract (STT) [43]. Pain, temperature, and some touch affer-\nents end in the posterior horn, where second-order fibers cross the midline\nto form the spinothalamic tract, ascending to the thalamus and projecting to\nvarious cortical areas. Along the way, collaterals connect to the reticular for-\nmation. Due to the rostral inclination of fibers in Lissauer\u2019s tract, cordotomy\nmust be performed several segments above the pain level for effective relief. 12\n2.2 Pain classification [48]: (A)Nociceptive pain , which results from detecting\npotentially harmful stimuli and serves a protective function. (B)Inflamma-\ntory pain is linked to tissue damage and immune cell infiltration, increas-\ning pain sensitivity during healing. (C)Pathological pain is a disease state\ncaused by either nervous system damage (neuropathic) or abnormal nervous\nsystem function (dysfunctional). . . . . . . . . . . . . . . . . . . . . . . . 14\n3.1 The number of studies utilizing these specific datasets. Note that various\nstudies used multiple datasets to conduct their experiments. . . . . . . . . . 25\n4.1 The PQRST waveform. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55\n4.2 The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure. . 55\n4.3 The signal preprocessing using the Pan-Tompkins algorithm. . . . . . . . . 57\n4.4 Results for the Gender Scheme . . . . . . . . . . . . . . . . . . . . . . . . . 60\n4.5 Results for the Age Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.6 Results for the Gender-Age Scheme . . . . . . . . . . . . . . . . . . . . . . 64\n4.7 The proposed MTL network: The sizes of the extracted vectors for the net-\nwork are as follows: for the Pain classifier, n\u02c61, where nis the number of\npain estimation tasks ( e.g.,2for binary classification, 5for multi-class clas-\nsification); for the Age classifier, 36\u02c61, where 36represents the possible\nage values of the subjects; for the Gender classifier, 2\u02c61, corresponding to\nthe two possible gender categories ( i.e., males and females). . . . . . . . . 66\n4.8 Results for the proposed Schemes. . . . . . . . . . . . . . . . . . . . . . . 69\n4.9 Comparison of performances utilizing various neural networks approaches. 72\nxix5.1 The application of face alignment illustrates landmarks in 2D (left) and 3D\n(right) space. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2 An overview of our proposed transformer-based framework for automatic\npain assessment. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.3 The impact of the number of input frames on accuracy (left) and on runtime\nin milliseconds (right). Runtime calculated during inference on a NVIDIA\nRTX-3090 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 82\n5.4 Relevance Maps. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.5 Outline of the proposed framework. . . . . . . . . . . . . . . . . . . . . . 86\n5.6 Comparison of mean accuracy and inference period for unimodal and multi-\nmodal strategies across NP versus P 4and MC tasks. The diagram adopts a\ndual-y-axis configuration\u2014accuracy measurements on the left and time met-\nrics on the right\u2014to outline the balance between performance efficacy and\ncomputational load, categorizing the methodologies along the x-axis. . . . . 98\n5.7 Regions highlighted in yellow and red denote areas of significant attention.\n(a) (1strow) Sequence of original frames. (2ndrow) Derived from the\nSpatial-Module after initial stage pretraining. (3rdrow) Derived from the\nSpatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module trained on the BioVid dataset. (b) (1strow) Derived from\ntheTemporal-Module incorporating video embeddings. (2ndrow) Derived\nfrom the Temporal-Module with heart rate embeddings. (3rdrow) Derived\nfrom the Temporal-Module using a combined embedding of video and heart\nrate. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100\n6.1 Illustration of the procedure for creating thermal images, featuring the archi-\ntecture of the Generator G(Encoder, mid-stage ResNet, Decoder), and the\nDiscriminator D. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 106\n6.2 Representation of the proposed framework, illustrating its components and\ntheir main functions: (a)The Vision-MLP module, tasked with extracting\nfeature embeddings from video frames. (b)TheToken-Mixer , an important\nsub-module of Vision-MLP , generates the wave representation for the tokens.\n(c)The Channel-Mixer , a crucial sub-module within Vision-MLP .(d)The\nMLP, a core component of the Channel-Mixer .(e)The fusion procedure\nthat combines RGB and synthetic thermal embeddings, succeeded by the\nTransformer module, which conducts the final pain assessment. . . . . . . . 107\n6.3 Gradual blurring of RGB and synthetic thermal facial images: a series dis-\nplaying varying levels of Gaussian blur applied, with kernel sizes gradually\nincreased from k\u201c0(no blur) to k\u201c191(extensively blurred). . . . . . . 1136.4 Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain)\nclasses in RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191\n(heavily blurred). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 114\n7.1 PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer hav-\ning varying depths, showcasing how token resolution decreases at each stage;\n(b)Composition of the Token-Mixer module, featuring elements like depth-\nwise convolution (DWConv) and batch normalization; (c)Architecture of the\nFeed-Forward Network (FFN) within the Token-Mixer ;(d)The Cascaded\nAttention mechanism implemented across multiple heads, illustrating how\noutputs from preceding heads are incorporated to refine the self-attention\nprocess, culminating in the final output projection; (e)Configuration of the\nproposed multimodal pipeline, employing videos and fNIRS. The embed-\ndings from PainViT\u20131 are represented as waveform diagrams, which are\nmerged into a single diagram that illustrates both modalities before entering\nPainViT\u20132 for final pain evaluation. . . . . . . . . . . . . . . . . . . . . . . 151\n7.2 Waveform illustrations for various data types: (a)original fNIRS signal,\n(b)video embedding derived from PainViT\u20131 , and (c)fNIRS embedding\nobtained from PainViT\u20131 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 152\n7.3 Attention maps from the PainViT\u20132 . . . . . . . . . . . . . . . . . . . . . . 152\n7.4 Overview of primary models and their components outlined in this research:\n(a)PainFormer is structured hierarchically into four stages, incorporating\nSpectral andSelf-Attention Layers to extract embeddings from the inputs;\n(b)The Spectral Layer , a key element of PainFormer , uses FFT to ana-\nlyze frequency-specific data along with a learnable filter Kto highlight\ncritical frequencies; (c)The Self-Attention Layer , crucial for PainFormer ,\nenables parallel processing of features and their interconnections; (d)The\nEmbedding-Mixer , employing both cross and self-attention mechanisms, func-\ntions as the component for the final classification of embeddings in pain as-\nsessment; (e)TheVideo-Encoder , designed for compact and efficient encod-\ning, compresses video data into a reduced dimensional form; (f)TheMLP-1\nis part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)TheMLP-3 configuration is integrated into the Embedding-Mixer\nandVideo-Encoder . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153\n7.5 Examples of different vision modalities in frame samples: (a)RGB frame,\n(b)synthetic thermal frame, and (c)depth estimation frame. . . . . . . . . 153\n7.6 Examples of different visual representations for biosignals: (a)waveform ,\n(b)spectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD . . 1547.7 An overview of the presented framework. PainFormer , the foundational\nmodel, excels in deriving high-quality embeddings from a diverse array of\nbehavioral and physiological modalities. The evaluation of RGB, thermal,\nand depth videos, alongside various representations of ECG, EMG, GSR,\nand fNIRS such as waveforms and spectrograms, underscores the rich infor-\nmation captured within these embeddings. Leveraging the embeddings from\nPainFormer facilitates the creation of various and diverse unimodal and mul-\ntimodal pipelines designed for the pain assessment task. Each pipeline can\nbe customized to suit the specific modalities involved, dataset characteristics,\nand the demands of the intended application or clinical setting. Our assess-\nments included the development and implementation of several pipelines\nin both unimodal and multimodal contexts, achieving leading-edge results\nacross various modalities and data representations. . . . . . . . . . . . . . 154\n7.8 Attention maps from the PainFormer :(a)(1strow) frames from RGB, ther-\nmal, and depth video modalities; (a)(2ndrow) corresponding attention maps;\n(b)(1strow) attention maps for ECG and EMG; (b)(2ndrow) attention maps\nfor EDA and fNIRS modalities. . . . . . . . . . . . . . . . . . . . . . . . . 155\n1 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 205\n2 Attention maps generated by the Spatial-Module .Yellow and red colors sig-\nnify intense focus on specific areas. (1strow) Sequence of original frames.\n(2ndrow) Derived from the Spatial-Module after initial stage pretraining.\n(3rdrow) Derived from the Spatial-Module post second stage pretraining.\n(4throw) Derived from the Spatial-Module following training on BioVid (re-\nfer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206\n3 Additional attention maps from the PainViT\u20132 (refer to Section 7.2). . . . . 207List of Tables\n3.1 Most commonly utilized pain databases. . . . . . . . . . . . . . . . . . . . 24\n3.2 Vision-based studies with static analysis. . . . . . . . . . . . . . . . . . . . 32\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 33\n3.2 Vision-based studies with static analysis (continued). . . . . . . . . . . . . 34\n3.3 Vision-based studies with temporal utilization. . . . . . . . . . . . . . . . . 39\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 40\n3.3 Vision-based studies with temporal utilization (continued). . . . . . . . . . 41\n3.4 Touch sensor-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.5 Audio-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43\n3.6 Multimodal-based studies. . . . . . . . . . . . . . . . . . . . . . . . . . . 46\n3.7 Interpretation approaches. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n4.1 Results for the Basic Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2 Results for the Gender Scheme (1). . . . . . . . . . . . . . . . . . . . . . . 60\n4.3 Results for the Age Scheme (1). . . . . . . . . . . . . . . . . . . . . . . . . 62\n4.4 Results for the Gender-Age Scheme (Males) (1). . . . . . . . . . . . . . . . 62\n4.5 Results for the Gender-Age Scheme (Females) (1). . . . . . . . . . . . . . . 63\n4.6 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (1). 63\n4.7 Hyper-parameters used in our approach. . . . . . . . . . . . . . . . . . . . 65\n4.8 Results for the Basic Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . 68\n4.9 Results for the Gender Scheme (2). . . . . . . . . . . . . . . . . . . . . . . 68\n4.10 Results for the Age Scheme (2). . . . . . . . . . . . . . . . . . . . . . . . . 68\n4.11 Results for the Gender-Age Scheme (2). . . . . . . . . . . . . . . . . . . . 68\n4.12 Comparison of results adopting the feature augmentation approach. . . . . . 70\n4.13 Comparison of results adopting the MT-NN approach. . . . . . . . . . . . . 71\n4.14 Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2). 72\n5.1 Training details for the automatic pain assessment. . . . . . . . . . . . . . 79\n5.2 Results on the pain estimation tasks. . . . . . . . . . . . . . . . . . . . . . 81\n5.3 Results for the pain estimation tasks using various numbers of input frames. 82\n5.4 Comparison of studies utilizing BioVid , RGB videos, and LOSO validation. 84\n5.5 Datasets utilized for the pre-training process of the framework. . . . . . . . 91\nxxiii5.6 Training details for the automatic pain assessment. . . . . . . . . . . . . . 92\n5.7 Results utilizing the video modality. . . . . . . . . . . . . . . . . . . . . . 93\n5.8 Results utilizing the heart rate modality. . . . . . . . . . . . . . . . . . . . 95\n5.9 Results utilizing the video &the heart rate modality. . . . . . . . . . . . . . 95\n5.10 Comparison of studies utilizing BioVid &LOSO validation, reported on ac-\ncuracy %. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 97\n5.11 Module parameters and computational cost in FLOPS for the proposed frame-\nwork. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98\n6.1 Datasets utilized for the pretraining process of the framework. . . . . . . . 110\n6.2 Training specifications, and number of parameters and FLOPS for each mod-\nule. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.3 Results utilizing the RGB video. . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Results utilizing the synthetic thermal video. . . . . . . . . . . . . . . . . . 112\n6.5 Results utilizing the RGB &the synthetic thermal video. . . . . . . . . . . . 113\n6.6 Results utilizing the fusion of RGB &synthetic thermal video. . . . . . . . 115\n6.7 Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.116\n6.8 Comparison with the MIntPAIN dataset. . . . . . . . . . . . . . . . . . . . 116\n7.1 Number of parameters and FLOPS for the components of the proposed Twins-\nPainViT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 121\n7.2 Datasets utilized for the pretraining process of the framework. . . . . . . . 123\n7.3 Training details for the automatic pain assessment. . . . . . . . . . . . . . 124\n7.4 Results utilizing the video modality & Addition method. . . . . . . . . . . . 125\n7.5 Results utilizing the video modality & Concatenation method. . . . . . . . . 125\n7.6 Results utilizing the HbR & Addition method. . . . . . . . . . . . . . . . . 126\n7.7 Results utilizing the HbR & Concatenation method. . . . . . . . . . . . . . 126\n7.8 Results utilizing the HbO & Addition method. . . . . . . . . . . . . . . . . 126\n7.9 Results utilizing the HbO & Concatenation method. . . . . . . . . . . . . . 127\n7.10 Results utilizing the HbR, HbO & Addition method. . . . . . . . . . . . . . 127\n7.11 Results utilizing the videos, HbO & Addition method. . . . . . . . . . . . . 127\n7.12 Results utilizing the videos, HbO & Single Diagram method. . . . . . . . . 128\n7.13 Comparison with the validation baseline provided by the AI4PAIN challenge\norganizers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.14 Number of parameters and FLOPS for the modules of the proposed framework.130\n7.15 Details of the PainFormer\u2019s architecture. . . . . . . . . . . . . . . . . . . . 132\n7.16 Datasets utilized for the multitask learning-based pretraining process of the\nframework. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1347.17 Training details of the proposed framework. . . . . . . . . . . . . . . . . . 135\n7.18 Results utilizing the video modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 138\n7.19 Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 139\n7.20 Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140\n7.21 Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy,\nrecall and F1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 141\n7.22 Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and\nF1 score. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143\n7.23 Results on the validation set of AI4Pain dataset, multilevel classification task,\nreported on accuracy, recall and F1 score. . . . . . . . . . . . . . . . . . . 144\n7.24 Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 146\n7.25 Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4\ntask and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . . 147\n7.26 Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs.\nP4task and LOSO validation. . . . . . . . . . . . . . . . . . . . . . . . . . 148\n7.27 Comparison of studies on the testing set of AI4Pain dataset. . . . . . . . . . 148\n1 Results utilizing the video modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 201\n2 Results utilizing the heart rate modality reported on precision, recall, and F1\nscore (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . . . . . . . . 202\n3 Results utilizing the video &the heart rate modality reported on precision,\nrecall and F1 score (refer to Section 5.3). . . . . . . . . . . . . . . . . . . . 202\n4 Results utilizing the RGB video modality, reported on recall and F1 score\n(refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 203\n5 Results utilizing the synthetic thermal video modality, reported on recall and\nF1 score (refer to Section 6.2). . . . . . . . . . . . . . . . . . . . . . . . . 203\n6 Results utilizing the fusion of RGB &synthetic thermal video modality, re-\nported on recall and F1 score (refer to Section 6.2). . . . . . . . . . . . . . 204\n7 Results of the proposed approaches, reported on macro-averaged precision,\nrecall and F1 score (refer to Section 7.2). . . . . . . . . . . . . . . . . . . . 204Chapter 1\nIntroduction\nContents\n1.1 Context and Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1\n1.2 Scope and Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n1.3 Contributions \u2013 Peer-review Publications . . . . . . . . . . . . . . . . . . . . . 5\n1.4 Thesis Outline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n1.1 Context and Motivation\nPain is a complex and deeply personal experience that is subjective by nature. Traditionally,\nit has been described in terms of its sensory dimension [1]. However, extensive research\nhas highlighted the importance of affective, cognitive and social aspects in shaping this ex-\nperience [2]. Studies have explored physiological, psychological, and socio-environmental\nfactors that contribute to the experience of pain. It is understood as a result of biological evo-\nlution and as influenced by psychological and social factors. As Ridell et al. [3] noted, \u201cPain\nis a synthesis\u2013a sum that is greater than its parts. \u201d The brain\u2019s ability to alter the perception\nof sensory inputs through the interplay of emotion, cognition, and social processes is signifi-\ncant. Although natural systems establish the initial biological framework for pain perception,\nthis structure is highly adaptable, particularly in humans. Throughout a person\u2019s life, both\nbiological developments and personal experiences significantly reshape this framework.\nA key question driving pain research across biological, psychological, and computational\nfields is why this topic of pain is meaningful and important. This question also forms the\nbasis for initiating this thesis, highlighting the broader relevance of studying pain. Williams\nand Kappesser [4] provide a compelling explanation, stating, \u201cWe care because we are wired\nto care: to attend to other people\u2019s expression of pain and to understand its meaning; to feel\ndistress in relation to their distress; and to be motivated to reduce their distress, and ours,\nif we are able to do so. \u201d This highlights the intrinsic human response to empathize and\n12 CHAPTER 1. INTRODUCTION\nalleviate pain, underlining the fundamental importance of this research area. Indeed, from a\nDarwinian perspective, pain serves a crucial role. The manifestation of pain in humans and\nthe reactions it elicits are examined through an evolutionary lens. Pain facilitates recovery by\npromoting responses to harmful stimuli and behaviors that demonstrate the adverse nature\nof painful experiences, common among animals. Specifically, the facial expression of pain,\nwhich communicates discomfort directly to those nearby, is universally recognized across\ndifferent ages, ethnicities, roles, and relationships. Evidence from healed major fractures\n[5, 6] suggests that injured members of hominid groups were not left to fend for themselves\nbut were supported through their recovery, indicating the fundamental importance of pain\nexpression in our evolutionary history.\nPain is a widespread health concern globally, affecting up to 30% of the adult popula-\ntion [7] and between 83% and93% of elderly adults in residential care [8]. The Global Bur-\nden of Disease (GBD) study identifies pain as the primary cause of years lived with disability\n(YLD) [9], with major contributors including chronic back pain, musculoskeletal disorders,\nand neck pain [10]. Pain impacts individuals and poses significant clinical, economic, and\nsocial challenges. In the United States, the economic and healthcare costs related to pain\ndue to reduced work productivity range from $560 to$635 billion annually, surpassing the\ncosts associated with heart disease, cancer, and diabetes combined [11]. In Europe, chronic\npain\u2019s direct healthcare costs and indirect socioeconomic impacts account for 3%to10% of\nthe GDP [12]. In Australia, the average annual cost for individuals among the 15.4%living\nwith chronic pain ranges from AU $22,588to AU $42,979, including non-financial costs [13].\nBeyond direct effects on health, pain contributes to a range of adverse outcomes, such as opi-\noid dependency, drug overuse, addiction, declining social relationships, and psychological\ndisorders [14]. In the last two decades, prescription opioid use has surged in the United\nStates, where overdose deaths have increased more than fourfold from 1999 to2016 [15].\nAdditionally, side effects from these opioids, like lethargy, depression, anxiety, and nausea,\nseverely impact workforce productivity and overall life quality [16].\nAccurate pain assessment is crucial for early diagnosis, disease progression monitoring,\nand treatment effectiveness evaluation, particularly in managing chronic pain [17]. This criti-\ncal role has resulted in pain being recognized as \u201cthe fifth vital sign\u201d in nursing literature [18].\nPain assessment is also fundamental in physiotherapy, where therapists apply external stim-\nuli and need to gauge the patient\u2019s pain levels accurately [19]. Objective evaluation of pain is\nessential to provide appropriate care, especially for vulnerable populations who may not be\nable to communicate their pain effectively, such as infants, young children, individuals with\nmental health issues, and the elderly. Various methods are used for pain assessment, with\nself-reporting\u2013where individuals describe their pain experiences\u2013considered the gold stan-\ndard [20]. Pain evaluation methods in clinical environments include quantifiable measures\nlike the Numeric Pain Rating Scale (NPRS), Visual Analogue Scale (V AS), and quantitative1.1. CONTEXT AND MOTIVATION 3\nsensory testing techniques such as the pressure pain detection threshold (PPDT) [21]. Behav-\nioral indicators are also crucial and include facial expressions ( e.g., grimacing, open mouth,\nlifted eyebrows), vocalizations (like crying, moaning, or screaming), and movements of the\nbody and head [22]. Physiological measures such as electrocardiography (ECG), electromyo-\ngraphy (EMG), galvanic skin responses (GSR), and respiration rates further contribute to\nunderstanding pain\u2019s physiological aspects [17]. Additionally, brain monitoring techniques\nlike near-infrared spectroscopy (fNIRS) have effectively detected changes in hemodynamic\nactivity associated with pain stimuli [23].\nCaregivers and family members often determine the presence or absence of pain in pa-\ntients by observing their behavioral or physiological responses [17]. However, accurately\nassessing pain poses a significant challenge for clinicians [24], especially with nonverbal\npatients such as the elderly, who may have reduced expressive abilities or may be reluctant\nto communicate pain [25]. Extensive research indicates that pain manifestations vary signif-\nicantly across different genders and ages, adding to the complexity of its assessment [26].\nFurther complicating the assessment process are the heightened workload and fatigue ex-\nperienced by nursing staff due to the demands of patient monitoring [27]. Technological\nsolutions are necessary for continuous patient monitoring. Nevertheless, concerns remain\nabout the objectivity and accuracy of these observations, as inadequately trained or biased\nobservers may struggle to assess pain [28] accurately. Even among trained observers, in-\nterpretations of behaviors can vary [22], and social and interpersonal dynamics can signif-\nicantly affect the pain assessment process, influencing both the evaluators\u2019 judgments and\nthe patients\u2019 expressions of pain [29]. Additionally, the presence of an observer can lead pa-\ntients to modify their behavior [30], and expressing pain through scales and measurements\ncan be challenging [31]. While self-reporting is used because pain is inherently subjective,\nrelying solely on a one-dimensional pain score fails to capture this complex phenomenon,\noften leading to inadequate pain management [32].\nGiven the challenges described above, scientific computing (SC) researchers have fo-\ncused on developing models and algorithms to enhance automatic pain recognition systems\nover the last two decades. Their goal is to accurately determine the presence and intensity\nof pain by analyzing physiological and behavioral indicators. Adopting deep learning and\nartificial intelligence (AI) techniques has expanded these automatic methods, designed to\ninterpret the complex and varied nature of pain [17]. Numerous studies have underscored\nthe effectiveness of automated systems that utilize behavioral or physiological modalities\nfor pain assessment [33]. Sario et al. [34] have shown the capability of these systems to\naccurately recognize pain through facial expressions, proving their utility in clinical envi-\nronments. Multimodal sensing has shown particular promise, offering enhanced accuracy\nin pain detection systems [22]. Furthermore, including temporal aspects in these modalities\nhas proven to significantly improve the accuracy of pain assessments [17].4 CHAPTER 1. INTRODUCTION\n1.2 Scope and Challenges\nAlthough considerable research has been conducted on automatic pain assessment, studies\nhave yet to explore factors like demographics and social aspects from a computational angle.\nFurthermore, despite the existence of deep learning-based methods, the approaches we ob-\nserve are often outdated and repeatedly recycled. For these reasons, we aimed to address two\nissues by (i)attempting to evaluate the social or demographic context, which significantly\nimpacts and influences pain sensation and perception, and (ii)introducing innovative deep\nlearning methods inspired by the latest developments in AI and generative AI literature. We\nbelieve these approaches can forge new paths in pain research, enhance the accuracy of rec-\nognizing this complex phenomenon, and, ultimately, be adopted in real-world scenarios to\nassist those in need. Additionally, (iii)recognizing the skepticism towards new technologies\namong clinicians and the general public, especially regarding the limited understanding of\nhow deep learning models function, we have devoted a portion of our research to interpret-\ning these models to offer some level of explanation and help the adoption process of them in\nclinical settings.\nNevertheless, this thesis initially faced challenges related to our objectives and goals as\nthe research progressed. The availability of pain datasets (to be discussed in the next chapter)\nis limited. Only a few datasets are available, and crucially, they are limited in size. This re-\nstriction poses a significant challenge for developing deep learning models, which typically\nrequire a large volume of data. In automatic pain assessment, researchers who develop deep\nlearning methods typically confront a decision: either train their models from scratch, which\ncan introduce performance limitations, or employ pre-trained models. These pre-trained\nmodels are generally trained on broadly available image datasets that include a variety of\nsubjects like animals and objects, or they rely on older architectures that were trained explic-\nitly on facial datasets. In this thesis, we addressed these issues by independently pre-training\nour deep-learning models using diverse datasets related explicitly to human facial images\nand biosignals. This strategy allowed us to design specific architectures to meet our unique\nneeds for each scenario, free from the constraints of relying on models developed and trained\nby others. Furthermore, we explored and evaluated several pre-training techniques to assess\ntheir effectiveness in pain assessment applications.\nRegarding, our objective to explore methods that utilize various modalities individually\nand in combination in a multimodal manner further constrains our dataset options. More-\nover, as previously outlined, our interest in the sociodemographic aspects of pain necessitates\ndatasets that include this information type, intensifying our challenges. For these reasons,\nthis thesis focuses specifically on examining the impact of age and gender on pain. In addi-\ntion, led us to utilize two pain datasets that most closely match the characteristics necessary\nfor our research, particularly in terms of demographic elements and multimodality.1.3. CONTRIBUTIONS \u2013 PEER-REVIEW PUBLICATIONS 5\n1.3 Contributions \u2013 Peer-review Publications\nThis section outlines the publications and projects produced during the Ph.D. research on\nautomatic pain assessment, where I was the first author.\n1.Automatic assessment of pain based on deep learning methods: A systematic re-\nview [17]\nThis systematic literature review (SLR) was conducted at the start of this Ph.D. re-\nsearch. This paper aims to explore the surge in recent years of deep learning algorithms\nadopted by researchers to encode the multidimensional nature of pain into meaning-\nful features. Specifically, this systematic review examines the models, methods, and\ndata types used to establish the foundation for deep learning-based automatic pain\nassessment systems. It identified relevant original studies from digital libraries such\nasScopus ,IEEE Xplore , and ACM Digital Library , following defined inclusion and\nexclusion criteria for studies published until December 2021 . The findings highlight\nthe critical role of multimodal approaches in automatic pain estimation, particularly\nin clinical environments, and emphasize the substantial gains observed with the inclu-\nsion of temporal exploitation of modalities. The review also recommends selecting\nhigh-performing deep learning architectures and methods, encouraging the adoption\nof robust evaluation protocols and interpretability techniques to deliver reliable and\nunderstandable outcomes. Additionally, it underscores the current limitations of exist-\ning pain databases in adequately supporting the development, validation, and practical\napplication of deep learning models as decision-support tools in real-world settings.\nFurthermore, we believe this paper is valuable not only for this Ph.D. project but also\nfor other practitioners and researchers in the field.\n2.Automatic Pain Intensity Estimation based on Electrocardiogram and Demographic\nFactors [35]\nThis study investigated the relationship between gender, age, and pain sensation and\ntheir effects on the automatic pain assessment process. By analyzing physiological\nsignals, particularly electrocardiography (ECG), we estimated pain intensity and ex-\namined the influence of these demographic factors. Utilizing the Pan-Tompkins algo-\nrithm for feature extraction and applying well-established classification methods, we\nexplored the correlation between gender, age, and pain manifestation.\n3.Multi-task Neural Networks for Pain Intensity Estimation Using Electrocardiogram\nand Demographic Factors [36]\nInspired by the previous study, this research further explored the influence of gender\nand age on pain perception. In this work, we analyze electrocardiography signals\nto uncover variations in pain perception across different demographic groups. We6 CHAPTER 1. INTRODUCTION\nleveraged these insights by developing a novel multi-task neural network for automatic\npain estimation, incorporating age and gender data for each individual. The study\ndemonstrated the advantages of this approach compared to other existing methods.\n4.A Full Transformer-based Framework for Automatic Pain Estimation using Videos\n[37]\nThis study introduced an innovative full transformer-based framework featuring a Trans-\nformer in Transformer (TNT) model combined with cross-attention and self-attention\nblocks. We achieved state-of-the-art performance using video data from the BioVid\ndatabase, demonstrating the model\u2019s effectiveness, efficiency, and strong generaliza-\ntion across primary pain estimation tasks.\n5.Multimodal automatic assessment of acute pain through facial videos and heart rate\nsignals utilizing transformer-based architectures [38]\nThis study presented a multimodal automatic acute pain assessment framework, inte-\ngrating video and heart rate signals. The framework consists of four key modules:\ntheSpatial Module , which extracts embeddings from videos; the Heart Rate Encoder ,\nwhich maps heart rate signals into a higher-dimensional space; the AugmNet , which\ngenerates learning-based augmentations in the latent space; and the Temporal Mod-\nule, which leverages the video and heart rate embeddings for the final assessment.\nThe Spatial Module undergoes a two-stage pre-training process: first, it learns uni-\nversal facial features through face recognition, followed by emotion recognition in a\nmultitask learning approach, enabling high-quality embeddings for pain assessment.\nExperiments with facial videos and heart rate data extracted from electrocardiograms\nin the BioVid database, alongside direct comparisons to 29studies, demonstrate state-\nof-the-art performance in unimodal and multimodal settings while maintaining high\nefficiency. In the multimodal setting, the framework achieved 82.74% accuracy for bi-\nnary pain classification and 39.77% for multi-level pain classification, using only 9.62\nmillion parameters across the entire framework.\n6.Synthetic Thermal and RGB Videos for Automatic Pain Assessment utilizing a Vision-\nMLP Architecture [39]\nThis paper introduced synthetic thermal videos generated by Generative Adversarial\nNetworks , which are integrated into the pain recognition process to assess their effec-\ntiveness. The framework employs a Vision-MLP andTransformer -based module, lever-\naging RBG and synthetic thermal videos in unimodal and multimodal settings. Exper-\niments conducted using facial videos from the BioVid database highlighted synthetic\nthermal videos\u2019 effectiveness and showcased their potential benefits in pain recognition\ntasks.1.4. THESIS OUTLINE 7\n7.Twins-PainViT: Towards a Modality-Agnostic Vision Transformer Framework for\nMultimodal Automatic Pain Assessment using Facial Videos and fNIRS [40]\nThis study was submitted to the First Multimodal Sensing Grand Challenge for Next-\nGen Pain Assessment (AI4PAIN) . The proposed multimodal framework leverages fa-\ncial videos and fNIRS, offering a modality-agnostic approach that eliminates the need\nfor domain-specific models. Utilizing a dual ViT configuration and waveform repre-\nsentations for both fNIRS and the extracted embeddings from the two modalities, the\nmethod demonstrates its effectiveness, achieving an accuracy of 46.76% in the multi-\nlevel pain assessment task.\n8.PainFormer: a Vision Foundation Model for Automatic Pain Assessment [41]1\nThis study introduces PainFormer , a vision foundation model built on multi-task learn-\ning principles and trained across 14distinct tasks and datasets comprising 10.9million\nsamples. As an embedding extractor for various input modalities, PainFormer provides\nfeature representations to the Embedding-Mixer , a transformer-based module respon-\nsible for conducting the final pain assessment. Extensive experimentation using both\nbehavioral modalities\u2013including RGB, synthetic thermal, and estimated depth videos\u2013\nand physiological modalities like ECG, EMG, GSR, and fNIRS revealed PainFormer \u2019s\nability to extract high-quality embeddings from diverse inputs. Tested on the BioVid\nandAI4Pain datasets and compared to more than 60existing methods, the framework\ndemonstrated state-of-the-art performance in unimodal and multimodal settings, posi-\ntioning itself as a step toward developing general-purpose models for automated pain\nevaluation.\n1.4 Thesis Outline\nThe dissertation is organized into the following chapters:\nChapter 2 introduces the foundational concepts of pain from biological, psychological, and\nclinical perspectives.\nChapter 3 reviews existing literature on automatic pain assessment using deep learning\nmethods and details the pain datasets used.\nChapter 4 outlines and proposes methods for evaluating demographic variables, their uti-\nlization, and their integration into an automatic pain assessment framework.\nChapter 5 discusses methods that utilize video and wearable device data, exploring the\ntrade-offs between efficiency and accuracy. It also proposes efficient, fast, effective models\nsuitable for real-world applications.\nChapter 6 explores synthetic data in pain assessment and introduces synthetic thermal im-\n1Under Review8 CHAPTER 1. INTRODUCTION\nagery techniques to enhance performance in automatic pain recognition.\nChapter 7 discusses general-purpose models, introduces a modality-agnostic framework,\nand presents the first foundation model used in automatic pain assessment.\nChapter 8 concludes the thesis with a final discussion, offering perspectives and ideas for\nfuture research in automatic pain assessment.Chapter 2\nClinical Pain Assessment\nContents\n2.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\n2.2 Biology of Pain . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10\n2.3 Classification and Characteristics of Pain . . . . . . . . . . . . . . . . . . . . . 11\n2.4 Pain Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.1 Behavioral Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\n2.4.2 Physiological Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.4.3 Biochemical Indicators . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\n2.5 Sociodemographic and Psychological Variables . . . . . . . . . . . . . . . . . . 15\n2.5.1 Sex and Gender . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.2 Age . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16\n2.5.3 Psychological Factors . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17\n2.5.4 Race and Culture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.5.5 Observer\u2019s Impact on Pain . . . . . . . . . . . . . . . . . . . . . . . . . . 18\n2.6 Impact of Inadequate Pain Management . . . . . . . . . . . . . . . . . . . . . 19\n2.7 Pain Measurement Scales and Metrics . . . . . . . . . . . . . . . . . . . . . . . 20\n2.1 Chapter Overview\nThis chapter provides an anatomical and physiological overview of pain, focusing on the\nmechanisms responsible for generating, transmitting, processing, and interpreting pain sig-\nnals. It examines the various types of pain and explores the actions and expressions typically\nassociated with pain. Additionally, it reviews current pain assessment methods used in clin-\nical settings for adults, children, and newborns. The chapter also discusses developing and\nvalidating existing clinical pain assessment tools. This foundational knowledge is essen-\ntial for understanding the development and validation of computer-assisted pain assessment\n910 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nmethods discussed in later chapters. Finally, it highlights the challenges faced in clinical\npain assessment and underscores the need for automated pain assessment techniques.\n2.2 Biology of Pain\nPain, according to the International Association for the Study of Pain (IASP) [42], is \u201can\nunpleasant sensory and emotional experience associated with actual or potential tissue dam-\nage, or described in terms of such damage\u201d. Biologically, pain is an undesirable sensation\noriginating from the peripheral nervous system. Its fundamental function is to engage sen-\nsory neurons, notifying the organism of potential harm and playing a vital role in recognizing\nand responding to threats [43].\nThe transmission of a noxious stimulus from the periphery to the central nervous sys-\ntem involves a complex pathway through the spinal cord, resulting in the physical sensation\nof pain and a corresponding emotional response and memory. This process culminates in\nthe perception of pain. The initial stage of pain processing occurs when a stimulus at noci-\nceptive sensory fibers in the periphery is converted into an action potential. A nerve signal\nis generated if the stimulus is strong enough to surpass the action potential threshold [44].\nThis signal travels along the primary afferent fiber toward the central nervous system. As the\nstimulus intensity grows, more nerve fibers and areas of the nervous system are engaged [44].\nDue to their branching nature, primary afferent fibers typically relay information from sev-\neral pain receptors. These fibers and their receptors comprise a sensory unit, which gathers\ndata from a specific receptive field [44]. When receptive fields are larger and overlap with\nnearby fields, it becomes more challenging for the sensory system to locate the source of pain\naccurately. The primary afferent neuron is a pseudounipolar neuron that splits into a periph-\neral and central axon. The cell bodies of these neurons are located in the peripheral nervous\nsystem, within the posterior or cranial root ganglia. The peripheral axon extends to the skin,\nmuscles, tendons, or joints, branching into terminal fibers that connect with somatosensory\nreceptors. In contrast, the central axon leads to the central nervous system [45].\nPeripheral somatosensory fibers are categorized into three main groups. The first group\nincludes A\u00b4\u03b1,A\u00b4\u03b2,A\u00b4\u03b3fibers, large, myelinated fibers that rapidly conduct sig-\nnals [46]. These fibers involve touch and proprioception but are not associated with pain\nperception. The second group consists of A\u00b4\u03b4fibers, which are smaller and slower con-\nducting. Certain A\u00b4\u03b4fibers play a key role in pain sensation, with some responding only\nto intense mechanical stimuli and others reacting to noxious and non-noxious heat. The\nthird group comprises Cfibers, which are small, unmyelinated, and conduct signals very\nslowly. Most Cfibers are polymodal for pain perception, responding to various noxious\nmechanical, thermal, and chemical stimuli. These fibers are mainly linked to burning pain\nsensations [43]. The sensation of pain, known as nociception, is primarily facilitated by2.3. CLASSIFICATION AND CHARACTERISTICS OF PAIN 11\nvarious intracellular and extracellular molecular messengers. When activated by a specific\nstimulus, nociceptors relay information through glutamate, an excitatory neurotransmitter.\nAdditionally, inflammatory mediators are released at the injury site, further stimulating no-\nciceptor activation by releasing chemicals such as neurotransmitters ( e.g., serotonin), lipids\n(e.g., prostaglandins), peptides ( e.g., bradykinin), and neurotrophins ( e.g., nerve growth fac-\ntor) [46]. There are ascending tracts responsible for transmitting sensory information from\nthe periphery to the central nervous system. Fibers that convey two-point discrimination, tac-\ntile information, pressure, vibration, and proprioception ascend via the dorsal column of the\nspinal cord, forming the gracile and cuneate fasciculi. Fibers transmitting pain, temperature,\nand crude touch from somatic and visceral structures travel through the lateral spinothalamic\ntract. The anterior spinothalamic tract also transmits pain, temperature, and touch informa-\ntion to the brainstem and diencephalon (Figure 2.1) [47].\n2.3 Classification and Characteristics of Pain\nAccording to neurobiologist Clifford Woolf [48], pain can be classified into three categories\nbased on its function and characteristics: nociceptive ,inflammatory , and pathological pain.\nThese classes and their respective functions are illustrated in Figure 2.2.\nNociceptive pain (refer to Figure. 2.2(A)), arising from tissue damage, is a high-threshold\npain that activates only in response to intense stimuli [49], serving as a vital warning signal\nto the body. The neurobiological system responsible for nociceptive pain evolved from the\nability of even the most primitive nervous systems to detect impending or actual tissue dam-\nage caused by external stimuli. Its protective role requires immediate attention and action,\nachieved through the withdrawal reflex it initiates, the unpleasant sensation it produces, and\nthe emotional distress it triggers. Nociceptive pain demands avoidance in the present mo-\nment, and when activated, it overrides most other neural processes [48].\nInflammatory pain (refer to Figure. 2.2(B)) is also protective and adaptive, increasing\nsensory sensitivity following tissue damage to aid healing by discouraging movement and\ncontact with the injured area. This heightened sensitivity, or tenderness, helps prevent further\nharm and supports recovery, as seen after surgical wounds or inflamed joints where normally\nnon-painful stimuli now cause pain. It is triggered by immune system activation in response\nto tissue injury or infection. Despite its adaptive role, this pain often needs to be alleviated in\npatients with persistent inflammation, such as in rheumatoid arthritis or severe injuries [48].\nPathological pain (Figure. 2.2(C)) is maladaptive, arising from abnormal nervous sys-\ntem functioning and not serving a protective role. Unlike nociceptive and inflammatory pain,\npathological pain is a disease state of the nervous system itself. It may occur following\nnerve damage (neuropathic pain) or in conditions without apparent damage or inflammation\n(dysfunction l pain). Examples of dysfunctional pain include fibromyalgia, irritable bowel12 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFIGURE\n 2: Spinothalamic tract.\nPain, temperature, and some touch and pressure afferents end in the posterior horn. Second- or\nhigher-order fibers cross the midline, form the spinothalamic tract, and ascend to the ventral\nposterolateral (VPL) nucleus of the thalamus (and also to other thalamic nuclei not shown).\nThalamic cells then project to the somatosensory cortex of the postcentral gyrus, to the insula,\nand to other cortical areas (also not shown). Along their course through the brainstem,\nspinothalamic fibers give off many collaterals to the reticular formation (RF). The inset to the left\nshows the lamination of fibers in the posterior columns and the spinothalamic tract in a leg-\nlower trunk-upper trunk-arm sequence. The inset to the right shows the longitudinal formation\nof the spinothalamic tract. Primary afferents ascend several segments in Lissauer\u02bcs tract before\nall their branches terminate; fibers crossing to join the spinothalamic tract do so with a rostral\ninclination. As a result, a cordotomy incision at any given level would spare most of the\ninformation entering the contralateral side of the spinal cord at that level, and to be effective,\nthe incision must be made several segments rostral to the highest dermatomal level of pain.\n2017 Khalid et al. Cureus 9(10): e1754. DOI 10.7759/cureus.1754\n5\n of \n14\nFigure 2.1: The spinothalamic tract (STT) [43]. Pain, temperature, and some touch afferents\nend in the posterior horn, where second-order fibers cross the midline to form the\nspinothalamic tract, ascending to the thalamus and projecting to various cortical\nareas. Along the way, collaterals connect to the reticular formation. Due to the ros-\ntral inclination of fibers in Lissauer\u2019s tract, cordotomy must be performed several\nsegments above the pain level for effective relief.\nsyndrome, tension headaches, and temporomandibular joint disease, where significant pain\nexists without an apparent noxious stimulus or peripheral pathology. Pathological pain, a\nlow-threshold pain primarily driven by amplified sensory signals in the central nervous sys-\ntem, is the clinical pain syndrome with the greatest unmet need. To analogize, while nocicep-\ntive pain acts as a fire alarm for intense heat, and inflammatory pain reacts to warm tempera-\ntures, pathological pain is a false alarm triggered by a system malfunction. Thus, treatment\nmust specifically target the underlying mechanisms causing each type of pain [48].\nPain from a time-duration perspective can be categorized by duration into acute and2.4. PAIN INDICATORS 13\nchronic , with chronic pain persisting or recurring for more than three months [50]. Acute\npain is typically related to identifiable physiological damage from injury, surgery, illness,\ntrauma, or medical procedures and generally subsides once the underlying cause is resolved.\nHowever, if untreated, it may develop into chronic pain. Acute pain is further classified\nintoprocedural pain, caused by medical interventions such as muscular injections [51], and\npostoperative pain, which occurs after surgery and is a significant concern for both patients\nand healthcare providers. Effective management is crucial to aid recovery and prevent the\ntransition to chronic pain [52]. Chronic pain manifests in various forms, including chronic-\nrecurrent pain, like migraine headaches, and chronic-continuous pain, such as persistent low\nback pain [53].\n2.4 Pain Indicators\nPain can manifest in numerous ways and is often shaped by individual characteristics and\nenvironmental influences. Various human expressions, actions, and bodily responses have\nbeen linked to pain, serving both communicative and coping purposes. These pain indicators\nare generally categorized into three primary groups: (i)behavioral, (ii)physiological, and\n(iii)biochemical. While these indicators are universally present, certain expressions are more\nprominent in specific groups. For instance, crying is a common pain response across all age\ngroups but is more frequently observed in younger infants. This may be due to contextual\nfactors\u2014such as culture, social status, age, and ego\u2014influencing how pain is expressed\nover time. Adults, for example, may suppress crying in favor of other vocalizations, such as\ngroans and moans, as crying could be perceived as inappropriate in certain contexts. These\nmediating factors are often considered when interpreting pain indicators. The following\nsections will delve into each of these three categories [51].\n2.4.1 Behavioral Indicators\nBehavioral indicators such as facial expressions ( e.g., grimacing, open mouth, raised eye-\nbrows), vocalizations ( e.g., crying, moaning, screaming), and various bodily movements\n(e.g., changes in posture, signs of tension) are vital markers used in assessing pain [22].\nFacial expressions and limb movements in response to acute pain are typically rapid and\ninvoluntary. Facial reactions include brow bulging, eye squeezing, nasolabial furrow forma-\ntion [54], grimacing, clenched teeth, jaw-dropping, and tightened lips [55]. Body movements\nassociated with pain include bracing (gripping an object or the affected area during move-\nment), rubbing (massaging the painful area), restlessness (constant shifting of position) [55],\nand knee flexion [56]. Non-verbal vocalizations such as groaning, moaning, sighing, crying,\nand gasping [57] also indicate pain. Verbal expressions like \u201couch\u201d ,\u201cstop\u201d ,\u201cthat hurts\u201d ,14 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nFigure 2.2: Pain classification [48]: (A)Nociceptive pain , which results from detecting po-\ntentially harmful stimuli and serves a protective function. (B)Inflammatory pain is\nlinked to tissue damage and immune cell infiltration, increasing pain sensitivity dur-\ning healing. (C)Pathological pain is a disease state caused by either nervous sys-\ntem damage (neuropathic) or abnormal nervous system function (dysfunctional).\n\u201cthat is enough\u201d , and even cursing [55] also serve as pain indicators. Interestingly, swearing\nhas been found to significantly alleviate pain, although its effect diminishes with frequent\nuse over a short period [58, 59].2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 15\n2.4.2 Physiological Indicators\nVital signs can reflect the state of the central nervous system, and since pain is mediated\nthrough this system, trends in vital signs can provide insights into pain levels. Clinical stud-\nies [60, 61] have examined physiological changes in response to pain and established em-\npirical solid evidence linking pain to vital sign alterations. However, as vital signs can also\nchange due to other non-pain-related pathological conditions, it is recommended that they be\nassessed alongside behavioral pain indicators for accuracy. Physiological pain responses are\nconsidered more reliable than behavioral signals, as they cannot be consciously controlled\nor altered. Physiological measurements such as electrocardiography (ECG), electromyogra-\nphy (EMG), galvanic skin responses (GSR), and respiration rate provide critical insights into\nthe body\u2019s reaction to pain [17]. In addition, brain monitoring techniques like near-infrared\nspectroscopy (fNIRS) have demonstrated the ability to detect pain-related hemodynamic\nchanges [23]. At the same time, functional magnetic resonance imaging (fMRI) has been\nexplored for assessing pain in both normal and pathological conditions [62].\n2.4.3 Biochemical Indicators\nCompared to other pain indicators, biochemical changes are the most precise and sensitive\nreactions to pain. However, their routine use in pain assessment is restricted due to the\ninvasive nature of measurement techniques [63]. These biochemical responses are most\nevident during surgical procedures with limited anesthesia, leading to increased levels of\nendorphins, norepinephrine, cortisol, growth hormones, renin, glucagon, aldosterone, and\ncatecholamines, along with a decrease in insulin levels [60].\n2.5 Sociodemographic and Psychological Variables\nIn1965 , Melzack and Wall [64] introduced the \u201cGate Control Theory\u201d , which interprets pain\nfrom two perspectives. The first involves the mechanisms of nociceptive signal transmission\nand modulation, while the second emphasizes pain as a psychophysiological phenomenon\narising from the interaction between physiological and psychological factors [53]. Observa-\ntions, empirical research, and theoretical models increasingly suggest that a comprehensive\nunderstanding of pain requires a biopsychological approach. It is also becoming apparent\nthat, although pain is often regarded as private and subjective, it is also fundamentally a so-\ncial experience [53]. Pain is not solely explained by biomedical components ( e.g., muscle\ndamage) but also involves psychological ( e.g., cognitive, affective) and social factors ( e.g.,\nfriends, family, health professionals), leading to what is known as a biopsychosocial sensa-\ntion [65]. Numerous factors contribute to how painful experiences are expressed and per-\nceived, varying wildly due to social and personal biases. These factors prompted Williams16 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nand Craig [2] to define pain as \u201ca distressing experience associated with actual or potential\ntissue damage with sensory, emotional, cognitive, and social components. \u201d\n2.5.1 Sex and Gender\nSeveral studies have explored the relationship between gender and pain expressiveness, as\nwell as variations in pain reporting. Research indicates that women generally exhibit a lower\npain threshold compared to men. A meta-analysis by Boerner et al. [66] on gender differ-\nences in children and adolescents found that girls over the age of 12reported higher pain\nintensity in response to cold-induced pain than boys. Furthermore, multiple studies suggest\nthat women tend to describe a greater degree of pain compared to men. In addition to bi-\nological differences, psychological aspects linked to gender also play a role. For instance,\nindividuals with a masculine identity may be less inclined to express or report their pain or\nseek assistance [67].\nMoreover, the manifestation of pain is not only influenced by the individual\u2019s gender but\nalso by dyadic interactions between people of different sexes. Levine and Desimone [68]\nconducted one of the initial studies on this phenomenon, showing that male participants in\na cold pressure experiment reported lower pain intensity when a female experimenter was\npresent. Similarly, McClelland and McCubbin [69] found that female participants expressed\nand reported higher pain levels when accompanied by a female friend. This dynamic also\nextends to patient-healthcare provider interactions. In studying health records, Vigil and\nAlcock [70] discovered that when the pain intensity was reported as high, the patients ( i.e.,\nmen and women) were examined by a female doctor or nurse. Additionally, studies exam-\nining gender differences among physicians in pain treatment options revealed that female\npatients were more likely to receive prescriptions for more potent drugs, such as analgesics,\nand female physicians were more likely to prescribe medications. Extensive research has\nalso shown that both lay observers and healthcare professionals tend to estimate higher pain\nlevels for female patients compared to male patients [71]. Hooper et al. [72] further noted\nthat clinicians communicate more effectively with female patients, often displaying greater\nempathy. Gender roles, beliefs, and expectations play a significant role in understanding\nhow social factors influence the differences in pain perception and experience between men\nand women [73].\n2.5.2 Age\nAge plays a crucial role in pain assessment and management. At the same time, there are\nsignificant challenges, limitations, and biases related to the patient\u2019s age group. Two of the\nmost vulnerable groups, albeit for different reasons, are the elderly and infants.\nPain recognition and interpretation among the elderly, particularly by caregivers, often2.5. SOCIODEMOGRAPHIC AND PSYCHOLOGICAL VARIABLES 17\npresent unique challenges. Older adults frequently exhibit stoicism and reluctance to ex-\npress their pain, while healthcare providers struggle to accurately assess the patient\u2019s pain,\nleading to inappropriate pain management decisions [74]. McPherson et al. [75] noted that\ndespite caregivers\u2019 accommodating and empathetic relationships with elderly patients, con-\nflicts still arise. Older patients may resist acknowledging their weaknesses and accepting\nhelp, which can cause them to conceal their pain. The situation becomes even more complex\nwhen dealing with dementia, a disorder encompassing a range of conditions ( e.g., Parkin-\nson\u2019s, Alzheimer\u2019s, Vascular dementia), characterized by abnormal brain changes that im-\npair cognitive and linguistic abilities. A person with dementia may find it challenging to\ncommunicate their pain verbally. However, non-verbal pain expressions remain intact even\nin moderate dementia, although such reactions can be exaggerated [76]. However, aggres-\nsive behavior and disturbances in dementia patients, often caused by pain, are frequently\nmisinterpreted as psychiatric symptoms, leading to improper medication that can have life-\nthreatening consequences [77]. Caregivers of dementia patients face additional challenges,\nnot only related to pain management but also in addressing dementia\u2019s impact on language\nand memory. Particularly in the later stages of dementia, patients encounter severe pain\ncommunication difficulties due to cognitive decline, necessitating that caregivers recognize\nbehavioral and contextual indicators of pain [74]. Age is also known to cause changes in\nskin characteristics, such as texture, rigidity, and elasticity, which impact the performance of\nemotional face recognition tasks [78].\nInfants represent another vulnerable age group where pain assessment requires special-\nized attention, particularly when they experience painful events. The first challenge is obvi-\nously their limited reporting ability to express their pain through language. Although crying\nmight appear to signal pain, this is an oversimplified and unreliable method, as crying can in-\ndicate a variety of situations, such as discomfort, hunger, or pain. Accurately discerning the\ntype of cry is only one part of the challenge; assessing pain in infants is far more complex and\ninfluenced by numerous factors, including the interpersonal relationships within their envi-\nronment. Riddell and Racine [79] found that through various distressing experiences, infants\ncan learn that specific signaling behaviors can prompt their caregiver\u2019s proximity. This at-\ntachment dynamic suggests that, to some extent, infants may consciously utilize pain-related\nbehaviors to elicit responses from their caregivers. Similarly, the context affects older chil-\ndren as well; for example, self-reports of pain tend to be significantly lower when a parent is\npresent compared to when the child is alone [80].\n2.5.3 Psychological Factors\nMultiple studies have revealed that several psychological factors are consistently linked with\npain-related behavior, including depression, pain-related fear, and catastrophizing. Research18 CHAPTER 2. CLINICAL PAIN ASSESSMENT\nfocusing on the impact of depression and anxiety on pain-related behavior has been con-\nducted mainly on patient populations. These studies have shown that depressed individ-\nuals exhibit more pronounced protective and communicative behaviors compared to non-\ndepressed patients [81]. Similarly, numerous studies suggest that patients with higher levels\nof anxiety demonstrate more pain-related behaviors than those with lower anxiety levels [82].\nDespite the frequent coexistence of pain with psychological conditions, research indicates\nthat these patients often experience underestimation of their pain. For instance, De Ruddere\net al. [83] found that patients dealing with psychological stressors such as anxiety, depres-\nsion, and daily life challenges are often perceived by physiotherapists as experiencing less\nsevere pain, illustrating the influence of psychosocial factors on the patient\u2019s pain experience.\n2.5.4 Race and Culture\nPain expression is generally understood across ethnicities and cultures, though differences\nexist in how it is conveyed [4]. However, cultural variations and the nuances of facial ex-\npressions related to emotion are complex and necessitate deeper study. Additionally, racial\nand cultural biases significantly influence pain assessment, judgment, and interpretation.\nExtensive research highlights the impact of a patient\u2019s race as a sociodemographic factor\non observer responses. The most examined topic relates to the different responses toward\nCaucasian versus non-Caucasian individuals, particularly African Americans, who are more\nlikely to have their pain underestimated and undertreated by healthcare providers [84].\nEthnocultural factors are crucial in shaping how individuals perceive and express pain.\nFor example, Western cultures often emphasize conservative expressions and self-control,\nleading to restrained responses in personal pain experiences and in perceiving others\u2019 pain\n[3]. Differences also arise in coping mechanisms; African Americans, for instance, are more\nprone to catastrophizing pain events compared to European Americans [85]. Furthermore,\nevidence shows racial biases in pain treatment across various racial groups, with certain\ngroups being more sensitive to pain but receiving lower-quality treatment [86]. For exam-\nple, Cleeland et al. [87] found that minority cancer patients, mainly Black and Hispanic\nindividuals, were more likely to experience inadequate analgesia compared to non-minority\npatients.\n2.5.5 Observer\u2019s Impact on Pain\nThe variability in pain management stems from the interplay of various elements, including\nsociocultural, biomedical, and psychosocial factors, especially in cases of chronic pain [88].\nWhen it comes to the observer responsible for assessing a patient\u2019s pain, several characteris-\ntics directly influence the objectivity of their evaluation. The first and perhaps most critical\nfactor is the observer\u2019s experience level. One would expect that more experience leads to2.6. IMPACT OF INADEQUATE PAIN MANAGEMENT 19\nbetter and more accurate assessments, but studies show that even experienced healthcare\nproviders consistently underestimate pain, much like laypersons [28]. The greater the expe-\nrience, the more pronounced the underestimation tends to be. This may be due to desensitiza-\ntion caused by repeated exposure to pain events, as seen in the differences between internists\nand surgeons in their evaluation of postoperative pain, with surgeons often encountering se-\nvere pain more regularly [89]. Another significant factor is the observer\u2019s knowledge and\nbeliefs about pain. For example, [83] found that laypersons and healthcare professionals\nwithout physical signs of pain might view the patient\u2019s complaints less seriously. Proper\ntraining is also essential for adequate pain assessment, which is why the Department of\nHealth and Human Services (DHHS) initiated a strategic program to improve healthcare\nproviders\u2019 education and knowledge regarding pain management, following evidence of in-\nadequate training in the field [90].\n2.6 Impact of Inadequate Pain Management\nThe experience of pain, particularly persistent pain, can have detrimental effects on the indi-\nvidual and their surrounding environment. Thoughts about severe pain often lead to grief and\nfear, causing individuals to perceive pain as a threat and feel incapable of managing it. This\ncan prompt avoidance behaviors aimed at escaping perceived harm [91]. Studies have shown\nthat children with a catastrophizing mindset about pain struggle with daily activities, while\nadolescents with chronic pain tend to have fewer friends and may miss out on social and\nentertainment opportunities, putting them at greater risk of victimization [92]. These adoles-\ncents often feel isolated and lonely compared to their healthy peers, and they may experience\nanxiety in social interactions [93]. Parental reactions to their children\u2019s pain can further com-\nplicate the situation, as parents with catastrophic tendencies tend to engage in overprotective\nbehaviors that hinder the child\u2019s functioning and psychosocial development [94]. Addition-\nally, the family\u2019s overall dynamic is affected, with the patient\u2019s sadness, sleep disorders, and\nchanges in leisure activities impacting the household [74].\nOn a biological level, pain, particularly when experienced early and severely, can alter\nthe brain and nervous system. These early pain experiences can disrupt neurobiological\ndevelopment and affect how pain is processed later in life [95]. A growing body of research\nlinks chronic pain to changes in the medial prefrontal cortex, a region crucial to emotional\nprocessing. Chronic pain is associated with structural and biochemical alterations in this\nbrain area, suggesting that these changes play a role in the pathophysiology of chronic pain\n[96].20 CHAPTER 2. CLINICAL PAIN ASSESSMENT\n2.7 Pain Measurement Scales and Metrics\nIn clinical settings, self-reporting remains the gold standard for assessing pain, allowing in-\ndividuals to describe their pain\u2019s intensity and location. Various self-report scales have been\ndeveloped for different age groups, such as the visual analog scale (V AS) [97] and the verbal\nrating scale (VRS) [98]. Additionally, observation-based scales, where a third party eval-\nuates the pain\u2019s severity, include tools like the Prkachin and Solomon pain intensity scale\n(PSPI) [99] and the neonatal/infant pain scale (NIPS) [100]. However, some studies suggest\nthat patients may exaggerate their pain severity to prompt more aggressive treatment inter-\nventions [101], raising concerns about the accuracy of self-reported symptoms. Therefore,\nobjective pain measurement remains clinically crucial.Chapter 3\nAutomatic Pain Assessment\u2013A Literature\nReview\nContents\n3.1 Chapter Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\n3.2 Modalities and Hardware for Automatic Pain Assessment . . . . . . . . . . . . 23\n3.3 Pain Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database . . . . 24\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database . . . . . . . . . . 25\n3.3.3 The EmoPain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database 26\n3.3.5 The AI4Pain Database . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4 Unimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27\n3.4.1 Vision-based: Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . 28\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach) . . . . . . . . . . 35\n3.4.3 Vision-based: Implicit Temporal Utilization . . . . . . . . . . . . . . . . . 36\n3.4.4 Vision-based: Explicit Temporal Utilization . . . . . . . . . . . . . . . . . 37\n3.4.5 Touch sensor-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.4.6 Audio-based . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42\n3.5 Multimodal studies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.1 Static Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.5.2 Temporal Utilization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44\n3.6 Summary of Automatic Pain Assessment Methods . . . . . . . . . . . . . . . . 47\n3.6.1 Input . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.2 Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 47\n3.6.3 Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48\n3.6.4 Pain Databases for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 49\n2122 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.6.5 Interpretation of Results . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\n3.7 Challenges and Future Directions . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.7.1 Current Challenges in Automatic Pain Assessment &Future Research Di-\nrections . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51\n3.1 Chapter Overview\nThis chapter corresponds to the publication [17], a systematic literature review (SLR) con-\nducted at the start of this Ph.D. research. This review facilitated an understanding of au-\ntomatic pain assessment methods, particularly those based on deep learning, and the tech-\nniques and strategies employed. It enabled the identification and proposal of new approaches\nthat could enhance the effectiveness of pain recognition.\nAdditionally, it allowed for identifying gaps in the literature from other reviews con-\nducted on this specific research topic. Every existing systematic review on pain assessment\nwas identified and assessed, revealing several insights. The first review on automatic pain\nassessment, published by Prkachin in 2009 [102], did not cover papers on deep learning,\nas the practical implementations of deep architectures only began around 2012. Zamzmi et\nal.[103] focused their review exclusively on infants, omitting deep learning methods. In\n2018, Chen et al. [104] reviewed automated pain detection methods using the Facial Ac-\ntion Coding System (FACS), noting only three publications that employed deep learning\ntechniques. In 2019, Hassan et al. [105] included only seven papers that used deep learn-\ning methods in their review. Similarly, Werner et al. [106], also in 2019, discussed pain\nassessment without restrictions on modalities or age groups, finding fewer than ten papers\nthat reported on deep learning methods. In 2020, Al-Eidan et al. [107] published the first\nsystematic literature review titled \u201cDeep-Learning-Based Models for Pain Recognition: A\nSystematic Review\u201d, which included fifteen papers but was critiqued for having significant\nlimitations and incorrect information. It was noted that some papers analyzed might not be\nrelevant, and there was confusion between \u201cneural networks\u201d and \u201cdeep learning\u201d. For in-\nstance, while study [105] mentioned using neural network approaches, they did not provide\nevidence of using deep learning methods. Moreover, in the study [104], the authors devel-\noped a neural network with only two layers combined with handcrafted features, which does\nnot qualify as a deep learning method. Additionally, studies [103, 107] focused on detect-\ning protective movement behaviors in chronic pain patients, which deviates from the central\ntopic of automatic pain assessment. Several reviews and SLRs on automatic pain assessment\nhave been published, but none exclusively or adequately focus on deep learning methods.\nThis SLR aims to bridge this gap by thoroughly reviewing deep learning techniques used for\nautomatic pain assessment.3.2. MODALITIES AND HARDWARE FOR AUTOMATIC PAIN ASSESSMENT 23\n3.2 Modalities and Hardware for Automatic Pain Assessment\nCreating an automatic pain assessment system hinges on capturing the necessary input data\nthrough various information channels, referred to as modalities. These modalities are cat-\negorized into behavioral and physiological types. A system utilizing only one modality is\ntermed unimodal, whereas a multimodal system incorporates multiple modalities.\nKey behavioral modalities encompass facial expressions, body movements, gestures, and\nauditory signals. Researchers use a range of optical and light sensors to record images or\nvideo sequences of facial and body movements. Commonly, researchers employ color RGB\ncameras, but depth and thermal sensors are also used to enhance visual data. Motion capture\nsensors are also employed to track movements, and microphones are frequently employed\nto capture sound. On the physiological front, modalities often involve biosignals that detect\nelectrical activities from various tissues and organs. Techniques such as electrocardiogra-\nphy (ECG), electromyography (EMG), electrodermal activity (EDA), photoplethysmogra-\nphy (PPG), blood oxygen saturation (SpO2), near-infrared spectroscopy (NIRS), respiration\nrate, and skin temperature are commonly used to gauge pain. Multiple sensors can mea-\nsure several modalities simultaneously \u2014 for instance, strain sensors and cameras can track\nrespiration rates.\nBesides the sensors that gather input data, the computational hardware is crucial. Deep\nlearning-based systems operate in two phases: training and inference. The training phase is\nparticularly resource-intensive, necessitating a graphics processing unit (GPU). The trained\nmodel makes predictions on new data during inference, typically processed on a central\nprocessing unit (CPU). The choice of hardware depends on various factors, especially in\nreal-time scenarios where low latency is crucial, compared to offline settings where data\nprocessing can be deferred. Additionally, characteristics of the model, such as floating point\noperations per second (FLOPS) and total computational operations, are significant consider-\nations.\n3.3 Pain Databases\nAccess to data is crucial for evaluating methods and algorithms in automatic pain assess-\nment. However, only a few databases have explicitly been developed for automatic pain\nrecognition based on human behavioral and physiological changes. Unlike the extensive\ndata found in most facial expression databases, publicly accessible pain datasets often offer\nlimited samples and suffer from significant class imbalance. This primarily stems from the\nethical concerns associated with collecting pain data. Table 3.1 lists the principal databases\nreviewed in the studies. Figure 3.1 shows how frequently each database was used. Most\nresearch utilized publicly available datasets, with some studies exploring multiple datasets.24 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nFew studies used private datasets, mainly those aimed at detecting pain in neonates. The\nUNBC-McMaster Shoulder Pain Archive Database [108] is the most utilized, followed by\nTheBioVid Heat Pain Database [109]. The former contains 200facial videos of 25individ-\nuals with shoulder pain. At the same time, the latter includes facial videos and biopotentials\nof90healthy participants subjected to experimentally induced heat pain at four intensity\nlevels. The following subsections provide a brief description of some of these datasets.\nTable 3.1: Most commonly utilized pain databases.\nDatabase Modality Population Annotation\nGranularityAnnotation Labels\nUNBC-McMaster\nShoulder PainA[108]RGB video of face 25 adults with shoulder painFrame level\nSequence levelFACS\nV AS, OPI\nBioVidA[109] RGB video of face, EDA, ECG,\nEMG87 healthy adults Sequence level stimulus\n(calibrated per person)\nMIntPAINA[110] RGB-Depth-Thermal video of\nface20 healthy adults Sequence level stimulus\n(calibrated per person),\nV AS\niCOPEA[111] RGB photographs of face 26 healthy neonates Frame level pain, cry, rest, air puff,\nfriction\niCOPEvidA[112] Grayscale video of face 49 neonates Sequence level pain, no pain\nNPAD-IA[113] RGB video of face & body, HR,\nSpO2, BP, NIRS36 healthy neonates & 9 neonates\nwith tissue injured by surgerySequence level NIPS, N-PASS\nAPN-dbA[114] RGB video of face 112 healthy neonates Sequence level NFLAPS, NIPS, NFCS\nEmoPainN[115] video, audio, EMG, MoCap 22 adults with chronic pack pain &\n28 healthy adultsSequence level self-report, naive OPI\nSenseEmotionN\n[116]video of face, audio, EDA, ECG,\nEMG, RSP45 healthy adults Sequence level stimulus\n(calibrated per person)\nX-ITEN[117] RGB-Thermal video of face,\nRGB-Depth video of body, au-\ndio, EDA, ECG, EMG134 healthy adults Sequence level stimulus\n(calibrated per person)\nA: Publicly available by request, complete or part of the dataset N: Not yet available Modality: HR: heart rate SpO2: oxygen saturation rate BP:\nblood pressure NIRS: near-infrared spectroscopy MoCap: motion capture RSP: respiration rate EDA: electrodermal activity ECG: electrocardiogram EMG:\nelectromyogram Annotation Labels: FACS: Facial Action Coding System V AS: visual analogue scale OPI: observer pain intensity NIPS: neonatal infant\nscale N-PASS: neonatal pain, agitation and sedation scale NFLAPS: neonatal face and limb acute pain scale NFCS: neonatal facial coding system\n3.3.1 The UNBC-McMaster Shoulder Pain Expression Archive Database\nTheUNBC McMaster Shoulder Pain Database [108] comprises 200video sequences show-\ning the facial expressions of 25subjects undergoing motion tests, including arm abduction\nand external and internal rotations. The data collection utilized both active and passive ap-\nproaches: in the active mode, subjects moved their affected arms to their bearable limit,\nwhile in the passive mode, a physiotherapist moved the subjects\u2019 arms. Each video sequence\ncontains about 60to700frames, totaling 48,398, with 82.71% of frames scoring a pain\nrating of zero, indicating a significant imbalance in the data. All frames are FACS-coded\nfor pain-related action units (AUs)\u2014AU4, AU6, AU7, AU9, AU10, AU12, AU20, AU25,\nAU26, AU27, and AU43\u2014with each AU coded for intensity from A to E, 0, or5, except\nfor AU43 (closed eyes), which is coded as either present or absent. Pain scores are assigned\nusing the PSPI metric based on the intensity of the AUs present. Additionally, the database3.3. PAIN DATABASES 25\n0815233038455360\nUNBC-McMasterBioVidEmoPainSenseEmotionX-ITEMIntPAINiCOPEiCOPEvidNPAD-IAPN-dbother\nTable 1Category AUNBC-McMaster59BioVid21EmoPain7SenseEmotion5X-ITE2MIntPAIN4iCOPE3iCOPEvid1NPAD-I5APN-db1other21\n1\nFigure 3.1: The number of studies utilizing these specific datasets. Note that various studies\nused multiple datasets to conduct their experiments.\nincludes 66facial landmarks per frame, determined by an active appearance model. Pain as-\nsessments also include self-reports using two Likert scales with 15options each and a visual\nanalog scale (V AS) from 1(no pain) to 10(extreme pain). One scale measures the sen-\nsory intensity from \u201cextremely weak\u201d to \u201cextremely intense\u201d , while the other assesses the\naffective-motivation aspect of pain from \u201cbearable\u201d to \u201cextremely excruciating\u201d. Indepen-\ndent observer pain intensity (OPI) ratings use a 6-point scale from 0(no pain) to 5(intense\npain). The UNBC database is currently the most extensively utilized dataset for automatic\npain recognition among publicly available resources.\n3.3.2 The Biopotential and Video (BioVid) Heat Pain Database\nTheBioVid dataset [109] is a prominent resource in pain research, comprising facial videos,\nelectrocardiograms, electromyograms, and galvanic skin response data from eighty-seven\npn=87qhealthy participants ( 44males and 43females, aged 20to65). The pain was in-\nduced using a thermode on the participants\u2019 right arm, with pain and tolerance thresholds\nestablished before data collection. These thresholds defined the range of pain from No Pain\n(NP) to Very Severe Pain (P 4), encompassing five levels of pain intensity. The temperatures\nfor the pain inductions ranged from P 1to P 4and did not exceed 50.5\u02ddC. Each participant\nunderwent 20inductions at each of four pain levels, with each induction lasting 4sfollowed\nby a recovery period of 8to12s. In addition, 20baseline measurements were taken at 32\u02ddC\n(NP), totaling 100stimulations per participant, randomly administered. Data processing\nsegmented these into 5.5sdurations starting 1safter the target temperature was reached, re-26 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsulting in 8,700samples across the five pain intensity classes, equally distributed among all\nmodalities for each participant. Video recordings were made at a frame rate of 25FPS, and\nbiosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\n3.3.3 The EmoPain Database\nTheEmoPain [115] dataset encompasses various pain indicators, including body movements,\naudio, biosignals, and postural and facial expressions. It features video and audio recordings\nof22patients ( 7male, 15female) exhibiting natural pain expressions while engaging in\nphysiotherapy-like exercises. These exercises, performed at regular and challenging levels,\ninclude a sitting-standing sequence, balancing on one leg for five minutes, and reaching for-\nward while standing. The video signals are captured in high resolution ( 1024\u02c61024 pixels)\nusing eight cameras positioned at various angles, enhanced by specialized lighting condi-\ntions. Audio is recorded with two microphones: an AKG C-1000S MKIII placed near the\ncameras and an AKG HC 577 L worn by the patients, both operating at a 48kHz sampling\nrate with bit Pulse Code Modulation. Body movements and postures are tracked using a mo-\ntion capture suit with 18sensors distributed across the body. Biosignals are monitored with\nfour sEMG sensors attached to the trapezius and lumbar para-spinal muscles. Additionally,\nthe dataset provides continuous frame-wise pain ratings for facial expressions by eight naive\nannotators and binary frame-wise annotations for protective behaviors by four experts, along\nwith coordinates from 26body nodes. Six annotated protective behaviors include stiffness,\nbracing, hesitation, limping, rubbing, and abrupt actions. Audio and EMG signals from the\neight activities per subject also contribute to multimodal pain recognition. Like the UNBC\ndatabase, EmoPain faces significant challenges due to data sparsity and imbalance\u2014only\n11.4%of frames show facial expressions of pain, and 8.6%show protective behaviors. This\nscarcity complicates pain recognition research, necessitating the development of methods\nthat efficiently utilize limited data to achieve optimal performance.\n3.3.4 The Experimentally Induced Thermal and Electrical (X-ITE) Pain Database\nTheX-ITE [117] dataset is one of the largest pain datasets but is not publicly available. It\ninvolved 134healthy adults ( 67men and 67women) aged between 18and50. The aver-\nage age was 31.4years (SD = 9.7), with men averaging 33.4years (SD = 9.3) and women\n32.9years (SD = 10.2). Participants had no chronic pain, depression, psychiatric disorders,\nneurological conditions, headache syndromes, or cardiovascular disease, nor had they taken\npain medication or painkillers before the experiment. Pain stimuli were stimulated using the\nMedoc PATHWAY Model ATS for heat pain on the forearm and the Digitimer DS7A for elec-\ntrical pain on the index and middle fingers. Both modalities featured phasic stimuli (short,\n5seconds) and tonic stimuli (long, 60seconds), each in three intensities. After calibration,3.4. UNIMODAL STUDIES 27\nparticipants underwent a 90-minute stimulation phase where phasic stimuli were repeated\n30times in a randomized sequence with 8-12-second pauses. The tonic stimuli were applied\nonce per intensity, totaling six per participant, each followed by a five-minute pause. The\nhighest intensity tonic stimuli for heat and electrical pain were induced at the experiment\u2019s\nending, with the other stimuli randomly interspersed during the phasic period. Simultane-\nous to the pain stimulation, various sensors collected multimodal pain response data: frontal\nand side view RGB videos for facial expression and head pose analysis, audio for paralin-\nguistic response analysis, electrocardiogram (ECG) to monitor heart rate variability, surface\nelectromyography (EMG) to assess muscle activity in the trapezius, corrugator supercilii,\nand zygomaticus major, electrodermal activity (EDA) to measure sweating, video for body\nmovement analysis, and thermal video for facial temperature changes.\n3.3.5 The AI4Pain Database\nThe AI4Pain Grand Challenge 2024 [118] dataset is a recent contribution to the pain re-\nsearch field, tailored for sophisticated pain recognition tasks using fNIRS and facial video\ndata. This dataset involves sixty-five volunteers pn=65q, including 23females, with ages\nranging from 17to52years (mean age of 29.06years and a standard deviation of 8.28years).\nAlthough it captures physiological signals such as photoplethysmography (PPG), electroder-\nmal activity (EDA), and respiration (RESP), these signals are not publicly available yet. The\ndataset is segmented into three parts: training ( 41volunteers), validation ( 12volunteers),\nand testing ( 12volunteers). The experimental setup includes fNIRS data recorded with an\nArtinis device, measuring changes in oxygenated and deoxygenated haemoglobin concentra-\ntions across 24channels targeting the prefrontal cortex. The optodes configuration includes\n10sources and 8detectors spaced 30mm apart, using near-infrared light at 760nm and 840\nnm, sampled at 50Hz. Additionally, facial movements are captured by a Logitech Stream-\nCam at30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low Pain ,\nandHigh Pain . It features 65instances of No Pain (each lasting 60s),780instances of Low\nPain (each lasting 10s), and 780instances of High Pain (each lasting 10s). The No Pain\ninstances, recorded during baseline, serve as control data. The Low Pain instances reflect\nmild pain responses, and the High Pain instances capture significant pain, both derived from\na pain tolerance test and reflected in the corresponding neurological and behavioral data\nrecorded.\n3.4 Unimodal studies\nThis section presents the studies that utilized only one information channel to estimate the\nsubject\u2019s pain condition.28 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.1 Vision-based: Static Analysis\nThe first publicly available pain database that significantly contributed to the development\nof automatic pain assessment methods was the UNBC-McMaster Shoulder Pain Database .\nNumerous studies have employed this dataset. Pedersen [119] implemented the first deep\nlearning approach in 2015 to address the pain assessment problem, utilizing a 4-layer contrac-\ntive autoencoder. He combined the encoded representations with a support vector machine\n(SVM), achieving high performance in frame-level pain detection. A significant advance-\nment in vision-based pain recognition methods was the EmoPain challenge in 2020, which\nbecame the first international competition to compare machine learning methods for chronic\npain assessment. Egede et al. [120] presented the EMOPAIN 2020 Challenge , utilizing a\ndataset composed of features extracted via both handcrafted methods and deep-learned mod-\nels. They utilized facial landmarks, histogram of oriented gradients (HOG), and deep vectors\nfrom VGG-16 [121] and ResNet-50 [122], both pre-trained on the Aff-Wild dataset1. The au-\nthors report that combining hand-engineered features with deep learning cues led to the best\nperformance. Similarly, Yang et al. [123] extracted both low- and high-level features from lo-\ncal descriptors and the pre-trained VGG-16 CNN, combining them through weighted coeffi-\ncients. Semwal and Londhe [124] demonstrated that fusing deep-learned features with facial\nlandmarks is beneficial for multi-class pain estimation. Lakshminarayan et al. [125] com-\nbined deep-learned features with handcrafted ones\u2014namely features from VGG-16 [121]\nandResNet-50 [122], HOG, action unit occurrence and intensity, facial landmarks, and head\npose\u2014through a fully connected network. Their study found that combining VGG-16 with\nhandcrafted features lowered regression error, whereas [126] achieved maximal performance\nusing only VGG-16 features with a fully connected network.\nConversely, Semwal and Londhe [127] noted the limitations of traditional handcrafted\nfeature engineering and the computational expense of deep neural networks. As a solution,\nthey proposed a relatively shallow 4-layer CNN, which reduces computational costs due to\nfewer parameters while achieving performance comparable to deeper models. A different\napproach came from [128], where the authors focused on representing facial expressions\nas compact binary codes for pain intensity classification. Feature extraction was conducted\nusing a pre-trained model [129], with a fully connected network used to generate the binary\ncodes.\nSeveral studies utilized CNN ensemble designs with varying architectures to exploit fea-\nture diversity. Semwal and Londhe [130] combined predictions from three compact CNNs\u2014\nVGG-16 ,M-MobileNet [131], and GoogleNet [132]\u2014using the average ensemble rule, re-\nsulting in improved classification performance. Kharghanian et al. [133] developed a con-\nvolutional deep belief network (CDBN) using unsupervised feature learning. An SVM used\n1https://ibug.doc.ic.ac.uk/resources/first-affect-wild-challenge3.4. UNIMODAL STUDIES 29\nthe extracted features to differentiate between two states for binary pain classification ( i.e.,\npain vs. no pain). Later, [134] added two layers to the CDBN, though the results were not\ndirectly comparable due to differing evaluation methods.\nSeveral papers suggest that because pain is predominantly expressed in specific facial\nregions, focusing on these areas rather than the whole face could improve model accuracy by\nreducing noise. Huang et al. [135] initially identified the left eye, right eye, nose, and mouth\nas key regions and utilized a multi-stream CNN for feature extraction, assigning learned\nweights to enhance attention on these regions. Xin et al. [136] employed a 9-layer CNN\nwith an attention mechanism to assign different weights to face regions, resulting in more\naccurate attention face maps and boosting prediction accuracy by up to 19%. Cui and Huang\n[137] introduced a multi-scale regional attention network (MSRAN), which uses multiple\ncropping regions from video frames. The framework includes self-attention and relation-\nattention modules to highlight pain-relevant regions and explore interrelationships. Li et\nal.[138] extended this concept by integrating contrastive and multi-task training through an\nautoencoder, building on the work of [139].\nOne challenge in pain intensity estimation is that individual facial features, such as face\nshape, can introduce significant variability in how pain is expressed. This makes it difficult\nto distinguish between adjacent intensity levels. To address this, Peng et al. [140] examined\nfacial shape information and developed a deep multi-task network to account for the rela-\ntionship between pain recognition and shape, which improved pain estimation performance.\nSimilarly, Xin et al. [141] proposed a novel multi-task framework that combines a CNN\nfeature learning module with an autoencoder attention component, also estimating subject\nidentity, as individual differences in pain manifestation are key. Their experiments achieved\nstate-of-the-art results on publicly available datasets.\nMost studies report results obtained from controlled laboratory settings, which typically\nfeature proper lighting, minimal head pose variability, and no occlusions. However, such\nconditions do not represent typical hospital environments. Semwal and Londhe [142] ad-\ndressed this by focusing on pain assessment in uncontrolled settings, developing a shallow\nCNN with three convolutional layers that performed comparably to deeper pre-trained mod-\nels. In a subsequent study [143], they introduced a more complex framework comprising\nthree modules that leveraged high-level spatial descriptors with both local and global geomet-\nric cues, achieving results comparable to models like GoogleNet [144] and VGG [121]. Lee\nand Wang [145] explored pain assessment in intensive care unit (ICU) settings, where par-\ntially occluded faces frequently complicate facial analysis. They developed a 4-layer CNN\ncombined with an extreme learning machine (ELM) for final estimation. Virrey and Cae-\nsarendra [146] used CNNs to classify sections of frames where pain was triggered, peaked,\nand subsided. Nugroho et al. [147] tackled pain detection in smart home-care settings, par-\nticularly for elderly patients, using relatively low-power mobile devices. They modified the30 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nOpenFace2library, based on pre-trained FaceNets [148], and showed that transfer learning\ncould enable real-time binary classification ( pain vs.no pain ), even on low-powered hard-\nware.\nResearchers like Dai et al. [149] and Menchetti et al. [150] have noted that most models,\nwhether deep or shallow, are trained on dataset-specific features rather than actual pain-\nrelated features. Moreover, most studies employ validation methods using the same dataset,\nwhile cross-dataset performance is rarely addressed, limiting real-world applicability. To\ntackle these issues, Dai et al. [149] combined pain and emotion detection datasets to develop\na real-time pain assessment system with better generalization capabilities. They emphasized\nthe importance of cross-corpus evaluation, real-time testing, and the need for well-balanced,\necologically valid pain datasets [151].\nSeveral studies have explored combining pain scales to improve prediction objectivity\nand reliability. Liu et al. [152] developed a two-stage personalized model trained using active\nappearance model (AAM) facial landmarks and multi-task learning, with visual analog scale\n(V AS) and observed pain index (OPI) as ground truth. Xu et al. [153] similarly reduced\nmean square error (MSE) by incorporating various pain scales with the VGG-Face model.\nHowever, Casti et al. [154] pointed out the limitations of original ground truth data due to\nsubjectivity and annotation inconsistencies. To address this, they re-annotated their dataset\nwith judgments from multiple experts, using multidimensional scaling to map frames to\nillumination-invariant 3D space, which they then fed into a pre-trained AlexNet [155].\nCelona and Manoni [156] investigated neonatal facial expressions to detect pain, achiev-\ning the highest accuracy when utilizing two pre-trained models: VGG-Face [157] and mapped\nLBP+CNN (MBPCNN) [158]. Similarly, Lu and Hao [159] found that pre-trained models\nwere crucial for small datasets like neonates, as training from scratch led to overfitting. They\nachieved optimal classification performance by fine-tuning the entire VGG-16 model [122].\nHowever, Zamzmi et al. [160] argue that most face recognition methods are tailored for\nadults and thus less applicable to infants. They developed a lightweight 2D CNN trained\nend-to-end and achieved high pain detection accuracy, but external validation on a different\nneonatal dataset revealed challenges with generalizability. In 2019, Brahnam et al. [112]\nintroduced the iCOPEvid neonatal video dataset, a significant contribution since the only\npublicly available neonatal pain dataset [111] previously contained only static images. Their\nexperiments showed that local descriptors based on the bag-of-features (BoF) approach out-\nperformed deep learning models like VGG-Face andResNet . Combining handcrafted and\ndeep-learned features offered only a marginal improvement in performance. In contrast, Za-\nmzmi et al. [161] found that the most effective approach for binary classification (pain vs.\nno pain) was the fusion of high-level features from VGG [162] and optical flow strains, with\n2http://cmusatyalab.github.io/openface3.4. UNIMODAL STUDIES 31\nnaive Bayes serving as the classifier. Celona and Brahnam [163] applied a Wasserstein gen-\nerative adversarial network with gradient penalty (WGAN-GP) [164], demonstrating that\ntraining set augmentation with synthetic samples improved classification performance. Ta-\nble 3.2 summarizes the vision-based studies focusing exclusively on the spatial dimension.32 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [112]F (RGB) texture\ndescriptors- FF 2D CNN`SVM SL C P O 49 k-fold iCOPEvid 79.80 AUC\n'15 [119]F (RGB) - - - AE SVM SeSL,\nSLC P PS 25 LOSO UNBC 86.10 ACC,\n96.50 AUC\n'20 [120]F (RGB) - - FF 2D CNN`NN SL R IC O 36 hold-out EmoPain 0.91 MAE;\n'18 [123]F (RGB) HOG,\nstatistics- FF 2D CNN`SVR SL R IC PS 25 LOSO UNBC 1.44 MSE;\n'21 [130]F (RGB) - - DF 2D CNN`- SL C ID PS 25 k-fold UNBC 93.87 ACC;\n'16 [133]F (RGB) - - - CDBN SVM UL C P PS 25 LOSO:UNBC 87.20 ACC;\n'21 [134]F (RGB) - - - CDBN SVM SL C P PS 25 LOSO UNBC 93.16 AUC\n'19 [135]F (RGB) - - FF 2D CNN - SL C ID1, IC PS 25 LOSO UNBC 88.191ACC\n'20 [136]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 51.10 ACC;\n'20 [140]F (RGB) - - FF 2D CNN`- SL R ID S 25 ? UNBC 79.94 ACC;\n'21 [142]F (RGB) - - - 2D CNN - SL C ID O 8 k-fold other 97.48 ACC;\n'19 [145]F (RGB) - - - 2D CNN ELM SL R IC PS 25 k-fold UNBC\u201a1.22 MSE;\n'19 [146]F (RGB) - - - 2D CNN - SL C TR, CL, DI PS 25 k-fold UNBC 60.00 ACC\n'19 [150]F (RGB) - - - 2D CNN`- SL C AUs-D PS 25, 43 k-fold UNBC & CK+1,\nWilkie97.701ACC;\n'17 [152]F (RGB) statistics - - NN GPM WSL R IC O, S 25 k-fold UNBC 2.18 MAE\n'20 [153]F (RGB) statistics - FF 2D CNN`NN SL R IC S 25 k-fold UNBC 1.95 MAE;\n'19 [154]F (RGB) LBP, MDS - - 2D CNN`- SL C ID O 25 hold-out UNBC 80.00 ACC\n'18 [159]F (RGB) - - - 2D CNN`- SL C ID O ? hold-out other 78.30 ACC\n`: Pre-trained model -:Not exist &: in Dataset indicates the utilization of cross-database training/validation ?: Not found :: The authors provide additional experiments with other validation methods \u201a: The authors\nutilized occluded facial images ;: The authors provide additional metrics Modality: F: face region Non deep features: LBP: local binary pattern MDS: multidimensional scaling Fusion: M: fusion of modalities E:\nfusion of deep learned features or hand-crafted features Deep models: AE: autoencoder RCNN: recurrent convolutional neural network CDBN: convolutional deep belief network CNN: convolutional neural network\nNN: neural network WGAN-GP: Wasserstein generative adversarial model with gradient penalty Non deep model: SVM: support vector machine GPM: Gaussian process regression model kNN: k-nearest neighbors\nNB: naive Bayes ELM: extreme learning machine Learning Method: SL: supervised learning SeSL: semi-supervised learning UL: unsupervised learning WSL: weakly supervised learning Classific./Regres.: C:\nclassification R: regression Objective: P: presence of pain ID: intensity in discrete scale IC: intensity in continuous scale TR: trigger CL: climax DI: diminishing AUs-D: Action Units detection GT: ground truth\nPS: Prkachin and Solomon S: self-report O: observer rating ST: stimulus Validation Method: LOSO: leave one subject out Metrics: AUC: Area Under the ROC Curve ACC: accuracy PPV: precision MSE: mean\nsquared error MAE: mean absolute error3.4. UNIMODAL STUDIES 33Table 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [124]F (RGB) facial landmarks - FF 2D CNN NN SL C, R ID, IC1P 25 LOSO:UNBC 0.171MSE;\n'20 [125]F (RGB) HOG, head pose,\nAUs intensity/\noccurrence, facial\nlandmarksFF - 2D CNN`NN SL R IC O 36 hold-out EmoPain 5.48 RMSE;\n'20 [126]F (RGB) - - - 2D CNN`NN SL R IC O 36 hold-out EmoPain 1.49 RMSE;\n'18 [127]F (RGB) - - - 2D CNN - SL C ID PS 25 hold-out UNBC 92.00 ACC;\n'18 [128]F (RGB) statistics, distance\nmetrics- FF 2D CNN`- SL C, R ID, IC PS 25 LOSO UNBC 0.81 PCC,\n0.69 MSE\n'21 [137]F (RGB) - - FF 2D CNN`- SL C, R ID, IC P 25 LOSO UNBC 91.13 ACC,\n0.78 PCC,\n0.46 MSE\n'18 [138]F (RGB) - - - AE`- SL R IC PS 25 k-fold UNBC 0.33 MAE;\n'21 [141]F (RGB) - - FF [AE, 2D CNN]Y- SL C, R ID1, IC2,\nP3P, ST 25, 87 LOSO UNBC1,\nBioVid (A)289.1711ACC,\n0.8121PCC,\n85.6532ACC,\n40.4012ACC\n'21 [143]F (RGB) entropy texture\ndescriptors- - 2D CNN`- SL C ID O 8 k-fold other 0.92 PPV;\n'18 [147]F (RGB) - - - 2D CNN`- SL C P PS 14 k-fold UNBC 93.00 ACC\n'19 [149]F (RGB) - - - 2D CNN - SL C P PS 25, 20 k-fold UNBC &\nBioVid (A)\u02db56.75 ACC\n'17 [156]F (RGB) HOG, LBP - FF 2D CNN`SVM SL C P O 26 LOSO iCOPE 73.78 ACC\n'19 [160]F (RGB) - - - 2D CNN - SL C P O 31 LOSO NPAD1,\niCOPE296.981ACC;,\n89.802ACC\n'21 [165]F (RGB) - - - 2D CNN`- FL C P PS 25 LOSO UNBC 76.00 ACC;\n'21 [166]F (RGB) - - - 2D CNN`- SL C P O 25 hold-out UNBC 75.49 ACC\n'21 [167]F (RGB) - - - 2D CNN`SVR SL R IC P 25 LOSO UNBC 0.34 MSE\n'21 [168]F (RGB) - - - 2D R-CNN - SL C P O ? hold-out other 87.80 PPV\nY: The authors combined the deep models into a unified framework \u02db: The authors experimented with additional datasets combinations Non deep features: AUs: actions units HOG: histogram of oriented gradients Non\ndeep model: SVR: support vector regression Learning Method: FL: federated learning Metrics: RMSE: root mean squared error34 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.2: Vision-based studies with static analysis (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/EDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [161]F (RGB) optical flow - FF 2D CNN`SVM,\nkNN, NBSL C P O 31 k-fold other 92.71 ACC,\n94.80 AUR\n'19 [163]F (RGB) - - - WGAN-GP - SL C P O 26 LOSO iCOPE 93.38 ACC\n'17 [169]F (RGB) - - - 2D CNN`- SL R IC PS 25 LOSO UNBC 0.99 MAE;\n'20 [170]F (RGB) - - - 2D CNN - SL C ID ST 87 hold-out BioVid (A) 36.60 ACC\n'20 [171]F (RGB) - - - 2D CNN - SL C P PS 25 hold-out UNBC 97.00 PPV;\n'21 [172]F (RGB) - - - 2D CNN - SL C ID P 28 LOSO:UNBC 90.30 ACC\n'19 [173]F (RGB) - - - 2D CNN - SL C P O 31 hold-out NPAD1,\niCOPE291.001ACC;,\n84.502ACC;\n'21 [174]F (RGB) - - - 2D CNN`- SL C P O 26, 30 hold-out iCOPE &\nUNIFESP89.90 ACC;\n'21 [175]F (RGB) - - - 2D CNN - SL C AUs-D P 10 hold-out Pain-ICU 77.00 ACC;3.4. UNIMODAL STUDIES 35\n3.4.2 Vision-based: Temporal Utilization (Non-ML Approach)\nPain assessment is particularly challenging due to its complex and dynamic nature. Rely-\ning on static, individual frames to assess pain fails to capture the phenomenon\u2019s temporal\nprogression and often leads to inaccurate estimations. Additionally, many studies highlight\nthe difficulties of applying deep learning techniques to small datasets, with one proposed\nsolution being the combination of deep learning and traditional feature extraction methods.\nEgede et al. [176] addressed this by extracting deep features from a pre-trained CNN, explic-\nitly targeting the eyes and mouth regions. Using a relevance vector regressor (RVR), they\ndemonstrated that combining deep and hand-crafted features led to optimal performance. De-\nspite the valuable insights the UNBC-McMaster database provides, its imbalanced sample\ndistribution\u2014particularly the limited number of frames showing pain\u2014poses a significant\nchallenge for deep learning models. In response, Egede and Valstar [177] devised a method\nbased on the observation that neighboring pain level classes share many common features.\nThis approach allowed them to avoid extracting all possible features for classes with fewer\nsamples, as certain features had already been utilized from other related classes. The study\nalso showed that combining deep and hand-crafted features improved performance. How-\never, in a later study [178], the authors applied a similar approach, using only deep-learned\nfeatures to address data imbalance, but could not replicate the same high-performance levels.\nTavakolian et al. [179] took a different approach, focusing on the detection of genuine\nversus acted pain through facial expressions, a technique with important applications in both\nmedical and forensic contexts. They developed a residual GAN (R-GAN) to capture subtle\nfacial changes and the dynamic nature of expressions, using a weighted spatio-temporal pool-\ning (WSP) method. In a subsequent study [180], the authors suggested that self-supervised\nlearning could reduce the time and effort needed for data labeling, as it does not require\ncomplete dataset annotation. They introduced a new similarity function for learning general-\nized representations with a Siamese network. They also employed statistical spatio-temporal\ndistillation (SSD) based on the Gaussian scale mixture (GSM) to improve computational effi-\nciency. This technique encodes spatiotemporal variations in facial videos into a single RGB\nimage, simplifying the model while maintaining effectiveness.\nOther studies also aim to capture the dynamic aspects of pain. For instance, [181] com-\nbined a random forest classifier with the pre-trained MobileNetV2 model [182], encoding\nvideos by selecting and merging three frames from different time points into a single image.\nOthman et al. [183] emphasized the importance of using diverse datasets\u2014including vary-\ning age, gender, pose, occlusion, and lighting conditions\u2014to improve model generalization.\nThey used multiple data combinations and a reduced version of MobileNetV2 , showing that\ncross-dataset training is essential for achieving better generalizability.36 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.3 Vision-based: Implicit Temporal Utilization\nSeveral studies have explored the application of 3D CNNs for pain assessment. Tavakolian\nand Hadid [184] developed a 3D CNN to capture dynamic facial representations from videos.\nThey noted that researchers often use fixed temporal kernel depths when employing 3D\nconvolution techniques, which limits the ability to capture short, mid, and long temporal\nranges simultaneously. To address this, they designed a model with parallel 3D convolutional\nlayers featuring variable temporal depths, allowing the capture of temporal dependencies\nfrom 32consecutive frames. Similarly, Wang and Sun [185] applied 3D convolutions based\non the architecture proposed in [186], consisting of 8convolutional layers with 3\u02c63\u02c63\nfilters. While they reported high performance, the authors acknowledged that extracting deep\nfeatures from small datasets posed a challenge for model generalization. In a related study,\nHuang et al. [187] developed a framework that integrated 3D, 2D, and 1D CNNs to extract\nspatio-temporal, spatial, and geometric features. For the 3D CNN component, they modified\nthe architecture from [188] by using discrete kernels of 1\u02c63\u02c63and3\u02c61\u02c61rather than the\ntraditional 3\u02c63\u02c63kernel. Other researchers have also proposed 3D deep CNNs with varying\ntemporal depths to capture short, mid, and long-range facial expression variations [189].\nRecognizing the difficulty and time consumption involved in training a deep 3D CNN from\nscratch, they introduced a cross-architecture knowledge transfer learning technique, utilizing\na pre-trained 2D CNN to assist in the training of the 3D CNN. In studies by Praveen et\nal.[190] and [191], the authors employed weakly-supervised domain adaptation, where the\nsource domain focused on human affective expressions and the target domain was explicitly\nrelated to pain expressions. Their framework featured an inflated 3D-CNN (I3D) [192],\nincorporating 3convolutional layers and 3inception modules [132] to capture both spatial\nand temporal information from video data.\nBargshady et al. [193] opted to use the HSV color space instead of RGB, arguing that it\nbetter reflects human visual perception for tasks such as skin pixel detection and multi-face\ndetection. They employed the pre-trained VGG-Face [157] for feature extraction, followed\nby a temporal convolutional network (TCN) using dilated causal convolutional operations to\nleverage temporal dependencies. Rezaei et al. [194] tackled the challenge of pain detection\nin people with dementia, a difficult task due to insufficient pain-related images or videos\nof elderly subjects in existing datasets. They developed a 10-layer 2D CNN that processed\npairs of pain and no-pain images, analyzing frame-to-frame changes and employing con-\ntrastive training methods [195]. The model demonstrated high performance in both healthy\nindividuals and people with dementia. In another study, Pandit and Schmitt [196] explored\nthe potential of using shallow 1D CNN architectures for real-time pain recognition. They ex-\ntracted facial action units from each frame using the OpenFace 2.03toolkit, with promising\n3https://github.com/TadasBaltrusaitis/OpenFace3.4. UNIMODAL STUDIES 37\nresults for pain detection in real-time settings.\n3.4.4 Vision-based: Explicit Temporal Utilization\nSeveral efforts have focused on addressing the limitations of static frames by developing\ndedicated temporal modules. Zhou et al. [197] tackled this issue using a regression frame-\nwork based on a 4-layer recurrent convolutional neural network (RCNN), each with a se-\nquence length of 3time steps. Rodriguez et al. [198] leveraged dynamic information by\ndesigning an LSTM model fed with feature vectors extracted from VGG-16 [122]. Simi-\nlarly, Bellantonio et al. [199] emphasized that facial expressions evolve, making it essential\nto analyze the spatio-temporal dimension of pain. They improved estimation performance\nusing a fine-tuned 16-layer CNN model [157], an LSTM processing 16frames as a time\nwindow, and super-resolution techniques. In another study, Bargshady et al. [200] com-\nbined the VGG-Face CNN [157] with a 3-layer LSTM to extract spatio-temporal features\nfrom grayscale images, applying zero-phase component analysis (ZCA). In [201], principal\ncomponent analysis (PCA) was used to reduce dimensionality. Mauricio et al. [202] also\nemployed VGG-Face but replaced LSTM with a 2-layer gated recurrent unit (GRU) to cap-\nture temporal dependencies. Thuseethan et al. [203] used a conventional 2D CNN and two\nRCNNs to extract temporal features from previous and subsequent frames, enhancing the\ntime dimension of expression analysis.\nA similar approach was followed by Bargshady et al. [204], who employed ensemble\nlearning with three distinct CNN-biLSTM modules, merging their outputs for the final pre-\ndiction. Salekin et al. [205] used a bilinear CNN (B-CNN) based on the VGG architecture\n[121], pre-trained on VGGFace24andImageNet5datasets, along with an LSTM to capture\ntemporal dependencies in image sequences. Kalischek et al. [206] explored deep domain\nadaptation for facial expression and pain detection, utilizing the self-ensembling approach\n[207] with a long-term recurrent convolutional network (LRCN). While they achieved state-\nof-the-art results for facial expression recognition, performance was lower for pain detection,\nlikely due to the subtle nature of pain-related expressions.\nDespite the availability of additional information in pain datasets, multi-task approaches\nremain limited. Martinez et al. [208] proposed a personalized multi-task learning method\nbased on individual physiological and behavioral pain responses. They extracted AAM fa-\ncial landmarks, processed them through a biLSTM to produce PSPI scores, and predicted the\nfinal V AS score. Erekat et al. [209] combined AlexNet [155] with 2 GRU layers to capture\ntemporal dependencies, using both self and observer-reported pain intensity as ground truth.\nVuet al. [210] developed a multi-task framework to estimate pain levels while reconstruct-\n4https://www.robots.ox.ac.uk/ \u02dcvgg/data/vgg_face\n5https://www.image-net.org38 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\ning heatmaps of action unit locations, improving model generalization with a CNN-LSTM\ncombination to capture micro facial movements.\nHuang et al. [211] noted that specific frames within a video sequence exhibit more pro-\nnounced pain expressions, requiring special handling. They developed a novel framework\nusing attention saliency maps with a VGG-16 model, GRUs and learned weights for each\nframe\u2019s contribution to pain intensity estimation. The study demonstrated that dynamic and\nsalient features can significantly improve performance. Similarly, Yu et al. [212] used VGG-\n11 (configuration A) and an LSTM to create an attention mechanism, predicting pain in-\ntensity from 16consecutive frames. Xu and Liu [213] adopted a ResNet-50 model with an\nattention mechanism to extract spatial features, followed by a transformer encoder to capture\ntemporal sequences, achieving promising results.\nIn other studies, Ragolta et al. [214] used extracted action units to train a 2-layer LSTM\npredicting pain on an 11-point scale, employing curriculum learning. Guo et al. [215] devel-\noped a convolutional LSTM (C-LSTM) to extract both spatial and temporal features from\nvideos, showing that temporal models outperform non-temporal models for pain estimation\naccuracy. Rasipuram et al. [216] utilized in-the-wild video data for pain detection, gener-\nating a 3D morphable model without relying on facial landmarks and combining it with an\nLSTM. Zhi and Wan [217] introduced sparse coding with LSTM (SLTM), using the iterative\nhard thresholding algorithm (ISTA) [218] to capture dynamic facial expressions. Although\nSLTM did not achieve high performance, it offers speed and efficiency for specific applica-\ntions. Finally, Thiam et al. [219] developed a method combining motion history and optical\nflow images with a 10-layer CNN and 2-layer biLSTM, showing that weighted score aggre-\ngation improves performance. Table 3.3 summarizes studies incorporating the modalities\u2019\ntemporal dimensions.3.4. UNIMODAL STUDIES 39Table 3.3: Vision-based studies with temporal utilization.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'17 [176] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVR SL R IC PS 25 LOSO UNBC 0.99 RMSE,\n0.67 PCC\n'17 [177] F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN`RVM SL R IC PS 25 LOSO UNBC 1.04 RMSE,\n0.64 PCC\n\u201918 [178] F (RGB) - - - NL 2D CNN - SL R IC PS 25 LOSO UNBC 1.20 RMSE,\n0.47 PCC\n'18 [184] F (RGB) - - - I 3D CNN - SL R IC PS 25 LOSO UNBC 0.53 MSE,\n0.84 PCC;\n'18 [185] F (RGB) HOG,\ngeometric\ndifference- DF I 3D CNN SVR SL R IC PS 25 LOSO UNBC 0.94 RMSE,\n0.67 PCC\n'20 [191] F (RGB) - - - I 3D CNN`- WSL R IC PS 24, ? LOSO UNBC\n& RECOLA0.64 MAE,\n0.82 PCC;\n'16 [197] F (RGB) - - FF E RCNN - SL R IC PS 25 LOSO UNBC 1.54 MSE,\n0.65 PCC\n'17 [198] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C, R P, IC1PS 25 LOSO UNBC 0.741MSE,\n0.781PCC;\n'17 [199] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 61.90 ACC\n'19 [200] F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C ID PS 25 LOSO UNBC 75.20 ACC\n'20 [201] F (RGB) PCA - DF E [2D CNN`,\n1D CNN, biLSTM]Y- SL C ID PS 25 LOSO:UNBC 85.00 ACC;\n'19 [202] F (RGB) - - - E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 85.40 ACC,\n0.62 MSE;\n'19 [203] F (RGB) - - FF E [2D CNN, RCNN]Y- SL R IC PS 25 LOSO UNBC 1.29 MSE,\n0.73 PCC\n'17 [208] F (RGB) - - FF E biLSTM HCRF,\nFCSL C IC O,\nS25 hold-out UNBC 2.46 MAE;\nNon deep features: PCA: principal component analysis Temporal Exploitation: NL: non-machine learning method I: implicit method E: explicit method Deep models: RCNN: recurrent convolutional neural network\nLSTM: long short memory networks biLSTM: bidirectional neural network GRU: gated recurrent unit Non deep models: SVM: support vector machine RVM: relevance vector machine GPM: Gaussian process regression\nmodel HCRF: hidden conditional random fields FC: fully connected SVR: support vector regression Objective: I2: intensity in binary pairs Metrics: PCC: Pearson correlation coefficient40 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'19 [114]F (RGB) HOG,\ndistance\nmetrics- DF NL 2D CNN RVR SL R IC O 13 LOSO APN-DB 1.71 MAE;\n'19 [179]F (RGB) - - - NL R-GAN - UL C genuine\nvs posedPS,\nST25,\n34,\n87,\n87? UNBC\n& STOIC\n& BioVid (A)\n& BioVid (D)90.97 ACC\n'20 [180]F (RGB) - - FF NL 2D CNN`- SSL C IC P,\nST25\n87LOSO UNBC1,\nBioVid (A)2\u20180.781PCC;,\n71.022AUC;\n'21 [181]F (RGB) AUs\nintensity- H NL 2D CNN`RF SL C ID ST 127 k-fold X-ITE 25.00 ACC\n'19 [183]F (RGB) - - - NL 2D CNN - SL C P ST 87\n134k-fold BioVid (A)\n& X-ITE\u201867.90 ACC\n'20 [209]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC O,\nS25 k-fold UNBC 2.34 MAE\n'20 [211]F (RGB) - - FF E [2D CNN`, GRU]Y- SL R IC PS 19 LOSO UNBC 0.21 MSE,\n0.89 PCC\n'19 [212]F (RGB) - - FF E [2D CNN, LSTM]Y- SL R IC PS 24 LOSO UNBC 1.22 MSE;,\n0.40 PCC;\n'20 [214]F (RGB) AUs\nintensity- - E LSTM - SL R IC O 36 hold-out EmoPain 2.12 RMSE,\n1.60 MAE;\n'20 [216]F (RGB) - - FF E [2D CNN`, LSTM]Y- SL C P O ? k-fold UNBC 78.20 ACC;\n'20 [219]F (RGB) - - DF E [2D CNN, biLSTM,\nNN]Y- SL C P ST 87\n40LOSO BioVid (A)1,\nSenseEmotion269.251ACC,\n64.352ACC\n'20 [220]F (RGB) - - FF E [2D CNN`, GRU]Y- SL C ID, IC PS 25 LOSO UNBC 0.84 ACC,\n0.69 PCC;\n\u2018: The authors provide experiments with cross-dataset settings Fusion: H: hybrid Non deep models: RF: random forest classifier3.4. UNIMODAL STUDIES 41Table 3.3: Vision-based studies with temporal utilization (continued).\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'21 [187]F (RGB) facial\nlandmarks- DF I [3D CNN`,\n2D CNN`,\n1D CNN, FC]Y- SL R IC PS 25 LOSO UNBC 0.76 MSE,\n0.82 PCC;\n'19 [189]F (RGB) - - - I [2D CNN`,\n3D CNN]Y- UL,\nSLC, R IC1, P2P,\nST25, 87 LOSO UNBC1,\nBioVid (A)20.9211PCC;,\n86.0222AUC\n'20 [190]F (RGB) - - - I 3D CNN`- WSL R IC PS 24,?,\n87, 18LOSO UNBC1\n& RECOLA\n& BioVid (A)2\u20180.741PCC,\n0.342PCC\n'20 [193]F (RGB) PCA - FF I [2D CNN`,\nTCN]Y- SL C ID P,\nST25, 20 LOSO:UNBC1,\nMIntPAIN292.441ACC;,\n89.002ACC;\n'20 [194]F (RGB) - - - I 2D CNN - SL C, R IC, P1P 95, 25 k-fold UofR & UNBC182.0011PCC;\n'20 [196]F (RGB) AUs\noccurrence- FF I 1D CNN - SL R IC P 24, 87 hold-\noutUNBC1,\nBioVid (A)0.801CCC\n'20 [204]F (RGB) PCA - DF E [2D CNN`, 1D\nCNN, biLSTM]Y- SL C ID PS,\nST25, 20 k-fold UNBC1,\nMIntPAIN286.001ACC;\n92.262ACC;\n'20 [205]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- SL R P, IC1O 45 LOSO NPAD 3.991MSE,\n1.552MAE\n'19 [206]F (RGB) - - FF E [2D CNN`,\nLSTM]Y- UL C P ST 40 LOSO SenseEmotion 60.61 ACC\n'21 [210]F (RGB) - - - E [2D CNN`,\nLSTM]Y- SL R IC P 25, 27 LOSO UNBC1,\nDISFA\u20180.60`MSE,\n0.82`PCC;\n'21 [213]F (RGB) - - - E [2D CNN`,\nTransformer]Y- SL R IC P 25 LOSO UNBC 0.40 MSE,\n0.76 PCC;\n'21 [215]F (RGB) - - - E 2D C-LSTM - SL C ID S 29 hold-\noutother 69.58 F1\n'19 [217]F (RGB) - - FF E SLSTM - SL C P1, ID2ST 85 LOSO BioVid (A) 61.701ACC\n29.702ACC\n'21 [221]F (RGB) - - - I 3D CNN`- SL R IC S 25 k-fold UNBC 0.66 ICC;\nFusion: H: hybrid Deep models: TCN: temporal convolutional neural network C-LSTM: convolutional-LSTM SLTM: sparse long short memory network Learning Method: SSL: self-supervised learning Metrics: F1:\nF1 score CCC: concordance correlation coefficient42 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.4.5 Touch sensor-based\nTouch (contact) sensors provide a viable alternative for pain assessment, often outperforming\nvision-based methods. Table 3.4 highlights studies that utilized contact sensor data to evalu-\nate pain. Yu et al. [222] analyzed three categories of pain-no pain, moderate pain, and severe\npain\u2014using EEG signals. They extracted several bands from the biosignals, including al-\npha, beta, and gamma, and applied a convolutional module. The study found that combining\nthese bands yielded better results than evaluating them independently. Similarly, [223] used\nEEG potentials with an autoencoder to compress the raw data and applied a logistic regressor\nfor classification.\nOther researchers, such as Rojas et al. [224], utilized functional near-infrared spec-\ntroscopy (fNIRS) for pain detection. They developed three models\u2014multilayer perceptron\n(MLP), LSTM, and biLSTM\u2014with biLSTM demonstrating superior accuracy. Addition-\nally, [225] focused on PPG signals, extracting hand-crafted features from the time and fre-\nquency domains, which were then combined with a deep belief network (DBN) to achieve\nover65% accuracy in a 4-class pain assessment task. Hu et al. [226] used kinematic data\nto compare healthy individuals with those suffering from low back pain (LBP). Their ap-\nproach, which employed two stacked LSTM layers, reached over 97% accuracy in binary\nclassification using raw motion data. Lastly, Mamontov et al. [227] were the first to apply\nevolutionary algorithms in the design of an optimized recurrent neural network (RNN) for\npain estimation, achieving 91.94% accuracy using EDA signals.\n3.4.6 Audio-based\nA few studies have explored using audio information for pain detection and intensity esti-\nmation, as outlined in Table 3.5. These methods are especially relevant for neonates, where\nfrequent facial and body occlusions make analyzing cries a more effective approach for pain\ndetection. Chang and Li [228] concentrated on infant cries to differentiate between hunger,\npain, and sleepiness. They transformed the audio signals into 2D spectrograms using a fast\nFourier transform (FFT) and trained a 2D CNN for feature extraction. Similarly, [229] uti-\nlized spectrograms generated from recorded sounds, employing a model identical to that\nused in [160]. Thiam and Schwenker [230] focused on detecting adult pain by analyzing\nbreathing sounds. They leveraged deep-learned features from spectrograms with Mel-scaled\nshort-time Fourier transform, combined with various handcrafted cues. A CNN followed by\na biLSTM was used to capture spatial and temporal dependencies, integrating both low- and\nhigh-level features. In a different approach, Tsai et al. [231] examined pain events during\nemergency triage. They developed an LSTM autoencoder framework to extract temporal\nfeatures from verbal behavior, reporting encouraging results.3.4. UNIMODAL STUDIES 43Table 3.4: Touch sensor-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'20 [222]EEG - - FF I 1D TCN - S C ID S 32 k-fold other 97.30 ACC;\n'20 [223]EEG - - - I AE (TCN) LR UL, S C P S 29 LOSO other 74.60 ACC\n'21 [224]fNIRS - - - E biLSTM - SL C ID S 18 k-fold other 90.60 ACC;\n'19 [225]PPG - - - NL DBN SBM U, SL C P1, ID2S 100 k-fold other 86.791ACC,\n65.572ACC\n'18 [226]kinematatics - - FF E LSTM - SL C P LBP 44 LOSO other 97.20 ACC;\n'19 [227]EDA - - FF E [RNN, LSTM,\nGRU, NN]YSelfCGA,\nselfCGP,\nPSOPBSL C P ST 40 LOSO Sense-\nEmotion81.94 ACC\n'21 [232]EDA - - - I NN - SL C P1, I2 ST 87,\n55LOSO BioVid (A)1,\nPainMonit284.2211ACC;,\n86.5012ACC;\nModality: PPG: photoplethysmogram fNIRS: functional near-infrared spectroscopy EEG: electroencephalography EDA: electrodermal activity Deep models: DBN: Deep belief network RNN: recurrent neural network\nNon deep models: SBM: selective bagging model LR: Logistic Regression SelfCGA: Self-Configuring Genetic Algorithm SelfCGP: Self-Configuring Genetic Programming PSOPB: Particle Swarm Optimisation with\nparasitic behaviour GT: LBP: low back pain vs healthy population\nTable 3.5: Audio-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'16 [228]audio (cry) - - - - 2D CNN - SL C P O ? k-fold other 78.50 ACC\n'19 [229]audio (cry) - - - - 2D CNN - SL C P O 31 LOSO:NPAD 96.77 ACC;\n'19 [230]audio\n(breathing)MFCCs,\nRASTA-\nPLP,\nDTD- FF E [2D CNN,\nLSTM]YRFc SL C P ST 40 LOSO Sense-\nEmotion64.39 ACC\n'17 [231]audio\n(voice)prosodic-\nspectral\nfeatures,\nSF- FF E LSTM`SVM UL,\nSLC P1, ID2S 63 LOSO other 72.301UAR,\n54.202UAR\nNon deep features: MFCCs: Mel Frequency Cepstral Coefficients RASTA-PLT: Relative Spectral Perceptual Linear Predictive DTD: descriptors from temporal domain SF: statistical features44 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\n3.5 Multimodal studies\nSince pain is a multidimensional phenomenon, combining multiple modalities in a multi-\nmodal system offers a promising approach. Heterogeneous information sources can com-\nplement one another, enhancing specificity and sensitivity. As reported in [106], when in-\ndividual modalities demonstrate good predictive performance, their fusion tends to yield\nimproved outcomes. Moreover, integrating cues from various channels may be helpful and\nnecessary, especially in clinical settings where specific modalities may become unavailable\n(for instance, if the patient turns and their face is occluded). The information channels can\noriginate from (1) the same hardware sensor but focus on different regions of interest, such\nas RGB facial images and RGB body images [233], (2) different hardware sensors but the\nsame region of interest, like RGB facial images and thermal facial images [110], or (3)\ndifferent hardware sensors and information sources, such as RGB facial images and ECG\nsignals [234]. Table 3.6 lists the studies utilizing multimodal approaches.\n3.5.1 Static Analysis\nA commonly used biosignal combination is those of EDA, EMG, and ECG, as these channels\nare found in all main pain reference databases. Thiam et al. [235] applied an early fusion\nmethod by merging these signals into a 2D representation and inputting it into a 9-layer 2D\nCNN. Their results showed a strong correlation between EDA and pain intensity, and com-\nbining all three modalities did not outperform using EDA alone. Al-Qerem et al. [236] used\nleast generative adversarial networks (LSGANs) to enhance EMG, EDA, and ECG samples,\nreporting a notable improvement in classification when using an SVM on the augmented\ndataset. Haque et al. [110] introduced the MIntPAIN dataset, which includes RGB, depth,\nand thermal videos for multi-class ( 5levels) pain recognition. They combined these three vi-\nsual modalities into a 5D matrix (RGB+D+T) and used it to train the pre-trained VGG-Face\nmodel [157], leading to better classification performance in their experiments.\n3.5.2 Temporal Utilization\nZhiet al. [237] proposed a multimodal stream-integrated neural network that leverages video\nand biosignal data. They combined raw facial video frames with optical flow images to cap-\nture spatio-temporal dependencies via 3D CNNs, integrating these with biosignal features\nextracted using LSTMs. The entire network was trained end-to-end, achieving superior re-\nsults compared to their unimodal methods. Beyond facial analysis, Salekin et al. [233]\nfocused on assessing neonatal pain through body movements in videos. After identifying\nrelevant body regions, video frames were fed into a pre-trained VGG-16 [121], connected to\nan LSTM to capture temporal dynamics. In a follow-up study, Salekin et al. [238] fused three3.5. MULTIMODAL STUDIES 45\nmodalities\u2014facial expressions, body movements, and crying sounds\u2013demonstrating that this\nmultimodal approach outperformed unimodal techniques. Similarly, Wang et al. [239] ex-\nplored combining EMG, EDA, and ECG biosignals with handcrafted and learned features\nfrom a biLSTM model. They applied the minimum relevance method (MRMR) to reduce\nthe number of features, resulting in notable outcomes.\nIn addition to EDA, EMG, and ECG, other biosignal combinations have been explored.\nZhao et al. [240] integrated PPG, EDA, and temperature signals, using 2D convolutions for\nspatial feature extraction and time windows for capturing temporal information. Yuan et\nal.[241] successfully estimated pain using whole-body MoCap sensors and EMG, utilizing\nLSTM layers with an attention mechanism in an autoencoder, which reduced training time\nby leveraging latent space representations of raw data. Similarly, Li et al. [242] employed\nMoCap and EMG as data sources and tested various LSTM configurations to predict pain\nintensity, achieving the best performance with a 3-layer vanilla LSTM combined with a 3-\nlayer fully connected network.46 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nTable 3.6: Multimodal-based studies.\nPaperInput Processing Evaluation\nModality Non deep\nfeaturesFusion\nM/ETemporal\nExploitationDeep model Non deep\nmodelLearning\nMethodClassific.\n{Regres.Objective GT Number\nsubjectsValidation\nMethodDataset Metrics\n'18 [110]F (RGB,\nthermal,\ndepth)- RF - - 2D CNN`- SL C ID S 20 k-fold MIntPAIN 36.55 ACC\n'19 [233]F, B (RGB) - FF - E [2D CNN`,\nLSTM]Y- SL C P O 31 LOSO other 92.48 ACC;\n'19 [234]F (RGB),\nECG, EDAbiosignals\u2019\nfeaturesmFF FF - 2D CNN`RFc SL C I2 S 85 k-fold BioVid (A) 74.00 ACC\n'19 [235]EDA,\nEMG,\nECG- RF - - 2D CNN - SL C P1I2,\nID2S 87,\n86LOSO BioVid (A)1\nBioVid (B)84.4011ACC;,\n36.5412ACC;\n'20 [236]EDA,\nEMG,\nECGBoruta\nfeaturesFF - - LSGAN SVM UL,\nSLC I2, ID1S 85 hold-\noutBioVid (A) 82.801ACC\n'21 [237]F (RGB),\nEDA,\nEMG, ECGoptical\nflowFF FF NL,\nE, I[3D CNN,\nLSTM]Y- SL C, R P1, I2,\nID2S 87,\n40k-fold:BioVid (A)1,\nMIntPain68.2011ACC;,\n28.1021ACC\n'21 [238]F, B (RGB),\nsound- DF - E [2D CNN`,\nLSTM]Y- SL C P O 45 LOSO NPAD 78.95 ACC;\n'20 [239]EDA,\nEMG,\nECGMRMR,\nbiosig-\nnals\u2019\nfeaturesRF\nFFE biLSTM NN SL C P1, I2 S 87 LOSO BioVid (A) 83.301ACC\n'20 [243]EDA,\nEMG,\nECG- FF - I [DDCAE,\nNN]Y- UL,\nSLC P1, I2 S 87 LOSO BioVid (A) 83.991ACC;\n'21 [244]EDA,\nEMG,\nECG, RSP- FF - I [DDCAE,\nNN]Y- UL,\nSL,\nSSLC, R P1, ID2,\nICS 87,\n40LOSO BioVid (A)1,\nSense-\nEmotion84.2511ACC;,\n35.4421ACC;\n'21 [245]EDA, ECG - FF - E 1D CNN,\nLSTM- UL C P1, I2 S 67 hold-\noutBioVid (A) 81.711ACC\n'20 [240]PPG, EDA,\ntemperature- RF - I 2D CNN - SL R\u02ddP1, ID2S 21 k-fold other 96.301ACC,\n95.232ACC\n'20 [241]MoCap,\nEMG- RF - E AE, LSTM - UL,\nSLC ID O 23 LOSO:EmoPain 52.60 ACC;\n'20 [242]MoCap,\nEMG- RF - E LSTM, NN - UL C ID O 30 hold-\noutEmoPain 80.00 ACC;\n'21 [246]MoCap,\nEMG- RF - E LSTM, NN - SL C ID O 30 LOSO:EmoPain 54.60 ACC;\nm: Not specifically described \u02dd: Ordinal Modality F: face region B: body region EMG: electromyography Non deep features: MRMR: Minimum Redundancy Maximum Relevance method Deep models: LSGAN:\nLeast Square Generative Adversarial Networks3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 47\n3.6 Summary of Automatic Pain Assessment Methods\nThis section presents an analysis of the reviewed studies, summarizing the main conclusions\non current methods for automatic pain assessment, their advantages, and corresponding lim-\nitations. Additionally, it offers recommendations for future research directions that could\nadvance the field of pain research from a computational perspective.\n3.6.1 Input\nFirst, we observe a clear imbalance between unimodal and multimodal approaches in pain\nassessment studies. More than 86% of the reported research focuses on unimodal methods,\neven though the databases often contain multiple information channels. Notably, contact\nsensor-based and audio-based approaches are underrepresented, with only seven and four\nstudies, respectively, compared to 84studies that utilize a vision-based approach.\nMultimodal approaches are even less explored, with only 15studies falling into this\ncategory, making it difficult to draw strong conclusions about the effectiveness of specific\nmodality combinations. However, there are indications that EDA sensor data is particularly\nvaluable compared to other biopotentials. Researchers have primarily focused on visual data,\nlikely due to the complexity of implementing multimodal frameworks or the impracticality\nof contact sensors in non-laboratory settings. Further exploration of diverse modality com-\nbinations is necessary to evaluate their potential for pain assessment fully\u2014additionally, 28\nstudies employed non-deep features to enhance deep-learned representations.\nFinally, we identified three primary strategies in examining the approaches that utilize\ntemporal information: non-machine learning-based, machine learning-based (implicit), and\nmachine learning-based (explicit). Non-machine learning-based methods, such as motion\nhistory images [219] or temporal distillation [180], rely on traditional computer vision tech-\nniques. These methods tend to be more straightforward but are generally less sophisti-\ncated. In contrast, machine learning-based approaches [190] [217] offer richer temporal\ninformation and the flexibility to adapt to specific requirements, such as emphasizing certain\nvideo frames. Among the studies reviewed, 55% employed temporal features, with explicit\nmethods\u2014most commonly LSTM models\u2014being the predominant choice. Given that many\nstudies report superior performance when temporal information is incorporated, compared\nto non-temporal methods, it is evident that further emphasis on temporal approaches is war-\nranted.\n3.6.2 Processing\nRegarding machine learning approaches, various models and techniques have been employed\nfor pain estimation. CNN models remain the most widely used, with more than 75% of stud-48 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nies utilizing 1D, 2D, or 3D filters, highlighting the central role of convolution operations\nin deep learning. Sequential models, such as RNNs, GRUs, LSTMs, and biLSTMs, follow\nclosely behind in popularity. Almost half of the studies used pre-trained models to achieve\ntheir desired performance. This suggests that existing pain databases may not be adequate\nfor training deep-learning models from scratch. Non-deep learning models have also been\nemployed in 26studies as auxiliary decision components, with SVMs and shallow neural net-\nworks being the most common choices. There seems to be significant potential for adopting\nnewer deep learning architectures, especially transformer-based models, which have demon-\nstrated state-of-the-art results in various AI research fields and are particularly suited for\nexploiting temporal modality information [247].\nThe predominant learning method used across studies is supervised learning. How-\never, 16papers explored or adopted alternative methods such as unsupervised learning [119,\n133, 179, 189, 206, 223, 225, 231, 236, 241, 243], self-supervised [180, 244], self-supervised\nlearning [180, 244], semi-supervised learning [119], weakly supervised learning [190, 191],\nand federated learning [165]. Given the limited availability of pain data resources, self-\nsupervised learning appears to be the most appropriate method for future research and should\nbe further embraced by the community.\nLastly, it is notable that most studies\u2014approximately 70%\u2014treat pain assessment as a\nclassification problem rather than a regression problem. However, we believe that regres-\nsion more closely reflects the continuous nature of pain and is better suited to capturing the\ncomplexity of pain sensation.\n3.6.3 Evaluation\nThe primary objectives of the reviewed studies were (i)to estimate pain intensity on a dis-\ncrete scale (multi-class classification), (ii)to measure pain intensity on a continuous scale,\nand(iii)to determine the presence or absence of pain (binary classification). Notably, 25\nstudies focused on pain detection rather than pain intensity estimation, which, from a clin-\nical standpoint, is less informative as it does not provide sufficient data for effective pain\nmanagement. From an engineering perspective, detecting the presence or absence of pain is\nalso a more straightforward and less demanding task.\nA small subset of studies took a different approach to pain estimation. For instance, one\nstudy [179] sought to differentiate genuine pain from acted pain. Another [231] explored\npain events in emergency triage settings rather than controlled laboratory environments,\nwhile [234] examined the feasibility of real-time pain detection on IoT devices. Addition-\nally, [142] and [143] aimed to address the issue of occluded faces in pain estimation. So-\nciodemographic and psychological factors were also considered, as seen in studies like [245],\nwhich explored gender differences, and [194], which focused on pain assessment in elderly3.6. SUMMARY OF AUTOMATIC PAIN ASSESSMENT METHODS 49\npatients with dementia. The limited exploration of pain estimation in real-world settings\nor unconventional contexts suggests that current approaches may not be fully applicable in\npractical environments like clinics and hospitals.\nVarious annotation types are used regarding ground truth, such as self-reported ratings,\nFACS, and observer scales. Temporal features are critical for accurately estimating pain\nintensity, making the temporal granularity of the ground truth equally important. Several\nstudies have questioned the objectivity of PSPI scores, as noted in [248], which highlights\nthat PSPI scores can be zero even when pain is present or that there may be no visible facial\nexpressions in low-intensity pain. Pain expressions not captured by the FACS system, such\nas raising eyebrows or opening the mouth, further challenge the use of PSPI [249]. Addi-\ntionally, PSPI does not account for pain-related head and body movements, which are par-\nticularly valuable in newborn assessments [250]. For these reasons, we recommend moving\naway from PSPI as ground truth in favor of self-reports and observer scales at the video-\nsegment level.\nAround 54% of the studies employed the leave-one-subject-out (LOSO) validation method,\nwhich is widely regarded as more objective and better for assessing the generalizability of\nmodels. However, LOSO can be less practical due to the increased model size and longer\ntraining times. When researchers use other validation methods, such as k-fold or hold-out,\nit is essential to ensure that consecutive, highly correlated frames from the same subject do\nnot skew the training and validation results, leading to flawed estimations. Moreover, when\nresearchers define their own validation or testing sets, comparing results across studies\u2014\nespecially between classification and regression models\u2014becomes nearly impossible. We\nbelieve standardized evaluation protocols should be developed for each publicly available\ndatabase for these reasons.\n3.6.4 Pain Databases for Evaluation\nThe availability of suitable public databases is arguably the most crucial factor in addressing\nthe challenge of automatic pain assessment. Several aspects must be considered in evaluating\nthese datasets, including the number of subjects and their characteristics, such as age, sex,\nhealth status, and race. Moreover, the ground truth must be objective and offer meaningful\ninsights into the subject\u2019s pain experience [154].\nFig. 3.1 illustrates the number of papers corresponding to the pain database utilized in\neach study. It is clear from this figure that the UNBC andBioVid databases were the most\ncommonly used public datasets. However, the UNBC dataset does not record the subjects\u2019\nages, despite age being a known factor in pain expression [35,66]. While the BioVid dataset\ndoes document age, the oldest participants are only 65years old, which is notable since pain\nand its management are critical issues among individuals aged 65and older [251]. Simi-50 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nlar limitations are found in other pain datasets, such as X-ITE [117], EmoPain [115], and\nSenseEmotion [116].\nIt is well known that aging causes skin changes, including texture, rigidity, and elastic-\nity alterations, which can impact facial emotion recognition tasks [78]. Additionally, race-\nrelated factors can lead to inaccurate pain assessments due to variations in how pain is ex-\npressed [252]. Notably, one study by Nerella et al. [175] reported lower performance when\ntheir model was tested on African American patients. Furthermore, only one study [194]\nwas found that specifically addressed pain estimation in elderly individuals with dementia.\nIn summary, developing objective, automated, and generalizable deep learning-based\npain assessment systems will only be possible if balanced and representative datasets are\navailable for training and external validation.\n3.6.5 Interpretation of Results\nRecent advancements in AI have shown state-of-the-art performance across nearly every\nscientific discipline, often surpassing human accuracy in specific diagnostic tasks [253].\nHowever, a significant drawback of AI solutions, particularly deep neural networks, is their\nlack of transparency, commonly called \u201cblack box AI\u201d. This term highlights how these\nmodels learn intricate functions that are opaque and frequently incomprehensible to hu-\nmans [254]. This opacity is a primary reason for the criticism directed toward deep learning\ntechniques [255]. Various techniques, such as visualizations and gradients-backpropagation\nfocusing on specific units, have been developed to offer insights into how these models func-\ntion. For further reading, refer to the comprehensive review on explanatory techniques in\ndeep learning [256].\nTable 3.7 outlines the different approaches used to interpret model decisions. Only a\nsmall fraction of the reviewed studies\u2014 20out of 110\u2014implemented methods to explain\nhow their models work and which features or elements they focus on. It is important to\nnote that interpretable machine learning can be broadly defined as the \u201cextraction of rele-\nvant knowledge from a machine-learning model concerning relationships either contained\nin data or learned by the model\u201d [257]. To summarize: (i)18% of the reviewed studies\nprovided an approach to enhance the interpretability of the model\u2019s decision, (ii)all of these\nmethods were applied to studies using facial images as the input modality, and (iii)around\nhalf of these studies were conducted by just three specific research groups. These findings\nsuggest that the issue of interpretability and explainability within deep learning remains un-\nderexplored, particularly in the context of automatically classifying pain severity levels.3.7. CHALLENGES AND FUTURE DIRECTIONS 51\nTable 3.7: Interpretation approaches.\nPaper Year Modality Method\n[124] 2021 F (RGB) visualization (saliency maps)\n[128] 2018 F (RGB) visualization (heat maps)\n[130] 2021 F (RGB) visualization (saliency map)\n[133] 2016 F (RGB) visualization (learned filters)\n[134] 2021 F (RGB) visualization (learned filters)\n[135] 2019 F (RGB) visualization (heat maps),\nvalues of learned weights\n[138] 2018 F (RGB) visualization (saliency maps)\n[141] 2021 F (RGB) visualization (attention maps)\n[142] 2021 F (RGB) visualization (saliency map)\n[143] 2021 F (RGB) visualization (activation maps)\n[153] 2020 F (RGB) visualization (pixels contributions)\n[177] 2017 F (RGB) visualization (average saliency map)\n[179] 2019 F (RGB) visualization\n(generated intermediate representation)\n[194] 2020 F (RGB) visualization (saliency maps)\n[196] 2020 F (RGB) weights per AU (contribution of AUs)\n[173] 2019 F (RGB) visualization (feature maps)\n[174] 2021 F (RGB) visualization (integrated gradients)\n[210] 2021 F (RGB) visualization (heatmaps)\n[211] 2020 F (RGB) visualization (attention maps),\nvalues of learned weights\n[212] 2019 F (RGB) visualization (attention maps)\n3.7 Challenges and Future Directions\nThis section discusses the existing challenges in automatic pain assessment and proposes\nfuture research directions to further progress in the field.\n3.7.1 Current Challenges in Automatic Pain Assessment & Future Research Direc-\ntions\nSeveral limitations exist in the current pain databases. Important demographic factors such\nas sex, gender, and age are often missing, and there is an apparent lack of racial diversity\namong subjects. For example, facial structures and emotional expressions vary across Cau-\ncasian, Asian, and African populations [258]. Moreover, social interactions, such as the\npresence of a partner during assessments, could influence pain manifestation and should\nbe included in future datasets [69]. Estimating the location of pain, particularly for infants\nor individuals with communication impairments, is another vital aspect of pain assessment52 CHAPTER 3. AUTOMATIC PAIN ASSESSMENT\u2013A LITERATURE REVIEW\nsystems, which current databases largely overlook. Future datasets should incorporate stim-\nuli targeting various body locations. Furthermore, the videos in existing visual databases\noften have low to medium resolution and frame rates, which are inadequate for capturing\nfacial micro-expressions. Audio data is also sparsely represented, though it holds potential\nas a valuable modality. From an audio perspective, integrating natural language processing\n(NLP) methods to extract linguistic features and create multimodal systems is a promising\ndirection, as shown in affective computing research [259]. Finally, specific validation proto-\ncols should be provided with present and future datasets to ensure objective and consistent\ncomparisons across studies.\nFrom an engineering perspective, several issues must be addressed to advance automatic\npain assessment. Developing multimodal approaches is essential for creating robust systems\nwith enhanced capabilities. Not only do multimodal methods demonstrate better perfor-\nmance than unimodal ones, but they are also crucial in real-world scenarios where a specific\nmodality may become unavailable. Additionally, it is essential to exploit each modality\u2019s\ntemporal aspects fully. We encourage using machine learning models or other techniques\nthat can accommodate the dynamic nature of pain. More work is needed to improve the accu-\nracy of multi-level and low-intensity pain estimation. Another area of research involves the\nrelationship between pain and other affective states, such as negative emotions, which often\ncoexist during painful events. Detecting these emotions could improve pain assessment. Ad-\ndressing challenges like occlusions or poor lighting conditions in vision-based systems also\nrequires attention. Researchers should explore these scenarios, even if current databases do\nnot account for them. Real-time application of pain assessment systems is another critical\nfactor, so future studies should measure throughput, such as the number of images processed\nper second during inference. Generalization is another crucial concern for AI systems, and\nevaluating trained models across different pain databases could be valuable. Finally, to facil-\nitate the clinical adoption of AI-based pain assessment systems, the models\u2019 decisions need\ngreater explainability. Developing or adopting methods that improve interpretability will\nenhance their clinical viability.Chapter 4\nDemographic Variables: Their Role and\nImpact\nContents\n4.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 53\n4.2 ECG Analysis with Classical Machine Learning . . . . . . . . . . . . . . . . . 54\n4.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 54\n4.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58\n4.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59\n4.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 61\n4.3 ECG Analysis with Multitask Neural Networks . . . . . . . . . . . . . . . . . . 64\n4.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64\n4.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 66\n4.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67\n4.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71\n4.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73\n4.1 Chapter Overview: Introduction & Related Work\nThis chapter includes the findings published in [35, 36]. As discussed in Section 2, research\nhas demonstrated that biological and psychological differences can lead to variations in how\npain is perceived between men and women. Regarding age, it is known that infants who\ncannot express themselves directly and older adults with health issues require specific care\ndue to their unique needs. However, a significant question remains unanswered in pain\nresearch, both from clinical and biological perspectives: Does the sensation of pain change\nas individuals age? Specifically, does a person in pain perceive their situation differently\n5354 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nas they grow older than others in different age groups? Is the sensation of pain evolving\nthroughout life? To the best of our knowledge, this question has not yet been definitively\nexplored thoroughly. This chapter investigates the differences among males and females, as\nwell as age groups, using ECG signals. In addition, it proposes a computational framework\nthat utilizes these two demographic elements to improve pain assessment performance. This\nchapter analyzes ECG signals to explore the differences in pain perception between males\nand females and various age groups. Additionally, it introduces a computational framework\nincorporating these two demographic factors to enhance the accuracy of pain assessments.\nFrom a computational standpoint, the literature concerning the use of demographic fac-\ntors in pain assessment is scarce. The study in [260] utilized a range of biosignals, including\nEDA, respiration rate, diastolic blood pressure, and facial action units, to demonstrate dif-\nferences in pain perception between males and females. Similarly, in research [245], the\nauthors utilized a hybrid CNN-LSTM model that processed ECG and EDA data, highlight-\ning gender-based variations in pain response. Following the publications of our research,\nanother study by Ricken et al. [261] was released, which explored the differences in adap-\ntation and habituation between men and women. This study extracted handcrafted features\nfrom various biosignals (including ECG and EDA) and employed random forest classifiers\nto analyze the data.\n4.2 ECG Analysis with Classical Machine Learning\nWe explore a pain estimation process using ECG signals and examine variations across dif-\nferent demographic groups, focusing on gender and age. Specifically, we analyze how pain\nmanifestation differs between males and females, investigate variations in pain perception\nacross different age groups, and consider the combined effects of age and gender on pain\nperception.\n4.2.1 Methodology\nThis section will describe the electrocardiography processing algorithm and the methods\nused for feature extraction and classification algorithms.\nECG signal Processing and Analysis\nAn ECG signal captures the heart\u2019s electrical activity over time. Typically, a normal ECG\ndisplays a sequence of waves, identified as P, Q, R, S, T, and occasionally U. These waves\nand their intervals provide crucial insights into heart function. The P wave indicates atrial\ndepolarization, the QRS complex signifies ventricular depolarization and contraction, and\nthe T wave corresponds to the repolarization of the ventricles. Each heartbeat is depicted4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 55\nQR\nST\nPQRS  \nComplex\nPR\nInterval\nQT Interval\nFigure 4.1: The PQRST waveform.\nLow Pass Filter ECG  High Pass Filter Differentiation\nAdaptive ThresholdsMoving W indow\nIntegrationSquaringQRS\nComplexBand-Pass Filter\nLow Pass Filter ECG  High Pass Filter\nDifferentiation\nAdaptive\nThresholdsMoving W indow\nIntegrationSquaring\nQRSBand-Pass Filter\nFigure 4.2: The flowchart of the Pan-Tompkins algorithm\u2019s pre-processing procedure.\nthrough the PQRST complex (refer to Figure 1). Accurately detecting the R wave within the\nQRS complex is especially critical as it is the most pronounced peak in the complex. Precise\ndetection of the R wave allows for the calculation of heart rate (HR) and heart rate variability\n(HRV), the latter of which measures the time intervals between successive R waves, known\nas the R-R or Interbeat interval. The Pan-Tompkins algorithm, developed in 1985, is one of\nthe most extensively used real-time QRS detection algorithms [262]. Over the years, both the\noriginal algorithm and its variations have been rigorously tested, consistently proving their\neffectiveness even with noisy and low-quality data [263,264]. The Pan-Tompkins Algorithm\nis frequently cited as a benchmark in the field due to its robust performance, making it a stan-\ndard against which new QRS detection methods are compared [265]. Our research employed\nthe original Pan-Tompkins Algorithm to identify the QRS complex. We integrated the algo-\nrithm in two primary phases: preprocessing and decision-making. The preprocessing stage\nis crucial for conditioning the ECG by eliminating noise and artifacts, smoothing the signal,\nand enhancing the QRS slope. The preprocessing steps of the Pan-Tompkins algorithm are\ndepicted in the flow diagram shown in Figure 4.2.56 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nFeature Extraction\nThe subsequent phase involves extracting specific features based on the inter-beat intervals\n(IBIs). In our study, we calculated several metrics, including the mean of IBIs, the root mean\nsquare of successive differences (RMSSD), the standard deviation of IBIs (SDNN), the slope\nof the linear regression of IBIs, the ratio of SDNN to RMSSD, and the heart rate, as outlined\nbelow:\n1. Mean of IBIs\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (4.1)\nwhere RRrepresents consecutive Rpeaks.\n2. Root mean square of successive differences\nRMSSD\u201cgffe1\nn\u00b41n\u00b41\u00ff\ni\u201c1pRRi`1\u00b4RRiq2 (4.2)\n3. Standard deviation of IBIs\nSDNN\u201cd\n1\nn\u00b41n\u00ff\ni\u201c1pRRi\u00b4\u00b5q2 (4.3)\n4. Slope of the linear regression of IBIs\nATAx\u201cATb, (4.4)\nwhere is calculated using the least-square approximation, where bis the vector of RR\npeak intervals and Ais the corresponding time series.\n5. Ratio of SDNN to RMSSD\nSR\u201cSDNN\nRMSSD(4.5)\n6. Heartbeat rate\nHR\u201c60\u00a8FS\n\u00b5, (4.6)\nwhere FSis the sampling frequency of the ECG recording, typically 512Hz. Figure\n4.3 illustrates the raw ECG signal and the algorithm\u2019s stages.4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 57\n0 500 1000 1500 2000 2500\nTime (ms)Raw Signal\n0 500 1000 1500 2000 2500\nTime (ms)Band Pass Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Derivative Filtered\n0 500 1000 1500 2000 2500\nTime (ms)Squared\n500 1000 1500 2000 2500\nTime (ms)Moving Window Averaged\nSignal\nQRS\nNoise Level\nSignal Level\nAdaptive Threshold\nFigure 4.3: The signal preprocessing using the Pan-Tompkins algorithm.\nClassification Methods\nFor the classification phase, three widely recognized classifiers were utilized: Linear Dis-\ncriminant Analysis (LDA), Support Vector Machine (SVM) with a linear kernel, and SVM\nwith a Radial Basis Function (RBF) kernel.\n1. Linear Discriminant Analysis\nPpX|y\u201ckq\u201cexp\u00b4\n\u00b41\n2pX\u00b4\u00b5kqt\u03a3\u00b41\nkpX\u00b4\u00b5kqt\u00af\np2\u03c0qd{2|\u03a3k|1{2, (4.7)\nwhere Pdenotes the probability density function for the feature set X, conditional on\nthe target class y\u201ck.\n2. SVM with linear kernel\nKpx1, x2q\u201cxT\n1x2, (4.8)\nwhere x1andx2represent feature vectors from two separate classes.58 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n3. SVM with Radial Basis Function (RBF) kernel\nKpx1, x2q\u201cexp\u02dc\n\u00b4||x1\u00b4x2||2\n2\u03c32\u00b8\n, (4.9)\nwhere \u03c3is the parameter defining the width of the RBF kernel.\nDataset Details\nIn this study, we utilized the publicly available \u201cBioVid Heat Pain Database\u201d [109], which\ncontains facial videos and biosignals (ECG, EMG, EDA) from 87participants ( 44males and\n43females, aged 20\u00b465). This dataset is unique because it is the only publicly accessible\nresource that includes the subjects\u2019 age and gender. The data collection involved applying\na heat stimulus to the right arm of each participant using a thermode. Prior to recording,\nthe pain threshold (the temperature at which the participant first perceives heat as pain) and\npain tolerance (the temperature at which the pain becomes intolerable) were established for\neach participant. The study defined specific thresholds as the temperatures for the lowest\nand highest pain levels. Also, it included two intermediate levels, resulting in five pain\nconditions: No pain (NP), mild pain (P 1), moderate pain (P 2), severe pain (P 3), and very\nsevere pain (P 4). Each participant was exposed to 20stimulations for each intensity level,\ngenerating 100samples across the four modalities.\n4.2.2 Experiments\nIn the following experiments, we specifically used Part A of the BioVid , which includes\npre-processed ECG samples filtered through a Butterworth band-pass filter, totaling 8700\nsamples ( 87\u02c6100\u201c8700 ). All experiments were conducted in triplicate under identical\nconditions, using a distinct classifier for each iteration to compare their effectiveness. This\nwas based on the leave-one-subject-out (LOSO) cross-validation method, utilizing all avail-\nable subjects and ECG samples. The performance of each classifier was evaluated based on\naccuracy.\nUsing the previously mentioned classification algorithms, we conducted experiments to\nrecognize pain and its relationship with demographic factors. The classification tasks were\nstructured around the pain conditions in multi-class and binary classification formats. Specif-\nically, five distinct experiments were executed: (i)multi-class pain classification, (ii)NP vs.\nP1,(iii)NP vs. P 2,(iv)NP vs. P 3,(v)NP vs. P 4. In experiment (i), the goal was to cate-\ngorize an ECG signal into one of the five pain conditions, while experiments (ii)-(v) aimed\nto classify signals into one of two pain conditions, either no pain or the specified pain level.\nFurthermore, considering the gender and age of the subjects, we developed four different4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 59\nTable 4.1: Results for the Basic Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)AllMC 23.72 23.79 22.77\nNP vs P 1 50.97 52.38 49.97\nNP vs P 2 52.55 52.78 52.70\nNP vs P 3 55.20 55.37 53.87\nNP vs P 4 58.62 58.39 57.41\nMC: multi-classification NP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain\nP4: very severe pain LDA: Linear Discriminant Analysis LN:Linear RBF: Radial Basis\nFunction\nexperimental schemes: (i)theBasic Scheme , utilizing the entire dataset, (ii)theGender\nScheme , where data were segmented by the gender of the subjects into males and females,\n(iii) the Age Scheme , which grouped subjects into three age categories: \u201820-35\u2019 ,\u201836-50\u2019 ,\n\u201851-65\u2019 , and (iv) the Gender-Age Scheme , which combined both demographic factors, result-\ning in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\n\u2018Males 51-65\u2019 ,\u2018Females 51-65\u2019 . The most successful classification results are displayed\nin Figures 4.4-4.5 for each task and classification method, while Tables 4.1-4.5 detail the\noutcomes of each individual experiment.\n4.2.3 Results\nTable 4.1 shows the results from the entire dataset, where the multi-class pain classification\nachieved a 23.79% accuracy, and performance scores generally increased with pain intensity,\npeaking at 58.62% for NP vs. P 4. This progression highlights the difficulty in detecting\nlower levels of pain severity. Regarding the classification algorithms, SVM (linear) was\nmore effective, except for the highest pain level task, where SVM (RBF) was less successful.\nIn the Gender Scheme (see Table 4.2), notable differences were observed between males\nand females. Overall, females showed a 1.12% higher accuracy variation than males, with\nfemales achieving 60.69% in NP vs. P 4over males\u2019 56.07%. This 4.62% increase suggests\nthat females are more sensitive to higher levels of pain than males. Interestingly, in NP vs. P 1\nand NP vs. P 2, males outperformed females by 1.16% and1.78%, respectively. Consistent\nwith the first scheme, SVM (linear) yielded better results in most tasks. Figure 4.4 illustrates\nthe gender differences in classification accuracy.\nIn the Age-Scheme (refer to Table 4.3), the \u201820-35\u2019 age group achieved 25.06% accuracy\nin multi-level classification, compared to 23.27% and22.35% for the \u201836-50\u2019 and\u201851-65\u2019\ngroups, respectively, indicating that age significantly affects pain perception. The nearly 9%60 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.2: Results for the Gender Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)MalesMC 22.13 22.25 20.70\nNP vs P 1 51.53 52.61 47.72\nNP vs P 2 53.12 53.69 52.15\nNP vs P 3 54.94 54.71 51.36\nNP vs P 4 55.28 56.07 51.36FemalesMC 25.11 24.41 23.41\nNP vs P 1 50.23 51.45 49.06\nNP vs P 2 51.62 51.86 51.91\nNP vs P 3 55.98 55.87 55.29\nNP vs P 4 60.17 60.69 59.82\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.4: Results for the Gender Scheme .\ndifference in the NP vs P 4task between the youngest and oldest groups was particularly no-\ntable. Similar to the gender-based results, minor differences in low pain intensities among\nthe age groups became more pronounced as pain intensity increased. Specifically, the vari-\nance ( \u03c32) between the groups in NP vs. P 1was1.38%. At the same time, in the other tasks,\nit increased to 2.44%,6.35%, and 20.42%, respectively, showing that high pain intensities\nare necessary to discern perceptual differences among age groups. Regarding classification\naccuracy, the \u201820-35\u2019 group showed the highest sensitivity, followed by \u201836-50\u2019 and\u201851-\n65\u2019. Regarding classification methods, the SVM (RBF) performed best in the \u201851-65\u2019 group\nacross almost all tasks, while it underperformed in the \u201820-35\u2019 group, suggesting it is better\nsuited for more challenging, separable classes. Figure 4.5 displays the results from the age\nscheme.\nIn the final analysis, we examined the subjects more closely to gain deeper insights into\nthe relationship between pain and the demographic factors of gender and age. As shown4.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 61\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35MC2325BL vs P15352BL vs P25455BL vs P35754BL vs P46067018355370\nMCBL vs P1BL vs P2BL vs P3\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65\n1\nFigure 4.5: Results for the Age Scheme .\nin Tables 4.4-4.5, the \u2018Females 20-35\u2019 group achieved the highest accuracy in multi-class\npain classification at 24.80%, while \u2018Females 51-65\u2019 led in NP vs. P 1with 55.38%, again\nindicating higher pain sensitivity in females. Moreover, \u2018Females 51-65\u2019 followed by \u2018Males\n51-65\u2019 topped the performance in NP vs. P 2, and in NP vs. P 3,\u2018Females 36-50\u2019 surpassed\nthe next best group, \u2018Males 20-35\u2019 , by3.5%. In the final NP vs. P 4task, \u2018Females 20-\n35\u2019excelled with 67% accuracy, whereas \u2018Males 51-65\u2019 had the lowest at 54.50%, marking\nthem as the most and least pain-sensitive groups, respectively. It is noted that sometimes\nclassification accuracy decreases despite increased pain levels ( e.g.,\u2018Females 36-50\u2019 ). This\nmight be attributed to the subjects becoming accustomed to the stimulus over time during\nthe biosignal recording.\nFigure 4.6 illustrates the classification performance of the six groups in the Gender-Age\nScheme . Additionally, Table 7 compares our results with other studies that used ECG signals\nfrom the BioVid database and followed the same evaluation protocol, ensuring an objective\ncomparison. Our study achieved the best classification performance in both the multi-class\nsetting and the NP vs. P 1and NP vs. P 2tasks, with acceptable results in the remaining binary\nclassification tasks.\n4.2.4 Discussion\nWe analyzed ECG biosignals using the Pan-Tompkins algorithm to detect QRS complexes\nand extracted features about inter-beat intervals. We also evaluated three machine learning\ntechniques, assessing their performance in multi-class and binary pain classification across\nvarious pain intensities. We also examined the influence of gender and age on pain percep-\ntion, discovering significant differences: males generally showed lower sensitivity to high\npain levels. Regarding the age factor, significant variations suggest that pain sensitivity tends\nto diminish with age, potentially increasing the risk of further injury. In certain demographic\ngroups, the difference in pain perception exceeded 12%, underscoring the variability of pain\nsensation among individuals.62 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.3: Results for the Age Scheme (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)20-35MC 25.06 24.73 21.96\nNP vs P 1 52.83 52.83 49.90\nNP vs P 2 54.33 53.75 52.75\nNP vs P 3 55.58 56.16 54.66\nNP vs P 4 63.83 63.41 60.7536-50MC 23.27 22.06 23.03\nNP vs P 1 50.34 48.36 50.68\nNP vs P 2 49.13 51.20 50.17\nNP vs P 3 58.10 58.70 58.27\nNP vs P 4 58.10 57.75 55.9451-65MC 21.89 22.07 22.35\nNP vs P 1 52.23 51.87 52.58\nNP vs P 2 52.14 51.69 52.76\nNP vs P 3 53.66 53.39 54.10\nNP vs P 4 54.46 54.19 54.91\nTable 4.4: Results for the Gender-Age Scheme (Males) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Males 20-35MC 23.13 23.20 18.73\nNP vs P 1 52.50 52.83 45.83\nNP vs P 2 54.00 53.50 53.16\nNP vs P 3 56.33 56.50 54.83\nNP vs P 4 60.00 59.00 53.66Males 36-50MC 23.21 22.21 20.92\nNP vs P 1 50.53 50.53 46.42\nNP vs P 2 50.00 51.78 47.50\nNP vs P 3 54.64 56.25 47.32\nNP vs P 4 55.53 56.25 51.96Males 51-65MC 20.06 21.60 19.60\nNP vs P 1 52.66 51.66 50.66\nNP vs P 2 54.00 54.66 51.50\nNP vs P 3 53.00 54.66 51.50\nNP vs P 4 53.33 54.50 49.834.2. ECG ANALYSIS WITH CLASSICAL MACHINE LEARNING 63\nTable 4.5: Results for the Gender-Age Scheme (Females) (1).GroupTaskAlgorithm\nLDA SVM (LN) SVM (RBF)Females 20-35MC 24.73 24.80 23.26\nNP vs P 1 49.83 51.50 52.00\nNP vs P 2 54.50 53.66 46.50\nNP vs P 3 53.50 52.83 49.00\nNP vs P 4 65.83 67.00 62.16Females 36-50MC 23.06 22.73 21.93\nNP vs P 1 48.16 49.33 48.33\nNP vs P 2 48.66 49.83 47.83\nNP vs P 3 57.50 60.00 55.00\nNP vs P 4 59.00 58.83 56.16Females 51-65MC 21.23 21.84 23.92\nNP vs P 1 48.84 49.80 55.38\nNP vs P 2 51.15 48.65 55.96\nNP vs P 3 53.07 53.07 50.96\nNP vs P 4 52.69 55.00 56.34\nTable 4.6:\nComparison of studies utilizing BioVid , ECG signals\nand LOSO validation (1).\nMethod Task Results\nMartinez and Picard [266] NP vs P 4 57.69\nWerner et al. [267]NP vs P 1 48.70\nNP vs P 2 51.60\nNP vs P 3 56.50\nNP vs P 4 62.00\nThiam et al. [235]MC 23.23\nNP vs P 1 49.71\nNP vs P 2 50.72\nNP vs P 3 52.87\nNP vs P 4 57.04\nOursMC 23.79\nNP vs P 1 52.38\nNP vs P 2 52.78\nNP vs P 3 55.37\nNP vs P 4 58.6264 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n015304560\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nBasicGenderAgeGender-AgeSchemes (max scores)BasicGenderAgeGender-AgeMC24242423BL vs P152525252BL vs P253535353BL vs P355555656BL vs P459585959Gender (max scores)MalesFemalesMC2225BL vs P15351BL vs P25452BL vs P35556BL vs P45661018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMalesFemalesAge20-3536-5051-65MC252322BL vs P1535153BL vs P2545153BL vs P3565954BL vs P4645855018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\n20-3536-5051-65Gender-AgeMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65MC232523232224BL vs P1535251495355BL vs P2545552505556BL vs P3575456605553BL vs P4606756595456018355370\nMCBL vs P1BL vs P2BL vs P3BL vs P4\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65\n1\nFigure 4.6: Results for the Gender-Age Scheme .\n4.3 ECG Analysis with Multitask Neural Networks\nIn this section, we build on previous analysis 4.2 that explored variations in pain manifesta-\ntion across different demographic groups using ECG signals. It expands this investigation\nby implementing neural networks as the primary machine learning model and introduces a\nnovel multi-task learning (MTL) neural network. This network leverages demographic infor-\nmation to estimate age and gender in addition to pain levels, aiming to enhance the automatic\npain estimation system.\n4.3.1 Methodology\nThe following method we developed is a neural network-based approach. The feature extrac-\ntion process remains the same as previously described in 4.2.1, utilizing the Pan-Tompkins\nalgorithm for ECG signal processing.\nNeural Network\nThe proposed neural network was designed and trained using two distinct approaches: single-\ntask learning (STL) and multi-task learning (MTL). In the multi-task learning framework, the\nnetwork is simultaneous training for pain estimation and predicting age and/or gender.\nSingle-Task Neural Network: The proposed neural network comprises two components:\nthe encoder, which maps the original feature vectors into a higher dimensional space, and\nthe task-specific classifier. In our design, both the encoder and the classifier utilize fully-4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 65\nTable 4.7: Hyper-parameters used in our approach.\nEpochs Optimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsLabel\nsmoothEMA\n300 AdamW 1e-3 cosine 0.1 50 0.1 \u2713\nconnected (FC) layers, which are defined as follows:\nzipsq\u201cbi`nin\u00ff\nj\u201c1Wijsj for i\u201c1, .., n out, (4.10)\nwhere zirepresents the result of linearly combining the incoming inputs sj, with each input\nbeing weighted by Wijand adjusted by a bias bi. Each layer in the encoder is followed by a\nnonlinear activation function, specifically the rectified linear unit (ReLU), which is defined\nas:\n\u03c3pzq\u201c#\n1, z\u011b0\n0, z\u01030(4.11)\nThe classifier\u2019s layers are connected without nonlinearity, the encoder comprises four fully\nconnected (FC) layers with 256,512,1024 , and 1024 neurons respectively. The classifier\nincludes 2layers with 1024 andnneurons, where nrepresents the number of distinct pain\nclasses being classified. The hyperparameters of the network are detailed in Table 4.7.\nMulti-task neural network: This proposed machine learning method is based on the princi-\nple of sharing representations across related tasks, which helps the model better generalize\nto the primary task of pain estimation in this case. We kept the same encoder and pain\nclassifier in this configuration but introduced two additional auxiliary networks for age and\ngender estimation. The architecture of the proposed multi-task learning (MTL) neural net-\nwork is illustrated in Fig. 4.7. The objective of this network is to simultaneously minimize\nthree different losses. We adopt and expand upon the framework suggested by [268] for the\nmulti-task learning loss, where learned weights are applied to each loss function based on\nthe homoscedastic uncertainty of each task:\nLtotal\u201crew1LPain`w1sc1`rew2LAge`w2sc2`rew3LGender`w3sc3. (4.12)\nHere, Lrepresents the corresponding loss, wdenotes the weights, and care the coefficients\nthat modulate the losses LAgeandLGender to prioritize learning in the pain estimation task.\nIt should be noted that all tasks are treated as classification problems, utilizing cross-entropy66 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nL1\nL2\nL3 L4Feature\nvector\nPainAge\nGender6 x 1256 x 1512 x 11024 x 1 1024 x 1\n1024 x 1n x 1512 x 1256 x 1\nL1L2L1L2L336 x 1\n512 x 1256 x 1\nL1L2L32 x 1\nEncoderMTL LossPain\nEstimation\nFigure 4.7: The proposed MTL network: The sizes of the extracted vectors for the network are\nas follows: for the Pain classifier, n\u02c61, where nis the number of pain estimation\ntasks ( e.g.,2for binary classification, 5for multi-class classification); for the Age\nclassifier, 36\u02c61, where 36represents the possible age values of the subjects; for\nthe Gender classifier, 2\u02c61, corresponding to the two possible gender categories\n(i.e., males and females).\nloss with label smoothing:\nLD\u201c\u00b4\u00ff\n\u03b4PDnout\u00ff\ni\u201c1ppi|x\u03b4qlogrqpi|x\u03b4qs. (4.13)\nHere, Ddenotes the pain database, ppi|x\u03b4q \u201c1\u00b4\u03f5represents the probability of the true\nclass igiven the input x\u03b4, and ppi\u2030i\u03b4|x\u03b4q \u201c\u03f5{pnout\u00b41qis the probability distribution\nacross the other classes. This formulation spreads a small portion \u03f5of the probability across\nclasses other than the true class to implement label smoothing. Furthermore, qpi|x\u03b4qis the\nprobability distribution over the classes ias predicted by the network\u2019s output.\n4.3.2 Experiments\nSimilar to 4.2.2, we utilized the BioVid database, specifically focusing on its ECG signals.\nEmploying a single-task neural network (ST-NN), we conducted an initial series of experi-\nments to assess the impact of demographic factors. Building on the concept we proposed\nin the previous section, we devised five experimental schemes: (i)theBasic Scheme , which\nincluded all subjects from the database; (ii)theGender Scheme , which segregated subjects4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 67\ninto male and female groups; (iii)theAge Scheme , which categorized subjects into three age\ngroups\u2014 \u201820-35\u2019 ,\u201836-50\u2019 , and \u201851-65\u2019 ; and (iv)the\u2018Gender-Age Scheme\u2019 , which combined\nboth demographic factors, resulting in six distinct groups: \u2018Males 20-35\u2019 ,\u2018Females 20-35\u2019 ,\n\u2018Males 36-50\u2019 ,\u2018Females 36-50\u2019 ,\u2018Males 51-65\u2019 , and \u2018Females 51-65\u2019 . All experiments were\nconducted in both binary and multi-class classification formats. Specifically, the binary clas-\nsification tasks were (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3, and (4) NP vs. P 4, and the\nmulti-class task utilized all available pain classifications from the database.\n4.3.3 Results\nDemographic Groups\nTable 4.8 presents the classification results of the Basic Scheme , which utilized all subjects\nin the database. For the multi-class pain classification, we achieved an accuracy of 29.43%,\nwith NP vs. P 1scoring 61.15% and NP vs. P 4reaching 68.82%. These results indicate that\nas pain intensity increases, so performs, highlighting the difficulty in recognizing less severe\npain. According to the Gender Scheme (refer to Table 4.9), notable differences emerge\nbetween males and females, particularly at higher pain intensities. Specifically, in NP vs.\nP4, females achieved an accuracy of 69.48% compared to 66.48% for males, with an overall\nvariance of 1.63% between genders, suggesting that females exhibit higher pain sensitivity.\nFigure 4.8a illustrates these gender-based classification disparities. In the Age Scheme (see\nTable 4.10), the \u201820-35\u2019 age group outperformed the \u201836-50\u2019 and\u201851-65\u2019 groups in NP vs. P 4,\nwith accuracies of 72.58%,66.29%, and 64.91%, respectively. While the differences are less\npronounced at lower pain intensities, this scheme still shows that age significantly impacts\npain perception, particularly among the older population. Figure 4.8b shows the results from\nthe age scheme.\nIn the final scheme, by dividing subjects into more specific groups, we can analyze them\nmore precisely and gain better insights into the relationship between gender, age, and pain\nperception. Table 4.11 reveals that in the NP vs. P 4task, the group \u2018Females 20-35\u2019 reached\nthe highest accuracy of 71.67%, significantly outperforming the \u2018Males 51-65\u2019 group, which\nscored the lowest at 60.67%, marking them as the least sensitive group. This pattern is\nconsistent across the multi-class classification and other pain tasks, with \u2018Females 20-35\u2019\nand\u2018Males 51-65\u2019 exhibiting the highest and lowest accuracies, respectively. This supports\nthat females generally experience more pronounced pain responses, while older males have\na reduced pain sensation. It is noted that in some instances, such as with \u2018Males 20-35\u2019\nand\u2018Males 36-50\u2019 , higher pain levels do not necessarily correlate with higher classification\naccuracy, a phenomenon also noted in our previous experiments, in Section 4.2.3. A possible\nexplanation could be the subjects\u2019 habituation to pain stimuli, especially at lower intensities.\nFigure 4.8c visualizes the performance outcomes of the Gender-Age Scheme .68 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.8: Results for the Basic Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN 61.15 62.87 65.14 68.82 29.43\nST-NN: single-task neural network NP: no pain P1: mild pain P2: moderate pain P3: severe pain P4: very severe\npain MC: multi-classification\nTable 4.9: Results for the Gender Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales ST-NN 60.40 63.24 63.18 66.48 28.61\nFemales ST-NN 60.87 62.15 66.98 69.48 30.59\nTable 4.10: Results for the Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n20-35 ST-NN 61.58 64.08 66.08 72.58 31.07\n36-50 ST-NN 60.52 61.38 64.05 66.29 29.59\n51-65 ST-NN 61.70 60.80 62.50 64.91 27.82\nTable 4.11: Results for the Gender-Age Scheme (2).\nGroup AlgorithmTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nMales 20-35 ST-NN 62.83 62.33 65.50 71.33 29.73\nMales 36-50 ST-NN 61.79 60.00 59.64 64.11 27.14\nMales 51-65 ST-NN 59.50 58.67 57.33 60.67 26.07\nFemales 20-35 ST-NN 63.17 63.17 66.83 71.67 31.53\nFemales 36-50 ST-NN 59.50 61.00 65.83 67.00 29.13\nFemales 51-65 ST-NN 60.96 60.96 59.23 63.27 27.69\nAugmentation of Feature Vectors\nBuilding on the findings from the previous experiments about the impact of demographic\nfactors on pain perception, we explored the practical use of subjects\u2019 demographic data.\nExperiments were conducted using the Single-Task Neural Network (ST-NN) and feature4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 69\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(a) Gender\nAge-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59\nAge-1-1-120-3536-5051-65BL vs P1626162BL vs P2646161BL vs P3666463BL vs P4736665MC313028Gender-Age-1-1-1Males 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65BL vs P1636362606061BL vs P2626360615961BL vs P3666760665759BL vs P4717264676163MC303227292628Gender-1-1-1MalesFemalesBL vs P160,4060,87BL vs P263,2462,15BL vs P363,1867BL vs P466,4869MC28,6130,59020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMales 20-35Females 20-35Males 36-50Females 36-50Males 51-65Females 51-65020406080\nNP vs P1NP vs P2NP vs P3NP vs P4MC\n20-3536-5051-65018355370\nNP vs P1NP vs P2NP vs P3NP vs P4MC\nMalesFemales\n2\n(b) Age\nAge\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-Age\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\nBL vs P1 63 63 62 60 60 61\nBL vs P2 62 63 60 61 59 61\nBL vs P3 66 67 60 66 57 59\nBL vs P4 71 72 64 67 61 63\nMC 30 32 27 29 26 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales 20-35\n Females 20-35\n Males 36-50\n Females 36-50\n Males 51-65\n Females 51-65\nGender\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales\n Females\nAge-1\n20-35 36-50 51-65\nBL vs P1 62 61 62\nBL vs P2 64 61 61\nBL vs P3 66 64 63\nBL vs P4 73 66 65\nMC 31 30 28020406080\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\n20-35\n 36-50\n 51-65\nGender-1\nMales Females\nBL vs P1 60,40 60,87\nBL vs P2 63,24 62,15\nBL vs P3 63,18 67\nBL vs P4 66,48 69\nMC 28,61 30,59018355370\nBL vs P1 BL vs P2 BL vs P3 BL vs P4 MC\nMales\n Females\n020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\n20-35 36-50 51-65018355370\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales Females020406080\nNP vs P1 NP vs P2 NP vs P3 NP vs P4 MC\nMales 20-35 Females 20-35 Males 36-50 Females 36-50 Males 51-65 Females 51-65\n1\n(c) Gender-Age\nFigure 4.8: Results for the proposed Schemes.\nvectors enhanced with demographic attributes. Initially, the feature vectors, which originally\nconsisted of six features (see 4.3.1), were augmented by adding either one additional fea-\nture ( i.e., the subject\u2019s gender or age) or two additional features (both the subject\u2019s gender\nand age). We conducted the same pain estimation tasks using this enhanced set of features.\nAs shown in Table 4.12, the results demonstrate improved performance with the augmented\nfeature vectors. Specifically, the most effective augmentation involved combining gender70 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\nTable 4.12: Comparison of results adopting the feature augmentation approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAux: Auxiliary information -: original feature vectors F(G): feature vectors with the additional feature of gender F(A): feature\nvectors with the additional feature of age F(GA): feature vectors with the additional features of gender and age\nand age features, which increased the average pain estimation performance by 0.55%. Us-\ning these demographic features individually also improved classification accuracy, albeit\npartially.\nMulti-Task Neural Network\nThe final experiments utilized a multi-task learning framework with the proposed Multi-Task\nNeural Network (MT-NN) outlined in 4.3.1. The classification results for MT-NN, incorpo-\nrating additional tasks of (1) gender estimation, (2) age estimation, and (3) simultaneous\ngender and age estimation, are detailed in Table 4.13. For comparison, the results from ear-\nlier experiments using the Single-Task Neural Network (ST-NN) method are also included\nin Table 4.13. We noted that the task of gender estimation alone performed less effectively\nthan the other tasks. In contrast, the combined gender and age estimation delivered the high-\nest performance across four tasks. Specifically, in the multi-class classification, it achieved\n30.24%, and in NP vs. P 1, it reached 62.8%, marking the best results of any method pre-\nsented in this study. In NP vs. P 3and NP vs. P 4, the combined tasks outperformed the\nindividual gender and age tasks but were slightly less effective than the ST-NN approaches\nusing augmented features. Interestingly, in NP vs P 2, the age estimation task alone excelled,\nachieving 63.97%, the highest result recorded in this study.\nComparing the overall performances of MT-NN with the ST-NN approaches (using both\noriginal and augmented feature vectors), there is a noticeable improvement of 0.71% and\n0.39%, respectively, in average pain estimation accuracy across all tasks. Figure 4.9 visually\ncompare each neural network approach used in this study, encompassing multi-class and\nbinary classification tasks.4.3. ECG ANALYSIS WITH MULTITASK NEURAL NETWORKS 71\nTable 4.13: Comparison of results adopting the MT-NN approach.\nGroup Algorithm Aux.Task\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAll ST-NN - 61.15 62.87 65.14 68.82 29.43\nAll ST-NN F(G) 61.44 63.19 65.00 68.79 29.68\nAll ST-NN F(A) 61.21 62.67 65.66 69.57 29.71\nAll ST-NN F(GA) 61.09 63.48 66.21 69.54 29.86\nAll MT-NN T(G) 61.72 63.39 65.95 68.99 30.00\nAll MT-NN T(A) 62.72 63.97 65.40 69.28 29.79\nAll MT-NN T(GA) 62.82 63.68 66.12 69.40 30.24\nT(G): MT-NN with the additional task of gender estimation T(A): MT-NN with the additional task of age estimation T(GA):\nMT-NN with the additional task of gender and age estimation\nComparison with Existing Approaches\nIn this section, we benchmark the results achieved using the Multi-Task Neural Network\n(MT-NN), which incorporated additional tasks of gender and age estimation against relevant\nstudies. These comparative studies also utilized electrocardiography signals from Part A of\ntheBioVid database with all 87participants. To ensure a fair comparison, they followed the\nsame evaluation protocol, specifically the leave-one-subject-out (LOSO) cross-validation.\nThe comparative results are detailed in Table 4.14 and include research that employed hand-\ncrafted features with traditional machine learning algorithms [35] [267], end-to-end deep\nlearning models [269] [235], and finally, hybrid approaches combine hand-crafted features\nwith deep learning classifiers [266]. Our approach, which leverages hand-crafted engineered\nECG features and a high-dimensional mapping from the encoder in combination with multi-\ntask learning neural networks, demonstrated superior performance across all pain estimation\ntasks, whether in binary or multi-class classification settings.\n4.3.4 Discussion\nWe explored multi-task learning neural networks for automatic pain estimation from electro-\ncardiography signals. By implementing the Pan-Tompkins algorithm to identify QRS com-\nplexes, we extracted features associated with inter-beat intervals (IBIs). Numerous experi-\nments were conducted to explore how gender and age influence pain perception, highlighting\ntheir significant impact. Additionally, we introduced two approaches to enhance pain esti-\nmation results by leveraging demographic information. Firstly, we augmented the original\nfeature vectors by incorporating the subjects\u2019 demographic data, improving classification\naccuracy. Secondly, we employed a multi-task learning neural network that combined the72 CHAPTER 4. DEMOGRAPHIC V ARIABLES: THEIR ROLE AND IMPACT\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(a) Binary classification\n29,5529,830,0530,3\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nMC63,2565,567,7570\nST-NN (-)ST-NN (G)ST-NN (A)ST-NN (GA)MT-NN (G)MT-NN (A)MT-NN (GA)\nNP vs P1\nNP vs P2\nNP vs P3\nNP vs P4\n1\n(b) Multi-class classification\nFigure 4.9: Comparison of performances utilizing various neural networks approaches.\nTable 4.14: Comparison of studies utilizing BioVid , ECG signals and LOSO validation (2).\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nGkikas et al. [35]:52.38 52.78 55.37 58.62 23.79\nHuang et al. [269]\u2039d- - - 65.00 28.50\nMartinez and Picard [270]\u00b8- - - 57.69 -\nThiam et al. [235]\u203949.71 50.72 52.87 57.04 23.23\nWernel et al. [267]:48.70 51.60 56.50 62.00 -\nThis study\u00b862.82 63.68 66.12 69.40 30.24\n::hand-crafted features and classic machine learning \u2039: end-to-end deep learning \u00b8: hand-crafted features with deep\nlearning classification algorithms d: pseudo heart rate gain extracted from visual modality\ntasks of pain, gender, and age estimation. This approach yielded superior results compared\nto methods previously discussed in this chapter and other related research. These findings\nindicate that domain-specific features can achieve excellent outcomes when combined with\nwell-designed deep-learning architectures and demographic factors.4.4. SUMMARY 73\n4.4 Summary\nIn this chapter, we examine the impact of age and gender on pain perception using ECG\nsignals to extract relevant features. Our study involved a series of experiments where subjects\nwere categorized into different groups based on gender (males and females) and age (20-35,\n36-50, and 51-65 years). Additionally, we created combined groups that segregated age\ngroups within each gender. Our findings from both approaches provided strong evidence\nof significant differences in pain perception among these groups. Notably, we observed\na12.5%disparity in pain sensitivity between young females and older males. Generally,\nour results confirm that females exhibit higher pain sensitivity than males, aligning with\nfindings from other studies in pain research. A critical discovery from our study is that pain\nsensitivity appears to decrease with age, which may increase the risk of unnoticed injuries.\nWe presented two methods of incorporating demographic information into our models from a\ncomputational perspective. First, we augmented the feature vectors derived from ECGs with\ndemographic data. Second, we utilized a multi-task neural network approach to estimate\npain, gender, and age simultaneously. Both methods demonstrated improved performance\ncompared to the standard approach, indicating that integrating demographic information can\nenhance the accuracy of automatic pain assessment systems.\nWe recommend that clinical pain assessment tools be designed for specific demographic\ngroups to account for the distinct ways pain manifests across different populations. Ad-\nditionally, we emphasize to researchers developing new pain databases the importance of\nincluding demographic factors and information on the social context and psychological con-\nditions of subjects to enhance the quality and applicability of the data collected. Our study\nfocused on analyzing pain sensation through biosignals, specifically ECGs. We propose that\nfurther research should explore pain expressivity through visual mediums such as video. As\npreviously discussed in Section 2, the expression of pain is a crucial and complex issue. Peo-\nple vary in expressiveness; for various reasons, they might exaggerate or even feign pain,\nmaking accurate assessment challenging.74Chapter 5\nOptimization: Balancing Efficiency and Per-\nformance\nContents\n5.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 75\n5.2 Video Analysis with Vision Transformers . . . . . . . . . . . . . . . . . . . . . 77\n5.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 77\n5.2.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79\n5.2.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\n5.2.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 83\n5.3 Video &Heart Rate Analysis with Transformer Architectures . . . . . . . . . . 84\n5.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 84\n5.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92\n5.3.4 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99\n5.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101\n5.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the studies published in [37, 38]. As outlined in\n3.6.3, research in automatic pain assessment has rarely considered real-world situations. For\nexample, [231] implemented their study in an emergency triage setting, while [234] tested\ntheir approach within IoT devices. Additionally, we highlighted that the scarcity of stud-\nies exploring pain estimation in real-world settings or unconventional contexts suggests that\ncurrent methodologies might not be entirely suitable for practical environments like clinics\nor hospitals due to issues with generalization or operational factors such as efficiency and\ninference time. For these reasons, this chapter\u2019s objective is to explore approaches that (i)\n7576 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nutilize modalities readily available and applicable in the market and (ii)examine the impact\nof model size and computational cost on performance. In this context, our methodologies\nand experiments exclusively utilize RGB videos, a universally available modality, particu-\nlarly on mobile devices. Additionally, we incorporate heart rate data, which is commonly\naccessible from various types of wearable technology. It is important to note that although\nwe employ established pain datasets in our experiments, the videos and heart rate data ex-\ntracted from ECGs are akin to those that could be obtained from smartphones and wearables,\nserving as a proof of concept for application in real-world environments. Furthermore, with\nrespect to efficiency and the speed of inference, our goal is to develop the most compact pain\nassessment frameworks possible, ensuring they maintain adequate performance levels.\nNumerous studies highlight the capabilities of automated systems that utilize behavioral\nor physiological modalities for pain assessment [271]. Sario et al. [34] demonstrate the fea-\nsibility of accurately detecting and quantifying pain through facial expressions, establishing\ntheir value in clinical settings. The use of multimodal sensing appears especially promising,\noffering increased accuracy in pain monitoring systems [22]. An important aspect of pain\nmonitoring involves wearable devices that record biopotentials to estimate pain levels. Few\nstudies have investigated the use of mainstream, wearable technology for this purpose, possi-\nbly due to a research preference for more costly, highly precise medical equipment. Leroux\net al. [32] state, \u201cThe challenge is not whether wearable devices will provide useful clinical\ninformation but rather when we will start to use them in practice to enhance the field of pain. \u201d\nAdditionally, Claret et al. [272] explore the potential of using cardiac signals from wearable\nsensors for automatic emotion recognition, confirming the effectiveness of such methods.\nIn this chapter, our deep learning approaches are founded on transformer-based archi-\ntectures. Convolutional Neural Networks (CNNs) have been the cornerstone of mainstream\nneural architectures in computer vision (CV), especially in the field of automatic pain as-\nsessment using images and videos, as we discussed in Section 3. Inspired by the success\nof transformer architecture in natural language processing (NLP), where the self-attention\nmechanism is a fundamental element [273], researchers have developed similar models for\nvisual tasks. The introduction of Vision Transformers (ViT) [274] has established a new\nparadigm in the computer vision domain. This has led to a plethora of new approaches based\non ViT, such as the Transformer in Transformer (TNT) [275], which enhances local feature\nrepresentation by subdividing image patches into smaller sub-patches. While transformer-\nbased models have shown impressive results and offer great flexibility, they tend to scale\npoorly with input size and incur higher computational costs due to the self-attention layers\nthat compute interactions between all input pairs. Efforts to mitigate these challenges in-\nclude replacing self-attention with cross-attention [276] or combining both techniques [277]\nto improve the efficiency and reduce the complexity of these architectures.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 77\nFigure 5.1: The application of face alignment illustrates landmarks in 2D (left) and 3D (right) space.\n5.2 Video Analysis with Vision Transformers\nWe introduce a framework incorporating a vision transformer as a module extracting spatial\nfeatures for individual video frames, combined with a transformer-based model equipped\nwith cross and self-attention blocks, extracting temporal features from the video feature\nsequences. This configuration enables the effective utilization of the temporal dimensions of\nvideo data to deliver more accurate and reliable estimation of the continuous nature of pain.\n5.2.1 Methodology\nThis section outlines the preprocessing methods employed, the design of our framework, the\nimplementation details concerning the training procedure, and the database used.\nPre-processing\nBefore processing videos for pain estimation, applying face detection and alignment was\ncrucial to enhance performance and computational efficiency. We utilized the well-known\nface detector MTCNN [278] in combination with the Face Alignment Network (FAN) [279],\nwhich leverages 3D landmarks. This 3D approach is critical for addressing our specific chal-\nlenges, as head movements tend to increase, particularly during instances of high-intensity\npain, which can lead to inaccurate alignments with 2D methods. Additionally, it should be\nnoted that all experiments were carried out using video frames with a resolution of 224\u02c6224\npixels. Figure 5.1 illustrates the facial alignment process applied to a video frame.\nTransformer-based Framework\nOur framework is composed of two primary components: the \u201cspatial feature extraction\nmodule\u201d , specifically a TNT (Transformer in Transformer) model, and the \u201ctemporal fea-78 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nture extraction module\u201d , which is a transformer with both cross and self-attention blocks.\nThis framework, illustrated in Figure 5.2, includes approximately 24million parameters and\nperforms operations at 4.2GFLOPS.\nSpatial feature extraction module :Each frame is initially divided into npatches represented\nasFk\u201crF1\nk, F2\nk, . . . , Fn\nksPRn\u02c6p\u02c6p\u02c63, where p\u02c6pdenotes the resolution of each patch ( i.e.,\n16\u02c616) and 3represents the number of color channels. These patches are then subdivided\nintomsub-patches to facilitate the model\u2019s learning of global and local features. Each frame\nis thus transformed into a sequence of patches and sub-patches:\nFk\u00d1rFk,n,1, Fk,n,2, . . . , F k,n,ms, (5.1)\nwhere Fk,n,mPRs\u02c6s\u02c63is the m-th sub-patch of the n-th patch of the k-th frame, with each\nsub-patch having a resolution of s\u02c6s(i.e.,4\u02c64). Following this, the patches and sub-\npatches undergo a linear projection and are transformed into embeddings ZandY. Position\nembeddings are then added to retain spatial information:\nZ0\u00d0Z0`Epatch, (5.2)\nwhere Epatchare the position encodings for the patches. Correspondingly, for each sub-patch\nwithin a patch, a position encoding is also added:\nY0\ni\u00d0Y0\ni`Esub-patch , (5.3)\nwhere Esub-patch are the sub-patch position encodings and i\u201c1,2, . . . , m denotes the index of\na sub-patch. These sub-patches are then processed through an \u201cInner Transformer Encoder\u201d ,\nwhich consists of two multi-head self-attention blocks, crucial for dot product attention. The\nattention mechanism is defined as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n, (5.4)\nwhere QPRM\u02c6D, KPRM\u02c6C,andVPRM\u02c6C(Mis the input dimension, CandDare\nchannel dimensions) are projections of the corresponding input and represent the Query, Key,\nand Value matrices. They defined as Q\u201cXW Q,K\u201cXW K, and V\u201cXW V, where W\nare the learnable weight matrices and Xis the input. The output embedding from the \u201cInner\nTransformer Encoder\u201d is then added to the patch embedding and forwarded to the \u201cOuter\nTransformer Encoder\u201d . This encoder comprises three multi-head self-attention blocks, and\nits output is a feature vector d\u201c192. The \u201cspatial feature extraction module\u201d as a whole\nencompasses a depth of 12blocks.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 79\nTable 5.1: Training details for the automatic pain assessment.\nEpochs Optimizer Learning\nRateLR decay Weight\ndecayWarmup\nepochs\n200 AdamW 1e-4 cosine 0.1 5\nLabel\nSmoothingDropPath Attention\nDropOutLoss\nfunctionAugmentation methods\n0.1 0.1 0.1 Cross\nEntropyAugMix [281] &\nTrivialAugment [282]\nDropPath applied to the \u201cspatial feature extraction module\u201d , Attention DropOut applied to the \u201ctemporal\nfeature extraction module\u201d\nTemporal feature extraction module :The extracted embeddings of each input video frame\nare concatenated into a unified vector D, representing the entire video as V\u00f1D\u201c\npd1\"d2\", . . . , dkq. This vector is then processed through the temporal module, a transformer\narchitecture consisting of 1cross-attention and 2self-attention mechanisms, each followed\nby a fully connected neural network (FCN). The introduction of cross-attention, which em-\nploys asymmetry in the attention mechanism, helps reduce computational complexity and\nincrease the model\u2019s efficiency. Specifically, rather than projecting the input with dimen-\nsions M\u02c6D, theQin cross-attention is a learned matrix with dimensions N\u02c6D, where\nN\u0103M. This module\u2019s self-attention components function as detailed in Equation 7.3, with\nthe cross and self-attention units comprising 1and8heads, respectively. In addition, we\nincorporate Fourier feature position encoding [277].\nTraining Details: Before starting the automatic pain estimation training process, we pre-\ntrained the \u201cspatial feature extraction module\u201d using the VGGFace2 dataset [280], incorpo-\nrating over three million facial images from more than nine thousand individuals. Table 7.3\ndetails the hyperparameters of our method and the applied augmentation techniques.\nDatabase Details: For this approach, we used the publicly available BioVid dataset [109],\nas described in the previous chapters.\n5.2.2 Experiments\nIn this section, we detail the experiments conducted for pain estimation. Our experiments\nwere carried out in both binary and multi-level classification formats. Specifically, we con-\nducted binary classification tasks: (i)NP vs. P 1,(ii)NP vs. P 2,(iii)NP vs. P 3,(iv)NP vs. P 4,\nand(v)a multi-level pain classification utilizing all available pain classes from the database.\nWe employed the leave-one-subject-out (LOSO) cross-validation method as our evaluation80 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nprotocol. Additionally, the classification metrics used in this study include micro-average ac-\ncuracy, macro-average precision, macro-average recall (sensitivity), and macro-average F1\nscore.\n5.2.3 Results\nPain Estimation\nIn evaluating pain estimation tasks, we noted the following results: For the NP vs. P 1task,\naccuracy reached 65.95%, with precision almost identical at 65.90%. The F1 score was\nFigure 5.2: An overview of our proposed transformer-based framework for automatic pain as-\nsessment.5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 81\nTable 5.2: Results on the pain estimation tasks.\nMetricTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nAccuracy 65.95 66.87 69.22 73.28 31.52\nPrecision 65.90 66.89 69.18 73.31 31.48\nRecall 67.85 68.34 70.84 74.75 29.94\nF1 65.04 66.19 68.54 72.75 27.82\nNP: no pain P 1: mild pain P 2: moderate pain P 3: severe pain P 4: very severe pain MC: multi-level\nclassification\nslightly lower at 65.04%, and the recall stood out at 67.85%. In the NP vs. P 2task, accuracy\nincreased to 66.87%, and all related metrics improved, with the F1 score climbing by over\n1.15%, highlighting enhanced detection of true positives. The results were notably better\nfor the NP vs. P 3task, with an accuracy of 69.22% and a sensitivity of 70.84%. This is\nexpected as the pain at this level is considered severe, eliciting more pronounced responses\nfrom subjects. In the highest pain task, NP vs. P 4, the recall was particularly high at 74.75%,\nwith an accuracy of 73.28%, demonstrating that the detection of very severe pain is relatively\nmore straightforward due to the pain reaching tolerance thresholds, making it more visibly\nevident through subjects\u2019 facial expressions. However, in the multi-level classification task,\nperformance metrics were lower, illustrating the complexity of estimating all pain levels\nconcurrently; accuracy was only 31.52%, with a recall of 29.94%, pointing to significant\nchallenges in accurately identifying true positives across multiple pain levels.\nIt should be noted that our framework, encompassing both the architectural and procedu-\nral aspects of training, was consistent across all binary and multi-level classification tasks.\nThis was done to evaluate the generalization potential of our approach across all possible\nscenarios provided by the database, akin to real-world clinical settings. The detailed classifi-\ncation outcomes are presented in Table 5.2.\nVideo Sampling\nIn this section, we explore the impact of video frame sampling on automatic pain estimation.\nExperiments detailed in Section 5.2.3 utilized all available frames ( 138) from each video.\nSubsequent experiments employed frame sampling with strides of 2,3, and 4. Starting with\nall138frames, the video feature representation Dhas dimensions 138\u02c6192, totaling 26,496.\nA stride of 2reduces this to 69frames, with Dhaving dimensions 69\u02c6192and totaling\n13,248. With strides 3and4, the frame counts reduce to 46and35, resulting in Dsizes82 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.3: Results for the pain estimation tasks using various numbers of input frames.\nNumber of\nFramesTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\n138 65.95 66.87 69.22 73.28 31.52\n69 65.76 66.74 69.15 73.25 31.29\n46 65.66 66.70 68.50 71.78 31.20\n35 65.40 66.12 68.32 72.01 30.80\nFigure 5.3: The impact of the number of input frames on accuracy (left) and on runtime in\nmilliseconds (right). Runtime calculated during inference on a NVIDIA RTX-3090 .\nof8,832and6,720, respectively. Table 5.3 displays the classification accuracies achieved\nwith these varying frame counts for each pain estimation task. Concurrently, Figure 5.3\ndemonstrates how the number of frames affects mean accuracy across the five tasks and\nmean runtime during inference. We noted a performance increase of approximately 1.38%\nwhen using 138frames compared to 35frames. Additionally, the runtime increased by a\nfactor of 3. Despite the longer runtime, each sampling rate allows for real-time automatic\npain estimation when necessary.\nInterpretation\nResearch in deep learning, particularly relevant to healthcare, increasingly focuses on model\ninterpretability to explain decision-making processes. This is crucial for enhancing the trans-\nparency of models, a key factor for their acceptance and integration into clinical settings. In\nour study, we implemented the technique described in [283] to generate relevant maps illus-\ntrating which facial areas our model\u2014the \u201cspatial feature extraction module\u201d \u2014focuses on.\nAs shown in Figure 5.4, the model\u2019s attention is distributed over \u201carbitrary\u201d areas at the on-\nset of a facial expression sequence. However, as the expression of pain intensifies, the focus\nsharpens on specific regions indicative of pain. It is important to note from our relevance5.2. VIDEO ANALYSIS WITH VISION TRANSFORMERS 83\nFigure 5.4: Relevance Maps.\nmaps that no universal facial expressions are unique to pain. However, there is a noticeable\nconcentration on areas like the mouth and eyes.\nComparison with existing methods\nIn this section, we present a comparison of our results achieved using a transformer-based\nframework that utilizes all available video frames against other studies that also employed\nPart A of the BioVid database with all 87subjects, following the same leave-one-subject-out\n(LOSO) cross-validation protocol. This ensures objective and accurate comparisons, with\nresults detailed in Table 5.4. The studies compared fall into three main categories: i) those\nfocusing exclusively on pain detection (NP vs. P 4),ii) those examining both pain detection\nand multi-level pain estimation, and iii) those that cover all major pain-related tasks.\nOur method, tested across all tasks, recorded the highest performance metrics in bi-\nnary and multi-level pain estimations. Studies limited to specific aspects of pain detec-\ntion or multi-level pain estimation often yielded comparable or superior results, as indicated\nin [219], [180], and [284]. This highlights that while focused studies often show high perfor-\nmance, the broader impact lies in developing systems that perform well across all potential\nscenarios.\n5.2.4 Discussion\nThis research examined the application of transformer-based architectures for automatic pain\nestimation through video analysis. Our framework employed exclusively transformer mod-\nels, leveraging the spatial and temporal aspects of the video frames. The experiments demon-\nstrated the effectiveness of our approach in assessing pain, showing strong generalization\nacross various pain estimation tasks with notable results, particularly with low-intensity pain\nwhere facial expressions are less apparent. Additionally, the framework demonstrated high\nefficiency, suitable for real-time applications. A significant contribution of our work includes\ndeveloping relevance maps highlighting facial areas the model focuses on. We advocate for\ncontinued efforts within the affective computing field to enhance the interpretability of these\ndeep-learning methods.84 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.4: Comparison of studies utilizing BioVid , RGB videos, and LOSO validation.\nStudyTask\nNP vs P 1 NP vs P 2 NP vs P 3 NP vs P 4 MC\nThiam et al. [219] - 69.25 -\nTavakolian et al. [180] - - - 71.02 -\nPatania et al. [284] - - - 73.20 -\nHuang et al. [269] - - - 77.50 34.30\nXinet al. [141] - - - 86.65 40.40\nZhi and Wan [217] 56.50 57.10 59.60 61.70 29.70\nWerner et al. [248] 53.30 56.00 64.00 72.40 30.80\nOur approach 65.95 66.87 69.22 73.28 31.52\n5.3 Video & Heart Rate Analysis with Transformer Architectures\nWe introduce a proof of concept for an automatic pain assessment framework that integrates\nfacial video data captured by an RGB camera with heart rate signals. We build and extend\nour previous analysis in 5.2. Our main objectives include (1) evaluating the effectiveness and\nlimitations of video and heart rate data as standalone modalities in an unimodal setting, (2)\nexploring the efficacy of combining behavioral (video) and physiological (heart rate) markers\nto overcome challenges associated with their reliance on different sensing technologies and\ninformation representations, and (3) analyzing the performance and efficiency of recently\nintroduced transformer-based architectures.\n5.3.1 Methodology\nThis section details the preprocessing methods for video and ECG, the design of the proposed\nframework, the augmentation techniques developed, and the specifics of implementing the\npretraining process.\nPre-processing\nPreparatory processing was essential before feeding data into the pain assessment framework,\nparticularly for the raw ECG data used to compute heart rate. We focused on exploring heart\nrate as the primary feature due to its benefits: It\u2019s readily obtainable from wearable devices,\ncost-effective, and easily accessible. These advantages position heart rate as a potentially\nvaluable feature for automated pain assessment.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 85\nVideo Preprocessing: Video preprocessing included face detection to isolate the facial re-\ngion using the MTCNN face detector [278], which employs multitask cascaded convolu-\ntional networks to predict facial and landmark locations. Predicting landmarks facilitates\nface alignment, which is crucial for accurate facial analysis. However, we observed that\nface alignment reduced the expressiveness linked to head movements, a common behavioral\nindicator of pain. Consequently, face alignment was omitted from our proposed pipeline.\nAdditionally, the resolution of frames post-face detection was standardized at 448\u02c6448\npixels.\nECG preprocessing & analysis: Similar to Section 4, we utilize the Pan-Tompkins [262] al-\ngorithm to detect the QRS complex, the most prominent wave complex in an ECG signal.\nThis algorithm operates in two phases: preprocessing and decision-making. The preprocess-\ning phase focuses on noise removal, artifact elimination, signal smoothing, and enhancing\nthe QRS slope. The decision-making phase involves initial QRS detection using adaptive\nthresholds, a retrospective search to identify any missed QRS complexes, and a method for\ndistinguishing T waves. Following the accurate detection of R waves, the estimation of inter-\nbeat intervals (IBIs) was conducted, leading to the extraction of key features. Specifically,\nwe calculated the mean of the IBIs as follows:\n\u00b5\u201c1\nnn\u00ff\ni\u201c1pRRi`1\u00b4RRiq, (5.5)\nwhere nis the total number of IBIs, and RRidenotes the consecutive R\u00b4Rintervals.\nSubsequently, the heart rate was calculated using the following formula:\nHR\u201c60\u00a8FS\n\u00b5, (5.6)\nwhere FSdenotes the sampling frequency of the ECG recording.\nFramework architecture\nThe proposed framework, as illustrated in Figure 5.5, consists of four key components: the\nSpatial-Module that extracts embeddings from video data, the Heart Rate Encoder which\nmaps heart rate signals into a higher dimensional space, the AugmNet that generates aug-\nmentations in the latent space, and the Temporal-Module performs with the final assessment\nof pain.\nSpatial-Module: The architecture for this module draws inspiration from the Transformer\nin Transformer approach as detailed by [275]. The process begins with the initial video86 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(a) Video analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(b) ECG analysis pipeline.\n1\u03c7 \nheads \nFCN\nCross-A ttention\nHeart Rat e Encoder  \n1x \nBicubic \nInterpolation\nECG\n=\n+\nCross-A ttention\nFCN\nFCN\nSelf -Attention\nTemporal - Module  \n1x \nPain Estimation\nTilek,4\n...\nPatch1\nPatch2\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nTilek,3\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nTilek,2\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\n...\nPatchn\nSub-pat chn,m\nTilek,1\nPatch1\nSub-pat ch1,m\nPatch2\nSub-pat ch2,m\nPatchn\nSub-pat chn,m\n...\nFull  \nFramek\nPatch1\nPatch2\n...\nPatchn\nSub-pat ch1,m\nSub-pat ch2,m\nSub-pat chn,m\nVideo\n...\n...\n...\nTilek,4\n...\nTilek,3\n...\nTilek,2\n...\nTilek,1\n...\nFull Framek\n...\nConcatenation\nEmbeddings\n\u22c5c \nLinear Projection\nPatch + Position Embedding\nInner \nTransformer \nEncoder\nInner \nTransformer \nEncoder\nOut er Transformer Encoder\nInner \nTransformer \nEncoder\nSpatial - Module \n12x\n4x  \nheads\n10x  \nheads\n...\n=\n+\n(c) Fusion analysis pipeline.\nFigure 5.5: Outline of the proposed framework.5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 87\nframe at a resolution of 448\u02c6448pixels, segmented into 4quadrants, each at 224\u02c6224\npixels. This tiling method, which maximizes the utilization of the frame\u2019s resolution, is\ninfluenced by approaches seen in satellite imaging analysis. Our framework incorporates the\n4tiles and the original frame\u2014resized to 224\u02c6224pixels\u2014into our analysis pipeline. Thus,\neach video frame transforms into 5distinct images, denoted as:\nFk\u201crFk,1, Fk,2, . . . , Fk,ts, (5.7)\nwhere kstands for the frame number, and tencompasses the tile count, including the resized\nfull frame. Each tile is initially split into npatches, expressed as:\nFk,t\u201crFk,t,1, Fk,t,2, . . . , Fk,t,nsPRn\u02c6p\u02c6p\u02c63, (5.8)\nwhere p\u02c6pspecifies the resolution of each patch ( 16\u02c616), and 3denotes the RGB channels.\nThese patches are further segmented into msub-patches, allowing the model to capture the\nimage\u2019s global and localized features. Each tile from a frame thus transitions into a sequence\nof patches and sub-patches, represented as\nFk,t\u201crFk,t,n, 1, Fk,t,n, 2, . . . , Fk,t,n,ms.\nConsequently, each video frame is characterized by:\nFk\u00d1\u201c\nFk,t,n,m|tPr1,5s, nPr1,196s, mPr1,16s\u2030\n, (5.9)\nwhere Fk,t,n,mPRs\u02c6s\u02c63defines the m-th sub-patch within the n-th patch of the t-th tile\nfor the k-th frame, with each sub-patch having a resolution of s\u02c6s(4\u02c64). Each frame\nconsists of 5image representations, encompassing 196patches, and each patch contains 16\nsub-patches. The patches and sub-patches are then linearly projected into embeddings Zand\nY. Positional embedding is applied to maintain spatial information, employing 1D learnable\nposition encodings:\nZ0\u00d0Z0`Epatch, (5.10)\nwhere Epatch indicates the position encoding. Each sub-patch also receives its specific posi-\ntional encoding:\nYi\n0\u00d0Yi\n0`Esub\u00b4patch, (5.11)\nwhere Esub\u00b4patch denotes the positional encodings for sub-patches, and irepresents the index\nof a sub-patch within a patch. The sub-patches are processed in the Inner Encoder , which88 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nconsists of four self-attention heads [285], utilizing dot product attention:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dkV\u02d9\n. (5.12)\nThe output from the Inner Encoder integrates into the patch embedding, advancing to the\nOuter Encoder , which mimics the Inner Encoder with ten self-attention heads. The Spatial-\nModule consists of twelve parallel blocks, generating embeddings of dimensionality d\u201c100.\nFor each input video frame, 5distinct output embeddings of dimensionality 100are produced\nand combined to form a comprehensive frame representation:\nD\u201cdFullFrame`pdTile1`dTile2`dTile3`dTile4q\u00a8c, DPR100, (5.13)\nwhere cadjusts the contribution from the tile embeddings, subsequently, the embedding for\neach frame, D, is concatenated with those of other frames to construct a complete video\nrepresentation:\nVD\u201crD1}D2}. . .}Dfs, VDPRN, (5.14)\nwhere frepresents the total number of frames in the video, and Ndenotes the dimensionality\nof the final video embedding.\nHeart Rate Encoder: As mentioned in Section 5.3.1, heart rate is computed from the origi-\nnal ECG every second, producing a heart rate vector of length h\u201c\u03b8for recordings lasting\n\u03b8seconds. It should be noted that when beats per minute (BPM) fall below 60within any\n1-second segment of the ECG, making direct heart rate calculation impractical, the method\naverages heart rates from the data points immediately before and after to maintain a uniform\nset of \u03b8data points. The Heart Rate Encoder , which is part of a transformer-based archi-\ntecture similar to the Inner andOuter Encoders , utilizes one cross-attention head instead\nof self-attention followed by a fully connected neural network (FCN). This use of cross-\nattention introduces an asymmetry that reduces computational load and increases efficiency.\nUnlike traditional input projections that are M\u02c6Din dimension, as detailed in Section\n5.3.1, the Qmatrix in cross-attention is a learnable matrix sized N\u02c6Dwhere N\u0103M. The\nencoder\u2019s internal embeddings are set to a dimensionality of 512and contain only a single\nblock depth. Fourier feature position encodings [277] are also implemented to handle posi-\ntional information. The main goal of this encoder is to transform the initial heart rate vector\nhinto a more complex and richer feature space,\nhPR\u03b8\u00d1EhPR2048,5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 89\nwhere Ehrepresents the enhanced output embedding of this encoder. In the next step, the\noutput from the heart rate encoder is expanded dimensionally via a bicubic interpolation\nmodule. This process enhances the original heart rate\u2019s feature representation, allowing it to\nintegrate smoothly with the video\u2019s embedding representation through addition. The need\nfor identical dimensions in both embedding vectors is critical and is adeptly addressed by this\ninterpolation module. This non-learning-based approach proves to be efficient and effective\nfor encoding purposes. Additionally, interpolation provides the flexibility to dynamically set\nthe dimensionality of the final output embedding, unlike the fixed dimensions typically seen\nin neural network-based methods. Specifically:\nBh\u201c3\u00ff\ni\u201c03\u00ff\nj\u201c0aijpEhq\u00a8px\u00b4x0pEhqqi\u00a8py\u00b4y0pEhqqj, (5.15)\nwhere aijare the coefficients used for interpolation, and Bhis the resulting vector from the\nbicubic interpolation process. The dimension of BhisN, which is the same as that of VD.\nAugmNet: Inspired by recent developments in the augmentation literature [286], employs\na learning-based technique to identify augmentation patterns within the latent space. Un-\nlike conventional methods that perform image augmentations ( e.g., rotation, cropping) in the\npixel space, AugmNet universally applies transformations to the embeddings. This method\neliminates the necessity for specific transformations tailored individually to each modality,\ne.g., image, signal, and text. Incorporating this module within the automatic pain assessment\nframework helps to regularize the learning process and address overfitting issues. Moreover,\ncorrupting the input embeddings compels the following model, especially the Temporal-\nModel , to derive more precise and representative features, thereby improving performance\nin the pain assessment task. The modality-agnostic method effectively applies to embedding\nrepresentations from any original modality, including video and heart rate signals. AugmNet\nadopts an encoder-decoder architecture, where both the encoder and decoder consist of only\n2fully connected layers with the ELU nonlinear activation function applied after each layer.\nFor a session lasting \u03b8seconds, it produces \u03b8\u02c6frames per second frames and \u03b8\u02c6FS\ndata points for video and ECG, respectively. In the video analysis pipeline, the Spatial-\nModule constructs an embedding representation, VD(5.14), from the original video, dimen-\nsioned at d\u02c6FPS\u201cN. In the ECG analysis pipeline, a feature vector with dimension \u03b8\nis created after extracting the heart rate, one data point per second. The Heart Rate Encoder\nand bicubic interpolation then produce an embedding, Bh(5.15), with dimension N. The fu-\nsion of video and heart rate embeddings at the session level is performed, where VDandBh\nare merged by addition, integrating the data from the initial input modalities. The resulting90 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncombined embedding is then processed by AugmNet :\nAD\u00d0AugmNetpVD`Bhq (5.16)\nP\u00d0AD`pVD`Bhq, (5.17)\nwhere Prepresents the transformed embedding vector, serving as input for the final module,\ntheTemporal-Module .AugmNet is active only during training as a standard augmentation\nmethod and remains inactive during inference.\nTemporal-Module: Like the Heart Rate Encoder , this component operates on a transformer-\nbased architecture. It employs a combination of multi-head cross-attention and multi-head\nself-attention mechanisms. The architecture consists of one multi-head cross-attention block\nwith a single attention head and three subsequent multi-head self-attention blocks, each fea-\nturing eight attention heads. An FCN follows each attention block. The internal embed-\ndings in this module have a dimensionality of 128and encompass a single block in depth.\nThe position encoding method used here mirrors that of the Heart Rate Encoder , utilizing\nFourier feature position encoding. This module processes the input embedding Por the sum\npVD`BhqifAugmNet is inactive, to derive the final classification outcome. The learning\nerror is computed during this phase, and the framework undergoes further training.\nSupplementary augmentation methods\nWe also introduced additional augmentation strategies alongside the AugmNet module, which\napplies learned transformations to embeddings. The initial technique, dubbed Basic , com-\nbines polarity inversion and noise addition to manipulate the original inputs by reversing\ntheir polarity and injecting noise. Another technique, named Masking , involves nullifying el-\nements within the embeddings using randomly sized and placed masks that zero out 10\u00b420%\nof the embedding elements. These methods function within the latent space, similar to Augm-\nNet.\nPre-training\nBefore starting the training for automatic pain assessment, we individually pretrained all\nmodules except AugmNet . The Spatial-Module underwent a dual-phase pretraining process.\nInitially, it was pre-trained on VGGFace2 [280], a dataset designed for facial recognition\nto learn basic facial features. This was followed by an advanced training phase involving\nemotion recognition datasets in a multi-task learning framework. These datasets include the\nwidely used AffectNet [287], Compound Facial Expressions of Emotions Database [288],\nRAF Face Database basic [289], and RAF Face Database compound [289], enabling the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 91\nTable 5.5: Datasets utilized for the pre-training process of the framework.\nDataset # samples # classes Task\nVGGFace2 [280] 3.31M 9,131 Face\nAffectNet [287] 0.40M 8 Emotion\nCompound FEE-DB [288] 6,000 26 Emotion\nRAF-DB basic [289] 15,000 7 Emotion\nRAF-DB compound [289] 4,000 11 Emotion\nECG HBC Dataset [291] 0.45M 5 Arrhythmia\nTask: all tasks involve classification\nmodule to adapt to specific emotional expressions related to pain manifestations. During\nthis phase, the model is trained across these datasets simultaneously, employing a multi-task\nlearning loss approach as suggested by [290], where learned weights scale the individual\nlosses to account for the homoscedastic uncertainty of each task:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s`rew3LS3`w3s`rew4LS4`w4s,\nwhere LSdenotes the loss for each dataset and ware the weights adjusting the learning focus\nto optimize the overall loss Ltotal. The Temporal-Module is trained solely on the VGGFace2\ndataset. For this module, images are converted into 1D vectors prior to processing. The\nHeart Rate Encoder is pre-trained using the ECG Heartbeat Categorization Dataset [291],\nwhich includes heartbeat signals from the MIT-BIH Arrhythmia Dataset [292] and the PTB\nDiagnostic ECG Database [293] [294]. Table 5.5 provides a detailed list of the datasets used\nin our training process.\nDataset Details\nIn this study, to assess the developed framework, we utilized the BioVid Heat Pain Database\n[109], which includes facial videos, electrocardiograms, electromyograms, and skin conduc-\ntance levels from 87healthy individuals ( 44males and 43females, aged 20\u00b465). The\ndataset employs a thermode to induce varying pain levels in the participants\u2019 right arm. Ini-\ntially, each participant\u2019s pain threshold (the transition from heat sensation to pain) and pain\ntolerance (the point at which pain becomes unbearable) were determined. These thresholds\ndelineated the minimum and maximum pain levels, along with two intermediary levels, form-\ning five distinct pain intensities: No Pain (NP), Mild Pain (P 1), Moderate Pain (P 2), Severe\nPain (P 3), and Very Severe Pain (P 4). Pain stimuli temperatures ranged from P 1to P 4, capped\nat50.5\u00b0C. Each subject experienced 20stimulations at each of the four intensities (P 1to P 4).\nEach stimulation lasted 4seconds, interspersed with recovery intervals of 8to12seconds.92 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nTable 5.6: Training details for the automatic pain assessment.\nOptimizer Learning\nrateLR\ndecayWeight\ndecayWarmup\nepochsBatch\nsize\nAdamW 1e-4 cosine 0.1 50 32\nThis protocol, along with 20baseline measurements at NP ( 32\u00b0C), culminated in randomly\nordered 100stimulations per participant. The data was preprocessed to capture 5.5-second\nwindows starting 1-second after reaching the target temperature for each stimulation. This\nprocess produced 8,700samples, each 5.5seconds in duration, distributed evenly across the\nfive classes and modalities for all 87subjects.\n5.3.2 Experiments\nThe study leveraged videos and electrocardiograms from Part A of the BioVid dataset, using\nall available samples from the 87participants. The videos were recorded at a rate of 25\nframes per second (FPS), and the electrocardiogram (ECG) signals were sampled at 512\nHertz (Hz). Each recording session lasted 5.5seconds, generating 138video frames and\nECG vectors containing 2,816elements each, then converted into heart rate vectors of 5\ndata points. The complete set of frames and data points from both videos and cardiac signals\nwas utilized in the experiments. The experimental approach included iterative refinement\nof techniques, with the most effective combinations selected for extended training periods\nranging from 500to800epochs to improve feature extraction and performance outcomes.\nTable 5.6 details the training configurations for the automatic pain assessment tasks.\nPain assessment experiments were structured around binary and multi-level classification\nsetups, testing each modality individually and in combination. The binary classification task\ndifferentiated between No Pain (NP) and Very Severe Pain (P 4), whereas the multi-level\nclassification (MC) involved categorizing all pain intensities available in the dataset. The\nevaluation strategy adopted was the leave-one-subject-out (LOSO) cross-validation, and the\nassessment metrics included accuracy, precision, recall (sensitivity), and F1 score. Notably,\na consistent training regimen was applied across both the binary (NP vs. P 4) and multi-level\n(MC) classification tasks without varying the training schedule or optimization strategies.\n5.3.3 Results\nVideo modality\nExperiments related to the video modality explored the effects of pretraining on the Spatial-\nModule , the video analysis pipeline\u2019s performance, particularly the impact of tiling, and the5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 93\nTable 5.7: Results utilizing the video modality.\nEpochsPretraining stage Pipeline Augmentations Task\n1st2ndFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n500 \u2713 - \u2713 - \u2713 - - 72.56 31.22\n500 - \u2713 \u2713 - \u2713 - - 74.25 33.34\n500 - \u2713 - \u2713 \u2713 - - 68.07 31.49\n500 - \u2713 \u2713 \u2713 \u2713 - - 65.11 27.84\n500 - \u2713 \u2713 \u2713c\u2713 - - 74.86 33.86\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 - 73.05 32.14\n500 - \u2713 \u2713 \u2713c\u2713 - \u2713 74.83 33.73\n500 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 73.16 32.87\n800 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 77.10 35.39\nStage: referring to pretraining process for Spatial-Module Mask: Masking c: constant-coefficient applied exclusively to the tiles NP:\nNo Pain P 4: Very Severe Pain MC: multiclass pain level\nimplementation of new augmentation techniques. These experiments are detailed in Table\n5.7. Performance enhancements are evident when comparing the first and second pretraining\nstages of the Spatial-Module . For instance, in the NP vs. P 4task, initial pretraining alone\nachieved 72.56% accuracy, while including the second emotion-focused pretraining stage\nincreased accuracy to 74.25%. This trend is also notable in the multi-level classification,\nwhere the second stage added 1.12% to the performance, totaling 33.34%. Further experi-\nments assessed the effect of using tiles in the video representation. Initially, employing four\ntiles led to a performance decrease of over 6%in the binary classification task and 1.85% in\nthe multi-level task. This reduction likely results from the localized information in each tile,\nwhich may capture irrelevant details like non-expressive facial areas or background elements.\nIncluding the resized full-frame ( 224\u02c6224pixels) alongside tiles further decreased accuracy\nto65.11% and27.84% for binary and multi-level tasks, respectively. However, introducing\na coefficient ( c\u201c0.1) to adjust the tile embeddings restored some performance, achieving\n74.86% and33.86% in respective tasks.\nThe integration of two augmentation techniques, Masking andAugmNet , along with the\nBasic method, was then tested. Masking reduced performance by 1.81% and1.72%, and\nAugmNet showed smaller declines of 0.03% and0.13%. Using both techniques together\nresulted in better outcomes than Masking alone but did not independently surpass the per-\nformance of AugmNet . Despite these initial results, combining all augmentation methods\nproved advantageous for extended training periods. This approach addresses the risk of\noverfitting through a robust regularization strategy. Ultimately, this comprehensive strategy\nled to final accuracy rates of 77.10% and35.39% for binary and multi-level classifications,\ndemonstrating its effectiveness in an unimodal, vision-based pain assessment framework.94 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nHeart rate modality\nThe experiments concerning the heart rate modality explore the use of the encoder and vari-\nous augmentation methods. Table 5.8 details all experiments involving the heart rate modal-\nity. Initially, using the original heart rate vectors with a dimensionality of h\u201c5, we achieved\nclassification scores of 61.70% for NP vs P 4and27.60% for the multi-level task. After apply-\ning the Heart Rate Encoder to map these vectors to a higher-dimensional space of h\u201c2048 ,\nwe noted a slight improvement: a 0.23% increase for the binary task and 0.08% for the\nmulti-level task. Despite the considerable increase in embedding size, this modest enhance-\nment suggests that the intrinsic information within the limited heart rate data points does not\nsignificantly enhance the feature representation. Nonetheless, the encoder\u2019s use is vital for\nproducing larger embeddings, especially for our multimodal approach integrating video and\nheart rate data, which will be discussed in subsequent sections.\nWe also tested augmentation methods on the heart rate data. Applying Masking yielded\na slight improvement of 0.02% for the binary task and 0.05% for the multi-level task. Imple-\nmenting AugmNet further enhanced performance to 62.09% and28.11% for binary and multi-\nlevel tasks, respectively. However, combining all augmentations decreased performance to\n61.87% and27.96%. During an extended 800-epoch training period, we achieved 64.87%\naccuracy for the binary task and 29.81% accuracy for the multi-level task using all augmen-\ntations. Despite these gains, we found that augmentations pose more challenges for accurate\nheart rate classification than video. Therefore, we repeated the extended training without\nBasic andMasking , keeping only AugmNet , which improved binary task performance to\n67.04% and multi-level to 31.22%. This reduction in heart rate embedding corruption sig-\nnificantly enhanced performance. The differing effects of augmentations between heart rate\nand video modalities highlight the challenges of using a single, isolated feature in a machine\nlearning system. We infer that heart rate embeddings with limited informational content\nare more vulnerable to significant performance degradation from augmentations than richer\nvideo embeddings.\nMultimodality\nThe results of integrating video and heart rate modalities are detailed in Table 5.9. Based on\nthe insights gained from separate experiments with each modality, we extended the training\nduration to 800epochs. For this integrated approach, we utilized the tiles with a coefficient\nc\u201c0.1and applied AugmNet as the sole augmentation method. This fusion strategy re-\nsulted in a classification accuracy of 82.74% for NP vs. P 4and39.77% for the multi-level\nclassification task. These results mark a substantial enhancement, with improvements of\n5.64% and15.70% over the individual performances of the video and heart rate modalities,\nrespectively, for the binary classification. Similarly, for the multi-level classification, the in-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 95\nTable 5.8: Results utilizing the heart rate modality.\nEpochsHR\nEncoderAugmentations Task\nBasic Mask AugmNet NP vs P 4 MC\n500 - \u2713 - - 61.70 27.60\n500 \u2713 \u2713 - - 61.93 27.68\n500 \u2713 \u2713 \u2713 - 61.95 27.73\n500 \u2713 \u2713 - \u2713 62.09 28.11\n500 \u2713 \u2713 \u2713 \u2713 61.87 27.96\n800 \u2713 \u2713 \u2713 \u2713 64.84 29.81\n800 \u2713 - - \u2713 67.04 31.22\nTable 5.9: Results utilizing the video & the heart rate modality.\nEpochsHR\nEncoderPipeline Augmentations Task\nFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n800 \u2713 \u2713 \u2713c- - \u2713 82.74 39.77\ntegrated approach shows a 4.38% and8.55% increase compared to the standalone modalities.\nThe combination of these two pivotal modalities significantly boosts the efficacy of the pain\nassessment process, outperforming the results obtained by each modality on its own.\nComparison with existing methods\nIn this section, a comparative analysis is performed to evaluate the performance of our\nmethod against other existing approaches documented in the literature. This evaluation\nis based on Part A of the BioVid dataset, including all 87participants. The same evalua-\ntion protocol\u2014leave-one-subject-out (LOSO) cross-validation\u2014is adhered to for all com-\nparisons to ensure fairness and accuracy. Our method is contrasted with both unimodal and\nmultimodal approaches, divided into (1) video-based, (2) ECG-based, and (3) multimodal\nstudies regardless of the modalities involved. The outcomes are summarized in Table 5.10.\nFor video-based studies, our approach, achieving 77.10% in binary and 35.39% in multi-\nlevel classification tasks, is recognized as one of the highest-performing methods. It exceeds\nthe average results of comparative studies by about 4.7%for binary and 3.4%for multi-level\ntasks. Regarding ECG-based studies, our method shows superior performance, exceeding\nthe average by 8.5%and18.1%for binary and multi-level classifications, respectively. Re-\nmarkably, it records the highest classification accuracy in the multi-level task at 31.22%.96 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nThese findings underscore the effectiveness of using heart rate data extracted from ECG\nas a standalone feature, establishing our method\u2019s capability to assess pain accurately and\nachieve state-of-the-art results. In multimodal studies, our approach records an impressive\n82.74% accuracy in the NP vs. P 4task, making it one of the top-performing methods. It\nis slightly outperformed by studies [269] and [243], which achieved 88.10% and83.99%,\nrespectively. For multi-level tasks, comparisons are scarce; however, study [269] reached\n42.20%, and [235] reported 36.54%, positioning our method favorably within this context.\nInference time\nWe explored several methodologies in this study, including a video-based approach, video\nincorporating tiles, a heart rate-based approach, heart rate analysis with an encoder, and a\ncombined multimodal strategy. Figure 5.6 illustrates each method\u2019s inference time in sec-\nonds and the corresponding average accuracy performances across binary and multi-level\ntasks. Table 5.11 details the number of parameters and the computational cost of floating-\npoint operations (FLOPS) for each component. Inference tests were conducted on an Intel\nCore i7-8750H CPU, including the time for face detection in each frame but excluding the\nextraction of heart rate from electrocardiography, focusing on the potential use of automati-\ncally provided cardiac features from wearables.\nThe inference time for the video modality employing the standard pipeline is approxi-\nmately 26seconds. Utilizing the tile pipeline increases inference time significantly, soaring\nto about 130seconds due to processing five image representations per frame\u2014one full frame\nand four tiles\u2014compared to a single image representation in the non-tiled approach. In the\ncontext of heart rate signals, completing a pain assessment requires only 1.2seconds. With\nthe integration of the Heart Rate Encoder , the processing time remains virtually unchanged,\nshowing a negligible increase of less than half a second, highlighting this specific encoder\u2019s\nefficiency. Lastly, the comprehensive multimodal framework incorporating the tiles and the\nHeart Rate Encoder demands about 131seconds, illustrating the increased complexity and\ncomputational requirements when combining multiple modalities.\nInterpretation\nImproving model interpretability is essential for their acceptance and integration into clin-\nical settings. This study generates attention maps from both the Spatial-Module and the\nTemporal-Module , as illustrated in Figure 5.7. For the Spatial-Module , attention maps are de-\nrived from the last fully connected layer\u2019s weight contributions, which are then interpolated\nonto the images to highlight the model\u2019s focal areas. Figure 5.7a displays an original frame\nsequence along with three attention map variations: (1) post-initial pretraining, (2) after the\nsecond pretraining phase, and (3) post-training on the BioVid . Based on face recognition5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 97\nTable 5.10: Comparison of studies utilizing BioVid & LOSO validation, reported on accuracy %.\nStudy ModalityMethod Task\nFeatures Machine Learning Params (M) FLOPS (G) NP vs P 4 MC\n[295] Video optical flow RF - - 70.20 -\n[217] Video raw SLSTM - - 61.70 29.70\n[269] Video raw 3D CNN, - - 77.50 34.30\n[219] Video raw 2D CNN, biLSTM - - 69.25 -\n[180] Video raw 2D CNN 25.00\u00154.00 71.00 -\n[296] Video facial action\ndescriptorsDeep RF - - 72.40 30.80\n[296] Video facial 3D distances Deep RF - - 72.10 30.30\n[284] Video fiducial points GNN - - 73.20 -\n[135]:Video raw 2D CNN - - 71.30 37.60\n[211]:Video raw 2D CNN, GRU 150.00\u0015- 73.90 39.10\n[37] Video raw Transformer 24.00 4.20 73.28 31.52\n[267] Video facial landmarks,\n3D distancesRF 71.60 -\nOur Video raw Transformer 4.20\u00101.62 77.10 35.39\n[235] ECG raw 1D CNN 1.80\u0015- 57.04 23.23\n[270] ECG domain-specific\u00b8LR - - 57.69 -\n[35] ECG domain-specific\u00b8SVM - - 58.39 23.79\n[269] ECG heart rate\u20393D CNN - - 65.00 28.50\n[267] ECG domain-specific RF - - 62.00 -\n[36] ECG domain-specific FCN 4.09\u00120.40 69.40 30.24\n[297] ECG domain-specific\u00b8SVM - - 63.50 -\nOur ECG heart rate Transformer 6.03\u00101.25 67.04 31.22\n[235] ECG, EMG, GSR raw 2D CNN 10.00\u0015- 76.72 36.54\n[270] ECG, GSR domain-specific\u00b8SVM - - 72.20 -\n[269] Video1, ECG2raw1, heart rate2\u20393D CNN - - 88.10 42.20\n[267] ECG1, EMG1,\nGSR1domain-specific1\u00b8RF - - 74.10 -\n[267] Video1,ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 77.80 -\n[297] Video1, ECG2,\nGSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8RF - - 78.90 -\n[297] Video1, ECG2,\nEMG2, GSR2facial landmarks1,\n3D distances1,\ndomain-specific2\u00b8SVM - - 76.60 -\n[243] ECG, EMG, GSR raw DDCAE 4.00\u0015- 83.99 -\nOur Video1,ECG2raw1, heart rate2Transformer 8.60\u00102.44 82.74 39.77\nM: Millions G: Giga :: reimplemented for pain intensity estimation on BioVid by [269]\u2039: pseudo heart rate gain \u00b8: numerous\nfeatures \u0015: parameter count estimated from provided paper details \u0010:AugmNet excluded from parameter count, not used in inference\n\u0012: parameter count not mentioned in study, provided directly by authors -: missing value RF: Random Forest AE-ATT: Autoencoder\nAttention SVM: Support Vector Machines LR: Logistic Regression98 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\nVideo Video\n(Tiles)Heart rate Heart rate\n(Encoder)Multimodal0102030405060Accuracy (%)Average Accuracy\n020406080100120\nInference Time (s)\nInference Time\nFigure 5.6: Comparison of mean accuracy and inference period for unimodal and multimodal\nstrategies across NP versus P 4and MC tasks. The diagram adopts a dual-y-axis\nconfiguration\u2014accuracy measurements on the left and time metrics on the right\u2014\nto outline the balance between performance efficacy and computational load, cate-\ngorizing the methodologies along the x-axis.\nTable 5.11: Module parameters and computational cost in FLOPS for the proposed framework.\nModule Params (M) FLOPS (G)\nSpatial-Module 2.57 1.19\nHeart Rate Encoder 4.40 0.82\nAugmNet 1.02 0.02\nTemporal-Module 1.63 0.43\nTotal 9.62 2.46\ntasks, the Spatial-Module produces maps focusing broadly on the facial area, particularly\nthe zygomatic, buccal, oral, mental, and nasal regions. The second stage, oriented towards\nmulti-task emotion recognition, refines the focus, sharpening attention on specific facial ar-\neas, highlighted in the first stage but with greater clarity and emphasis. After training on the\nBioVid for pain assessment, the attention maps show further refined focus on specific facial\nareas, with reduced attention to less relevant regions, ensuring concentrated focus on key\nareas. These maps consistently demonstrate the model\u2019s capability to adjust its focus based\non pain-related facial expressions.\nAttention maps from the Temporal-Module , based on input embeddings, illustrate the\nweight contributions in the module\u2019s final layer, forming easy-to-visualize rectangular pat-5.3. VIDEO & HEART RATE ANALYSIS WITH TRANSFORMER ARCHITECTURES 99\nterns. Figure 5.7b shows examples for three scenarios: (1) video embedding, (2) heart rate\nembedding, and (3) a combined embedding of video and heart rate. The attention maps ex-\nhibit a grid-like pattern, possibly due to the Fourier position encoding used, akin to those\nseen in perceiver-like architectures. The video embedding map shows intense attention\nacross the input. In contrast, the heart rate embedding map focuses attention more sparsely,\nwith a notable concentration in specific areas indicated by intense red coloration. The com-\nbined embedding map displays moderate intensity, consistent with the blended nature of the\ninput. These maps tend to emphasize the latter part of the session, aligning with the timing\nof pain manifestation towards the session\u2019s end, indicating the model\u2019s responsiveness to\nreal-time pain expressions.\n5.3.4 Discussion\nThis research introduced a multimodal framework that integrates video and heart rate sig-\nnals to assess pain automatically. Our innovative approach includes four main modules,\neach characterized by effectiveness and efficiency. The Spatial Module , particularly no-\ntable for its compact size of only 2.57million parameters, ranks as one of the most efficient\nvision-based models in automatic pain assessment literature. Despite limited comparative\nstudies, our model has shown it can match or exceed the performance of larger models. Its\nhigh efficiency and robust performance are primarily due to a thorough pretraining regime\non datasets related to affective responses, crucial for enhancing model capabilities in pain\nestimation tasks. The Heart Rate Encoder , with 4.40million parameters, excels at transform-\ning heart rate data into complex, high-dimensional embeddings, which integrate seamlessly\nwith video data during inference, all within under half a second. This quick processing\nunderscores the encoder\u2019s efficiency, supported by bicubic interpolation that modifies input\ndynamically to achieve variable output dimensions without predefined constraints. AugmNet ,\na novel augmentation module, learns to modify latent space representations directly, prevent-\ning the need for specific augmentation techniques designed for each data type. However, this\nmodule requires careful application to avoid overfitting and other training challenges. The\nTemporal-Module , consisting of 1.63million parameters, is crucial for final pain level as-\nsessments. It leverages a mix of cross- and self-attention mechanisms to enhance efficiency\nand accuracy, demonstrating the potential of transformers in streamlined settings contrary to\ntheir typical use in large-scale applications.\nOur experiments demonstrate that videos are invaluable for discerning individual pain ex-\nperiences by capturing diverse behavioral indicators like facial expressions, eye movements,\nand even slight color changes under stress. Utilizing video data, our methodology reached\nan accuracy of 77.10% in binary classification, effectively distinguishing between no pain\nand very severe pain scenarios. Moreover, it achieved 35.39% accuracy in a multi-level clas-100 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\n(a) Attention maps from the Spatial-Module .\n(b) Attention maps from the Temporal-Module .\nFigure 5.7: Regions highlighted in yellow and red denote areas of significant attention. (a)\n(1strow) Sequence of original frames. (2ndrow) Derived from the Spatial-Module\nafter initial stage pretraining. (3rdrow) Derived from the Spatial-Module post sec-\nond stage pretraining. (4throw) Derived from the Spatial-Module trained on the\nBioVid dataset. (b) (1strow) Derived from the Temporal-Module incorporating\nvideo embeddings. (2ndrow) Derived from the Temporal-Module with heart rate\nembeddings. (3rdrow) Derived from the Temporal-Module using a combined em-\nbedding of video and heart rate.5.4. SUMMARY 101\nsification spanning five distinct pain intensities. The heart rate signal, tested as a standalone\nfeature from electrocardiography, showed that remarkable outcomes are possible with this\nsingle data feature, which is pivotal for validating the feasibility of heart rate as a viable\npain indicator. This is crucial as heart rate data is readily accessible from most wearable\ntechnologies, reducing the need for specialized algorithms to handle cardiac signals or raw\nbiosignals, thereby conserving both time and computational resources. Solely using heart\nrate, our model excelled, registering accuracies of 67.04% and31.22% for binary and multi-\nlevel classifications, respectively, among the highest reported. Incorporating video and heart\nrate data, our multimodal method yielded superior results\u2014 82.74% and39.77% for binary\nand multi-level classifications, respectively. These figures significantly enhance video-only\nresults by roughly 9%and heart rate-only outcomes by about 24%. Furthermore, with a total\nparameter count of just 9.62million, our approach stands out for its efficiency and effec-\ntiveness. Overall, this study showcases the synergy achievable by merging video and heart\nrate data, leading to a system outperforming its unimodal counterparts and emphasizing the\npotential of integrated multimodal pain assessment tools.\nUsing attention maps generated by the Spatial-Module , our framework analysis iden-\ntified crucial facial areas like the zygomatic and oral regions as significant for automatic\npain assessment. These maps demonstrated that different pretraining stages refine the focus,\nshowing more targeted attention with specialized training. Similarly, attention maps from\ntheTemporal-Module focused on the latter part of the input image, corresponding to where\npain manifestations are typically observed in the particular dataset.\n5.4 Summary\nThis chapter explores the interplay between model efficiency and performance in automatic\npain assessment tasks. We also aimed to mirror real-world conditions by leveraging read-\nily accessible and applicable modalities without relying on costly precision medical devices.\nConsequently, we utilized RGB videos with a resolution of approximately 1080x1080 and\nheart rate data. The videos utilized are of medium quality, comparable to those captured with\nmobile phone cameras, and the heart rate data simulates readings from wearable devices. It\nis important to note that wearables across various price ranges automatically provide heart\nrate information. Consequently, exploring the potential of using this readily available modal-\nity for pain assessment is crucial. This proof of concept is significant as it could enable\ncost-effective and accessible pain assessment solutions without dependence on specialized\nmedical equipment. Additionally, our study focuses on developing compact and efficient\nmodels that maintain robust performance.\nThe experiments detailed in this chapter reveal that reducing the number of frames used\nin a video-based pipeline by a factor of four minimizes the accuracy loss, under 1.5%, while102 CHAPTER 5. OPTIMIZATION: BALANCING EFFICIENCY AND PERFORMANCE\ncutting inference time threefold, facilitating near real-time pain assessment. Furthermore,\nour initially proposed framework, which incorporates 24million parameters and operates at\n4.3GFLOPS, demonstrated exceptionally high performance. In the subsequent experiments,\nwe showcased that heart rate data alone can be effectively used for pain assessment, achiev-\ning impressive results. This finding underscores the practical utility of data available from\nwearables. Additionally, combining video data with heart rate information yielded the high-\nest accuracy in our tests, illustrating that an integrated approach using both behavioral and\nphysiological modalities can significantly enhance performance. Additionally, we demon-\nstrated that creating one of the smallest models documented, with only 9.62million param-\neters and 2.46GFLOPS, allowed us to achieve top-tier results. This highlights that large\nmodels, commonly favored in the current era of AI, are not always necessary for effective\nperformance. However, it is important to note that extensive multi-stage pretraining across\nvarious datasets greatly aided this framework\u2019s success, which was critical in achieving such\nhigh efficiency and effectiveness.\nIn this chapter, we aimed to explore the effectiveness of compact models in achieving\nhigh performance with rapid inference times. We exclusively used heart rate data, mimicking\nthe information typically available from wearable devices, though our experiments did not\nextend to real-world conditions. Instead, they relied on the sole publicly accessible dataset\nexplicitly designed for pain assessment, including facial videos and cardiac signals collected\nin highly controlled laboratory settings. Participants were positioned facing forward un-\nder optimal lighting conditions, with physiological sensors precisely affixed. Recognizing\nthe potential challenges of applying these findings to clinical settings is essential. Issues\nlike inconsistent lighting, unforeseen facial movements, occlusions, or difficulties with sen-\nsor placement must be meticulously addressed to tailor these systems for real-world use.\nMoreover, depending solely on heart rate as a cardiac feature could be restrictive in more\ndemanding scenarios, highlighting the necessity to integrate multiple extracted features or\nuutilizeraw biosignals for comprehensive assessments.Chapter 6\nSynthetic Data: The Role of Thermal Imag-\ning\nContents\n6.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 103\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks . . . . . . . 104\n6.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 104\n6.3 Combination of RGB and Synthetic Thermal Videos . . . . . . . . . . . . . . . 105\n6.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105\n6.3.2 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110\n6.3.3 Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111\n6.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 115\n6.1 Chapter Overview: Introduction & Related Work\nThis chapter presents the findings from the study published in [39]. In recent years, syn-\nthetic data generation has gained traction as a viable approach to addressing data scarcity\nand privacy issues while also meeting the requirements for training AI algorithms on unbi-\nased data with adequate sample size and statistical robustness. Additionally, synthetic data\ncan increase the availability and diversity of real data, particularly in rare modalities, which\ncan be essential for training AI-driven diagnostic and predictive models. This enhance-\nment supports healthcare research and improves patient outcomes. [298]. In the literature\non automatic pain assessment, no studies have been reported concerning creating synthetic\nmodalities. This chapter introduces the generation process of synthetic thermal videos using\nGenerative Adversarial Networks (GANs).\nRegarding thermal modality, in recent years, the field of affective computing research\nhas increasingly adopted thermal imaging techniques [299]. This shift was motivated by\n103104 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nstudies showing that stress and cognitive load significantly affect skin temperature [300], at-\ntributable to the autonomic nervous system\u2019s (ANS) regulation of physiological signals such\nas heart rate, respiration rate, blood perfusion, and body temperature. These signals are vital\nindicators of human emotions and affect [299]. Moreover, muscle contractions can alter fa-\ncial temperature by transferring heat to the facial skin [301]. Consequently, thermal imaging\nhas emerged as a viable method for recording transient facial temperatures [302]. Research\nby the authors in [303] on thermal imaging and facial action units to evaluate emotions, such\nas frustration, boredom, and enjoyment, indicated that a multimodal approach yielded the\nmost accurate results. Within pain research, thermal imaging has been explored in limited\nstudies. For instance, [304] reported increased facial temperature in response to painful stim-\nuli, suggesting thermal cameras as effective tools for monitoring pain. Another study [305]\nintroduced a pain dataset consisting of RGB, thermal, and depth videos, finding that while\nthe RGB modality slightly outperformed the others, integrating all modalities provided the\nbest results. This prompted us to explore the specific modality of thermal imagery through\nthe prism of synthesis using generative deep learning.\n6.2 Synthetic Thermal Videos using Generative Adversarial Networks\nWe present a process of creating synthetic thermal videos using GANs in unimodal and\nmultimodal settings alongside RGB video modalities. Integrating a Vision Multilayer Per-\nceptron (MLP) model with a transformer-based module is at the core of our automatic pain\nassessment framework. Key contributions of this research include (1) generating synthetic\nthermal videos to enhance pain assessment as an additional vision modality, (2) assessing\nthe effectiveness of RGB and synthetic thermal videos as standalone modalities, (3) exam-\nining the utility of thermal-related information for pain assessment, and (4) evaluating the\nperformance and implications of the newly developed Vision-MLP architectures.\n6.2.1 Methodology\nThis section outlines the generation of synthetic thermal videos, which will be utilized sub-\nsequently and incorporated into an automatic assessment pipeline.\nSynthetic Thermal Videos\nAn image-to-image translation (I2I) approach has been utilized to create synthetic thermal\nvideos. I2I generative models are designed to bridge different image domains by learning the\ndata distributions inherent to each domain. Here, the source domain comprises RGB images;\nthe target domain is thermal images. In this research, conditional generative adversarial\nnetworks (cGANs) [306] were employed and trained in a supervised manner using paired6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 105\nimages. Fig. 6.1 provides a high-level overview of the method. The generator Gproduces\nimages that appear realistic, whereas the discriminator Dworks to differentiate between\ngenuine and synthetic images through the following minimax game:\nmin\nGmax\nDLcGANpG, Dq, (6.1)\nwhere the objective function LcGANpG, Dqis defined as:\nEx,yrlogDpx, yqs`Ex,zrlogp1\u00b4Dpx, Gpx, zqqqs, (6.2)\nwithxdenoting the actual data, yindicating the target data, and zrepresenting the random\nnoise vector. Here, Gseeks to minimize the objective function, whereas Doperates in\nopposition, striving to maximize it. Additionally, we incorporated the Wasserstein gradient\npenalty (WGAN-GP) [307] to enhance the stability of training. The overall objective is\narticulated as:\nLcGANpG, Dq`\u03bbE\u02c6x,yrp}\u2207\u02c6xDp\u02c6x, yq}2\u00b41q2s, (6.3)\nwhere \u03bbis the penalty coefficient. In the architectural design of our proposed method, which\ndraws inspiration from [308], the generator Gis divided into three distinct modules: an en-\ncoder, which includes two convolutional layers that downsample the input; a middle ResNet\nmodule, featuring nine residual blocks, each equipped with two convolutional layers; and a\ndecoder that upsamples the feature maps back to the final resolution ( i.e.,256\u02c6256) for the\nsynthetic sample. The discriminator D, based on the approach outlined in [309], employs a\npixel-level PatchGAN strategy using 1\u02c61kernels and consists of two convolutional layers.\n6.3 Combination of RGB and Synthetic Thermal Videos\n6.3.1 Methodology\nThis section presents the structure of the proposed automatic pain assessment framework,\nthe augmentation techniques developed, the pre-processing methods employed, and the pre-\ntraining strategy for the modules.\nFramework Architecture\nThe proposed framework consists of two primary modules: a Vision-MLP model that acts\nas a spatial embedding extractor for individual video frames and a transformer-based model\nthat functions as a temporal module, using the embedded representations of the videos for\ntemporal analysis and final pain assessment. Fig. 6.2 displays the modules and their main\ncomponents.106 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nDecoder\n\u2026Encoder\n\u2026Residual blocks\nDiscriminator\nGenerator\nAuthentic/Synthetic?\nFigure 6.1: Illustration of the procedure for creating thermal images, featuring the architecture\nof the Generator G(Encoder, mid-stage ResNet, Decoder), and the Discriminator\nD.\nVision-MLP: MLP-like models have recently emerged as a novel class of vision models,\nproviding an alternative to traditional Convolutional Neural Networks (CNNs) and Vision\nTransformers (ViT). These models are characterized by their straightforward architectures,\nwhich consist of fully connected layers combined with activation functions. They possess\na lower level of inductive bias and rely on basic matrix multiplication operations. Our\nmethodology is grounded in the principles outlined in [310] that introduced the Vision-\nMLP, and [311] that incorporates a wave representation for the patches (also known as\ntokens). Each video frame is initially partitioned into nnon-overlapping tokens Fm\u201c\nrfm,1, fm,2, . . . , f m,nsPRn\u02c6p\u02c6p\u02c63, where pspecifies the resolution of each token, i.e.,16\u02c616\npixels, and 3represents the number of color channels. Each token is subsequently linearly\nprojected into a dimension d\u201c768prior to entering the Vision-MLP (refer to Fig. 2a). The\nfirst principal sub-module, the Channel-Mixer (Fig. 2c), operates independently on each\ntoken fj, enabling interactions among different channels, and is formulated as:\nChannel-Mixer pfj, Wcq\u201cWcfj (6.4)\nwhere Wcdenotes the weight matrix with learnable parameters, and j\u201c1,2, . . . , n . Fol-\nlowing this, the next significant sub-module, the Token-Mixer (Fig. 2b), facilitates commu-\nnication among various tokens, aiding in the extraction of features from different spatial6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 107\nVision-MLPLinear ProjectionToken MixerChannel Mixer\nNormNorm2x\n3xStage-1Token MixerChannel Mixer\nNormNorm4xStage-2Token MixerChannel Mixer\nNormNorm18xStage-3Token MixerChannel Mixer\nNormNorm 3xStage-4\nInput Imagea\nChannel-MixerMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLP\nMLPChannelsTokensSkip-connectionscMLP\nFully-connectedGELUFully-connectedd\nChannel-MixerWave-BlockWave-Block\nToken-MixerTokensChannels\n......FCNb\nTransformer\n\u2026\nVision-MLP\nWeighted Fusion\nPain Assessment\nCross-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN\nSelf-Attention\nFCN1x\n8x\n8x4x\n8xe\nFigure 6.2: Representation of the proposed framework, illustrating its components and their\nmain functions: (a)The Vision-MLP module, tasked with extracting feature em-\nbeddings from video frames. (b)The Token-Mixer , an important sub-module of\nVision-MLP , generates the wave representation for the tokens. (c)The Channel-\nMixer , a crucial sub-module within Vision-MLP .(d)The MLP, a core component\nof the Channel-Mixer .(e)The fusion procedure that combines RGB and synthetic\nthermal embeddings, succeeded by the Transformer module, which conducts the\nfinal pain assessment.\nlocations. Typically, in MLP-based models, the token mixers are defined as:\nToken-MixerpF, Wtqj\u201c\u00ff\nkWt\njkdfk, (6.5)\nwhere Wtis the corresponding weight matrix for the tokens, and drepresents element-\nwise multiplication. Our proposed approach modifies tokens into wave-like representations\nto dynamically adjust the interactions between tokens and weights based on their semantic\ncontent. To depict a token fjas a wave \u02dcfjvia a wave function, both amplitude and phase\ninformation is necessary:\n\u02dcfj\u201c|fj|dei\u03b8j. (6.6)\nHere, iis the imaginary unit satisfying i2\u201c \u00b41. The term|fj|indicates the amplitude of\nthe signal, while ei\u03b8jis a periodic function, with \u03b8jrepresenting the phase of the signal. The108 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nabsolute value operation is typically omitted and substituted with 6.4 for simplicity. Each to-\nken\u2019s phase \u03b8jreflects its position within the wave cycle and can, therefore, be characterized\nusing fixed parameters, which are adjustable during the training phase. As such, 6.4 is also\napplied for phase estimation. Given that 6.6 characterizes a wave within the complex domain,\nthe Euler formula is employed to embed tokens within the neural network architecture:\n\u02dcfj\u201c|fj|dcos\u03b8j`i|fj|dsin\u03b8j. (6.7)\nCombining 6.5 and 6.7, a token is represented as:\nfj\u201c\u00ff\nWt\njkfkdcos\u03b8k`Wi\njkfkdsin\u03b8k (6.8)\n\u00f9\u00f1\u00ff\nWt\njkfkdcospWcfkq`Wi\njkfkdsinpWcfkq (6.9)\nwhere Wt,Wc, and Wiare the learnable weight matrices. The described process focuses\non wave-like representations and is conducted within the Token-Mixer , particularly in the\nWave-Block . The Token-Mixer architecture consists of three blocks: two Wave-Blocks and\noneChannel-Mixer operating in parallel. The Vision-MLP module is organized into four\nstages, each featuring a sequence of a Token-Mixer and a Channel-Mixer block, preceded\nby a normalization layer. The depth of parallel blocks in each stage is 3,4,18, and 3, re-\nspectively, allowing for hierarchical embeddings extraction with corresponding dimensions\nacross stages 64,128,320, and 100.\nFusion: For each input frame, the Vision-MLP extracts an embedding of dimensionality d\u201c\n100. The embeddings from the individual frames of a specific video are then concatenated\nto form a comprehensive embedding representation of the video:\nVD\u201crd1}d2}\u00a8\u00a8\u00a8} dms,VDPRN, (6.10)\nwhere mrepresents the number of frames in the video, and Nis the dimensionality of the\nultimate embedding. Following this, the embeddings from the RGB and synthetic thermal\nvideos are combined through a weighted fusion process:\nVFused\u201cw1\u00a8VRGB`w2\u00a8VThermal ,VFusedPRN. (6.11)\nThis fusion strategy integrates the respective embeddings using learned weights w1andw2,\nwhich adjust the influence of the RGB and thermal embeddings, respectively. This weighted\nsummation achieves an effective integration, capturing the relevance of each modality in the\nresultant fused representation VFused .6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 109\nTransformer: The fused embeddings are then input into a transformer-based module that\nincludes self-attention and cross-attention blocks (Fig. 2e). The self-attention mechanism is\nexpressed as:\nAttentionpQ, K, Vq\u201csoftmax\u02c6QKT\n?dk\u02d9\nV. (6.12)\nIn this formulation, QPRM\u02c6C,KPRM\u02c6C, and VPRM\u02c6Care the Query, Key, and Value\nmatrices respectively, where Mindicates the input dimension, and Crepresents the channel\ndimension. The cross-attention mechanism also utilizes a dot product operation; however,\nQfor cross-attention is dimensioned N\u02c6Crather than M\u02c6C, with N\u0103Mproviding\na reduction in computational cost. Each self-attention and cross-attention block features 1\nand8attention heads, respectively, and the entire Transformer module consists of 4parallel\nblocks. The output embeddings, with a dimensionality of 340, are used to perform the final\npain assessment via a fully connected neural network.\nAugmentation Methods: Two augmentation techniques have been implemented within the\nframework. Firstly, the method known as Basic is utilized, which combines polarity in-\nversion with the addition of noise. This approach modifies the original input embedding by\ninverting the polarity of data elements and adding random noise from a Gaussian distribution,\nthus introducing variability and perturbations. Secondly, the Masking technique involves ap-\nplying zero-valued masks to the embeddings, effectively eliminating sections of the vectors.\nThe dimensions of these masks are randomly determined, ranging from 10% to 50% of the\nembedding\u2019s total dimensions, and are randomly positioned within the embeddings.\nPre-processing: The pre-processing involves face detection to isolate the facial region. The\nMTCNN face detector [278] is used, which employs multitask cascaded convolutional neural\nnetworks to identify faces and landmarks. It is essential to highlight that the face detector\nwas applied exclusively to the individual RGB frames, and the coordinates of the detected\nface were then applied to the corresponding synthetic thermal frames. The resolution of all\nframes was standardized at 224\u02c6224pixels.\nPre-training: For the I2I approach, the SpeakingFaces dataset [312] was employed to train\nthe proposed GAN model for translating RGB to synthetic thermal videos. Additionally, be-\nfore commencing the automatic pain assessment training, the Vision-MLP andTransformer\nmodules underwent pre-training. The Vision-MLP was subject to a three-stage pre-training\nstrategy: initially, it was trained on DigiFace-1M [313] to acquire basic facial features. It\nwas then trained on AffectNet [287] and RAF Face Database basic [289] to learn features\nrelated to basic emotions through multi-task learning. Lastly, the Compound Facial Expres-\nsions of Emotions Database [288] and the RAF Face Database compound [289] were used110 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.1: Datasets utilized for the pretraining process of the framework.\nDataset # samples # classes Task\nSpeakingFaces [312] 4.58M 142 Face\u0015\nDigiFace-1M [313] 1.00M 10,000 Face\u0010\nAffectNet [287] 0.40M 8 Emotion\u0010\nCompound FEE-DB [288] 6,000 26 Emotion\u0010\nRAF-DB basic [289] 15,000 7 Emotion\u0010\nRAF-DB compound [289] 4,000 11 Emotion\u0010\n\u0015: includes face image pairs for the I2I task \u0010: includes images for face or emotion\nrecognition tasks\nto learn features of compound emotions in a similar multi-task framework. The multi-task\nlearning process is represented as:\nLtotal\u201crew1LS1`w1s`rew2LS2`w2s, (6.13)\nwhere LSis the loss for the specific task associated with different datasets, and ware the\nlearned weights that guide the learning process in minimizing the collective loss Ltotal, in-\ntegrating all the individual losses. The Transformer was pre-trained solely on the DigiFace-\n1M[313], adapting the input images into 1D vectors to suit its architectural needs. Table 6.1\noutlines the datasets utilized in the pre-training phase.\n6.3.2 Experiments\nThe proposed framework was assessed using the BioVid Heat Pain Database [109], which\ncomprises facial videos, electrocardiograms, electromyograms, and skin conductance levels\nfrom 87healthy subjects. Participants underwent heat-induced pain via a thermode on their\nright arm at five different intensities: no pain (NP), mild pain (P 1), moderate pain (P 2), se-\nvere pain (P 3), and very severe pain (P 4). Each level was applied 20times to each subject,\nresulting in 100samples per modality and a total of 1740 samples per class. Experiments\nwere structured around binary and multi-level classification schemes to assess pain, analyz-\ning each modality individually and collectively. Binary classification aimed to distinguish\nbetween no pain (NP) and very severe pain (P 4), while multi-level classification (MC) was\ntasked with categorizing all levels of pain intensity present in the dataset. The leave-one-\nsubject-out (LOSO) method was utilized for validation, and accuracy served as the metric\nfor performance evaluation. Table 6.2 outlines the training details for the automatic pain\nassessment, including parameter number and the computational cost measured in floating-\npoint operations (FLOPS) for each module.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 111\nTable 6.2: Training specifications, and number of parameters and FLOPS for each module.\nTraining Details Vision-MLP Transformer\nOptimizer: AdamW Params: 7.35 M Params: 7.96 M\nLearning rate: 2e-5 FLOPS: 30.95 G FLOPS: 30.90 G\nLR decay: cosine\nWeight decay: 0.1\nWarmup epochs: 5\nBatch size: 32\nTotal Params: 15.31 Millions FLOPS: 61.85 Giga\nTable 6.3: Results utilizing the RGB video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.37 30.23\n200 \u2713 20-50 0.7 70.26 28.50\n300 \u2713 30-50 0.9 70.05 30.02\nMasking: indicates the percentage of the input embedding to which zero-value masking is applied\nP(Aug): represents the probability of applying the augmentation methods of Basic & Masking NP: No\nPain P 4: Very Severe Pain MC: multiclass pain level\n6.3.3 Results\nRGB Videos\nWithin the context of the RGB video modality, we recorded an accuracy of 69.37% for the\nbinary classification task (NP vs. P 4) and 30.23% for the multi-class classification (MC).\nThe use of the Masking augmentation method, which obscured 20\u00b450% of the input em-\nbeddings, yielded a slight increase in binary classification accuracy by 0.89%. However, it\nled to a reduction in multi-class classification accuracy. By extending the training period to\n300epochs, modifying the Masking method to cover 30\u00b450% of the embeddings, and ap-\nplying a 90% probability to both augmentation methods, the accuracies improved to 70.05%\nand30.02% for the binary and multi-class tasks, respectively. This represents an average\naccuracy gain of just under 0.5%. The classification outcomes are detailed in Table 6.3.\nSynthetic Thermal Videos\nIn the synthetic thermal modality experiments conducted under identical conditions, the ini-\ntial accuracies were 69.97% for the binary classification and 30.04% for the multi-class clas-\nsification. Enhancing the intensity of the masking method yielded modest gains in accuracy112 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.4: Results utilizing the synthetic thermal video.\nEpochsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 10-20 0.7 69.97 30.04\n200 \u2713 20-50 0.7 70.20 30.50\n300 \u2713 30-50 0.9 70.69 29.60\nof0.23% for the binary classification and 0.46% for the multi-class classification. Subse-\nquent final accuracies were 70.69% for the binary classification and 29.60% for the multi-\nclass classification, reflecting an average increment of 0.28%. This discrepancy likely arises\nfrom the challenge of detecting nuanced facial changes linked to low-level pain, exacerbated\nby more aggressive augmentation that potentially reduces performance. The summarized\nresults are presented in Table 6.4.\nAdditional Analysis on RGB & Synthetic Thermal Videos\nThe results from the previous section showed a surprising similarity in performance between\nthe RGB and synthetic thermal modalities. Specifically, the RGB modality achieved maxi-\nmum accuracies of 70.26% for the NP vs. P4 task and 30.23% for the MC task. Similarly,\nthe synthetic thermal modality reached top accuracies of 70.69% and30.50% for the same\ntasks, respectively. On average, the thermal video performances were about 1%higher than\nthose for the RGB modality. This was unexpected, considering the synthetic modality was\ninitially considered inferior to the original. This prompted further investigation into why\nsynthetic modalities might perform comparably to or better than the original RGB modality.\nA key question involved the relevance and efficacy of the thermal-related data featured in the\nsynthetic videos. The theory proposed that minimizing facial expressions in thermal videos\ncould enhance the clarity of thermal data assessment.\nGaussian blurring was incrementally applied to RGB and synthetic thermal videos, as\nshown in Fig. 6.3, with kernel sizes increasing from 0to191. According to Table 6.5, at\na kernel size of k\u201c0, a performance differential of 0.47%, favoring the thermal modality,\nconfirms prior findings. This gap slightly expands to 0.49% atk\u201c41. Remarkably, at\nk\u201c91, the disparity enlarges to 2.13% and increases to 5.90% atk\u201c191, where blurring\nis most intense. The results reveal that as facial expressions become less visible through\nblurring, synthetic thermal videos consistently outperform RGB videos, with respective ac-\ncuracies of 66.24% versus 60.34%. As blurring intensifies from k\u201c0tok\u201c191, accuracy\nrates for synthetic thermal and RGB modalities decrease by 1.81% and7.13%, respectively.6.3. COMBINATION OF RGB AND SYNTHETIC THERMAL VIDEOS 113\nFigure 6.3: Gradual blurring of RGB and synthetic thermal facial images: a series displaying\nvarying levels of Gaussian blur applied, with kernel sizes gradually increased from\nk\u201c0(no blur) to k\u201c191(extensively blurred).\nTable 6.5: Results utilizing the RGB & the synthetic thermal video.\nEpochs Modality BlurAugmentations Task\nBasic Masking P(Aug) NP vs P 4\n100 RGB 0 \u2713 10-20 0.7 67.47\n100 Thermal 0 \u2713 10-20 0.7 68.05\n100 RGB 41 \u2713 10-20 0.7 66.61\n100 Thermal 41 \u2713 10-20 0.7 67.10\n100 RGB 91 \u2713 10-20 0.7 64.80\n100 Thermal 91 \u2713 10-20 0.7 66.93\n100 RGB 191 \u2713 10-20 0.7 60.34\n100 Thermal 191 \u2713 10-20 0.7 66.24\nBlur: Gaussian blurring with kernel sizes k\nThis suggests that critical information, such as visually represented facial temperature in\nthe synthetic modality, is unaffected or slightly impacted. Fig. 6.4 displays the embedding\ndistribution for the RGB and synthetic thermal modalities at k\u201c0andk\u201c191, highlight-\ning a distinct variation in distribution patterns, albeit with ambiguous data point separation.\nFork\u201c191, while RGB embeddings tend to cluster and potentially overlap, many points\nconspicuously stray from the central mass without any distinct arrangement. Conversely, the\ndata points for the synthetic modality spread more uniformly, possibly indicating better class\ndifferentiation.114 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nRGB-0\nThermal-0 Thermal-191RGB-191\nNP\nP4\nFigure 6.4: Distributions of 3D embeddings for NP (no pain) and P4 (very severe pain) classes\nin RGB and synthetic thermal videos, for k\u201c0(clear) and k\u201c191(heavily\nblurred).\nFusion\nThree fusion strategies were assessed for multimodal analysis involving RGB and synthetic\nthermal video data. Initially, the fusion strategy referenced in 6.11 utilized learned weights\nw1andw2to scale the contributions of each modality. A second strategy incorporated an\nadditional weight, w3, modifying the formula to w3\u00a8pw1\u00a8VRGB`w2\u00a8VThermalq. The\nthird method bypassed learned weights altogether, directly combining the embedding vectors\nfrom the modalities. The results, detailed in Table 6.6, indicate that omitting learned weights\nachieved accuracies of 64.92% and26.40% for the binary and multi-class tasks, respectively.\nThe introduction of w3reduced 0.5%in accuracy for both tasks. The strategy using only\nweights w1andw2yielded the best outcomes, with accuracies of 65.08% and26.50% for\nthe binary and multi-class tasks, respectively. By maintaining the use of weights w1and\nw2and increasing the training duration from 100to300epochs, while consistent with the\naugmentation settings, accuracies improved to 69.50% and29.80% for the binary and multi-\nclass tasks, respectively. Further prolonging the training to 500epochs, with no evidence\nof overfitting, led to further improved performances, with final accuracies of 71.03% and\n30.70% for the respective tasks.6.4. SUMMARY 115\nTable 6.6: Results utilizing the fusion of RGB & synthetic thermal video.\nEpochsFusion\nweightsAugmentations Task\nBasic Masking P(Aug) NP vs P 4 MC\n100 \u2013 \u2713 10-20 0.7 64.92 26.40\n100 W2 \u2713 10-20 0.7 65.08 26.50\n100 W3 \u2713 10-20 0.7 64.42 25.90\n300 W2 \u2713 10-20 0.7 69.50 29.80\n500 W2 \u2713 10-20 0.7 71.03 30.70\nW2: utilization of [w 1,w2] W3: utilization of [w 1,w2,w3]\nComparison with Existing Methods\nThis section compares the proposed method with other methodologies reported in the liter-\nature. We utilize Part A of the BioVid dataset, which includes all 87subjects, employing\nthe LOSO cross-validation protocol for validation. The results are displayed in Table 6.7.\nOur vision-based approach, which leverages RGB and synthetic thermal modalities, shows\nperformances comparable to or surpass those of prior studies. Relative to results in the lit-\nerature from [180, 217, 219, 295], our method achieved higher accuracy in both binary and\nmulti-level tasks. Notably, the research in [296] recorded accuracies of 72.40% and30.80%,\nmarking an improvement of 1.37% and0.10% over our method, respectively. The highest\nreported results are from [37], which utilized a transformer-based architecture.\nAdditionally, Table 6.8 compares our results with those from [305], where the authors\nintroduced the MIntPAIN dataset comprising both RGB and thermal videos for automatic\npain assessment across five intensity levels. Our analysis revealed that the accuracies of\nthe RGB and thermal modalities were closely matched at 18.55% and18.33%, respectively,\nwhich parallels our observations of similar performance between RGB and synthetic ther-\nmal modalities. By integrating these modalities, the authors in [305] reported a significant\nperformance increase of 30.77%, surpassing our modest gains. It should be emphasized that\nin [305], the performance levels of the individual modalities were below the random guess\nprediction threshold of 20%. It was only through their combination that performance was\nelevated above this threshold.\n6.4 Summary\nThis chapter investigated the creation of synthetic thermal imagery via GAN models to as-\nsess its utility in automatic pain evaluation. Additionally, a novel framework incorporating\naVision-MLP and supported by a Transformer module as the core of the assessment sys-\ntem was introduced. The experiments demonstrated the effectiveness of the synthetic ther-116 CHAPTER 6. SYNTHETIC DATA: THE ROLE OF THERMAL IMAGING\nTable 6.7: Comparison of studies that utilized BioVid , videos, and LOSO cross-validation.\nStudy MethodTask\nNP vs P 4 MC\nWerner et al. [296] Deep RF 72.40 30.80\nWerner et al. [295] RF 70.20 \u2013\nZhiet al. [217] SLSTM 61.70 29.70\nThiam et al. [219] 2D CNN, biLSTM 69.25 \u2013\nTavakolian et al. [180] 2D CNN 71.00 \u2013\nGkikas et al. [37] Vision-Transformer 73.28 31.52\nOur Vision-MLP 71.03 30.70\nTable 6.8: Comparison with the MIntPAIN dataset.\nStudy Dataset ModalityTask\nMC\nHaque et al. [305] MIntPAINRGB 18.55\nThermal\u02dd18.33\nFusion 30.77\nOur BioVidRGB 30.02\nThermal\u203929.69\nFusion 30.70\n\u02dd:real\u2039: synthetic\nmal modality, which achieved performances comparable to or better than the original RGB\nmodality. This research also delved into the factors contributing to this effectiveness, par-\nticularly the role of temperature color representations in the analysis. Furthermore, various\nfusion techniques were used to evaluate the combination of the two vision modalities, high-\nlighting the potential for performance improvements over single-modality approaches. It\nis important to note that further enhancements and experimental work, especially with the\nmultimodal approach, could improve outcomes. The generation and integration of synthetic\nmodalities, such as thermal imagery, within an automatic pain assessment framework exhibit\nconsiderable promise, warranting additional exploration and research.Chapter 7\nGeneral-Purpose Models\nContents\n7.1 Chapter Overview: Introduction &Related Work . . . . . . . . . . . . . . . . 117\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment . . . . . . . . . . 119\n7.2.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119\n7.2.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . . . . . 124\n7.2.3 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128\n7.3 A Foundation Model for Automatic Pain Assessment . . . . . . . . . . . . . . . 129\n7.3.1 Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 129\n7.3.2 Experimental Evaluation &Results . . . . . . . . . . . . . . . . . . . . . . 136\n7.3.3 Comparison with existing methods . . . . . . . . . . . . . . . . . . . . . . 144\n7.3.4 Interpretation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145\n7.3.5 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 149\n7.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150\n7.1 Chapter Overview: Introduction & Related Work\nThis chapter discusses the approaches and findings from [40] and [41]. In recent years,\nthe literature on deep learning has shown a trend towards adopting general-purpose models.\nThese models are characterized by architectures not tied to specific modalities or pre-training\non datasets derived solely from a single domain. We will explore two approaches: first, a\nmodality-agnostic pipeline for automatic pain assessment, and second, the development of a\nfoundation model explicitly applied for automatic pain assessment tasks.\nRegarding modality-agnostic approaches, in 5.3, we introduced the concept specifically\nfor augmentations. These augmentations were designed not directly for the image or biosig-\nnal space but for the latent space. Thus, regardless of the original input modality, whether\n117118 CHAPTER 7. GENERAL-PURPOSE MODELS\nimages, biosignals, or others, the augmentations were applied to their feature representa-\ntions. This chapter advances our work by developing a modality-agnostic multimodal fusion\npipeline evaluated in pain assessment tasks. The literature on modality-agnostic approaches\nremains limited. In [314], the researchers pursued a novel approach by exploring modality-\nagnostic representations through knowledge distillation for semantic segmentation. Their\ngoal was to reduce the modality gap and diminish semantic ambiguity, enabling the com-\nbination of various modalities under any visual conditions. In [315], the authors addressed\nthe persistent challenges of temporal asynchrony and modality heterogeneity in multimodal\nsequence fusion, often leading to performance bottlenecks. To overcome these issues, they\ndeveloped a strategy integrating modality-exclusive and modality-agnostic representations\nfor multimodal ideo sequence fusion. This approach enabled them to capture reliable con-\ntext dynamics within individual modalities and enhance distinctive features across modality-\nexclusive spaces. Additionally, they designed a hierarchical cross-modal attention module\nto uncover significant element correlations across different modalities within the modality-\nagnostic space.\nThe literature on foundation models is considerably more extensive. With the emerging\nparadigm of building AI systems around foundation models, there has been a shift toward\ncreating more adaptable and scalable systems that generalize across various tasks and do-\nmains. A foundation model is defined as any model trained on vast datasets, often through\nextensive self-supervision, which can then be adapted\u2014such as through fine-tuning\u2014to a\nwide range of downstream tasks. Despite their reliance on traditional deep learning and\ntransfer learning techniques, the extensive scale of foundation models fosters the develop-\nment of new capabilities and enhances effectiveness across many tasks [316]. Numerous\nexamples have surfaced recently in academic literature. For instance, the SAM model [317],\na foundation model for image segmentation, was initially trained from scratch on 11million\nimages. In later studies [318, 319], researchers have adapted SAM for medical imaging by\noptimizing it for smaller, specialized datasets. Additionally, a notable paradigm shift has oc-\ncurred with the introduction of generalist models [320], a novel class of foundation models\ntrained simultaneously on various tasks under a unified learning strategy, typically super-\nvised. This approach is particularly advantageous in computer vision, enabling handling\ndiffering embedding representations across tasks and various visual modalities [321].\nTo the best of our knowledge, there are no modality-agnostic or foundation models in\nthe field of automatic pain assessment. Currently, the majority of the approaches utilize\npre-trained models; however, these models typically adhere to the traditional methodology\nof pre-training on a general, large-scale dataset and then fine-tuning for the specific task of\npain assessment. Studies such as those detailed in [305, 322] rely on transfer learning tech-\nniques derived from facial recognition datasets. The majority of these studies are reviewed\nin Chapter 3.7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 119\n7.2 A Modality-Agnostic Pipeline for Automatic Pain Assessment\nWe present a modality-agnostic multimodal framework that leverages both video and fNIRS\ndata. The pipeline operates on a dual Vision Transformer (ViT) format, which obviates the\nnecessity for modality-specific architectures or expansive feature engineering. It interprets\nthe inputs as unified images through 2D waveform representations.\n7.2.1 Methodology\nThis section outlines the pipeline for the proposed multi-modal automatic pain assessment\nframework, describing the model architectures, pre-processing methods, pre-training strate-\ngies, and augmentation techniques.\nFramework Architecture\nThe proposed framework, Twins-PainViT , includes two modules: PainViT\u20131 andPainViT\u20132 .\nBoth models share uniform architectures and parameters and follow to identical pre-training\nprotocol. PainViT\u20131 is tasked with processing the individual input video frames and the\nvisualized fNIRS waveforms, serving as an embedding extractor. Meanwhile, PainViT\u20132\nmanages the visual representation of these embeddings and performs the final pain classifi-\ncation task.\nPainViT: Vision Transformers (ViTs) [323] have established themselves as a leading frame-\nwork in computer vision tasks, recognized for their impressive performance. However,\ndespite their effectiveness, transformer-based models encounter scalability challenges with\nlarger input sizes, significantly increasing computational demands. This inefficiency mainly\nstems from the element-wise operations in the multi-head self-attention mechanism. Efforts\nto improve the efficiency and simplify the architecture of transformer-based models have\nincluded modifications to the self-attention module and overall structural adjustments [324]\n[325]. Our methodology builds on the principles of [326], which incorporates hierarchical\narchitectures into vision transformers, and [327], which introduces mechanisms to enhance\nefficiency and processing speed.\nPainViT\u2013block: A block consists of two key elements: the Token-Mixer and the Cascaded-\nAttention . The architecture places the Cascaded-Attention module centrally, combined with\naToken-Mixer module before and after it. For every input image I, overlapping patch em-\nbedding is utilized, generating 16\u02c616patches. Each patch is then projected into a token\nwith a dimensionality of d.120 CHAPTER 7. GENERAL-PURPOSE MODELS\nToken-Mixer: To better integrate local structural information, the token Tis processed\nthrough a depthwise convolution layer:\nYc\u201cKc\u02daTc`bc. (7.1)\nHere, Ycdenotes the output of the depthwise convolution for channel cof the token Tc.Kc\nrepresents the convolutional kernel for channel c,Tcindicates the c-th channel of the token\nT, and bcis the bias term added to the convolution output for channel c. The symbol \u02da\nindicates the convolution operation. After the depthwise convolution, batch normalization is\nthen applied to the output:\nZc\u201c\u03b3c\u02dc\nYc\u00b4\u00b5Ba\n\u03c32\nB`\u03f5\u00b8\n`\u03b2c. (7.2)\nHere, Zcrepresents the batch-normalized output for channel cof the token T. The learnable\nparameters \u03b3cand\u03b2care specific to channel c, adjusting the scale and shift of the normalized\ndata. \u00b5Bdenotes the batch mean of Yc,\u03c32\nBindicates the batch variance of Yc, and \u03f5is a\nsmall constant included for numerical stability to prevent division by zero. Subsequently, a\nfeed-forward network (FFN) is used to enhance communication between different feature\nchannels:\n\u03a6FpZcq\u201cW2\u00a8ReLUpW1\u00a8Zc`b1q`b2. (7.3)\nHere, \u03a6FpZcqrepresents the output of the feed-forward network for the input Zc. The weight\nmatrices for the first and second linear layers are denoted by W1andW2, respectively; b1\nandb2are the corresponding bias terms for these layers. The activation function employed\nhere is ReLU.\nCascaded-Attention Respecting the attention mechanism, it includes a single self-attention\nlayer. For each input embedding:\nXi`1\u201c\u03a6ApXiq. (7.4)\nTheXirefers to the entire input embedding for the i-thPainViT-block . The Cascaded-\nAttention module uses a cascaded mechanism that splits the full input embedding into smaller\nsegments, with each segment directed to a specific attention head. This method distributes\nthe computational load across the heads, improving efficiency by managing long input em-\nbeddings more effectively. The attention mechanism operates as follows:\nrXij\u201cAttnpXijWQ\nij, XijWK\nij, XijWV\nijq, (7.5)\nrXi`1\u201cConcatrrXijsj\u201c1:hWP\ni, (7.6)7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 121\nTable 7.1: Number of parameters and FLOPS for the components of the proposed Twins-PainViT .\nModule Params (M) FLOPS (G)\nPainViT\u20131 16.46 0.59\nPainViT\u20132 16.46 0.59\nTotal 32.92 1.18\nwhere each j-th head is responsible for computing the self-attention of Xi,j, thej-th segment\nof the full input embedding Xi. This embedding is organized as rXi1, Xi2, . . . , X ihs, with j\nranging from 1 to h, where his the total number of heads. The projection layers WQ\nij,WK\nij,\nandWV\nijtransform each segment of the input embedding into distinct subspaces for queries,\nkeys, and values, respectively. The process concludes with WP\ni, a linear layer that combines\nthe outputs of all heads, restoring them to the original input dimensionality. Additionally, the\ncascaded design aids in developing more complex representations for the Q,K, andVlayers.\nThis enhancement occurs as the output from each head is fed into the subsequent head,\nallowing for a progressive accumulation of information throughout the layers. Specifically:\nX1\nij\u201cXij`rXipj\u00b41q. (7.7)\nHere, X1\nijdenotes the sum of the j-th input segment Xijand the output \u02dcXipj\u00b41qfrom thepj\u00b4\n1q-th head. This summation is the new input embedding for the j-th head in the self-attention\ncomputation. Additionally, depthwise convolution is applied to each Qin every attention\nhead, enhancing the self-attention process\u2019s ability to capture both global representations\nand local details.\nThe architecture consists of three PainViT\u2013blocks with depths of 1,3, and 4. This hier-\narchical design progressively reduces the number of tokens by subsampling the resolution\nby2\u02c6at each stage, enabling the extraction of embeddings with dimensions dacross the\nblocks, particularly 192,288, and 500. Each block also features a multihead self-attention\nmechanism, employing 3,3, and 4heads, respectively. Fig. 1(a-d) depicts the PainViT archi-\ntecture and its core components, while Table 7.1 details the number of parameters and the\ncomputational costs measured in floating-point operations (FLOPS).\nEmbedding extraction & Fusion\nFor every frame in a video, V\u201crv1, v2, . . . , v ns,PainViT\u20131 generates a corresponding em-\nbedding. These embeddings are combined to produce a composite feature representation of\nthe video. Similarly, for each fNIRS signal channel, C\u201c rc1, c2, . . . , c ms,PainViT\u20131 ex-\ntracts embeddings which are then compiled to form a complete representation of the fNIRS122 CHAPTER 7. GENERAL-PURPOSE MODELS\nsignal. The following equations outline this procedure:\nEV\u00d0n\u00ff\ni\u201c1PainViT\u20131pviq, (7.8)\nEC\u00d0m\u00ff\ni\u201c1PainViT\u20131pciq, (7.9)\nwhere EVandECrepresent the embedding representations for the video and fNIRS, re-\nspectively. After these embeddings are extracted, EVandECare visualized as waveform\ndiagrams. The waveforms from both modalities\u2014video and fNIRS\u2014are then combined into\na single image with a resolution of 224\u02c6224. This integrated visual representation is input\ninto PainViT\u20132 for final pain assessment. (Fig. 1e) provides a high-level overview of the\nmultimodal proposed pipeline.\nPre-processing\nThe preprocessing steps include face detection in video frames and generating waveform\ndiagrams from the original fNIRS data. The MTCNN face detector [278], which uses a\nseries of cascaded convolutional neural networks, was employed to identify faces and facial\nlandmarks with the faces set at a resolution of 224\u02c6224pixels. All fNIRS channels are used\nto create waveform diagrams, which visually represent the signal\u2019s wave shape and form\nover time, displaying amplitude, frequency, and phase. This method offers a straightforward\napproach to visualizing a signal without requiring transformations or complex computations\ntypical of spectrograms, scalograms, or recurrence plots. Similarly, embeddings extracted\nbyPainViT\u20131 are visualized using waveform diagrams. Although these embeddings are not\nsignals per se, the 1D vectors can still be plotted in a 2D space for further analysis or use by\ndeep-learning vision models. Waveform diagrams created from fNIRS data and embeddings\nare formatted as images with a 224\u02c6224pixels resolution. Fig. 7.2 shows waveform\nrepresentations of channel-specific fNIRS signals, an embedding extracted from a video,\nand an embedding derived from a channel-specific fNIRS sample.\nPre-training\nBefore the automatic pain assessment training, the Twins-PainViT models underwent pre-\ntraining using a multi-task learning approach. This pre-training incorporated four datasets de-\nsigned for emotion recognition tasks. The AffectNet [287] and RAF-DB basic [289] datasets\nsupplied facial images to train on basic emotions, whereas the Compound FEE-DB [288]\nandRAF-DB compound [289] datasets focused on complex emotional states. In addition,\nfive datasets containing various biosignals were utilized. The EEG-BST-SZ [328] dataset in-7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 123\nTable 7.2: Datasets utilized for the pretraining process of the framework.\nDataset # samples # classes Modality\nAffectNet [287] 0.40M 8 Facial Images\nRAF-DB basic [289] 15,000 7 Facial Images\nRAF-DB compound [289] 4,000 11 Facial Images\nCompound FEE-DB [288] 6,000 26 Facial Images\nEEG-BST-SZ [328] 1.5M 2 EEG\nSilent-EMG [329] 190,816 8 EMG\nBioVid [109] 8,700 5 ECG\nBioVid [109] 8,700 5 EMG\nBioVid [109] 8,700 5 GSR\nEEG: electroencephalogram EMG: electromyogram ECG: electrocardiogram GSR: galvanic\nskin response\ncludes electroencephalograms for schizophrenia analysis, and the Silent-EMG [329] contains\nelectromyograms that help pinpoint the origin of EMG activities, such as from the throat or\nmid-jaw areas. The BioVid [109] dataset provided electrocardiogram, electromyogram, and\ngalvanic skin response samples for pain assessment. All biosignals were represented as\nwaveforms, as detailed in 7.2.1. The equation defines the multi-task learning framework:\nLtotal\u201c9\u00ff\ni\u201c1rewiLSi`wis, (7.10)\nwhere LSirepresents the loss for each specific task from the various datasets, and wiare\nthe learned weights that drive the learning process to minimize the combined loss Ltotal,\nconsidering all individual losses. Table 7.2 outlines the datasets involved in the pre-training\nphase.\nAugmentation Methods & Regularization\nSeveral augmentation methods have been employed to train the proposed framework. For the\npre-training process, RandAugment [330] and TrivialAugment [282] were used, along with\nauxiliary noise from a uniform distribution and a custom MaskOut technique that masks\nout random square sections of input images. In the automatic pain assessment task, Aug-\nMix[281] is used in addition to the previously mentioned methods. Moreover, Label Smooth-\ning[331] and DropOut [332] are implemented as regularization techniques to optimize the\ntraining outcome.124 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.3: Training details for the automatic pain assessment.\nOptimizer LR LR\ndecayWeight\ndecayEpochs Warmup\nepochsCooldown\nepochsBatch\nsize\nAdamW 2e-5 cosine 0.1 100 10 10 32\nLR: learning rate\n7.2.2 Experimental Evaluation & Results\nWe employ the dataset provided by the challenge organizers [118,333], which includes facial\nvideos and fNIRS data from 65participants. The dataset is partitioned into 41 training, 12\nvalidation, and 12 testing subjects, all recorded at the Human-Machine Interface Laboratory,\nUniversity of Canberra, Australia. Electrodes for transcutaneous electrical nerve stimula-\ntion, used as pain stimuli, were placed on the right hand\u2019s inner forearm and back. The study\nmeasures both pain threshold\u2014the lowest stimulus intensity perceived as painful\u2014and pain\ntolerance\u2014the maximum intensity of pain a participant can tolerate. For the fNIRS mea-\nsurements, 24channels were utilized for both HbO and HbR, and each video in the dataset\ncontains 30frames. This paper focuses on the results from the validation segment of the\ndataset, which are structured into a multi-level classification setting for No Pain ,Low Pain ,\nandHigh Pain . Table 7.3 details the training framework for the automatic pain assessment. It\nshould be noted that while numerous experiments were conducted across different modalities\nand their combinations, only the most successful outcomes are discussed in the subsequent\nsections and detailed in the corresponding tables.\nFacial Videos\nFor facial videos, two embedding fusion methods were implemented: the Addition tech-\nnique, which aggregates 30embeddings into a single vector of dimension d\u201c500, and the\nConcatenation approach, which merges the embeddings into a larger vector of d\u201c15,000.\nWith the Addition method, the initial accuracy reached 41.90% under basic augmentation\nand regularization settings. Enhancing the augmentation intensity and adjusting MaskOut\nimproved the accuracy incrementally, achieving a peak of 44.91%. Adjusting DropOut and\nother augmentation parameters refined the performance to 43.52%. These findings are de-\ntailed in Table 7.4. Employing the Concatenation method with initial uniform augmentation\nprobabilities resulted in a starting accuracy of 40.28%. Strategic increases in MaskOut and\nmaintaining other augmentations at moderate levels led to a gradual accuracy improvement,\nculminating in a high of 43.75% when all augmentations were maximized except MaskOut ,\npaired with high regularization settings. The results of this approach are outlined in Table\n7.5.7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 125\nTable 7.4: Results utilizing the video modality & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.1 0.1 0.1 0.1 |3 0.1 0.5 41.90\n0.5 0.5 0.5 0.7 |3 0.0 0.5 44.91\n0.5 0.5 0.5 0.7 |10 0.0 0.5 42.13\n0.5 0.5 0.5 0.7 |3 0.0 0.6 42.36\n0.9 0.9 0.9 0.7 |3 0.3 0.7 43.52\nRand :RandAugment Trivial : TrivialAugment LS:Label Smoothing MS: multiclass pain\nassessment. For Augmentation & Regularization the first number represents the probability\nof application, while in MaskOut the number followed |indicates the number of square\nsections applied.\nTable 7.5: Results utilizing the video modality & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.3 0.3 0.3 0.3 |3 0.1 0.5 40.28\n0.5 0.5 0.5 0.8 |5 0.0 0.5 41.44\n0.9 0.9 0.9 0.7 |3 0.2 0.7 42.13\n0.9 0.9 0.9 0.7 |1 0.4 0.5 41.90\n0.9 0.9 0.9 0.6 |3 0.4 0.5 43.75\nfNIRS\nSimilar to facial videos, the Addition andConcatenation methods were also applied to the\nfNIRS channels, excluding two faulty ones from the original 24. For the HbR with the\nAddition method, the initial accuracy was 39.35%, set with uniform probabilities of 0.5for\nAugMix ,Rand , and Trivial , and MaskOut adjusted to 0.6|5. Modifying MaskOut to0.7|3and\nincreasing LSslightly reduced accuracy, while subsequent adjustments in LSandDropOut\nimproved it to 41.20% (refer to Table 7.6). In the HbR with the Concatenation method, start-\ning with MaskOut at0.7|3led to an accuracy of 40.97%. Escalating all augmentations to\n0.9, while keeping MaskOut at0.7|3, achieved a peak accuracy of 42.13% (refer to Table\n7.7). For the HbO with the Addition method, accuracies started at 43.06% with uniform aug-\nmentation probabilities of 0.3andMaskOut at0.3|3. Elevating MaskOut to0.7|3with minor\nadjustments in LSandDropOut maintained similar accuracies, while optimizing MaskOut\nto0.8|3enhanced performance to 44.68% (refer to Table 7.8). The HbO with the Concate-\nnation method began with an accuracy of 42.13% under an augmentation probability of\n0.1. A balanced augmentation setup at 0.9andMaskOut at0.7|3lifted the peak accuracy\nto44.44%, demonstrating the effectiveness of increased overall augmentation coupled with\nhigh regularization. Further adjustments slightly reduced accuracy, highlighting the critical126 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.6: Results utilizing the HbR & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.6 |5 0.0 0.5 39.35\n0.5 0.5 0.5 0.7 |3 0.4 0.5 38.89\n0.9 0.9 0.9 0.7 |3 0.1 0.9 40.05\n0.9 0.9 0.9 0.7 |5 0.4 0.5 41.20\n0.5 0.5 0.5 0.7 |3 0.0 0.4 40.51\nTable 7.7: Results utilizing the HbR & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.0 0.0 0.7 0.7 |3 0.0 0.5 40.97\n0.5 0.5 0.5 0.7 |1 0.0 0.5 41.44\n0.9 0.9 0.9 0.7 |3 0.1 0.8 42.13\n0.9 0.9 0.9 0.7 |3 0.4 0.5 41.20\n0.5 0.5 0.5 0.7 |3 0.0 0.3 39.81\nTable 7.8: Results utilizing the HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.3 0.3 0.3 0.3 |3 0.1 0.5 43.06\n0.5 0.5 0.5 0.7 |3 0.2 0.5 42.82\n0.9 0.9 0.9 0.7 |3 0.4 0.8 43.29\n0.9 0.9 0.9 0.7 |9 0.4 0.5 44.44\n0.9 0.9 0.9 0.8 |3 0.4 0.5 44.68\nnature of optimal augmentation settings (refer to Table 7.9). Generally, enhanced perfor-\nmance is observed with HbO compared to HbR, as noted in other studies [334], attributed\nto its superior signal-to-noise ratio. Combining HbR and HbO using the Addition method\ninitially showed an accuracy of 42.82% with all augmentations at zero except for MaskOut\nat0.7|3. Increasing AugMix ,Rand , and Trivial to0.5while raising MaskOut to0.7|7slightly\nimproved accuracy to 43.29%. Adjustments to MaskOut back to 0.7|3with a slight increase\ninLSled to a minor reduction in accuracy to 42.59%. However, further increasing all aug-\nmentations to 0.9andLSto0.3while maintaining MaskOut at0.7|3maximized accuracy\nto43.75%. A reduction in DropOut to0.1in the final setup slightly reduced accuracy to\n43.06%, emphasizing the importance of optimizing regularization alongside augmentation\nstrategies for achieving optimal results (refer to Table 7.10).7.2. A MODALITY-AGNOSTIC PIPELINE FOR AUTOMATIC PAIN ASSESSMENT 127\nTable 7.9: Results utilizing the HbO & Concatenation method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.1 0.1 0.1 0.1 |3 0.1 0.5 42.13\n0.5 0.5 0.5 0.0 |0 0.0 0.5 43.98\n0.5 0.5 0.5 0.7 |1 0.0 0.5 42.36\n0.9 0.9 0.9 0.7 |3 0.4 0.9 44.44\n0.5 0.5 0.5 0.7 |3 0.0 0.8 43.52\nTable 7.10: Results utilizing the HbR, HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.0 0.0 0.7 0.7 |3 0.0 0.5 42.82\n0.5 0.5 0.5 0.7 |7 0.0 0.5 43.29\n0.5 0.5 0.5 0.7 |3 0.1 0.5 42.59\n0.9 0.9 0.9 0.7 |3 0.3 0.9 43.75\n0.5 0.5 0.5 0.7 |3 0.0 0.1 43.06\nTable 7.11: Results utilizing the videos, HbO & Addition method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.4 |5 0.0 0.5 42.36\n0.5 0.5 0.5 0.7 |9 0.0 0.5 41.67\n0.9 0.9 09. 0.7 |3 0.1 0.6 42.59\n0.9 0.9 0.9 0.7 |3 0.3 0.9 43.06\n0.9 0.9 0.9 0.7 |5 0.4 0.5 43.75\nFusion\nThis section explores the fusion of facial videos and fNIRS, explicitly utilizing HbO due\nto its demonstrated superior performance over HbR. Two fusion methods were employed:\ntheAddition method, which aggregates embeddings from video frames and fNIRS channels\ninto a unified vector, and the Single-Diagram method, where aggregated embeddings from\nboth modalities are visualized simultaneously in a single image. For the Addition method,\ninitial configurations with moderate augmentation levels ( 0.5forAugMix ,Rand ,Trivial ) and\nMaskOut at0.4|5achieved an accuracy of 42.36%. Increasing the augmentation levels to 0.9\nand adjusting the regularization parameters ( LSup to 0.4andDropOut up to 0.9) enhanced\nthe accuracy, peaking at 43.75% (refer to Table 7.11). For the Single Diagram method, accu-\nracy improvements were noted, as shown in Table 7.12. Starting with lower MaskOut levels128 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.12: Results utilizing the videos, HbO & Single Diagram method.\nAugmentation Regularization Task\nAugMix Rand Trivial MaskOut LS DropOut MC\n0.5 0.5 0.5 0.3 |5 0.0 0.5 45.83\n0.9 0.9 0.9 0.7 |3 0.1 0.6 46.76\n0.9 0.9 0.9 0.7 |3 0.3 0.6 46.53\n0.9 0.9 0.9 0.9 |3 0.4 0.5 45.83\n0.5 0.5 0.5 0.7 |3 0.0 0.7 45.14\nTable 7.13: Comparison with the validation baseline provided by the AI4PAIN challenge organizers.\nApproachModality\nVideo fNIRS Fusion\nBaseline 40.00 43.20 40.20\nOur 44.91 44.68 46.76\nat0.3|5and standard augmentation probabilities ( 0.5), the accuracy was 45.83%. Utilizing\naugmentation probabilities to 0.9andMaskOut adjustments to 0.7|3significantly improved\nperformance, achieving a high of 46.76%.\nInterpretation & Comparison\nIn the framework\u2019s analysis, attention maps from the last layer of PainViT\u20132 were generated,\nillustrating the processed unified image that integrates both the video and HbO embedding\nwaveforms. This layer consists of 500neurons, each specifically engaging with different in-\nput aspects. Figure 7.3 displays four examples demonstrating how specific neurons predom-\ninantly focus on the video embedding waveform. In contrast, others concentrate on the HbO\nwaveform, and some attend to both, highlighting various details. Table 7.13 compares the\nproposed pipeline and the baseline results set by the challenge organizers. The video-based\napproach utilizing the Addition method exceeded the baseline by 4.91%. Implementing the\nHbO with the Addition method showed a minor improvement of 1.48%. However, the fu-\nsion of modalities through the Single Diagram method achieved a more substantial gain of\n6.56%.\n7.2.3 Discussion\nThis chapter contributes to the First Multimodal Sensing Grand Challenge for Next-Gen\nPain Assessment (AI4PAIN) , utilizing facial videos and fNIRS in a modality-agnostic frame-\nwork. The Twins-PainViT , a dual Vision Transformer configuration, was pre-trained across7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 129\nmultiple datasets using a multi-task learning approach. A key feature of our approach is the\nwaveform representation applied to the original fNIRS data and the learned embeddings, al-\nlowing for their integration into a single image diagram. This method effectively eliminates\nthe need for domain-specific models for each modality. Our experiments demonstrated high\nperformance in both unimodal and multimodal configurations, outperforming the established\nbaselines. The analysis of PainViT\u20132 through attention maps further revealed that specific\nneurons specifically target different modalities or distinct aspects within them, suggesting a\ncomprehensive analytical approach. Future research should continue to explore multimodal\nstrategies, as they have shown superior efficacy in real-world pain assessment settings. De-\nveloping interpretative methods is crucial for integrating these advanced frameworks into\nclinical practice.\n7.3 A Foundation Model for Automatic Pain Assessment\nWe introduce PainFormer , a multi-task learning vision foundation model tailored for auto-\nmatic pain assessment. This initiative is the first to develop and deploy a foundation model\nfor pain recognition, inspired by the frameworks discussed in [320]. Our method involves\ntraining across various datasets and tasks, leveraging large-scale corpora to enhance repre-\nsentation learning for pain assessment applications. This research makes three key contri-\nbutions: (1) it introduces a foundation model capable of extracting robust embeddings from\ndiverse modalities, (2) it incorporates synthetic thermal and estimated depth videos as inno-\nvative modalities, and (3) it evaluates the performance of these modalities in both unimodal\nand multimodal settings.\n7.3.1 Methodology\nThis section outlines the structure and components of the suggested framework. It covers\nthe foundation model\u2019s pretraining based on multi-task learning, the methods for augmen-\ntation, and the training configurations for pretraining and pain evaluation tasks. It also de-\nscribes how synthetic thermal and depth videos are generated and details the visualization\ntechniques for biosignal modalities employed in this research.\nFramework Architecture\nThe framework integrates three models: the PainFormer , a foundation model that extracts\nembeddings from input data; the Embedding-Mixer , which applies these embeddings, either\nsingly or in combination, to classify pain; and the Video-Encoder , which reduces video data\ninto a lower-dimensional latent space for use in the multimodal approaches that are explained\nlater. This framework operates in two separate stages: initially extracting embeddings and130 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.14: Number of parameters and FLOPS for the modules of the proposed framework.\nModule Params\n(Millions)FLOPS\n(Giga)\nPainFormer 19.60 5.82\nEmbedding-Mixer 9.85 2.94\nVideo-Encoder 3.37 0.86\nTotal 32.82 9.62\nthen deploying them according to the demands of specific modality pipelines. Table 7.14\ndetails the parameters and computational costs for each module\u2019s floating-point operations\n(FLOPS).\nPainFormer: Vision Transformers (ViT) have become increasingly widespread for various\nimage-processing tasks, demonstrating the effectiveness of their self-attention mechanisms.\nMoreover, developing Vision Multilayer Perceptron (Vision-MLP) models that use spectral\nmixing techniques\u2014substituting self-attention layers with Fourier transformation layers\u2014\nillustrates that simpler structures with fewer inductive biases can achieve similar outcomes.\nOur strategy incorporates two key concepts: hierarchical Vision Transformers (ViT) [326],\nwhich use multiple embedding extraction stages to boost performance and scalability, and\nthe Fourier transform\u2019s efficient token information mixing as shown in [335]. PainFormer\nintegrates spectral layers using the Fast Fourier Transform (FFT) with self-attention layers.\nBoth spectral and self-attention layers are initially applied, whereas later stages rely solely\non self-attention. The architecture of PainFormer is illustrated in Fig. 1(a). Each 2D input\nimage Iis segmented into nnon-overlapping patches, each patch PRn\u02c6h\u02c6w\u02c63, where hand\nware the patch resolution set at 16\u02c616, and 3represents the RGB channels. Each patch\nis linearly projected into a dimension d\u201c768, followed by positional encoding. Applying\nDiscrete Fourier Transform (DFT) to a 1D sequence of Nelements, xrns, ranging from 0to\nN\u00b41, yields:\nXrks\u201cN\u00b41\u00ff\nn\u201c0xrns\u00a8e\u00b4i2\u03c0k\nNn:\u201cN\u00b41\u00ff\nn\u201c0xrns\u00a8Wkn\nN, (7.11)\nwhere iis the imaginary unit, and WNis defined as e\u00b4i2\u03c0\nN. The sequence can be transformed\nback to the time domain by applying the inverse Discrete Fourier Transform (IDFT):\nxrns\u201c1\nNN\u00b41\u00ff\nk\u201c0Xrks\u00a8ei2\u03c0k\nNn, (7.12)7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 131\nwhere xrnsrepresents the original sequence. Furthermore, for two-dimensional inputs, xrm, ns,\nwith0\u010fm\u010fM\u00b41and0\u010fn\u010fN\u00b41, the formula extends to:\nXru, vs\u201cM\u00b41\u00ff\nm\u201c0N\u00b41\u00ff\nn\u201c0xrm, ns\u00a8e\u00b4i2\u03c0`um\nM`vn\nN\u02d8\n, (7.13)\nwhere Xru, vsis the frequency-domain representation of the input xrm, ns.\nSpectral Layer: For the tokens xfrom image I, a 2D FFT is applied across the spatial\ndimensions to transform xinto the frequency domain:\nX\u201cFrxsPCh\u02c6w\u02c6d. (7.14)\nAfter applying the FFT to extract the various frequency components of the image, we employ\na learnable filter, KPCh\u02c6w\u02c6dacts as a gate to regulate the significance of each frequency\ncomponent. This spectrum modulation allows for the identification and learning of features\nsuch as lines and edges. Specifically:\n\u02dcX\u201cKdX, (7.15)\nwhereddefines the element-wise multiplication. Afterward, the inverse Fast Fourier Trans-\nform (IFFT) is applied, which converts the spectral space back into the physical space:\nx\u00d0F\u00b41r\u02dcXs, (7.16)\nwhere the physical space is referred to as the spatial domain in this case. The final component\nof a spectrum layer is an MLP module, which enables efficient channel mixing communica-\ntion:\n\u03a6pxq\u201cW2\u00a8GELUpDWConvpW1\u00a8x`b1qq`b2, (7.17)\nwhere DWConv denotes a depthwise convolution layer. In addition, layer normalization is\nemployed before and after the FFT and IFFT processes, refer to Fig. 1(b).\nSelf-Attention Layer: In this layer, the standard self-attention mechanism characteristic\nof transformers is utilized. For a given token sequence X, the attention mechanism is defined\nas:\nAttpXq:\u201csoftmax\u02c6XW qpXW kqT\n?\nd\u02d9\nXW v, (7.18)\nwhere Att maps RN\u02c6dtoRN\u02c6d, with Nrepresenting hw. The matrices Wq,Wk, and Wvin\nRd\u02c6dcorrespond to the query, key, and value weights, respectively. Layer normalization is132 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.15: Details of the PainFormer\u2019s architecture.\nStage # Spectral\nLayers# Self-Attention\nLayers# Self-Attention\nHeadsDimension\nd\n1 2 1 2 64\n2 2 2 4 128\n3 \u2013 12 10 320\n4 \u2013 3 16 160\nd: token dimensions\napplied both before and after the attention mechanism, mirroring the approach used in the\nspectral layer. Additionally, the MLP component within this layer is expressed as:\n\u03a6pxq\u201cW2\u00a8GELUpW1\u00a8x`b1q`b2. (7.19)\nFig. 1(c) illustrates the design of this layer.\nStages: A stage-based architecture was developed to create a hierarchical representa-\ntion. PainFormer is organized into four stages, each followed by a single-layer 2D CNN that\ndownsamples the resolution by a factor of 2to reduce the token dimensions. Additionally,\neach stage incorporates a distinct combination of spectral and self-attention layers, with vary-\ning numbers of heads in the self-attention layers and different dimensions for the extracted\ntokens. Table 7.15 presents the relevant details.\nEmbedding-Mixer: The model is built on a transformer architecture, leveraging cross- and\nself-attention mechanisms. Echoing insights from prior research [277], it employs an asym-\nmetrical attention scheme using cross-attention with fewer latent variables to reduce compu-\ntational demands and boost efficiency. Cross-attention operation parallels self-attention, as\ndetailed in Eq. (7.18). Nonetheless, the dimensions for Wq,Wk, and Wvadjust to n\u02c6d\nfrom d\u02c6d, with n\u0103dandnspecifically set at 256. The Embedding-Mixer includes 2\nlayers, each equipped with 1cross-attention and 2self-attention modules. The head counts\nfor cross- and self-attention are 1and8, respectively. The final classification task utilizes an\noutput embedding of length 512, as shown in Fig. 1(d).\nVideo-Encoder: The design of this specific module mirrors the Embedding-Mixer . How-\never, it is simplified for efficiency by including only a 1layer that contains a single cross-\nattention module with a 1head. The number of latent variables, n, is maintained at 256,\nand the output embedding length is set at 40. This module functions exclusively within a7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 133\nparticular framework as part of one of the multimodal strategies, integrating video and GSR\nembeddings. The module\u2019s architecture is depicted in Fig. 1(e).\nSynthetic Thermal & Depth Videos\nThis section incorporates thermal and depth vision modalities alandGB videos into our pain\nassessment frameworks. For the thermal modality, we utilize thermal videos from our ear-\nlier research [39], described in Chapter 6, which introduced an image-to-image translation\n(I2I) method using a conditional generative adversarial network (cGAN). This network was\ndesigned and trained to map the data distribution from the RGB to the thermal domain, fa-\ncilitating the creation of synthetic thermal images from new RGB videos. For the depth\nvideos, we employ the \u201cDepth Anything\u201d technique [336], which is a pioneering model for\nmonocular depth estimation (MDE) that uses a vision transformer-based encoder-decoder\narchitecture with semi-supervised learning. Fig. 7.5 displays a frame sample from the RGB,\nsynthetic thermal, and depth modalities.\nBiosignal Visualization\nGiven that the core model in this research is vision-based, it necessitates using 2D represen-\ntations for physiological modalities. We explore four distinct visualizations: (1) waveform\ndiagrams, which outline the signal\u2019s progression over time, showcasing its amplitude, fre-\nquency, and phase characteristics; (2) spectrogram-angle , which displays the phase angles\nassociated with different frequencies; (3) spectrogram-phase , which reveals phase details\nand incorporates unwrapping to rectify discontinuities; and (4) spectrogram-PSD , which de-\nlineates the power spectral density, indicating the distribution of power across frequencies\nover time. Fig. 7.6 provides an example of each visualization type.\nFoundation Training\nPainFormer , our proposed foundation model, serves as an embedding extractor as previously\noutlined. It has undergone extensive training on 14datasets, which include a total of 10.9\nmillion samples; for further details, see Table 7.16. The training datasets cover a variety\nof human-centric data, ranging from facial recognition datasets such as VGGFace2 [280]\nandDigiFace-1M [313] to datasets aimed at recognizing basic and compound emotions,\nlike AffectNet [287] and RAF-DB [289]. Furthermore, datasets based on biosignals like\nEEG, EMG, and ECG have been incorporated. Regarding training methodology, PainFormer\nemploys a multi-task learning strategy, with each dataset representing a separate supervised\nlearning task. Architecturally, the model adheres to its initial design as specified in 7.3.1\nbut now includes additional task-specific auxiliary classifiers. These classifiers comprise\na single-layer, fully connected network with an ELU (Exponential Linear Unit) activation134 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.16: Datasets utilized for the multitask learning-based pretraining process of the framework.\nDataset # samples # classes Modality\nVGGFace2 [280] 3.31M 9,131 Facial Images\nSpeakingFaces RGB [312]\u00150.76M 142 Facial Images\nSpeakingFaces Thermal [312]\u00150.76M 142 Facial Images\nDigiFace-1M [313] 0.72M 10,000 Facial Images\nDigiFace-1M [313] 0.50M 100,000 Facial Images\nAffectNet [287] 0.40M 8 Facial Images\nSFace [337] 1.84M 10,341 Facial Images\nCACIA-WebFace [129] 0.50M 10,575 Facial Images\nRAF-DB basic [289] 15,000 7 Facial Images\nRAF-DB compound [289] 4,000 11 Facial Images\nCompound FEE-DB [288] 6,000 26 Facial Images\nEEG-BST-SZ [328]\u00121.50M 2 EEG signals\nSilent-EMG [329]\u00120.19M 8 EMG signals\nECG HBC Dataset [291]\u00120.45M 5 ECG signals\nTotal: 14 datasets\u2013tasks 10.9M\nEEG: electroencephalogram EMG: electromyography ECG \u0015: The datasets were also used for the I2I process\ndescribed in 7.3.1, in addition to the training of the PainFormer \u0012: The samples were transformed into spectrograms\nbefore being employed.\nfunction. The objective during training is to simultaneously learn from all 14datasets/tasks.\nThe following equation formalizes this approach:\nLtotal\u201c14\u00ff\ni\u201c1rewiLSi`wis, (7.20)\nwhere LSiindicates the loss linked to each dataset/task, and wiare the adaptive weights that\naim to minimize the overall loss Ltotal, encompassing all individual task losses. The model\nwas trained using this methodology over 200epochs.\nAugmentation & Regularization Methods\nVarious augmentation and regularization strategies were applied during the pre-training of\nPainFormer and in the downstream pain assessment tasks. For foundational training, Triv-\nialAugment [282] and AugMix [281] were used. A customized augmentation method that\nmodifies brightness, contrast, and saturation and involves image cropping was also imple-\nmented. The pre-training regime included adding random noise sourced from a Gaussian\ndistribution. Moreover, a technique was devised to obscure random square portions of the\ninput images. Regularization during pre-training was achieved using DropPath [338] and\nLabel Smoothing [331]. Within the pain assessment framework, two specific augmentation7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 135\nTable 7.17: Training details of the proposed framework.\nTask Optimizer LR LR decay Weight\ndecayWarmup\nepochsCooldown\nepochsBatch size\nMTL AdamW 2e-5 cosine 0.1 5 10 126\u0010\nPain AdamW 2e-5 cosine 0.1 10 10 32\nMTL: multi-task learning for pre-training the foundation model Pain: pain assessment task LR: learning rate \u0010: batch\nsize is proportionally distributed across the 14 tasks\nmethods were integrated. The first, termed Basic , involves polarity inversion and the addition\nof noise, which alters the original input embeddings by reversing data elements\u2019 polarity and\nadding random noise from a Gaussian distribution, thereby inducing variability. The second\nmethod, Masking , implements zero-valued masks on the embeddings, effectively nullify-\ning sections of the vectors. These masks are randomly sized and placed, obscuring 10%\nto20% of the embedding\u2019s total dimensions. For further regularization, techniques such as\nDropOut [332] and Label Smoothing [331] were employed. Additional specifics on the two\ntraining methodologies are detailed in Table 7.17.\nDataset Details\nTo evaluate the effectiveness and resilience of our proposed framework, we performed tests\non two specific pain datasets, BioVid [109] and AI4Pain [118]. These datasets offer a varied\nand solid foundation for validating the performance of our model in pain assessment tasks.\nBioVid Heat Pain Database: This dataset is recognized and well-established within the do-\nmain of pain research. It encompasses facial videos, electrocardiograms, electromyograms,\nand galvanic skin response measurements from eighty-seven pn=87qhealthy participants ( 44\nmales and 43females, aged between 20and65). The experiment involved applying a ther-\nmode to the participants\u2019 right arm to induce pain. Before data collection, each participant\u2019s\npain and tolerance thresholds were determined, defining the minimum and maximum levels\nof pain experienced. This setup included two additional intermediate levels, culminating in\nfive distinct pain intensities: No Pain (NP), Mild Pain (P 1), Moderate Pain (P 2), Severe Pain\n(P3), and Very Severe Pain (P 4). The temperature for inducing these pain levels ranged from\nP1to P 4but did not exceed 50.5\u02ddC. Participants underwent 20inductions at each of the four\nspecified intensity levels (P 1to P 4), with each stimulus lasting 4s, followed by a recovery\ninterval of 8to12s. Additionally, 20baseline measurements at 32\u02ddC (NP) were conducted,\nresulting in 100total stimulations per participant, administered randomly. After reaching\nthe target temperature for each induction, the data was subsequently segmented into 5.5s\nintervals starting at 1s. This segmentation generated 8,700samples, each 5.5slong, evenly\ndistributed across the five pain intensity classes for each modality, encompassing all 87sub-136 CHAPTER 7. GENERAL-PURPOSE MODELS\njects. The video recordings were captured at a frame rate of 25frames per second (FPS),\nwhile the biosignal (ECG, EMG, GSR) recordings were sampled at 512Hz.\nAI4Pain Dataset: The AI4Pain Grand Challenge 2024 dataset is a recent addition tailored\nfor advanced pain recognition tasks using fNIRS and facial video data. Sixty-five pn=65q\nvolunteers participated, including 23females, with ages ranging from 17to52years. The\ndataset additionally includes physiological signals like photoplethysmography (PPG), elec-\ntrodermal activity (EDA), and respiration (RESP), though these are not currently publicly\navailable. The dataset is segmented into three parts: training ( 41volunteers), validation\n(12volunteers), and testing ( 12volunteers). The data collection setup for this dataset in-\nvolves comprehensive fNIRS and video recording to capture both brain activity and facial\nexpressions. The fNIRS recordings were conducted using an Artinis device (Artinis Medical\nSystems, Gelderland, the Netherlands), measuring fluctuations in oxygenated haemoglobin\n(HBO2) and deoxygenated haemoglobin (HHB) concentrations (in \u00b5mol/L). This fNIRS sys-\ntem uses 24channels to cover the prefrontal cortex with optodes ( 10sources and 8detectors)\nplaced 30mm apart. It emits near-infrared light at wavelengths of 760nm and 840nm and\nsamples at a rate of 50Hz. The video data is recorded using a Logitech StreamCam at a\nframe rate of 30FPS. The AI4Pain dataset categorizes pain into three levels: No Pain ,Low\nPain, and High Pain . It includes 65instances of No Pain (lasting 60s each), 780instances\nofLow Pain (lasting 10s each), and 780instances of High Pain (also lasting 10s each). The\nNo Pain instances represent baseline data. In contrast, Low Pain andHigh Pain are derived\nfrom the pain tolerance tests, capturing subtle and significant changes in neurological and\nbehavioral responses via fNIRS and video data.\n7.3.2 Experimental Evaluation & Results\nThis research devised various testing scenarios, including unimodal and multimodal settings,\nto assess the effectiveness of the proposed foundational model. The aim is to utilize a variety\nof behavioral and physiological modalities to ascertain the capability of PainFormer to gener-\nate and provide high-quality embeddings for pain assessment. The experimental framework\nutilizes a comprehensive set of modalities, encompassing RGB, synthetic thermal imaging,\ndepth videos, and physiological measurements such as ECG, EMG, GSR, and fNIRS, with\nwaveform and spectrogram representations. Additionally, specific pipelines were tailored to\nsingle modalities or their combinations based on each integration need. This adaptability\nis a cornerstone of our approach, as different pipelines might be required depending on the\nspecific demands, data availability, or intended application. We aim to offer robust feature\nrepresentations for any given input modality and excel in performance across all modalities\nand testing scenarios. Figure 7.7 displays a high-level view of the proposed framework. Note7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 137\nthat all images, including video frames and biosignal visual representations, are standardized\nto a resolution of 224\u02c6224pixels.\nThis research employed Part A of the BioVid dataset, focusing on pain assessment in\nbinary terms, differentiating between No Pain (NP) and Very Severe Pain (P4). Validation\nwas conducted using the leave-one-subject-out (LOSO) cross-validation technique. In the\ncase of AI4Pain , a multilevel classification scheme was applied, categorizing pain into three\nlevels: No Pain ,Low Pain , and High Pain . The challenge organizers\u2019 hold-out method\n(training, validation, testing) was utilized for validation. For both datasets, the evaluation\nmetrics included accuracy, recall (sensitivity), and F1 score. It is also important to mention\nthat all experiments used a deterministic approach, ensuring no influence from random ini-\ntializations. This practice assures that any differences in performance observed are strictly\nattributable to specific optimization parameters, modalities, and other controlled variations\nrather than any random factors.\nBioVid\nNumerous experiments were performed using the BioVid dataset. Beyond the original RGB\nvideos, synthetic thermal and depth videos were developed to provide additional visual con-\ntexts, as detailed in 7.3.1. As specified in 7.3.1, four distinct representations of ECGs, EMGs,\nand GSRs were assessed for biosignals. Combinations of these representations were also ex-\nplored.\nVideo: For behavioral modalities within the BioVid dataset, PainFormer generates an em-\nbedding of dimension d\u201c160for each video frame. These embeddings are concatenated to\ncreate a comprehensive representation of each video:\nVD\u201crd1}d2}\u00a8\u00a8\u00a8} dms, DPRN1, (7.21)\nwhere mrepresents the number of frames per video, and N1is the dimensionality of the total\nembedding, computed as m\u02c6d\u00d1138\u02c6160\u201c22,080. This unified embedding is fed\ninto the Embedding-Mixer for final pain assessment. Starting with a training duration of 200\nepochs and using augmentation only on RGB videos, an accuracy of 71.83% and a recall\nof74.52% were recorded. Thermal and depth videos achieved accuracies of 69.83% and\n69.00%, respectively. When the training was extended to 300epochs with intensified aug-\nmentations and incorporating Label Smoothing for regularization, RGB accuracy improved\nto72.50%, although recall decreased slightly by 0.46%. Thermal modality performance de-\ncreased overall, highlighting its sensitivity to augmentations and regularization techniques.\nConversely, depth modalities responded well to these changes, showing improved metrics\nwith an accuracy of 70.08%, a recall of 71.27%, and an F1 score of 69.63%. In the final138 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.18: Results utilizing the video modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1RGB200 0.5 0.5 |10-20| 0.0 0.0 71.83 74.52 70.29\n300 0.7 0.7 |15-20| 0.1 0.0 72.50 74.06 70.93\n600 0.5 0.5 |15-20| 0.1 0.5 76.29 77.56 75.56Thermal200 0.5 0.5 |10-20| 0.0 0.0 69.83 71.51 69.17\n300 0.7 0.7 |15-20| 0.1 0.0 68.83 69.77 68.41\n600 0.5 0.5 |15-20| 0.1 0.5 71.55 72.83 71.12Depth200 0.5 0.5 |10-20| 0.0 0.0 69.00 69.44 67.94\n300 0.7 0.7 |15-20| 0.1 0.0 70.08 71.27 69.63\n600 0.5 0.5 |15-20| 0.1 0.5 71.67 72.84 71.26\nLS: Label Smoothing For Augmentation and Regularization, the number denotes the probability of application,\nwhile in Masking , the number in | |indicates the size of the mask applied.\nexperimental phase, training was extended to 600epochs, employing lighter augmentations\nat a0.7probability, alongside 0.1Label Smoothing and0.5DropOut . This regimen resulted\nin the highest performance for RGB videos, achieving an accuracy of 76.29% and a recall\nof77.56%. The F1 score also significantly increased, rising over 5%to75.56%. Similar pat-\nterns were observed for the thermal and depth videos in this final experimental setup, albeit\nwith minor improvements. Accuracy for thermal videos was 71.55% and for depth videos\n71.67%, with recall rates closely matching at 72.83% and72.84%, respectively. These find-\nings demonstrate consistent enhancement across all visual modalities with refined training\nparameters and extended training durations. Table 7.18 consolidates these experimental out-\ncomes, indicating that the RGB modality consistently surpasses others, while the thermal\nand depth modalities show comparable performance levels. Moreover, although thermal and\ndepth enhancements are modest, they suggest a plateau in potential performance increases.\nECG: The training configuration used for the video data was similarly applied to the ECG\nsignals. As previously indicated, four visual representations were utilized. Each represen-\ntation corresponds to an image dimension of 224\u02c6224pixels, from which embeddings of\ndimensionality d\u201c160are extracted and then inputted into the Embedding-Mixer . Starting\nwith200epochs and employing minimal augmentation without regularization, the waveform\nrepresentation reached an accuracy of 69.58%, with recall and F1 scores of 72.67% and\n68.10%, respectively. The spectrogram-angle had lower performance in all metrics, achiev-\ning an accuracy of 65.58%. Meanwhile, the spectrogram-phase showed better accuracy,7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 139\nTable 7.19: Results utilizing the ECG modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 69.58 72.67 68.10\n300 0.7 0.7 |15-20| 0.1 0.0 71.08 72.74 70.22\n600 0.5 0.5 |15-20| 0.1 0.5 73.36 74.75 72.52Angle200 0.5 0.5 |10-20| 0.0 0.0 65.58 66.68 64.89\n300 0.7 0.7 |15-20| 0.1 0.0 66.33 68.22 65.22\n600 0.5 0.5 |15-20| 0.1 0.5 68.25 71.24 66.99Phase200 0.5 0.5 |10-20| 0.0 0.0 70.08 71.54 69.40\n300 0.7 0.7 |15-20| 0.1 0.0 72.33 73.73 71.69\n600 0.5 0.5 |15-20| 0.1 0.5 72.70 74.19 72.14PSD200 0.5 0.5 |10-20| 0.0 0.0 71.08 73.13 70.19\n300 0.7 0.7 |15-20| 0.1 0.0 71.50 73.14 70.18\n600 0.5 0.5 |15-20| 0.1 0.5 75.49 77.15 74.90\nsurpassing the prior two by 0.5%and4.5%, respectively. The spectrogram-PSD achieved\nthe highest results, recording 71.08% accuracy, 73.13% recall, and 70.19% F1 score. Fur-\nther improvements were seen in the 300-epoch configuration across all visual representations\nand metrics. In the ultimate experimental setup extending to 600epochs, enhancements were\nnoted universally, but the spectrogram-PSD showed the most considerable gains, nearly 4%,\nachieving 75.49% accuracy, 77.15% recall, and 74.90% F1 score. This indicates that integrat-\ning amplitude and frequency information, as the PSD representation provides, is particularly\neffective and valuable for analyzing ECG signals. Table 7.19 documents the outcomes for\nthe ECG modality.\nEMG: For EMG signals, the initial training configuration of 200 epochs demonstrated\ncomparable accuracy across the waveform ,spectrogram-phase , and spectrogram-PSD rep-\nresentations, recording scores of 68.75%,68.33%, and 69.25% respectively. However, the\nspectrogram-angle representation underperformed with an accuracy of 66.42%, mirroring\nits lower performance in the ECG modality. In subsequent training sessions with increased\nepochs and enhanced augmentation and regularization, the spectrogram-angle representa-\ntion showed a notable decline in performance across all metrics. Despite some marginal\nimprovements in the 300-epoch configuration, it still trailed behind its initial results, posting\nan accuracy of 65.32%, with recall and F1 scores of 68.15% and63.17%, respectively. This\npattern suggests that the angle representation, which lacks phase unwrapping, is less effec-140 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.20: Results utilizing the EMG modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 68.75 70.55 67.93\n300 0.7 0.7 |15-20| 0.1 0.0 69.83 72.52 68.68\n600 0.5 0.5 |15-20| 0.1 0.5 72.07 73.64 71.48Angle200 0.5 0.5 |10-20| 0.0 0.0 66.42 68.57 65.26\n300 0.7 0.7 |15-20| 0.1 0.0 63.92 66.33 62.67\n600 0.5 0.5 |15-20| 0.1 0.5 65.32 68.15 63.77Phase200 0.5 0.5 |10-20| 0.0 0.0 68.33 69.75 67.68\n300 0.7 0.7 |15-20| 0.1 0.0 68.58 70.00 67.97\n600 0.5 0.5 |15-20| 0.1 0.5 69.37 71.17 68.66PSD200 0.5 0.5 |10-20| 0.0 0.0 69.25 70.38 68.84\n300 0.7 0.7 |15-20| 0.1 0.0 69.67 71.06 69.12\n600 0.5 0.5 |15-20| 0.1 0.5 72.10 72.82 71.82\ntive for pain assessment tasks in EMG signals. Conversely, the other visual representations\ndemonstrated consistent improvements in each training configuration. The spectrogram-\nPSD achieved the highest accuracy at 72.10% and an F1 score of 71.82%. The waveform\nrepresentation obtained the highest recall at 73.64%. These results are presented in Table\n7.20 for the EMG modality.\nGSR: For the GSR modality, distinct performance variations among the four representa-\ntions are evident. The waveform -based representations significantly outshine the others,\nstarting with an initial accuracy of 87.75% in the 200-epoch configuration, which is over\n14% higher than other metrics. With extended training sessions, a modest improvement is\nnoted across all representations, indicating that the GSR modality might have reached its\nmaximum potential performance. Among the spectrograms, the spectrogram-phase proves\nto be the most informative, culminating in final accuracy, recall, and F1 scores of 76.41%,\n77.23%, and 76.47%, respectively. The waveform representation emerges as the most effec-\ntive, achieving the highest metrics with an accuracy of 88.99%, recall of 89.55%, and an\nF1 score of 88.88%. The distinct performance of these representations can be related to the\ninherent characteristics of the GSR signal. As depicted in Fig. 7.7, GSR typically presented\nas a smooth curve with gradual slopes, indicative of slow and steady changes in skin conduc-\ntivity due to variations in sweat gland activity triggered by stress or arousal. In comparison,\nEMG signals are marked by sharp spikes and erratic fluctuations, reflecting rapid electrical\nactivities from skeletal muscle contractions. On the other hand, ECG signals display distinct7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 141\nTable 7.21: Results utilizing the GSR modality, NP vs. P 4task, reported on accuracy, recall\nand F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Wave200 0.5 0.5 |10-20| 0.0 0.0 87.75 88.68 87.56\n300 0.7 0.7 |15-20| 0.1 0.0 88.50 89.16 88.34\n600 0.5 0.5 |15-20| 0.1 0.5 88.99 89.55 88.88Angle200 0.5 0.5 |10-20| 0.0 0.0 73.67 75.00 73.26\n300 0.7 0.7 |15-20| 0.1 0.0 73.08 74.60 72.66\n600 0.5 0.5 |15-20| 0.1 0.5 73.24 75.02 72.83Phase200 0.5 0.5 |10-20| 0.0 0.0 75.17 76.13 74.79\n300 0.7 0.7 |15-20| 0.1 0.0 75.92 76.60 75.57\n600 0.5 0.5 |15-20| 0.1 0.5 76.41 77.23 76.47PSD200 0.5 0.5 |10-20| 0.0 0.0 72.83 73.91 72.34\n300 0.7 0.7 |15-20| 0.1 0.0 73.08 73.96 72.68\n600 0.5 0.5 |15-20| 0.1 0.5 73.96 74.81 73.50\ncyclical patterns, including the P and T waves and the QRS complex. These observations\nimply that the simpler patterns in GSR are not as well suited for spectral and frequency do-\nmain analyses, which are more effectively captured by spectrograms. However, waveform\nrepresentations excel in capturing critical physiological data from GSR signals, outperform-\ning all other modalities and visual representations due to their ability to effectively represent\nthe essential dynamics of GSR activity. The results for the GSR modality are summarized in\nTable 7.21.\nFusion: Various fusion techniques were tested to evaluate whether combining different rep-\nresentations or modalities could enhance performance. In this research, using inputs from\nthe same sensor type, such as RGB with depth-estimation videos or ECG waveforms with\nECG spectrogram-PSD , was considered an unimodal fusion approach. On the other hand,\ncombining inputs from different sensor types, like GSR with EMG, was treated as a multi-\nmodal fusion. Three primary methods of fusion were explored: feature fusion and decision\nfusion. Feature fusion includes strategies such as addition, where embeddings from various\ninputs are summed before progressing to the following processing stage, and concatenation,\nwhich aligns them along the y-axis . Decision fusion, meanwhile, involves processing each\nembedding through the Embedding-Mixer , which then aggregates the predictions from each\ninput to generate a final decision. All related experiments were conducted under the previ-\nously detailed 600-epoch training configuration, with results compiled in Table 7.22.\nIn video modality fusion, we assessed combinations of RGB with thermal, RGB with142 CHAPTER 7. GENERAL-PURPOSE MODELS\ndepth, and thermal with depth, plus a three-input amalgamation of RGB, thermal, and depth.\nThe RGB and thermal blend underperformed compared to RGB alone, with the best perfor-\nmance ( 75.66% accuracy) achieved through decision fusion. The RGB and depth combina-\ntion similarly yielded optimal results through decision fusion, achieving 75.53% accuracy\nbut falling short of RGB-only performance. Notably, merging thermal and depth videos im-\nproved upon using depth alone, particularly via decision fusion, which attained a 73.02%\naccuracy rate. The combination of RGB, thermal, and depth inputs was the sole group that\noutperformed the standalone RGB setup, with decision fusion delivering the highest metrics:\n76.55% accuracy, 77.91% recall, and 76.11% F1 score, indicating marginal improvements\nacross all measures. Decision fusion consistently outperformed the addition method in all\nvideo-based experiments.\nFor biosignals, experiments focused on ECG and EMG using the waveform and the rep-\nresentations of spectrogram-PSD . No fusion experiments were conducted for GSR due to\nthe waveform\u2019s dominance in performance. For ECG, all fusion methods were less effective\nthan the spectrogram-PSD alone, except for the addition method, which slightly improved\nrecall by 0.21%. EMG results were enhanced by all fusion techniques, with concatenation\nproving to be the most beneficial, leading to increases in accuracy, recall, and F1 score by\n0.74%,0.36%, and 0.64%, respectively.\nPhysiological and behavioral modalities were integrated into our multimodal setup, com-\nbining GSR signals with RGB, synthetic thermal, and estimated depth videos. The GSR\u2019s\nwaveform representation and video features, described in 7.21, were merged into a unified\nvector of dimension 22,080. This vector was then processed through the Video-Encoder into\na smaller space of 40. The resulting combined vector of 160`40\u201c200dimensions was\nformed by concatenating the GSR and video embeddings, represented as:\nMh\u201cGd}Enc\u201c\npVRGB\nD`VThermal\nD`VDepth\nDq\u2030\n, hPRN2, (7.22)\nwhere Gdenotes the GSR embedding and Mthe fused vector with N2equal to 200. This\napproach, visualized in Fig. 7.7 (bottom right), achieved the highest performance in the\nstudy, with accuracy, recall, and F1 scores of 89.08%,89.88%, and 88.87%, respectively.\nThis method slightly surpassed the performance of GSR used independently, especially in\naccuracy and recall.\nAI4Pain\nIn the AI4Pain dataset, experiments were conducted utilizing both unimodal and multimodal\napproaches. The original RGB videos were employed for the behavioral modality, while\nwaveforms from the fNIRS\u2019s HBO2 channels were used for the physiological modality. It\nis important to note that out of the 24available HBO2 channels, 2were excluded due to7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 143\nTable 7.22: Results on fusion settings\u02da, NP vs. P 4task, reported on accuracy, recall and F1 score.\nModality Input FusionMetric\nAcc Rec F1\nVideoRGB, ThermalAdd 75.09\n-1.2076.97\n-0.5973.98\n-1.58\nDF 75.66\n-0.6377.23\n-0.3375.08\n-0.48\nRGB, DepthAdd 74.93\n-1.3676.41\n-1.1573.38\n-2.18\nDF 75.53\n-0.7677.18\n-0.3875.00\n-0.56\nThermal, DepthAdd 71.44\n-0.2373.15\n+0.3170.73\n-0.50\nDF 73.02\n+1.3574.46\n+1.6272.59\n+1.33\nRGB, Thermal, DepthAdd 76.26\n-0.0377.70\n+0.1475.78\n+0.22\nDF 76.55\n+0.2677.91\n+0.3576.11\n+0.55\nECG Wave, PSDAdd 75.43\n-0.0677.36\n+0.2174.75\n-0.15\nConcat 74.74\n-0.7576.77\n-0.3874.00\n-0.90\nEMG Wave, PSDAdd 72.79\n+0.6974.15\n+0.5172.28\n+0.46\nConcat 72.84\n+0.7474.00\n+0.3672.46\n+0.64\nVideo, GSRRGB, Thermal, Depth,\nWaveAdd &\nConcat89.08\n+0.0989.88\n+0.3388.87\n-0.01\n\u02da: All experiments follow the augmentation and regularization settings for the 600 epoch con-\nfiguration outlined in the unimodal experiments. + and - indicate an increase or decrease in\nperformance, respectively, compared to the best unimodal input approach. DF: Decision Fusion\nAdd: Addition Concat: Concatenation\nmalfunctions. Table 7.23 presents the corresponding results.\nVideo: Similar to 7.3.2, an embedding of dimension d\u201c160is extracted for every frame\nin the AI4Pain dataset. However, in this instance, the extracted embeddings are aggregated\ninto a unified vector:\nVd\u201crd1`d2`\u00a8\u00a8\u00a8` dms, dPRN3, (7.23)\nwhere mrepresents the number of frames in a video, and N3is the dimensionality of the\nunified embedding, set at 160. After processing the embedding through the Embedding-\nMixer and employing the same 600-epoch training configuration as used in prior experi-\nments, this setup achieved an accuracy of 49.77%, with recall and F1 scores of 50.11% and\n49.77%, respectively. Increasing the DropOut rate to 0.3improved the accuracy and F1\nscores to 51.39% and51.31%. Further elevating the DropOut rate to 0.8enhanced the recall\nto52.74%.144 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.23: Results on the validation set of AI4Pain dataset, multilevel classification task, re-\nported on accuracy, recall and F1 score.\nInput EpochsAugmentation Regularization Metric\nBasic Masking LS DropOut Acc Rec F1Video600 0.5 0.5 |15-20| 0.1 0.5 49.77 50.11 49.77\n600 0.5 0.5 |15-20| 0.1 0.3 51.39 51.50 51.31\n600 0.5 0.5 |15-20| 0.1 0.8 48.38 52.74 46.69fNIRS600 0.5 0.5 |15-20| 0.1 0.5 43.06 42.80 42.07\n600 0.5 0.5 |15-20| 0.1 0.3 44.44 45.55 43.74\n600 0.4 0.4 |15-20| 0.1 0.1 43.06 44.18 42.44Fusion600 0.5 0.5 |15-20| 0.1 0.5 50.00 51.01 48.54\n600 0.1 0.1 |15-20| 0.1 0.8 50.23 50.25 50.24\n600 0.4 0.4 |15-20| 0.1 0.6 51.85 51.87 51.35\nFusion: the Addition method of the modalities applied\nfNIRS: For the fNIRS modality, embeddings were aggregated across the 22HBO2 chan-\nnels to produce a feature representation of Od\u201c160. The 600-epoch training setup initially\nyielded 43.06% accuracy, 42.80% recall, and 42.07% F1 score. By increasing the DropOut\nrate to 0.3, peak performance metrics of 44.44% accuracy, 45.55% recall, and 43.74% F1\nscore were achieved.\nFusion: For the fusion of video and fNIRS data, the following aggregation approach was\nutilized:\nFd\u201cVd`Od, dPRN3, (7.24)\nwhere Fdrepresents the combined feature representation. Starting with the same 600-epoch\ntraining configuration, the initial results were 50.00% accuracy, 51.01% recall, and 48.54%\nF1 score. Increasing the DropOut rate to 0.8slightly improved the accuracy and F1 score\nby0.23% and1.7%, respectively, though recall decreased by 0.75%. The optimal DropOut\nsetting of 0.6achieved peak performances of 51.85% accuracy, 51.57% recall, and 51.35%\nF1 score.\n7.3.3 Comparison with existing methods\nTo evaluate PainFormer , we compared it against studies from the literature that utilized the\nBioVid dataset ( Part A ), included all available subjects ( 87), conducted the same task, ad-\nhered to the leave-one-subject-out (LOSO) validation protocol, and reported accuracy met-\nrics. For the AI4Pain dataset, our comparisons were made with studies that strictly followed\nthe evaluation guidelines outlined in the corresponding challenge.7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 145\nInBioVid , the proposed approach using RGB, thermal, and depth video inputs is among\nthe top performers in video-based studies, achieving an accuracy of 76.55%. This surpasses\nall methods utilizing hand-crafted features, as referenced in [284,295,339] and outperforms\nmost deep learning-based methods cited in [37, 180, 217, 219]. Exceptions include results\nfrom [38] at 77.10% and [340] at 78.90%, with the study in [269] achieving 77.50% using a\n3D CNN approach, which, when combined with pseudo heart rate data extracted from videos,\nreached the highest reported result of 88.10%. These results are documented in Table 7.24.\nRegarding biosignals, in ECG-based studies, PainFormer achieved the highest accuracy\nin the literature at 75.49% using the spectrogram-PSD representation, significantly outper-\nforming subsequent studies [36,38] by over 6%and8%, respectively. In EMG-based studies\nutilizing waveform andspectrogram-PSD representations, we achieved a 72.84% accuracy,\nsignificantly exceeding the nearest study [339] at 63.10%. For GSR-based studies, utiliz-\ning solely waveform representation led to the highest performance with an 88.99% accuracy.\nStudies using raw biosignals instead of extracting domain-specific features generally exhib-\nited better results, with the second [341] and third [342] ranked studies achieving 85.56%\nand84.80% accuracy, respectively. Table 7.25 presents these biosignal results.\nIn multimodal scenarios, our approach combining video inputs and GSR achieved the\nhighest reported accuracy of 89.08% (refer to Table 7.26). Notably, with one exception [38],\nall studies incorporated the GSR signal. GSR is consistently recognized as the most effective\nmodality for pain assessment, with the second-highest-performing study [343] using a GSR\nand ECG combination achieving 87.06%. A study including videos, ECG, EMG, and GSR\n[344] reached 86.00% accuracy.\nFor the AI4Pain dataset, PainFormer achieved a 53.67% accuracy using the RGB video\nmodality, outperforming [345] at 49.00% but falling behind [346] at 55.00% utilizing a\ntransformer-based masked autoencoder. Using only fNIRS, an accuracy of 52.60% was\nachieved. In a multimodal approach combining videos and waveform representations, an\naccuracy of 55.69% was attained, surpassing [40] by over 9%and establishing the highest\nperformance on this dataset to date.\n7.3.4 Interpretation\nEnhancing the interpretability of models is crucial for their acceptance and effective integra-\ntion into clinical settings. In this study, PainFormer generates attention maps, as illustrated\nin Fig. 7.8. The weights from the \u201cStage 4\u201d self-attention heads are applied by interpolating\nthem onto the input images, enabling visualization of the model\u2019s focal areas.\nIn Fig. 7.8(a), (1strow), we display examples from the RGB, thermal, and depth modali-\nties, and in Fig. 7.8(a), (2ndrow), the corresponding attention maps are presented. Observa-\ntions indicate that the model primarily focuses on the glabella region (the area between the146 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.24: Comparison of video-based studies utilizing BioVid (Part-A) , NP vs. P 4task and\nLOSO validation.\nStudyMethod\nAcc%\nFeatures ML\n[217] raw SLSTM 61.70\n[219] raw 2D CNN, biLSTM 69.25\n[295] optical flow RF 70.20\n[180] raw 2D CNN 71.00\n[39] raw\u0015Vision-MLP 71.03\n[135]:raw 2D CNN 71.30\n[267] facial landmarks, 3D distances RF 71.60\n[296] facial 3D distances Deep RF 72.10\n[296] facial action descriptors Deep RF 72.40\n[297] facial landmarks, 3D distances RF 72.70\n[284] fiducial points GNN 73.20\n[37] raw Transformer 73.28\n[211]:raw 2D CNN, GRU 73.90\n[339] facial landmarks, head pose RF 76.60\n[38] raw Transformer 77.10\n[269] raw 3D CNN, 77.50\n[340] raw, rPPG\u27263D CNN 78.90\n[269] raw, heart rate\u26053D CNN 88.10\nOur raw\u273fTransformer 76.55\n:: reimplemented for pain intensity estimation on BioVid by [269] \u0015: RGB, syn-\nthetic thermal videos \u2726: remote photo plethysmography (estimated from videos)\n\u2605: pseudo heart rate gain (estimated from videos) \u273f: RGB-thermal-depth (DF)\nRF: Random Forest GNN: Graph Neural Networks MLP: Multi-Layer Percep-\ntron\neyebrows) in the RGB frame, a key area for facial expressions. Additional focus is observed\non the mental protuberance area (the chin), which is also associated with expressions of pain.\nFor the thermal frame, the model concentrates on areas around the eyes and the sides of\nthe mouth, where brighter colors in the thermal imagery suggest higher temperatures, indi-\ncating that temperature variations rather than facial expressions drive the model\u2019s attention\nin the thermal modality. In the depth frame, the model targets areas showing variations in\ndepth, particularly across the horizontal eye region, with slight attention to the frame\u2019s lower\nleft and right edges, highlighting depth differences in body parts beyond the face, which\nillustrates a nuanced understanding of the model\u2019s representation of depth.\nThe ECG attention maps in Fig. 7.8(b), (top left), primarily emphasizes a distinct R peak\nin the trace\u2019s center. Significant attention is also directed towards the T waves, especially\nthose following the central R peak, highlighting the model\u2019s sensitivity to these elements in\nthe signal. In the EMG attention maps of Fig. 7.8(b), (top right), PainFormer mainly focuses\non the initial and middle sections of the signals. Despite a muscle contraction burst appearing7.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 147\nTable 7.25: Comparison of biosignal-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation.\nStudy ModalityMethod\nAcc%\nFeatures ML\n[235] ECG raw 1D CNN 57.04\n[347] ECG domain-specific\u00b8LR 57.40\n[270] ECG domain-specific\u00b8LR 57.69\n[35] ECG domain-specific\u00b8SVM 58.39\n[342] ECG raw 1D CNN, biLSTM 61.20\n[267] ECG domain-specific\u00b8RF 62.00\n[297] ECG domain-specific\u00b8SVM 62.40\n[297] ECG domain-specific\u00b8SVM 62.40\n[348] ECG raw 2D CNN, biLSTM 63.20\n[339] ECG domain-specific\u00b8RF 64.00\n[269] ECG heart rate\u26053D CNN 65.00\n[38] ECG heart rate Transformer 67.04\n[36] ECG domain-specific\u00b8FCN 69.40\nOur ECG raw\u2726Transformer 75.49\n[347] EMG domain-specific\u00b8LR 58.59\n[235] EMG raw 2D CNN 58.65\n[339] EMG domain-specific\u00b8RF 63.10\nOur EMG raw\u2740Transformer 72.84\n[339] GSR domain-specific\u00b8RF 71.90\n[270] GSR domain-specific\u00b8LR 74.21\n[297] GSR domain-specific\u00b8RF 74.40\n[349] GSR domain-specific\u00b8RF 80.40\n[350] GSR domain-specific\u00b8RF 81.90\n[347] GSR domain-specific\u00b8LR 82.36\n[351] GSR domain-specific\u00b8SVM 83.30\n[348] GSR raw 1D CNN, biLSTM 83.60\n[232] GSR domain-specific\u00b8MLP 84.22\n[235] GSR raw 1D CNN 84.57\n[342] GSR raw 1D CNN, biLSTM 84.80\n[341] GSR raw 1D CNN,\nTransformer85.56\nOur GSR raw\u273fTransformer 88.99\n\u00b8: numerous features \u2605: pseudo heart rate gain (estimated from videos) \u2726: PSD\n\u2740: waveform-PSD (Concat) \u273f: waveform SVM: Support Vector Machines LR:\nLogistic Regression\nlater in the sequence, the model exhibits less attention to this portion. This observation may\nbe related to the PainFormer \u2019s pre-training on the Silent-EMG dataset [329], which might\ninfluence its responsiveness to specific sections of the EMG signals.\nFor the GSR signal in Fig. 7.8(b), (bottom left), mild attention is noted at the onset of the\nresponse, marking the start of the conductance increase, with the most intense attention near\nthe peak amplitude, where conductance reaches its maximum level. In the fNIRS signal\nshown in Fig. 7.8(b) (bottom right), the attention map predominantly highlights regions148 CHAPTER 7. GENERAL-PURPOSE MODELS\nTable 7.26: Comparison of multimodal-based studies utilizing BioVid (Part-A) , NP vs. P 4task\nand LOSO validation.\nStudy ModalityMethod\nAcc%\nFeatures ML\n[270] ECG, GSR domain-specific\u00b8SVM 72.20\n[267] ECG, EMG, GSR domain-specific\u00b8RF 74.10\n[267] Video1,ECG2, EMG2,\nGSR2facial landmarks1, 3D distances1,\ndomain-specific2\u00b8RF 77.80\n[297] Video1, ECG2, GSR2facial landmarks1, 3D distances1,\ndomain-specific2\u00b8RF 78.90\n[339] Video1, ECG2, EMG2,\nGSR2facial landmarks1, head pose1,\ndomain-specific2RF 80.60\n[38] Video1, ECG2raw1, heart rate2Transformer 82.74\n[350] Video1, ECG2, EMG2,\nGSR2geometric1, appearance1,\ndomain-specific2RF 83.10\n[347] ECG, EMG, GSR domain-specific LR 83.20\n[239] ECG, EMG, GSR domain-specific biLSTM 83.30\n[243] ECG, EMG, GSR raw DDCAE 83.99\n[352] ECG, EMG, GSR raw DDCAE, NN 84.25\n[235] ECG, EMG, GSR raw 2D CNN 84.40\n[353] GSR, ECG domain-specific\u00b8NN 84.58\n[348] Video, GSR raw 2D CNN,\nbiLSTM84.80\n[342] ECG, GSR raw 1D CNN,\nbiLSTM84.80\n[354] ECG, EMG, GSR domain-specific RF 85.70\n[355] ECG, EMG, GSR domain-specific RF 85.80\n[344] Video1, ECG2, EMG2,\nGSR2facial descriptors1, domain-specific2RF 86.00\n[343] GSR, ECG domain-specific\u00b8NN 87.06\nOur Video\u2722, GSR\u273draw Transformer 89.08\n\u2722: RGB-thermal-depth \u273d: waveform \u00b8: numerous features DDCAE: deep denoising convolutional autoencoders\nNN: neural network\nTable 7.27: Comparison of studies on the testing set of AI4Pain dataset.\nStudyModality\nML Acc%\nVideo fNIRS Fusion\n[40] \u2013 \u2013 \u2713 Transformer 46.67\n[345] \u2713 \u2013 \u2013 2D CNN 49.00\n[346] \u2713 \u2013 \u2013 Transformer 55.00\nOur\u2713 \u2013 \u2013\nTransformer53.67\n\u2013 \u2713 \u2013 52.60\n\u2013 \u2013 \u2713 55.697.3. A FOUNDATION MODEL FOR AUTOMATIC PAIN ASSESSMENT 149\naligning with peaks and rapid changes in HbO2 levels. Significant attention is concentrated\nin the map\u2019s left, middle, and right sections, where distinct peaks and dips in the signal are\nobserved, indicating that PainFormer consistently focuses on substantial fluctuations in the\nHbO2 signal, likely associated with pain conditions. Areas with lower or moderate attention\ncorrespond to segments of the time series with stable or minor variations in HbO2, reflecting\nlower levels of brain activation typically associated with mild or no pain responses.\n7.3.5 Discussion\nThis research introduces PainFormer , a foundation model utilizing a vision-transformer ar-\nchitecture tailored for pain assessment across diverse modalities. The model was pre-trained\non14datasets, totaling 10.9million samples, using a multi-task learning framework to en-\nhance its capability in processing behavioral and physiological inputs. PainFormer is com-\nplemented by the Embedding-Mixer for analyzing embeddings and the Video-Encoder for re-\nducing the dimensionality of video embeddings. We tested our model using the BioVid and\nAI4Pain datasets, experimenting with modalities including RGB, synthetic thermal, depth\nvideos, and biosignal representations such as ECG, EMG, GSR, and fNIRS. The evaluation\ndemonstrated that RGB videos provided the highest accuracy among behavioral modalities,\nrecording 76.29% accuracy in the BioVid dataset. Thermal and depth modalities also per-\nformed well, with accuracies of 71.55% and71.67%, respectively. Interestingly, integrat-\ning thermal and depth modalities improved accuracy by 1.35%, suggesting their potential\nto match the efficacy of RGB while addressing privacy concerns associated with direct fa-\ncial recordings. ECG was particularly effective regarding physiological signals, with the\nspectrogram-PSD achieving the highest accuracy at 75.49%. Although combining differ-\nent ECG representations did not enhance performance, EMG signals showed exceptional\naccuracy above 72% when combining waveform andspectrogram-PSD . GSR, known for\nits efficacy in pain assessment, achieved an impressive accuracy of 88.9%using waveform\nrepresentations alone. Our multimodal approach that integrated GSR with RGB, thermal,\nand depth video embeddings led to a notable accuracy of 89.08% in the BioVid dataset, un-\nderscoring the strength of combining multiple modalities. Further, the creation of attention\nmaps revealed PainFormer \u2019s consistent focus on key areas indicative of pain across all tested\nmodalities. This ability highlights the model\u2019s utility in clinical settings, where understand-\ning pain through various indicators is crucial. However, the influence of pre-training on\nspecific areas requires further investigation to ensure the model\u2019s generalizability and accu-\nracy in real-world applications. Our findings place PainFormer at the forefront of current\nmethodologies, achieving state-of-the-art results across the board. While our model excels\nwith video-based and biosignal inputs in unimodal and multimodal settings, continuous ad-\nvancements and further explorations are needed, especially with the newer AI4Pain dataset.150 CHAPTER 7. GENERAL-PURPOSE MODELS\n7.4 Summary\nThis chapter has introduced general-purpose models and pipelines for automatic pain as-\nsessment. Such approaches have attained popularity recently, driven by the development of\nadvanced architectures and the availability of substantial data and computing resources nec-\nessary for training these models. While this combination has led to notable achievements\nin the broader fields of deep learning and AI, its effect on pain research has been virtu-\nally nonexistent. This distinction motivated our exploration of these methodologies in pain\nrecognition tasks. We presented a modality-agnostic method that homogenizes the pipeline\nirrespective of the input type. Our experiments with RGB videos and fNIRs showed promis-\ning results in both unimodal and multimodal settings. However, there is potential for further\nimprovement. We believe increasing the pre-training data for models within a modality-\nagnostic framework could significantly improve performance. It is generally accepted that\nfoundation models deliver superior outcomes. Our study introduced the first foundation\nmodel specifically developed for and applied to pain assessment. The results demonstrated\nthat this approach is highly effective, not only for well-established applications such as lan-\nguage understanding but also for automatic pain assessment. While further investigation\nis necessary, it is essential to acknowledge a fundamental challenge: the generally limited\navailability of pain-related data, which could restrict the effectiveness of these models.7.4. SUMMARY 151\n\u2026\nVideo\n\u2026\nfNIRSaddwaveform \ngeneration embedding \nextraction# frames \naddwaveform \ngeneration embedding \nextraction# channels \n PainViT-1\nPainViT-2\nPain \nAssessmenteFFN\nFully-connectedRELUFully-connectedc\nOutput\nInput\nHead h\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 2\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 1\nK\nQ\nV\nDWConv\nSelf-attention\nSegment\u2026\nConcatenation & Projectiond\nCascaded Attention PainViTLinear ProjectionToken Mixer\nCascaded Attention\nToken Mixer\n1xBlock-1+Token Mixer\nCascaded Attention\nToken Mixer\n3xBlock-2+Token Mixer\nCascaded Attention\nToken Mixer\n4xBlock-3+a\nInput ImageFFN\nDWConvBatchNorm++\nToken-Mixerb\nFFN\nFully-connectedRELUFully-connectedc\nOutput\nInput\nHead h\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 2\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 1\nK\nQ\nV\nDWConv\nSelf-attention\nSegment\u2026\nConcatenation & Projectiond\nCascaded Attention PainViTLinear ProjectionToken Mixer\nCascaded Attention\nToken Mixer\n1xBlock-1+Token Mixer\nCascaded Attention\nToken Mixer\n3xBlock-2+Token Mixer\nCascaded Attention\nToken Mixer\n4xBlock-3+a\nInput ImageFFN\nDWConvBatchNorm++\nToken-Mixerb\n\u2026\nVideo\n\u2026\nfNIRSaddwaveform \ngeneration embedding \nextraction# frames \naddwaveform \ngeneration embedding \nextraction# channels \nPainViT-1\nPainViT-2\nPain \nAssessment\nFigure 7.1: PainViT :(a)Hierarchical arrangement of the PainViT blocks, each layer having\nvarying depths, showcasing how token resolution decreases at each stage; (b)Com-\nposition of the Token-Mixer module, featuring elements like depthwise convolu-\ntion (DWConv) and batch normalization; (c)Architecture of the Feed-Forward\nNetwork (FFN) within the Token-Mixer ;(d)The Cascaded Attention mechanism\nimplemented across multiple heads, illustrating how outputs from preceding heads\nare incorporated to refine the self-attention process, culminating in the final out-\nput projection; (e)Configuration of the proposed multimodal pipeline, employing\nvideos and fNIRS. The embeddings from PainViT\u20131 are represented as waveform\ndiagrams, which are merged into a single diagram that illustrates both modalities\nbefore entering PainViT\u20132 for final pain evaluation.152 CHAPTER 7. GENERAL-PURPOSE MODELS\nabc\nFigure 7.2: Waveform illustrations for various data types: (a)original fNIRS signal, (b)video\nembedding derived from PainViT\u20131 , and (c)fNIRS embedding obtained from\nPainViT\u20131 .\nabc\nFigure 7.3: Attention maps from the PainViT\u20132 .7.4. SUMMARY 153\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nEmbedding-Mixerd\n++++++\nx2\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nVideo-Encodere\n++\nx1MLP-3\nFully-connectedGeGLUFully-connectedhMLP-1\nFully-connectedGELUFully-connected\nDWConvf\nLayer Norm\nLayer NormMLP-2\nMHSA x(2-4-10-16)\nSelf-Attention Layer+\n+c\nLayer Norm\nLayer NormMLP-1\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+ba\nPainFormerInput ImagePosition Embedding\nLinear ProjectionStage-1x2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2x2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3x12 Self-Attention LayerDownsamplerStage-4x3 Self-Attention LayerDownsampler\nMLP-2\nFully-connectedGELUFully-connectedg\nFigure 7.4: Overview of primary models and their components outlined in this research: (a)\nPainFormer is structured hierarchically into four stages, incorporating Spectral\nandSelf-Attention Layers to extract embeddings from the inputs; (b)The Spec-\ntral Layer , a key element of PainFormer , uses FFT to analyze frequency-specific\ndata along with a learnable filter Kto highlight critical frequencies; (c)TheSelf-\nAttention Layer , crucial for PainFormer , enables parallel processing of features and\ntheir interconnections; (d)TheEmbedding-Mixer , employing both cross and self-\nattention mechanisms, functions as the component for the final classification of\nembeddings in pain assessment; (e)TheVideo-Encoder , designed for compact and\nefficient encoding, compresses video data into a reduced dimensional form; (f)The\nMLP-1 is part of the Spectral Layer; (g)TheMLP-2 is included in the Self-Attention\nLayer ;(h)The MLP-3 configuration is integrated into the Embedding-Mixer and\nVideo-Encoder .\nabc\nabcd\na\nb\nc\nd\nFigure 7.5: Examples of different vision modalities in frame samples: (a)RGB frame, (b)\nsynthetic thermal frame, and (c)depth estimation frame.154 CHAPTER 7. GENERAL-PURPOSE MODELS\na\nb\nc\nd\nFigure 7.6: Examples of different visual representations for biosignals: (a)waveform ,(b)\nspectrogram-angle ,(c)spectrogram-phase , and (d)spectrogram-PSD .\nFusion N\nEmbedding-Mixer\nadd\nadd\nadd\nVideo-Encoder\nconcatFusion 2DF\n Embedding-Mixer\n add\nadd\nadd\nFusion 1\nadd\nEmbedding-Mixer\naddFusion Pipelines\nadd\nEmbedding-Mixer Modality 1\nconcat\nModality 2\n Embedding-MixerUnimodal Pipelines\nPainFormer\n\u2026Depth\n\u2026Thermal\n\u2026\nRGB\nECG\nEMG\nGSR\n\u2026\nfNIRS\nEmbeddingsadd:      addition\nconcat: concatenation\nDF:      decision fusion\nFusion N\nEmbedding-Mixer\nadd\nadd\nadd\nVideo-Encoder\nconcat\nadd\nFusion 1\nadd\n Embedding-Mixer\naddMultimodal Pipelines\nadd\nEmbedding-MixerModality 1\nconcat\nModality 2\n Embedding-Mixer\nadd\nEmbedding-Mixer Modality NUnimodal Pipelines\nPainFormer\n\u2026Depth\n\u2026Thermal\n\u2026\nRGB\nECG\nEMG\nGSR\n\u2026\nfNIRS\nEmbeddings\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nEmbedding-Mixerd\n++++++\nx2\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nVideo-Encodere\n++\nx1MLP-3\nFully-connectedGeGLUFully-connectedhMLP-1\nFully-connectedGELUFully-connected\nDWConvf\nLayer Norm\nLayer NormMLP-2\nMHSA x(2-4-10-16)\nSelf-Attention Layer+\n+c\nLayer Norm\nLayer NormMLP-1\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+ba\nPainFormerInput ImagePosition Embedding\nLinear ProjectionStage-1x2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2x2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3x12 Self-Attention LayerDownsamplerStage-4x3 Self-Attention LayerDownsampler\nMLP-2\nFully-connectedGELUFully-connectedg\nLayer Norm\nLayer Norm\nCross-AttentionMLP-1\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-1\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-1\nx8\nEmbedding-Mixerb\n++++++\nx2\nLayer Norm\nLayer Norm\nCross-AttentionMLP-1\nx1\nVideo-Encoderc\n++\nx1MLP-3\nFully-connectedGELUFully-connectedhMLP-1\nFully-connectedGeGLUFully-connectedf\nMLP-2\nFully-connectedGELUFully-connected\nDWConvg\nLayer Norm\nLayer NormMLP-3\nMHSA\nSelf-Attention Layer+\n+e\nLayer Norm\nLayer NormMLP-2\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+d a\nPainFormerInput ImagePosition Embedding\nLinear ProjectionStage-1\nx2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2\nx2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3 x12 Self-Attention LayerDownsamplerStage-4 x3 Self-Attention LayerDownsampler\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nVideo-Encoderf\n++\nx1\nLayer Norm\nLayer Norm\nCross-AttentionMLP-3\nx1\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nSelf-Attention\nLayer Norm\nLayer NormMLP-3\nx8\nEmbedding-Mixerf\n++++++\nx2MLP-2\nFully-connectedGELUFully-connectede\nMLP-3\nFully-connectedGeGLUFully-connectedgMLP-1\nFully-connectedGELUFully-connected\nDWConvc\nLayer Norm\nLayer NormMLP-2\nMHSA\nSelf-Attention Layer+\n+d\nLayer Norm\nLayer NormMLP-1\nFFT\nIFFT\nFrequency Domain Features X\nLearnable Filter K\nSpectral Layer+ba\nPainFormerInput ImagePotition Embedding\nLinear ProjectionStage-1\nx2x1\nSpectral LayerSelf-Attention LayerDownsamplerStage-2\nx2x2\nSpectral LayerSelf-Attention LayerDownsamplerStage-3 x12 Self-Attention LayerDownsamplerStage-4 x3 Self-Attention LayerDownsamplerFFN\nFully-connectedRELUFully-connectedc\nOutput\nInput\nHead h\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 2\nK\nQ\nV\nDWConv\nSelf-attention+\nHead 1\nK\nQ\nV\nDWConv\nSelf-attention\nSegment\u2026\nConcatenation & Projectiond\nCascaded Attention PainViTLinear ProjectionToken Mixer\nCascaded Attention\nToken Mixer\n1xBlock-1+Token Mixer\nCascaded Attention\nToken Mixer\n3xBlock-2+Token Mixer\nCascaded Attention\nToken Mixer\n4xBlock-3+a\nInput ImageFFN\nDWConvBatchNorm++\nToken-Mixerb\n\u2026\nVideo\n\u2026\nfNIRSaddwaveform \ngeneration embedding \nextraction# frames \naddwaveform \ngeneration embedding \nextraction# channels \n PainViT-1\nPainViT-2\nPain \nAssessmente\nFigure 7.7: An overview of the presented framework. PainFormer , the foundational model,\nexcels in deriving high-quality embeddings from a diverse array of behavioral and\nphysiological modalities. The evaluation of RGB, thermal, and depth videos, along-\nside various representations of ECG, EMG, GSR, and fNIRS such as waveforms\nand spectrograms, underscores the rich information captured within these embed-\ndings. Leveraging the embeddings from PainFormer facilitates the creation of var-\nious and diverse unimodal and multimodal pipelines designed for the pain assess-\nment task. Each pipeline can be customized to suit the specific modalities involved,\ndataset characteristics, and the demands of the intended application or clinical set-\nting. Our assessments included the development and implementation of several\npipelines in both unimodal and multimodal contexts, achieving leading-edge re-\nsults across various modalities and data representations.7.4. SUMMARY 155\na\nb\nFigure 7.8: Attention maps from the PainFormer :(a)(1strow) frames from RGB, thermal, and\ndepth video modalities; (a)(2ndrow) corresponding attention maps; (b)(1strow)\nattention maps for ECG and EMG; (b)(2ndrow) attention maps for EDA and fNIRS\nmodalities.156Chapter 8\nConclusions, Perspectives and Future Work\nContents\n8.1 Summary of Thesis Achievements . . . . . . . . . . . . . . . . . . . . . . . . . 157\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment . . . . . . . . 157\n8.1.2 Insights from Gender and Age Analysis . . . . . . . . . . . . . . . . . . . 158\n8.1.3 Pain Assessment with Compact, High-Performance Models . . . . . . . . 158\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy . . . . . . . . . 158\n8.1.5 Universal Modeling for Automatic Pain Assessment . . . . . . . . . . . . 159\n8.1.6 Explainable Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . 159\n8.2 Perspectives for Automatic Pain Assessment Methods . . . . . . . . . . . . . . 160\n8.3 Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 160\n8.1 Summary of Thesis Achievements\nThe primary objective of this thesis was to explore and improve methods for automatic pain\nassessment. We developed innovative methods that either improved assessment accuracy\nor introduced new approaches, potentially paving the way for advanced methodologies in\nthe future. Additionally, this thesis aimed to integrate ideas and insights from psychology,\nbiology, and nursing, translating them into engineering concepts.\n8.1.1 Reporting on Deep Learning in Automatic Pain Assessment\nChapter 3 presented a comprehensive review of deep learning-based approaches in the field.\nThis systematic review, conducted at the beginning of this Ph.D. project, laid the groundwork\nfor our understanding of the domain. We believe that this foundational work will continue\nto benefit other researchers interested in pain assessment from a machine learning and AI\nperspective. In addition to documenting existing approaches, this work identified emerging\ntrends and suggested potential improvements, offering valuable insights for future research.\n157158 CHAPTER 8. CONCLUSIONS, PERSPECTIVES AND FUTURE WORK\n8.1.2 Insights from Gender and Age Analysis\nIn Chapter 4, we explored the impact of demographic factors on an automatic pain assess-\nment pipeline, focusing specifically on gender and age. For decades, it has been recognized\nthat these factors significantly affect pain expression, sensation, and perception in diverse\nand intriguing ways. Our study utilized ECG signals to explore variations in pain sensation\namong people. Our findings confirmed significant differences between males and females,\nwith the latter group exhibiting higher sensitivity\u2014a phenomenon well-documented in pain\nliterature from psychological and biological perspectives. Moreover, substantial variations\nwere observed across different age groups. A critical discovery was that pain sensation\ntends to decrease with age. This observation is particularly concerning as it implies that\nolder individuals might sustain injuries without adequate perception of pain due to neurolog-\nical reasons, potentially leading to further harm. To our knowledge, such explorations and\nfindings have not been previously addressed in automatic pain assessment. We expect our re-\nsearch will inspire more studies into these phenomena from computational and engineering\nperspectives.\n8.1.3 Pain Assessment with Compact, High-Performance Models\nIn Chapter 5, we introduced methods aimed at developing effective and efficient approaches\nsuitable for real-world applications, focusing on computational cost and efficiency. A pri-\nmary concern addressed is the tendency in both automatic pain research and the broader\nfield of deep learning to rely on large models that require high-end GPUs for basic infer-\nence. We investigated whether achieving comparable performance with more efficient and\nfaster models is feasible. Additionally, we conducted one of the first studies to use heart\nrate as the sole feature for pain assessment. This choice was motivated by the widespread\navailability of heart rate data from consumer wearables, prompting us to examine its viabil-\nity for pain assessment. Our findings indicate that well-designed and optimized models can\ndeliver performance on par with, or even superior, systems that use more complex features\nor raw ECG signals. This is significant, as effective real-world applications must balance\npeak performance with manageable computational demands, particularly in clinical or home\nmonitoring settings.\n8.1.4 Synthetic Data for Improved Pain Assessment and Privacy\nIn Chapter 6, we explored the creation of synthetic data and its potential utility within an\nautomatic pain assessment framework. Motivated by literature suggesting that thermal im-\nagery can reflect skin temperature increases during arousal events like pain, we generated\nsynthetic thermal videos. Our efforts led to the development of this new synthetic modality,8.1. SUMMARY OF THESIS ACHIEVEMENTS 159\nand we assessed its effectiveness compared to authentic RGB videos. The results demon-\nstrated that the synthetic videos we produced were not only of high quality but also reached\nthe performance of RGB videos. This significant outcome may pave the way for new au-\ntomatic systems that rely on synthetic data. Such systems could ( i) utilize modalities that\nare rare, challenging, or costly to record and ( ii) enhance privacy by concealing or partially\nconcealing the identities of individuals involved. Furthermore, it opens the possibility for\nsystems that integrate both authentic and synthetic modalities, leveraging the strengths of\neach.\n8.1.5 Universal Modeling for Automatic Pain Assessment\nIn Chapter 7, we explored and presented general-purpose models and pipelines for auto-\nmatic pain assessment tasks. Our initial focus was on conceiving a single pipeline applica-\nble across all input modalities, simplifying the development process by eliminating the need\nfor specialized models and modules for each modality. Our strategy employed vision-based\nmodels, which are highly effective since all inputs can be transformed into image formats\u2014\nfor instance, a video frame remains an image. Signals can be visualized as images, such\nas spectrograms or waveforms. This approach proved straightforward and effective in our\nexperiments with videos and FNIRs, applying waveforms in unimodal and multimodal sce-\nnarios. A major advancement in our research was the development of the PainFormer , a\nfoundation model specifically crafted for automatic pain assessment. This model is notable\nfor being the first in its field and represents a significant step forward in pain assessment tech-\nnology. The most important finding from this study is the exceptional performance of the\nfoundation model across a range of modalities, including RGB, thermal, depth videos, ECG,\nEMG, GSR, and fNIRs. It consistently delivered state-of-the-art results, demonstrating its\nversatility and effectiveness. We believe that this research will encourage further investiga-\ntions into similar models, enhancing their performance in benchmarks and possibly leading\nto broader real-world applications by proving their effectiveness and utility.\n8.1.6 Explainable Deep Learning\nAlthough explainability was not the primary focus of the thesis, our research consistently\naimed to provide insights and explanations on how the model performs and makes decisions.\nUsing attention maps, we aimed to deliver these explanations, which were often clear and\nvaluable. This approach is crucial, especially if such models are to be implemented in clinical\nsettings in the future.160 CHAPTER 8. CONCLUSIONS, PERSPECTIVES AND FUTURE WORK\n8.2 Perspectives for Automatic Pain Assessment Methods\nAutomatic pain assessment is a fascinating subject from engineering and research perspec-\ntives and a critical matter in healthcare and clinical environments. Our observations and\ncritiques focus on the datasets, the cornerstone of any computational science research. In\nSection 3.7.1, we discussed the challenges in automatic pain assessment. The observations\nand statements remain largely unchanged a few years after this writing.\nThe comments provided here cannot be addressed with existing datasets; they are in-\ntended for future ones. The most superficial data to incorporate involves demographic fac-\ntors such as age and gender. As explored in our research, both are crucial and including\nthem should be considered a minimum standard for future datasets. Another factor criti-\ncal to the generalization of future models is the inclusion of individuals from various racial\nbackgrounds. We have observed notable improvements in this area; for instance, the AI4Pain\ndataset, a recent addition, features participants from diverse backgrounds. One important el-\nement not yet considered in any dataset is social interactions, which, as discussed, influence\npain perception. Future datasets should incorporate this factor, designing experiments to\nassess responses to stimuli when individuals are alone, accompanied by a female and male\nperson. Additionally, attractiveness also affects pain perception. Incorporating these aspects\ninto experiments is feasible and should be relatively straightforward. Recording sound and\nspeech is essential in terms of modalities. This data will benefit signal processing and lan-\nguage analysis and be valuable for assessing pain.\nLastly, we observe that even though significant funds are invested in equipment for\nrecording biosignals, video capture quality in some datasets is only medium to low. Fu-\nture research must utilize cameras capable of recording high-resolution videos, such as 4K.\nIt is important to note that while mobile phones may meet these specifications on paper,\nthe quality of their sensors and lenses is often inadequate. More professional video record-\ning equipment is necessary. Additionally, a high frame rate is essential for capturing facial\nmicro-expressions. Furthermore, as we have discussed, incorporating visual modalities be-\nyond RGB, such as thermal and depth imaging, can be highly valuable. The cost of these\ntechnologies is comparable to that of biosignal recording equipment and should not be pro-\nhibitively expensive.\n8.3 Future Work\nBased on the work and findings presented in this thesis, we propose recommendations for\nfuture research. As emphasized earlier, any modality must incorporate and extract features\nthat capture the temporal dimension. This is vital because pain is a dynamic condition,\nexpressed and evolving over time, not static. This principle applies equally to many other8.3. FUTURE WORK 161\naffective-related tasks.\nWe have a primary recommendation regarding the computational methods used in auto-\nmatic pain assessment. We have observed that researchers in this field often recycle the same\nmethods repeatedly. This repetition does not appear to stem from efforts to enhance or refine\nexisting approaches. We encourage future researchers to be more adventurous and explore\nnew concepts and techniques. The literature on deep learning, which is readily accessible\nand constantly evolving, provides a solid foundation. Researchers should strive to adopt and\nadapt these ideas to their specific problems and tasks and seek to innovate and improve upon\nthem.\nA more specific technical recommendation regarding methods concerns the use of foun-\ndation models. In our research, we explored this concept and developed PainFormer . It is\nimportant to note that the specific model is not large compared to others in the literature,\nespecially the well-known language models; it can be considered reasonably compact. This\ndemonstrates that small, efficient models can be developed for tasks like pain assessment and\nbeing effective. We believe greater efforts should be directed toward creating foundational,\ngeneral-purpose models incorporating multimodality. This approach represents a promising\nand significant path for future research. In addition, we believe that methods related to gener-\nation processes hold valuable potential. In particular, until new, high-quality pain datasets are\ndeveloped, techniques like super-resolution (upscaling) to produce higher-resolution video\nframes and methods for generating auxiliary frames to increase FPS are promising areas for\nexploration.162Bibliography\n[1] HAFD Merskey. Pain terms: a list with definitions and notes on usage. recommended\nby the iasp subcommittee on taxonomy. Pain, 6:249\u2013252, 1979.\n[2] Amanda C de C Williams and Kenneth D Craig. Updating the definition of pain. Pain,\n157(11):2420\u20132423, nov 2016.\n[3] Rebecca Pillai Riddell and Kenneth D Craig. Developmental Dimensions in Under-\nstanding Interpersonal Features of Pain , pages 43\u201355. Springer International Publish-\ning, Cham, 2018.\n[4] Amanda Williams and Judith Kappesser. Why Do We Care? Evolutionary Mecha-\nnisms in the Social Dimension of Pain , pages 3\u201322. 2018.\n[5] Steven J. Mithen. The Prehistory of the Mind: A Search for the Origins of Art, Reli-\ngion and Science . Orion Publishing Group, 1996.\n[6] Rebecca Redfern. A regional examination of surgery and fracture treatment in iron\nage and roman britain. International Journal of Osteoarchaeology , 20(4):443\u2013471,\n2010.\n[7] Steven P Cohen, Lene Vase, and William M Hooten. Chronic pain: an update on\nburden, best practices, and new advances. The Lancet , 397(10289):2082\u20132097, 2021.\n[8] Aza Abdulla, Nicola Adams, Margaret Bone, Alison M Elliott, Jean Gaffin, Derek\nJones, Roger Knaggs, Denis Martin, Liz Sampson, Pat Schofield, and British Geriatric\nSociety. Guidance on the management of pain in older people. Age and ageing , 42\nSuppl 1:1\u201357, March 2013.\n[9] Global, regional, and national incidence, prevalence, and years lived with disabil-\nity for 354 Diseases and Injuries for 195 countries and territories, 1990-2017: A\nsystematic analysis for the Global Burden of Disease Study 2017. The Lancet ,\n392(10159):1789\u20131858, nov 2018.\n[10] US Burden of Disease Collaborators. The State of US Health, 1990-2010: Burden of\nDiseases, Injuries, and Risk Factors. JAMA , 310(6):591\u2013606, 08 2013.\n163164 BIBLIOGRAPHY\n[11] Darrell J. Gaskin and Patrick Richard. The Economic Costs of Pain in the United\nStates. The Journal of Pain , 13(8):715\u2013724, aug 2012.\n[12] Harald Breivik, Elon Eisenberg, and Tony O\u2019Brien. The individual and societal bur-\nden of chronic pain in europe: the case for strategic prioritisation and action to im-\nprove knowledge and availability of appropriate care. BMC public health , 13:1\u201314,\n2013.\n[13] Deloitte Access Economics. The cost of pain in australia, 2019.\n[14] Pradeep Dinakar and Alexandra Marion Stillman. Pathogenesis of Pain. Seminars in\nPediatric Neurology , 23(3):201\u2013208, aug 2016.\n[15] Puja Seth, Rose A. Rudd, Rita K. Noonan, and Tamara M. Haegerich. Quantifying the\nepidemic of prescription opioid overdose deaths. American Journal of Public Health ,\n108(4):500\u2013502, 2018.\n[16] Ramsin Benyamin, Andrea M Trescot, Sukdeb Datta, Ricardo M Buenaventura, Ra-\njive Adlaka, Nalini Sehgal, Scott E Glaser, and Ricardo Vallejo. Opioid complications\nand side effects. Pain Physician , 2s;11(3;2s):S105\u2013S120, 03 2008.\n[17] Stefanos Gkikas and Manolis Tsiknakis. Automatic assessment of pain based on\ndeep learning methods: A systematic review. Computer Methods and Programs in\nBiomedicine , 231:107365, 2023.\n[18] Lucille A Joel. The fifth vital sign: pain. AJN The American Journal of Nursing ,\n99(2):9, 1999.\n[19] Aleksandra Badura, Aleksandra Maslowska, Andrzej My \u00b4sliwiec, and Ewa Pietka.\nMultimodal signal analysis for pain recognition in physiotherapy using wavelet scat-\ntering transform. Sensors , 21(4), 2021.\n[20] Kyle Vader, Geoff P. Bostick, Lisa C. Carlesso, Judith Hunter, Giulia Mesaroli, Kadija\nPerreault, Yannick Tousignant-Laflamme, Susan Tupper, David M. Walton, Timo-\nthy H. Wideman, and Jordan Miller. The revised iasp definition of pain and accompa-\nnying notes: Considerations for the physiotherapy profession. Physiotherapy Canada ,\n73(2):103\u2013106, 2021.\n[21] Lauren N Straatman, Michael J Lukacs, Joshua Y Lee, Maryam Ghodrati, Emily A\nLalone, and David M Walton. Are people good prognosticators of their own pain?\nan exploration of the relationship between sex-specific pain beliefs and clinical pain\nevaluation. Musculoskeletal Science and Practice , 62:102667, December 2022.BIBLIOGRAPHY 165\n[22] Raul Fernandez Rojas, Nicholas Brown, Gordon Waddington, and Roland Goecke.\nA systematic review of neurophysiological sensing for the assessment of acute pain.\nNPJ Digital Medicine , 6(1):76, 2023.\n[23] Raul Fernandez Rojas, Mingyu Liao, Julio Romero, Xu Huang, and Keng-Liang Ou.\nCortical network response to acupuncture and the effect of the hegu point: An fnirs\nstudy. Sensors , 19(2), 2019.\n[24] Seyed Amir Hossein Aqajari, Rui Cao, Emad Kasaeyan Naeini, Michael-David\nCalderon, Kai Zheng, Nikil Dutt, Pasi Liljeberg, Sanna Salanter \u00a8a, Ariana M Nelson,\nand Amir M Rahmani. Pain assessment tool with electrodermal activity for postop-\nerative patients: method validation study. JMIR mHealth and uHealth , 9(5):e25258,\n2021.\n[25] H H Yong, S J Gibson, D J Horne, and R D Helme. Development of a pain atti-\ntudes questionnaire to assess stoicism and cautiousness for possible age differences.\nThe journals of gerontology. Series B, Psychological sciences and social sciences ,\n56(5):P279\u201384, sep 2001.\n[26] E J Bartley and R B Fillingim. Sex differences in pain: a brief review of clinical and\nexperimental findings. British journal of anaesthesia , 111(1):52\u201358, jul 2013.\n[27] Jean-Michel Rou \u00b4e, Iris Morag, Wassim M Haddad, Behnood Gholami, and Kanwal-\njeet JS Anand. Using sensor-fusion and machine-learning algorithms to assess acute\npain in non-verbal infants: a study protocol. BMJ open , 11(1):e039292, 2021.\n[28] Boaz Gedaliahu Samolsky Dekel, Alberto Gori, Alessio Vasarri, Maria Cristina\nSorella, Gianfranco Di Nino, and Rita Maria Melotti. Medical evidence influence\non inpatients and nurses pain ratings agreement. Pain Research and Management ,\n2016.\n[29] Kelly M. Hoffman, Sophie Trawalter, Jordan R. Axt, and M. Norman Oliver. Racial\nbias in pain assessment and treatment recommendations, and false beliefs about bio-\nlogical differences between blacks and whites. Proceedings of the National Academy\nof Sciences , 113(16):4296\u20134301, apr 2016.\n[30] Francis J Keefe, Tamara J Somers, David A Williams, and Suzanne J Smith. Assess-\nment of pain behaviors. In Handbook of pain assessment, 3rd ed. , pages 134\u2013150.\nThe Guilford Press, New York, NY , US, 2011.\n[31] Nicole Miglio and Jessica Stanier. Beyond pain scales: A critical phenomenology of\nthe expression of pain. Frontiers in Pain Research , 3, 2022.166 BIBLIOGRAPHY\n[32] Andrew Leroux, Rachael Rzasa-Lynn, Ciprian Crainiceanu, and Tushar Sharma.\nWearable Devices: Current Status and Opportunities in Pain Assessment and Man-\nagement. Digital Biomarkers , 5(1):89\u2013102, 04 2021.\n[33] Philipp Werner, Daniel Lopez-Martinez, Steffen Walter, Ayoub Al-Hamadi, Sascha\nGruss, and Rosalind Picard. Automatic recognition methods supporting pain assess-\nment: A survey. IEEE Transactions on Affective Computing , 2019.\n[34] Gioacchino D. De Sario, Clifton R. Haider, Karla C. Maita, Ricardo A. Torres-\nGuzman, Omar S. Emam, Francisco R. Avila, John P. Garcia, Sahar Borna, Christo-\npher J. McLeod, Charles J. Bruce, Rickey E. Carter, and Antonio J. Forte. Using ai to\ndetect pain through facial expressions: A review. Bioengineering , 10(5), 2023.\n[35] Stefanos Gkikas., Chariklia Chatzaki., Elisavet Pavlidou., Foteini Verigou., Kyriakos\nKalkanis., and Manolis Tsiknakis. Automatic pain intensity estimation based on\nelectrocardiogram and demographic factors. In Proceedings of the 8th International\nConference on Information and Communication Technologies for Ageing Well and\ne-Health - ICT4AWE, , pages 155\u2013162. INSTICC, SciTePress, 2022.\n[36] Stefanos Gkikas, Chariklia Chatzaki, and Manolis Tsiknakis. Multi-task neural net-\nworks for pain intensity estimation using electrocardiogram and demographic factors.\nInInformation and Communication Technologies for Ageing Well and e-Health , pages\n324\u2013337. Springer Nature Switzerland, 2023.\n[37] Stefanos Gkikas and Manolis Tsiknakis. A full transformer-based framework for au-\ntomatic pain estimation using videos. In 2023 45th Annual International Conference\nof the IEEE Engineering in Medicine & Biology Society (EMBC) , pages 1\u20136, 2023.\n[38] Stefanos Gkikas, Nikolaos S. Tachos, Stelios Andreadis, Vasileios C. Pezoulas, Dim-\nitrios Zaridis, George Gkois, Anastasia Matonaki, Thanos G. Stavropoulos, and Dim-\nitrios I. Fotiadis. Multimodal automatic assessment of acute pain through facial videos\nand heart rate signals utilizing transformer-based architectures. Frontiers in Pain Re-\nsearch , 5, 2024.\n[39] Stefanos Gkikas and Manolis Tsiknakis. Synthetic thermal and rgb videos for auto-\nmatic pain assessment utilizing a vision-mlp architecture. In 2024 12th International\nConference on Affective Computing and Intelligent Interaction Workshops and Demos\n(ACIIW) , pages 4\u201312, 2024.\n[40] Stefanos Gkikas and Manolis Tsiknakis. Twins-painvit: Towards a modality-agnostic\nvision transformer framework for multimodal automatic pain assessment using facialBIBLIOGRAPHY 167\nvideos and fnirs. In 2024 12th International Conference on Affective Computing and\nIntelligent Interaction Workshops and Demos (ACIIW) , pages 13\u201321, 2024.\n[41] Stefanos Gkikas, Raul Fernandez Rojas, and Manolis Tsiknakis. Painformer: a\nvision foundation model for automatic pain assessment, 2025. arXiv preprint\narXiv:2505.01571.\n[42] Srinivasa N Raja, Daniel B Carr, Milton Cohen, Nanna B Finnerup, Herta Flor,\nStephen Gibson, Francis J Keefe, Jeffrey S Mogil, Matthias Ringkamp, Kathleen A\nSluka, Xue-Jun Song, Bonnie Stevens, Mark D Sullivan, Perri R Tutelman, Takahiro\nUshida, and Kyle Vader. The revised international association for the study of pain\ndefinition of pain: concepts, challenges, and compromises. Pain, 161(9):1976\u20131982,\nSeptember 2020.\n[43] Khalid S and Tubbs RS. Neuroanatomy and Neuropsychology of Pain. Cureus , 9(10),\n2017.\n[44] Eric L. Garland. Pain Processing in the Human Nervous System: A Selective Review\nof Nociceptive and Biobehavioral Pathways. Primary Care: Clinics in Office Practice ,\n39(3):561\u2013571, sep 2012.\n[45] Tatiana F. Almeida, Suely Roizenblatt, and Sergio Tufik. Afferent pain pathways:\na neuroanatomical review. Brain Research , 1000(1):40\u201356, 2004. Brain Research\nV olume 1000.\n[46] A. Vania Apkarian, M. Catherine Bushnell, Rolf-Detlef Treede, and Jon-Kar Zubieta.\nHuman brain mechanisms of pain perception and regulation in health and disease.\nEuropean Journal of Pain , 9(4):463\u2013463, 2005.\n[47] Mary Beth Babos, Brittany Grady, Warren Wisnoff, and Christy McGhee. Pathophys-\niology of pain. Disease-a-Month , 59(10):330\u2013358, 2013. Pathophysiology of pain.\n[48] Clifford J. Woolf. What is this thing called pain? The Journal of Clinical Investigation ,\n120(11):3742\u20133744, 11 2010.\n[49] Allan I Basbaum, Diana M Bautista, Gr \u00b4egory Scherrer, and David Julius. Cellular and\nmolecular mechanisms of pain. Cell, 139(2):267\u2013284, 2009.\n[50] Rolf-Detlef Treede, Winfried Rief, Antonia Barke, Qasim Aziz, Michael I Bennett,\nRafael Benoliel, Milton Cohen, Stefan Evers, Nanna B Finnerup, Michael B First,\net al. A classification of chronic pain for icd-11. Pain, 156(6):1003\u20131007, 2015.168 BIBLIOGRAPHY\n[51] Joy Onyekachukwu Egede. Automatic pain assessment from face video (continuous\npain intensity estimation in adults and newborns) . PhD thesis, University of Notting-\nham, 2019.\n[52] Raymond Sinatra. Causes and consequences of inadequate management of acute pain.\nPain Medicine , 11(12):1859\u20131871, 12 2010.\n[53] Lies De Ruddere and Raymond Tait. Facing Others in Pain: Why Context Matters ,\npages 241\u2013269. Springer International Publishing, Cham, 2018.\n[54] MVM De Oliveira, JAL De Jesus, and RM Tristao. Psychophysical parameters of\na multidimensional pain scale in newborns. Physiological measurement , 33(1):39,\n2011.\n[55] K Feldh. The checklist of nonverbal pain indicators. Pain Management Nursing, I ,\n1:13\u201321, 2000.\n[56] JC Evans, DG V ogelpohl, CM Bourguignon, and CS Morcott. Pain behaviors in lbw\ninfants accompany some\u201d nonpainful\u201d caregiving procedures. Neonatal network: NN ,\n16(3):33\u201340, 1997.\n[57] Huda Huijer Abu-Saad, Gerrie JJW Bours, Bonnie Stevens, and Jan PH Hamers. As-\nsessment of pain in the neonate. In Seminars in perinatology , volume 22, pages 402\u2013\n416. Elsevier, 1998.\n[58] Richard Stephens, John Atkins, and Andrew Kingston. Swearing as a response to\npain. Neuroreport , 20(12):1056\u20131060, 2009.\n[59] Richard Stephens and Claudia Umland. Swearing as a response to pain\u2013effect of daily\nswearing frequency. The Journal of Pain , 12(12):1274\u20131281, 2011.\n[60] Jacob Greisen, Claus B Juhl, Thorbj\u00f8rn Gr\u00f8fte, Hendrik Vilstrup, Troels S Jensen,\nand Ole Schmitz. Acute pain induces insulin resistance in humans. The Journal of the\nAmerican Society of Anesthesiologists , 95(3):578\u2013584, 2001.\n[61] Bonnie J Stevens and C Celeste Johnston. Physiological responses of premature in-\nfants to a painful stimulus. Nursing research , 43(4):226\u2013231, 1994.\n[62] Hunter G Hoffman, Todd L Richards, Barbara Coda, Aric R Bills, David Blough,\nAnne L Richards, and Sam R Sharar. Modulation of thermal pain-related brain activity\nwith virtual reality: evidence from fmri. Neuroreport , 15(8):1245\u20131248, 2004.\n[63] PJ Mathew and Joseph L Mathew. Assessment and management of pain in infants.\nPostgraduate medical journal , 79(934):438\u2013443, 2003.BIBLIOGRAPHY 169\n[64] R Melzack and P D Wall. Pain mechanisms: a new theory. Science (New York, N.Y.) ,\n150(3699):971\u2013979, nov 1965.\n[65] Thomas Hadjistavropoulos, Kenneth D Craig, Steve Duck, Annmarie Cano, Liesbet\nGoubert, Philip L Jackson, Jeffrey S Mogil, Pierre Rainville, Michael J L Sullivan,\nAmanda C de C Williams, Tine Vervoort, and Theresa Dever Fitzgerald. A biopsy-\nchosocial formulation of pain communication. Psychological bulletin , 137(6):910\u2013\n939, nov 2011.\n[66] Katelynn E Boerner, Kathryn A Birnie, Line Caes, Meghan Schinkel, and Christine T\nChambers. Sex differences in experimental pain among healthy children: a systematic\nreview and meta-analysis. Pain, 155(5):983\u2013993, may 2014.\n[67] Edmund Keogh. Men, masculinity, and pain. Pain, 156(12):2408\u20132412, dec 2015.\n[68] Fredric M Levine and Laura Lee De Simone. The effects of experimenter gender on\npain report in male and female subjects. Pain, 44(1):69\u201372, jan 1991.\n[69] Laura E McClelland and James A McCubbin. Social influence and pain response in\nwomen and men. Journal of behavioral medicine , 31(5):413\u2013420, oct 2008.\n[70] Jacob M Vigil and Joe Alcock. Tough guys or sensitive guys? Disentangling the role\nof examiner sex on patient pain reports. Pain Research & Management : The Journal\nof the Canadian Pain Society , 19(1):e9, 2014.\n[71] K A Raftery, R Smith-Coggins, and A H Chen. Gender-associated differences in\nemergency department pain management. Annals of emergency medicine , 26(4):414\u2013\n421, oct 1995.\n[72] E M Hooper, L M Comstock, J M Goodwin, and J S Goodwin. Patient characteristics\nthat influence physician behavior. Medical care , 20(6):630\u2013638, jun 1982.\n[73] Edmund Keogh. Sex and Gender as Social-Contextual Factors in Pain , pages 433\u2013\n453. Springer International Publishing, 2018.\n[74] Thomas Hadjistavropoulos and Natasha L Gallant. Pain in Older Adults: Caregiver\nChallenges , pages 415\u2013429. Springer International Publishing, 2018.\n[75] Christine J McPherson, Thomas Hadjistavropoulos, Alana Devereaux, and\nMichelle M Lobchuk. A qualitative investigation of the roles and perspectives of\nolder patients with advanced cancer and their family caregivers in managing pain in\nthe home. BMC palliative care , 13:39, 2014.170 BIBLIOGRAPHY\n[76] T Hadjistavropoulos, D L LaChapelle, F K MacLeod, B Snider, and K D Craig. Mea-\nsuring movement-exacerbated pain in cognitively impaired frail elders. The Clinical\njournal of pain , 16(1):54\u201363, mar 2000.\n[77] Clive Ballard, Maria Luisa Hanney, Megan Theodoulou, Simon Douglas, Rupert Mc-\nShane, Katja Kossakowski, Randeep Gill, Edmund Juszczak, Ly-Mee Yu, and Robin\nJacoby. The dementia antipsychotic withdrawal trial (DART-AD): long-term follow-\nup of a randomised placebo-controlled trial. The Lancet Neurology , 8(2):151\u2013157,\nfeb 2009.\n[78] Ryuta Ochi and Akira Midorikawa. Decline in Emotional Face Recognition Among\nElderly People May Reflect Mild Cognitive Impairment. Frontiers in Psychology , 12,\n2021.\n[79] R Pillai Riddell and Nicole Racine. Assessing pain in infancy: The caregiver context.\nPain Research & Management : The Journal of the Canadian Pain Society , 14(1):27,\n2009.\n[80] Tine Vervoort, Line Caes, Zina Trost, Michael Sullivan, Karoline Vangronsveld, and\nLiesbet Goubert. Social modulation of facial pain display in high-catastrophizing chil-\ndren: an observational study in schoolchildren and their parents. Pain, 152(7):1591\u2013\n1599, jul 2011.\n[81] Francis J. Keefe, Robert H. Wilkins, Wesley A. Cook, James E. Crisson, and\nLawrence H. Muhlbaier. Depression, Pain, and Pain Behavior. Journal of Consulting\nand Clinical Psychology , 54(5):665\u2013669, oct 1986.\n[82] John W. Burns, Phillip Quartana, Wesley Gilliam, Erika Gray, Justin Matsuura, Carla\nNappi, Brandy Wolfe, and Kenneth Lofland. Effects of Anger Suppression on Pain\nSeverity and Pain Behaviors Among Chronic Pain Patients: Evaluation of an Ironic\nProcess Model. Health Psychology , 27(5):645\u2013652, sep 2008.\n[83] Lies De Ruddere, Liesbet Goubert, Micha \u00a8el Andr \u00b4e Louis Stevens, Myriam Deveugele,\nKenneth Denton Craig, and Geert Crombez. Health Care Professionals\u2019 Reactions\nto Patient Pain: Impact of Knowledge About Medical Evidence and Psychosocial\nInfluences. The Journal of Pain , 15(3):262\u2013270, mar 2014.\n[84] Lisa J Staton, Mukta Panda, Ian Chen, Inginia Genao, James Kurz, Mark Pasanen,\nAlex J Mechaber, Madhusudan Menon, Jane O\u2019Rorke, JoAnn Wood, Eric Rosenberg,\nCharles Faeslis, Tim Carey, Diane Calleson, and Sam Cykert. When race matters:\ndisagreement in pain perception between patients and their physicians in primary care.\nJournal of the National Medical Association , 99(5):532\u2013538, may 2007.BIBLIOGRAPHY 171\n[85] Samantha M. Meints, Madison Stout, Samuel Abplanalp, and Adam T. Hirsh. Pain-\nRelated Rumination, But Not Magnification or Helplessness, Mediates Race and Sex\nDifferences in Experimental Pain. The Journal of Pain , 18(3):332\u2013339, mar 2017.\n[86] Brian Blake Drwecki. Race and Pain: A Dual Injustice , pages 455\u2013480. Springer\nInternational Publishing, 2018.\n[87] C S Cleeland, R Gonin, A K Hatfield, J H Edmonson, R H Blum, J A Stewart, and\nK J Pandya. Pain and its treatment in outpatients with metastatic cancer. The New\nEngland journal of medicine , 330(9):592\u2013596, mar 1994.\n[88] Lies De Ruddere and Raymond Tait. Facing Others in Pain: Why Context Matters ,\npages 241\u2013269. Springer International Publishing, Cham, 2018.\n[89] Raymond C. Tait, John T. Chibnall, Laura Miller, and Chas A. Werner. Judging pain\nand disability: effects of pain severity and physician specialty. Journal of Behavioral\nMedicine 2010 34:3 , 34(3):218\u2013224, nov 2010.\n[90] Relieving pain in America: A blueprint for transforming prevention, care, education,\nand research . National Academies Press, oct 2011.\n[91] Line Caes, Liesbet Goubert, and Laura Simons. An ecological and lifespan approach\nof social influences on childhood pain experiences. Social and Interpersonal Dynam-\nics in Pain , pages 395\u2013413, may 2018.\n[92] Geert Crombez, Patricia Bijttebier, Chris Eccleston, Tamara Mascagni, Gustaaf\nMertens, Liesbet Goubert, and Katrien Verstraeten. The child version of the pain\ncatastrophizing scale (PCS-C): a preliminary validation. Pain, 104(3):639\u2013646, aug\n2003.\n[93] Paula A Forgeron, Patrick McGrath, Bonnie Stevens, Joan Evans, Bruce Dick,\nAllen G Finley, and Torie Carlson. Social information processing in adolescents with\nchronic pain: my friends don\u2019t really understand me. Pain, 152(12):2773\u20132780, dec\n2011.\n[94] Liesbet Goubert and Laura E. Simons. Cognitive styles and processes in paediatric\npain. In Oxford Textbook of Paediatric Pain , pages 95\u2013101. Oxford University Press,\nnov 2013.\n[95] Maria Fitzgerald and Suellen M Walker. Infant pain management: a developmental\nneurobiological approach. Nature clinical practice. Neurology , 5(1):35\u201350, jan 2009.172 BIBLIOGRAPHY\n[96] David Kang, Negin Hesam-Shariati, James H. McAuley, Monzurul Alam, Zina Trost,\nCaroline D. Rae, and Sylvia M. Gustin. Disruption to normal excitatory and inhibitory\nfunction within the medial prefrontal cortex in people with chronic pain. European\nJournal of Pain , jul 2021.\n[97] Domenica A Delgado, Bradley S Lambert, Nickolas Boutris, Patrick C McCulloch,\nAndrew B Robbins, Michael R Moreno, and Joshua D Harris. Validation of Digi-\ntal Visual Analog Scale Pain Scoring With a Traditional Paper-based Visual Analog\nScale in Adults. Journal of the American Academy of Orthopaedic Surgeons. Global\nresearch & reviews , 2(3):e088, mar 2018.\n[98] Mathias Haefeli and Achim Elfering. Pain assessment. European spine journal :\nofficial publication of the European Spine Society, the European Spinal Deformity\nSociety, and the European Section of the Cervical Spine Research Society , 15 Suppl\n1(Suppl 1):S17\u201324, jan 2006.\n[99] Kenneth M. Prkachin and Patricia E. Solomon. The structure, reliability and validity\nof pain expression: Evidence from patients with shoulder pain. Pain, 139(2):267\u2013274,\noct 2008.\n[100] J Lawrence, D Alcock, P McGrath, J Kay, S B MacMurray, and C Dulberg. The\ndevelopment of a tool to assess neonatal pain. Neonatal network : NN , 12(6):59\u201366,\nsep 1993.\n[101] David E Weissman and David J Haddox. Opioid pseudoaddiction\u2013an iatrogenic syn-\ndrome. Pain, 36(3):363\u2013366, mar 1989.\n[102] Kenneth M. Prkachin. Assessing pain by facial expression: Facial expression as nexus.\nPain Research and Management , 14(1):53\u201358, 2009.\n[103] Ghada Zamzmi, Rangachar Kasturi, Dmitry Goldgof, Ruicong Zhi, Terri Ashmeade,\nand Yu Sun. A Review of Automated Pain Assessment in Infants: Features, Classi-\nfication Tasks, and Databases. IEEE Reviews in Biomedical Engineering , 11:77\u201396,\nnov 2018.\n[104] Zhanli Chen, Rashid Ansari, and Diana Wilkie. Automated Pain Detection from Fa-\ncial Expressions using FACS: A Review. arXiv , 2018.\n[105] Teena Hassan, Dominik Seus, Johannes Wollenberg, Katharina Weitz, Miriam Kunz,\nStefan Lautenbacher, Jens-Uwe Garbas, and Ute Schmid. Automatic Detection of\nPain from Facial Expressions: A Survey. IEEE Transactions on Pattern Analysis and\nMachine Intelligence , pages 1\u20131, apr 2019.BIBLIOGRAPHY 173\n[106] Philipp Werner, Daniel Lopez-Martinez, Steffen Walter, Ayoub Al-Hamadi, Sascha\nGruss, and Rosalind Picard. Automatic Recognition Methods Supporting Pain As-\nsessment: A Survey. IEEE Transactions on Affective Computing , 2019.\n[107] Rasha M. Al-Eidan, Hend Al-Khalifa, and AbdulMalik Al-Salman. Deep-Learning-\nBased Models for Pain Recognition: A Systematic Review. Applied Sciences ,\n10(17):5984, aug 2020.\n[108] Patrick Lucey, Jeffrey F. Cohn, Kenneth M. Prkachin, Patricia E. Solomon, and Iain\nMatthews. Painful data: The UNBC-McMaster shoulder pain expression archive\ndatabase. In 2011 IEEE International Conference on Automatic Face and Gesture\nRecognition and Workshops, FG 2011 , pages 57\u201364, 2011.\n[109] Steffen Walter, Sascha Gruss, Hagen Ehleiter, Junwen Tan, Harald C. Traue, Stephen\nCrawcour, Philipp Werner, Ayoub Al-Hamadi, Adriano O. Andrade, and Gustavo Mor-\neira Da Silva. The biovid heat pain database: Data for the advancement and systematic\nvalidation of an automated pain recognition. In 2013 IEEE International Conference\non Cybernetics , pages 128\u2013131, 2013.\n[110] Mohammad A. Haque, Ruben B. Bautista, Fatemeh Noroozi, Kaustubh Kulkarni,\nChristian B. Laursen, Ramin Irani, Marco Bellantonio, Sergio Escalera, Golamreza\nAnbarjafari, Kamal Nasrollahi, Ole K. Andersen, Erika G. Spaich, and Thomas B.\nMoeslund. Deep multimodal pain recognition: A database and comparison of spatio-\ntemporal visual modalities. In 13th IEEE International Conference on Automatic\nFace and Gesture Recognition, FG 2018 , pages 250\u2013257. Institute of Electrical and\nElectronics Engineers Inc., jun 2018.\n[111] Sheryl Brahnam, Chao-Fa Chuang, Frank Y Shih, and Melinda R Slack. SVM Clas-\nsification of Neonatal Facial Images of Pain. In Isabelle Bloch, Alfredo Petrosino,\nand Andrea G B Tettamanzi, editors, Fuzzy Logic and Applications , pages 121\u2013128,\nBerlin, Heidelberg, 2006. Springer Berlin Heidelberg.\n[112] Sheryl Brahnam, Loris Nanni, Shannon McMurtrey, Alessandra Lumini, Rick Brat-\ntin, Melinda Slack, and Tonya Barrier. Neonatal pain detection in videos using the\niCOPEvid dataset and an ensemble of descriptors extracted from Gaussian of Local\nDescriptors. Applied Computing and Informatics , 2019.\n[113] Ghada Zamzmi, Pai Chih-Yun, Dmitry Goldgof, R. Kasturi, Terri Ashmeade, and\nYu Sun. A Comprehensive and Context-Sensitive Neonatal Pain Assessment Using\nComputer Vision. IEEE Transactions on Affective Computing , 2019.174 BIBLIOGRAPHY\n[114] Joy Egede, Michel Valstar, Mercedes Torres Torres, and Don Sharkey. Automatic\nNeonatal Pain Estimation: An Acute Pain in Neonates Database. 2019 8th Inter-\nnational Conference on Affective Computing and Intelligent Interaction, ACII 2019 ,\npages 475\u2013481, 2019.\n[115] Min S H Aung, Sebastian Kaltwang, Bernardino Romera-Paredes, Brais Martinez,\nAneesha Singh, Matteo Cella, Michel Valstar, Hongying Meng, Andrew Kemp,\nMoshen Shafizadeh, Aaron C Elkins, Natalie Kanakam, Amschel de Rothschild, Nick\nTyler, Paul J Watson, Amanda C de C Williams, Maja Pantic, and Nadia Bianchi-\nBerthouze. The Automatic Detection of Chronic Pain-Related Expression: Require-\nments, Challenges and the Multimodal EmoPain Dataset. IEEE transactions on affec-\ntive computing , 7(4):435\u2013451, 2016.\n[116] Maria Velana, Sascha Gruss, Georg Layher, Patrick Thiam, Yan Zhang, Daniel\nSchork, Viktor Kessler, Sascha Meudt, Heiko Neumann, Jonghwa Kim, Friedhelm\nSchwenker, Elisabeth Andr \u00b4e, Harald C. Traue, and Steffen Walter. The senseemo-\ntion database: A multimodal database for the development and systematic validation\nof an automatic pain and emotion-recognition system. In Friedhelm Schwenker and\nStefan Scherer, editors, Multimodal Pattern Recognition of Social Signals in Human-\nComputer-Interaction , pages 127\u2013139, Cham, 2017. Springer International Publish-\ning.\n[117] Sascha Gruss, Mattis Geiger, Philipp Werner, Oliver Wilhelm, Harald C Traue, Ayoub\nAl-Hamadi, and Steffen Walter. Multi-Modal Signals for Analyzing Pain Responses\nto Thermal and Electrical Stimuli. Journal of visualized experiments : JoVE , (146),\napr 2019.\n[118] Raul Fernandez Rojas, Niraj Hirachan, Calvin Joseph, Ben Seymour, and Roland\nGoecke. The ai4pain grand challenge 2024: Advancing pain assessment with mul-\ntimodal fnirs and facial video analysis. In 2024 12th International Conference on\nAffective Computing and Intelligent Interaction . IEEE, 2024.\n[119] Henrik Pedersen. Learning appearance features for pain detection using the UNBC-\nMcMaster shoulder pain expression archive database. Lecture Notes in Computer\nScience (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes\nin Bioinformatics) , 9163:128\u2013136, 2015.\n[120] Joy O. Egede, Siyang Song, Temitayo A. Olugbade, Chongyang Wang, Amanda C.\nDe C. Williams, Hongying Meng, Min Aung, Nicholas D. Lane, Michel Valstar, and\nNadia Bianchi-Berthouze. Emopain challenge 2020: Multimodal pain evaluationBIBLIOGRAPHY 175\nfrom facial and bodily expressions. In 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition (FG 2020) , pages 849\u2013856, 2020.\n[121] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-\nscale image recognition. In 3rd International Conference on Learning Representa-\ntions, ICLR 2015 - Conference Track Proceedings . International Conference on Learn-\ning Representations, ICLR, sep 2015.\n[122] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning\nfor image recognition. In Proceedings of the IEEE Computer Society Conference\non Computer Vision and Pattern Recognition , volume 2016-Decem, pages 770\u2013778.\nIEEE Computer Society, dec 2016.\n[123] Ruijing Yang, Xiaopeng Hong, Jinye Peng, Xiaoyi Feng, and Guoying Zhao. Incor-\nporating high-level and low-level cues for pain intensity estimation. In Proceedings -\nInternational Conference on Pattern Recognition , volume 2018-Augus, pages 3495\u2013\n3500. Institute of Electrical and Electronics Engineers Inc., 2018.\n[124] Ashish Semwal and Narendra D Londhe. Computer aided pain detection and inten-\nsity estimation using compact CNN based fusion network. Applied Soft Computing ,\n112:107780, 2021.\n[125] Saandeep Aathreya Sidhapur Lakshminarayan, Saurabh Hinduja, and Shaun Cana-\nvan. Three-level Training of Multi-Head Architecture for Pain Detection. In Gomez-\nFernandez F Struc V ., editor, Proceedings - 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition, FG 2020 , pages 839\u2013843. Institute of\nElectrical and Electronics Engineers Inc., 2020.\n[126] Van Thong Huynh, Hyung Jeong Yang, Guee Sang Lee, and Soo Hyung Kim. Mul-\ntimodality Pain and related Behaviors Recognition based on Attention Learning. In\nGomez-Fernandez F Struc V ., editor, Proceedings - 2020 15th IEEE International\nConference on Automatic Face and Gesture Recognition, FG 2020 , pages 814\u2013818.\nInstitute of Electrical and Electronics Engineers Inc., 2020.\n[127] Ashish Semwal and Narendra D. Londhe. Automated Pain Severity Detection Using\nConvolutional Neural Network. In Rajpurohit V S Nadkatti M N Niranjan S.K. Desai\nV ., editor, Proceedings of the International Conference on Computational Techniques,\nElectronics and Mechanical Systems, CTEMS 2018 , pages 66\u201370. Institute of Electri-\ncal and Electronics Engineers Inc., 2018.\n[128] Mohammad Tavakolian and Abdenour Hadid. Deep binary representation of facial\nexpressions: A novel framework for automatic pain intensity recognition. In Proceed-176 BIBLIOGRAPHY\nings - International Conference on Image Processing, ICIP , pages 1952\u20131956. IEEE\nComputer Society, 2018.\n[129] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning Face Representation from\nScratch, 2014.\n[130] Ashish Semwal and Narendra D. Londhe. ECCNET: An ensemble of compact convo-\nlution neural network for pain severity assessment from face images. In Proceedings\nof the Confluence 2021: 11th International Conference on Cloud Computing, Data\nScience and Engineering , pages 761\u2013766, 2021.\n[131] Vernon J. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, Stephen M. Gordon,\nChou P. Hung, and Brent J. Lance. EEGNet: A compact convolutional neural network\nfor EEG-based brain-computer interfaces. Journal of Neural Engineering , 15(5), nov\n2018.\n[132] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi.\nInception-v4, inception-ResNet and the impact of residual connections on learning.\nIn31st AAAI Conference on Artificial Intelligence, AAAI 2017 , pages 4278\u20134284.\nAAAI press, feb 2017.\n[133] Reza Kharghanian, Ali Peiravi, and Farshad Moradi. Pain detection from facial im-\nages using unsupervised feature learning approach. In Proceedings of the Annual\nInternational Conference of the IEEE Engineering in Medicine and Biology Society,\nEMBS , volume 2016-Octob, pages 419\u2013422. Institute of Electrical and Electronics\nEngineers Inc., 2016.\n[134] Reza Kharghanian, Ali Peiravi, Farshad Moradi, and Alexandros Iosifidis. Pain detec-\ntion using batch normalized discriminant restricted Boltzmann machine layers. Jour-\nnal of Visual Communication and Image Representation , 76, 2021.\n[135] Dong Huang, Zhaoqiang Xia, Lei Li, Kunwei Wang, and Xiaoyi Feng. Pain-\nawareness multistream convolutional neural network for pain estimation. Journal\nof Electronic Imaging , 28(04):1, 2019.\n[136] Xuwu Xin, Xiaoyan Lin, Shengfu Yang, and Xin Zheng. Pain intensity estimation\nbased on a spatial transformation and attention CNN. PLoS ONE , 15(8 August\n2020):1\u201315, aug 2020.\n[137] S Cui, D Huang, Y Ni, and X Feng. Multi-scale regional attention networks for pain\nestimation. In 2021 13th International Conference on Bioinformatics and Biomedical\nTechnology , pages 1\u20138. Association for Computing Machinery, 2021.BIBLIOGRAPHY 177\n[138] Conghui Li, Zhaocheng Zhu, and Yuming Zhao. Saliency Supervision: An Intuitive\nand Effective Approach for Pain Intensity Regression. Lecture Notes in Computer\nScience (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes\nin Bioinformatics) , 11307 LNCS:455\u2013464, 2018.\n[139] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified em-\nbedding for face recognition and clustering. In 2015 IEEE Conference on Computer\nVision and Pattern Recognition (CVPR) , pages 815\u2013823. IEEE Computer Society, mar\n2015.\n[140] Xianlin Peng, Dong Huang, and Haixi Zhang. Pain intensity recognition via multi-\nscale deep network. IET Image Processing , 14(8):1645\u20131652, 2020.\n[141] X Xin, X Li, S Yang, X Lin, and X Zheng. Pain expression assessment based on a\nlocality and identity aware network. IET Image Processing , 15(12):2948\u20132958, 2021.\n[142] Ashish Semwal and Narendra D. Londhe. S-PANET: A Shallow Convolutional Neural\nNetwork for Pain Severity Assessment in Uncontrolled Environment. In 2021 IEEE\n11th Annual Computing and Communication Workshop and Conference, CCWC 2021 ,\npages 800\u2013806. Institute of Electrical and Electronics Engineers Inc., jan 2021.\n[143] Ashish Semwal and Narendra D. Londhe. MVFNet: A multi-view fusion network for\npain intensity assessment in unconstrained environment. Biomedical Signal Process-\ning and Control , 67, may 2021.\n[144] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir\nAnguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going\ndeeper with convolutions. In IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , volume 07-12-June, pages 1\u20139. IEEE Computer Society, oct\n2015.\n[145] Jiann Shu Lee and Chuan Wei Wang. Facial pain intensity estimation for ICU patient\nwith partial occlusion coming from treatment. In 3rd International Conference on Bi-\nological Information and Biomedical Engineering, BIBE 2019 , pages 106\u2013109. VDE\nVerlag GmbH, 2019.\n[146] Reneiro Andal Virrey, Wahyu Caesarendra, Muhammad Iskandar Bin Pg Hj Petra,\nEmeroylariffion Abas, Asmah Husaini, and Chandratilak De Silva Liyanage. Mile-\nstone of Pain Intensity Evaluation from Facial Action Units. In ICECOS 2019 - 3rd\nInternational Conference on Electrical Engineering and Computer Science, Proceed-\ning, pages 54\u201357. Institute of Electrical and Electronics Engineers Inc., 2019.178 BIBLIOGRAPHY\n[147] Hermawan Nugroho, Dani Harmanto, and Hamada Rasheed Hassan Al-Absi. On the\nDevelopment of Smart Home Care: Application of Deep Learning for Pain Detection.\nIn2018 IEEE-EMBS Conference on Biomedical Engineering and Sciences (IECBES) ,\npages 612\u2013616, 2019.\n[148] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A Unified Em-\nbedding for Face Recognition and Clustering. Proceedings of the IEEE Computer So-\nciety Conference on Computer Vision and Pattern Recognition , 07-12-June:815\u2013823,\nmar 2015.\n[149] Laduona Dai, Joost Broekens, and Khiet P. Truong. Real-time pain detection in facial\nexpressions for health robotics. In 2019 8th International Conference on Affective\nComputing and Intelligent Interaction Workshops and Demos, ACIIW 2019 , pages\n277\u2013283. Institute of Electrical and Electronics Engineers Inc., 2019.\n[150] Guglielmo Menchetti, Zhanli Chen, DIana J. Wilkie, Rashid Ansari, Yasemin\nYardimci, and A. Enis Cetin. Pain detection from facial videos using two-stage deep\nlearning. In GlobalSIP 2019 - 7th IEEE Global Conference on Signal and Information\nProcessing, Proceedings . Institute of Electrical and Electronics Engineers Inc., 2019.\n[151] Chittaranjan Andrade. Internal, external, and ecological validity in research design,\nconduct, and evaluation. Indian Journal of Psychological Medicine , 40(5):498\u2013499,\n2018. PMID: 30275631.\n[152] Dianbo Liu, Peng Fengjiao, Ognjen (Oggi) Rudovic, and Rosalind Picard. Deep-\nfacelift: Interpretable personalized models for automatic estimation of self-reported\npain. In Neil Lawrence and Mark Reid, editors, Proceedings of IJCAI 2017 Work-\nshop on Artificial Intelligence in Affective Computing , volume 66 of Proceedings of\nMachine Learning Research , pages 1\u201316. PMLR, 20 Aug 2017.\n[153] Xiaojing Xu, Jeannie S Huang, and Virginia R De Sa. Pain Evaluation in Video using\nExtended Multitask Learning from Multidimensional Measurements. In Proceedings\nof the Machine Learning for Health NeurIPS Workshop , volume 116, pages 141\u2013154.\nPMLR, apr 2020.\n[154] Paola Casti, Arianna Mencattini, Maria Colomba Comes, Giuseppina Callari, Davide\nDi Giuseppe, Silvia Natoli, Mauro Dauri, Elena Daprati, and Eugenio Martinelli. Cal-\nibration of Vision-Based Measurement of Pain Intensity with Multiple Expert Ob-\nservers. IEEE Transactions on Instrumentation and Measurement , 68(7):2442\u20132450,\n2019.BIBLIOGRAPHY 179\n[155] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton. ImageNet classification\nwith deep convolutional neural networks. In Advances in Neural Information Process-\ning Systems 25 (NIPS 2012) , 2012.\n[156] Luigi Celona and Luca Manoni. Neonatal Facial Pain Assessment Combining Hand-\nCrafted and Deep Features. Lecture Notes in Computer Science , 10590 LNCS:197\u2013\n204, 2017.\n[157] Omkar M. Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep Face Recognition.\nInProceedings of the British Machine Vision Conference (BMVC) , pages 41.1\u201341.12.\nBritish Machine Vision Association and Society for Pattern Recognition, dec 2015.\n[158] Gil Levi and Tal Hassner. Emotion recognition in the wild via convolutional neural\nnetworks and mapped binary patterns. In ICMI 2015 - Proceedings of the 2015 ACM\nInternational Conference on Multimodal Interaction , pages 503\u2013510, New York, NY ,\nUSA, 2015. ACM.\n[159] Guanming Lu, Qiang Hao, Kaiting Kong, Jingjie Yan, Haibo Li, and Xiaonan Li.\nDeep convolutional neural networks with transfer learning for neonatal pain expres-\nsion recognition. In Xiao G Ning X Li K Li M Xiao Z. Wang L., editor, 2018 14th\nInternational Conference on Natural Computation, Fuzzy Systems and Knowledge\nDiscovery (ICNC-FSKD) , pages 251\u2013256. Institute of Electrical and Electronics En-\ngineers Inc., 2018.\n[160] Ghada Zamzmi, Rahul Paul, Md. Sirajus Salekin, Dmitry Goldgof, Rangachar Kas-\nturi, Thao Ho, and Yu Sun. Convolutional Neural Networks for Neonatal Pain Assess-\nment. IEEE Transactions on Biometrics, Behavior, and Identity Science , 1(3):192\u2013\n200, 2019.\n[161] Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, and Yu Sun. Neonatal Pain\nExpression Recognition Using Transfer Learning. arXiv , jul 2018.\n[162] Ken Chatfield, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Return of\nthe devil in the details: Delving deep into convolutional nets, 2014.\n[163] Luigi Celona, Sheryl Brahnam, and Simone Bianco. Getting the most of few data\nfor neonatal pain assessment. In ACM International Conference Proceeding Series ,\npages 298\u2013301. Association for Computing Machinery, 2019.\n[164] Martin Arjovsky, Soumith Chintala, and L \u00b4eon Bottou. Wasserstein GAN. arXiv , jan\n2017.180 BIBLIOGRAPHY\n[165] Ognjen Rudovic, Nicolas Tobis, Sebastian Kaltwang, Bj \u00a8orn Schuller, Daniel Rueckert,\nJeffrey F. Cohn, and Rosalind W. Picard. Personalized Federated Deep Learning for\nPain Estimation From Face Images. arXiv , jan 2021.\n[166] Kornprom Pikulkaew, Ekkarat Boonchieng, Waraporn Boonchieng, and Varin Chou-\nvatut. Pain detection using deep learning with evaluation system. In Advances in In-\ntelligent Systems and Computing , volume 1184, pages 426\u2013435. Springer Singapore,\n2021.\n[167] S El Morabit, A Rivenq, M.-E.-N. Zighem, A Hadid, A Ouahabi, and A Taleb-Ahmed.\nAutomatic pain estimation from facial expressions: A comparative analysis using off-\nthe-shelf cnn architectures. Electronics , 10(16), 2021.\n[168] C Li, A Pourtaherian, L Van Onzenoort, W.E.T.A. Ten, and P H N De With. Infant\nFacial Expression Analysis: Towards a Real-Time Video Monitoring System Using\nR-CNN and HMM. IEEE Journal of Biomedical and Health Informatics , 25(5):1429\u2013\n1440, 2021.\n[169] Feng Wang, Xiang Xiang, Chang Liu, Trac D. Tran, Austin Reiter, Gregory D. Hager,\nHarry Quon, Jian Cheng, and Alan L. Yuille. Regularizing face verification nets for\npain intensity regression. In Proceedings - International Conference on Image Pro-\ncessing, ICIP , volume 2017-Septe, pages 1087\u20131091. IEEE Computer Society, 2018.\n[170] Marilena Ctlina Dragomir, Corneliu Florea, and Valentin Pupezescu. Automatic Sub-\nject Independent Pain Intensity Estimation using a Deep Learning Approach. In 2020\n8th E-Health and Bioengineering Conference, EHB 2020 , pages 1\u20134, 2020.\n[171] Ashish Semwal and Narendra D. Londhe. Automated facial expression based pain\nassessment using deep convolutional neural network. In Proceedings of the 3rd Inter-\nnational Conference on Intelligent Sustainable Systems, ICISS 2020 , pages 366\u2013370.\nInstitute of Electrical and Electronics Engineers Inc., 2020.\n[172] N Rathee, S Pahal, and P Sheoran. Pain detection from facial expressions using do-\nmain adaptation technique. Pattern Analysis and Applications , 2021.\n[173] Ghada Zamzmi, Rahul Paul, Dmitry Goldgof, Rangachar Kasturi, and Yu Sun. Pain\nAssessment from Facial Expression: Neonatal Convolutional Neural Network (N-\nCNN). In Proceedings of the International Joint Conference on Neural Networks ,\nvolume 2019-July. Institute of Electrical and Electronics Engineers Inc., 2019.\n[174] L P Carlini, L A Ferreira, G S Coutrin, V V Varoto, T M Heiderich, R X Balda,\nM M Barros, R Guinsburg, and C E Thomaz. A Convolutional Neural Network-\nbased Mobile Application to Bedside Neonatal Pain Assessment. In Conference onBIBLIOGRAPHY 181\nGraphics, Patterns and Images (SIBGRAPI) , pages 394\u2013401, Los Alamitos, CA, USA,\n2021. IEEE Computer Society.\n[175] Subhash Nerella, Julie Cupka, Matthew Ruppert, Patrick Tighe, Azra Bihorac, and\nParisa Rashidi. Pain action unit detection in critically ill patients. In 2021 IEEE\n45th Annual Computers, Software, and Applications Conference (COMPSAC) , pages\n645\u2013651, 2021.\n[176] Joy Egede, Michel Valstar, and Brais Martinez. Fusing deep learned and hand-crafted\nfeatures of appearance, shape, and dynamics for automatic pain estimation. In 12th\nIEEE International Conference on Automatic Face & Gesture Recognition , pages 689\u2013\n696, 2017.\n[177] Joy O. Egede and Michel Valstar. Cumulative attributes for pain intensity estimation.\nInICMI 2017 - Proceedings of the 19th ACM International Conference on Multi-\nmodal Interaction , volume 2017-Janua, pages 146\u2013153. Association for Computing\nMachinery, Inc, nov 2017.\n[178] Shashank Jaiswal, Joy Egede, and Michel Valstar. Deep learned cumulative attribute\nregression. In Proceedings - 13th IEEE International Conference on Automatic Face\nand Gesture Recognition, FG 2018 , pages 715\u2013722. Institute of Electrical and Elec-\ntronics Engineers Inc., 2018.\n[179] Mohammad Tavakolian, Carlos Guillermo Bermudez Cruces, and Abdenour Hadid.\nLearning to detect genuine versus posed pain from facial expressions using residual\ngenerative adversarial networks. In Proceedings - 14th IEEE International Confer-\nence on Automatic Face and Gesture Recognition, FG 2019 . Institute of Electrical\nand Electronics Engineers Inc., may 2019.\n[180] Mohammad Tavakolian, Miguel Bordallo Lopez, and Li Liu. Self-supervised pain in-\ntensity estimation from facial videos via statistical spatiotemporal distillation. Pattern\nRecognition Letters , 140:26\u201333, 2020.\n[181] Ehsan Othman, Philipp Werner, Frerk Saxen, Ayoub Al-Hamadi, Sascha Gruss, and\nSteffen Walter. Automatic vs. Human recognition of pain intensity from facial expres-\nsion on the x-ite pain database. Sensors , 21(9), may 2021.\n[182] Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and Liang-\nChieh Chen. Mobilenetv2: Inverted residuals and linear bottlenecks. Proceedings\nof the IEEE Computer Society Conference on Computer Vision and Pattern Recogni-\ntion, pages 4510\u20134520, jan 2018.182 BIBLIOGRAPHY\n[183] Ehsan Othman, Philipp Werner, Frerk Saxen, Ayoub Al-Hamadi, and Steffen Walter.\nCross-database evaluation of pain recognition from facial video. In International\nSymposium on Image and Signal Processing and Analysis, ISPA , volume 2019-Septe,\npages 181\u2013186. IEEE Computer Society, sep 2019.\n[184] Mohammad Tavakolian and Abdenour Hadid. Deep Spatiotemporal Representation of\nthe Face for Automatic Pain Intensity Estimation. In 2018 24th International Confer-\nence on Pattern Recognition (ICPR) , volume 2018-Augus, pages 350\u2013354. Institute\nof Electrical and Electronics Engineers Inc., 2018.\n[185] Jinwei Wang and Huazhi Sun. Pain intensity estimation using deep spatiotem-\nporal and handcrafted features. IEICE Transactions on Information and Systems ,\nE101D(6):1572\u20131580, 2018.\n[186] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri.\nLearning Spatiotemporal Features with 3D Convolutional Networks. In 2015 IEEE\nInternational Conference on Computer Vision (ICCV) , pages 4489\u20134497, 2015.\n[187] Yibo Huang, Linbo Qing, Shengyu Xu, Lu Wang, and Yonghong Peng. HybNet: a\nhybrid network structure for pain intensity estimation. Visual Computer , 2021.\n[188] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethink-\ning spatiotemporal feature learning: Speed-accuracy trade-offs in video classification.\nInComputer Vision \u2013 ECCV 2018 , volume 11219 LNCS, pages 318\u2013335, dec 2018.\n[189] Mohammad Tavakolian and Abdenour Hadid. A Spatiotemporal Convolutional Neu-\nral Network for Automatic Pain Intensity Estimation from Facial Dynamics. Interna-\ntional Journal of Computer Vision , 127(10):1413\u20131425, oct 2019.\n[190] Gnana Praveen R, Eric Granger, and Patrick Cardinal. Deep Domain Adaptation\nfor Ordinal Regression of Pain Intensity Estimation Using Weakly-Labeled Videos.\nCoRR , aug 2020.\n[191] R. Gnana Praveen, Eric Granger, and Patrick Cardinal. Deep Weakly Supervised Do-\nmain Adaptation for Pain Localization in Videos. In Gomez-Fernandez F Struc V .,\neditor, 15th IEEE International Conference on Automatic Face and Gesture Recogni-\ntion, pages 473\u2013480. Institute of Electrical and Electronics Engineers Inc., 2020.\n[192] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model\nand the kinetics dataset. In Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR) , July 2017.BIBLIOGRAPHY 183\n[193] Ghazal Bargshady, Xujuan Zhou, Ravinesh C. Deo, Jeffrey Soar, Frank Whittaker,\nand Hua Wang. The modeling of human facial pain intensity based on Temporal\nConvolutional Networks trained with video frames in HSV color space. Applied Soft\nComputing Journal , 97, 2020.\n[194] Siavash Rezaei, Abhishek Moturu, Shun Zhao, Kenneth M. Prkachin, Thomas Had-\njistavropoulos, and Babak Taati. Unobtrusive pain monitoring in older adults with\ndementia using pairwise and contrastive training. IEEE Journal of Biomedical and\nHealth Informatics , 25(5):1450\u20131462, 2021.\n[195] Hinton GE. Training products of experts by minimizing contrastive divergence. Neu-\nral computation , 14(8):1771\u20131800, aug 2002.\n[196] Vedhas Pandit, Maximilian Schmitt, Nicholas Cummins, and Bj \u00a8orn Schuller. I see it\nin your eyes: Training the shallowest-possible CNN to recognise emotions and pain\nfrom muted web-assisted in-the-wild video-chats in real-time. Information Processing\nand Management , 57(6):102347, nov 2020.\n[197] Jing Zhou, Xiaopeng Hong, Fei Su, and Guoying Zhao. Recurrent Convolutional\nNeural Network Regression for Continuous Pain Intensity Estimation in Video. IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition Work-\nshops , pages 1535\u20131543, may 2016.\n[198] Pau Rodriguez, Guillem Cucurull, Jordi Gonalez, Josep M. Gonfaus, Kamal Nasrol-\nlahi, Thomas B. Moeslund, and F. Xavier Roca. Deep Pain: Exploiting Long Short-\nTerm Memory Networks for Facial Expression Classification. IEEE Transactions on\nCybernetics , feb 2017.\n[199] Marco Bellantonio, Mohammad A. Haque, Pau Rodriguez, Kamal Nasrollahi, Taisi\nTelve, Sergio Escarela, Jordi Gonzalez, Thomas B. Moeslund, Pejman Rasti, and Gho-\nlamreza Anbarjafari. Spatio-temporal pain recognition in CNN-based super-resolved\nfacial images. Lecture Notes in Computer Science (including subseries Lecture Notes\nin Artificial Intelligence and Lecture Notes in Bioinformatics) , 10165 LNCS:151\u2013162,\n2017.\n[200] Ghazal Bargshady, Jeffrey Soar, Xujuan Zhou, Ravinesh C. Deo, Frank Whittaker,\nand Hua Wang. A joint deep neural network model for pain recognition from face.\n2019 IEEE 4th International Conference on Computer and Communication Systems,\nICCCS 2019 , pages 52\u201356, 2019.184 BIBLIOGRAPHY\n[201] Ghazal Bargshady, Xujuan Zhou, Ravinesh C. Deo, Jeffrey Soar, Frank Whittaker, and\nHua Wang. Enhanced deep learning algorithm development to detect pain intensity\nfrom facial expression images. Expert Systems with Applications , 149, 2020.\n[202] Antoni Mauricio, F \u00b4abio Cappabianco, Adriano Veloso, and Guillermo C \u00b4amara. A\nSequential Approach for Pain Recognition Based on Facial Representations. Lecture\nNotes in Computer Science (including subseries Lecture Notes in Artificial Intelli-\ngence and Lecture Notes in Bioinformatics) , 11754 LNCS:295\u2013304, 2019.\n[203] Selvarajah Thuseethan, Sutharshan Rajasegarar, and John Yearwood. Deep hybrid\nspatiotemporal networks for continuous pain intensity estimation. Lecture Notes in\nComputer Science (including subseries Lecture Notes in Artificial Intelligence and\nLecture Notes in Bioinformatics) , 11955 LNCS:449\u2013461, 2019.\n[204] Ghazal Bargshady, Xujuan Zhou, Ravinesh C. Deo, Jeffrey Soar, Frank Whittaker,\nand Hua Wang. Ensemble neural network approach detecting pain intensity from\nfacial expressions. Artificial Intelligence in Medicine , 109, 2020.\n[205] Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho,\nand Yu Sun. First Investigation into the Use of Deep Learning for Continuous As-\nsessment of Neonatal Postoperative Pain. In Gomez-Fernandez F Struc V ., editor,\nProceedings - 2020 15th IEEE International Conference on Automatic Face and Ges-\nture Recognition, FG 2020 , pages 415\u2013419. Institute of Electrical and Electronics\nEngineers Inc., 2020.\n[206] Nikolai Kalischek, Patrick Thiam, Peter Bellmann, and Friedhelm Schwenker. Deep\nDomain Adaptation for Facial Expression Analysis. In 2019 8th International Con-\nference on Affective Computing and Intelligent Interaction Workshops and Demos,\nACIIW 2019 , pages 317\u2013323. Institute of Electrical and Electronics Engineers Inc.,\n2019.\n[207] Geoffrey French, Michal Mackiewicz, and Mark Fisher. Self-ensembling for visual\ndomain adaptation. In 6th International Conference on Learning Representations,\nICLR 2018 - Conference Track Proceedings , jun 2018.\n[208] Daniel Lopez Martinez, Ognjen Rudovic, and Rosalind Picard. Personalized Auto-\nmatic Estimation of Self-Reported Pain Intensity from Facial Expressions. In IEEE\nComputer Society Conference on Computer Vision and Pattern Recognition Work-\nshops , volume 2017-July, pages 2318\u20132327. IEEE Computer Society, jun 2017.\n[209] Diyala Erekat, Zakia Hammal, Maimoon Siddiqui, and Hamdi Dibeklioglu. Enforcing\nmultilabel consistency for automatic spatio-temporal assessment of shoulder pain in-BIBLIOGRAPHY 185\ntensity. In ICMI 2020 Companion - Companion Publication of the 2020 International\nConference on Multimodal Interaction , pages 156\u2013164. Association for Computing\nMachinery, Inc, 2020.\n[210] Manh Tu Vu, Marie Beurton-Aimar, Pierre-yves Dezaunay, and Marine Cotty Es-\nlous. Automated Pain Estimation based on Facial Action Units from Multi-Databases.\nInJoint International Conference on Informatics, Electronics Vision (ICIEV) and In-\nternational Conference on Imaging, Vision Pattern Recognition (icIVPR) , pages 1\u20138,\n2021.\n[211] Dong Huang, Zhaoqiang Xia, Joshua Mwesigye, and Xiaoyi Feng. Pain-attentive\nnetwork: a deep spatio-temporal attention model for pain estimation. Multimedia\nTools and Applications , 79(37-38):28329\u201328354, 2020.\n[212] Jun Yu, Toru Kurihara, and Shu Zhan. Frame by Frame Pain Estimation Using Locally\nSpatial Attention Learning. Lecture Notes in Computer Science (including subseries\nLecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics) , 11868\nLNCS:229\u2013238, 2019.\n[213] Haochen Xu and Manhua Liu. A Deep Attention Transformer Network for Pain Es-\ntimation with Facial Expression Video. In Jianjiang Feng, Junping Zhang, Manhua\nLiu, and Yuchun Fang, editors, Biometric Recognition , pages 112\u2013119, Cham, 2021.\nSpringer International Publishing.\n[214] Adria Mallol-Ragolta, Shuo Liu, Nicholas Cummins, and Bjorn Schuller. A Cur-\nriculum Learning Approach for Pain Intensity Recognition from Facial Expressions.\nIn Gomez-Fernandez F Struc V ., editor, 2020 15th IEEE International Conference\non Automatic Face and Gesture Recognition (FG 2020) , pages 829\u2013833. Institute of\nElectrical and Electronics Engineers Inc., 2020.\n[215] Yikang Guo, Li Wang, Yan Xiao, and Yingzi Lin. A Personalized Spatial-Temporal\nCold Pain Intensity Estimation Model Based on Facial Expression. IEEE journal of\ntranslational engineering in health and medicine , 9, 2021.\n[216] Sowmya Rasipuram, Bukka Nikhil Sai, Dinesh Babu Jayagopi, and Anutosh Maitra.\nUsing Deep 3D Features and an LSTM Based Sequence Model for Automatic Pain\nDetection in the Wild. In Gomez-Fernandez F Struc V ., editor, 2020 15th IEEE Inter-\nnational Conference on Automatic Face and Gesture Recognition (FG 2020) , pages\n781\u2013785. Institute of Electrical and Electronics Engineers Inc., 2020.\n[217] Ruicong Zhi and Ming Wan. Dynamic facial expression feature learning based on\nsparse RNN. In Proceedings of 2019 IEEE 8th Joint International Information Tech-186 BIBLIOGRAPHY\nnology and Artificial Intelligence Conference, ITAIC 2019 , pages 1373\u20131377. Institute\nof Electrical and Electronics Engineers Inc., may 2019.\n[218] Thomas Blumensath and Mike Davies. Iterative Thresholding for Sparse Approxima-\ntions. Journal of Fourier Analysis and Applications , 14:629\u2013654, 2008.\n[219] Patrick Thiam, Hans A. Kestler, and Friedhelm Schwenker. Two-stream attention\nnetwork for pain recognition from video sequences. Sensors (Switzerland) , 20(3):839,\nfeb 2020.\n[220] Antoni Mauricio, Jonathan Pe \u02dcna, Erwin Dianderas, Leonidas Mauricio, Jose D \u00b4\u0131az, and\nAntonio Mor \u00b4an. Chronic Pain Estimation Through Deep Facial Descriptors Analysis.\nCommunications in Computer and Information Science , 1070 CCIS:173\u2013185, 2020.\n[221] Jie Ting, Yi-Cheng Yang, Li-Chen Fu, Chu-Lin Tsai, and Chien-Hua Huang. Distance\nOrdering: A Deep Supervised Metric Learning for Pain Intensity Estimation. In 2021\n20th IEEE International Conference on Machine Learning and Applications (ICMLA) ,\npages 1083\u20131088, 2021.\n[222] Mingxin Yu, Yichen Sun, Bofei Zhu, Lianqing Zhu, Yingzi Lin, Xiaoying Tang,\nYikang Guo, Guangkai Sun, and Mingli Dong. Diverse frequency band-based convo-\nlutional neural networks for tonic cold pain assessment using EEG. Neurocomputing ,\n378:270\u2013282, 2020.\n[223] Jiahao Wang, Mengying Wei, Li Zhang, Gan Huang, Zhen Liang, Linling Li, and\nZhiguo Zhang. An Autoencoder-based Approach to Predict Subjective Pain Percep-\ntion from High-density Evoked EEG Potentials. In International Conference of the\nIEEE Engineering in Medicine Biology Society , volume 2020-July, pages 1507\u20131511.\nInstitute of Electrical and Electronics Engineers Inc., 2020.\n[224] Raul Fernandez Rojas, Julio Romero, Jehu Lopez-Aparicio, and Keng-Liang Ou.\nPain Assessment based on fNIRS using Bi-LSTM RNNs. In 10th International\nIEEE/EMBS Conference on Neural Engineering (NER) , pages 399\u2013402. IEEE, may\n2021.\n[225] Hyunjun Lim, Byeongnam Kim, Gyu Jeong Noh, and Sun K. Yoo. A deep neural\nnetwork-based pain classifier using a photoplethysmography signal. Sensors (Switzer-\nland) , 19(2), jan 2019.\n[226] Boyi Hu, Chong Kim, Xiaopeng Ning, and Xu Xu. Using a deep learning network to\nrecognise low back pain in static standing. Ergonomics , 61(10):1374\u20131381, oct 2018.BIBLIOGRAPHY 187\n[227] Danila Mamontov, Iana Polonskaia, Alina Skorokhod, Eugene Semenkin, Viktor\nKessler, and Friedhelm Schwenker. Evolutionary algorithms for the design of neu-\nral network classifiers for the classification of pain intensity. Multimodal Pattern\nRecognition of Social Signals in Human-Computer-Interaction , 11377 LNAI:84\u2013100,\n2019.\n[228] Chuan Yu Chang and Jia Jing Li. Application of deep learning for recognizing in-\nfant cries. In 2016 IEEE International Conference on Consumer Electronics-Taiwan,\nICCE-TW 2016 . Institute of Electrical and Electronics Engineers Inc., jul 2016.\n[229] Md Sirajus Salekin, Ghada Zamzmi, Rahul Paul, Dmitry Goldgof, Rangachar Kas-\nturi, Thao Ho, and Yu Sun. Harnessing the Power of Deep Learning Methods in\nHealthcare: Neonatal Pain Assessment from Crying Sound. In 2019 IEEE Healthcare\nInnovations and Point of Care Technologies, HI-POCT 2019 , pages 127\u2013130. Institute\nof Electrical and Electronics Engineers Inc., 2019.\n[230] Patrick Thiam and Friedhelm Schwenker. Combining deep and hand-crafted features\nfor audio-based pain intensity classification. Lecture Notes in Computer Science (in-\ncluding subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioin-\nformatics) , 11377 LNAI:49\u201358, 2019.\n[231] Fu Sheng Tsai, Yi Ming Weng, Chip Jin Ng, and Chi Chun Lee. Embedding stacked\nbottleneck vocal features in a LSTM architecture for automatic pain level classifica-\ntion during emergency triage. In 2017 7th International Conference on Affective Com-\nputing and Intelligent Interaction, ACII 2017 , volume 2018-Janua, pages 313\u2013318.\nInstitute of Electrical and Electronics Engineers Inc., 2018.\n[232] P Gouverneur, F Li, W M Adamczyk, T M Szikszay, K Luedtke, and M Grzegorzek.\nComparison of feature extraction methods for physiological signals for heat-based\npain recognition. Sensors , 21(14), 2021.\n[233] Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho,\nand Yu Sun. Multi-channel neural network for assessing neonatal pain from videos.\nInIEEE International Conference on Systems, Man and Cybernetics , volume 2019-\nOctob, pages 1551\u20131556. Institute of Electrical and Electronics Engineers Inc., oct\n2019.\n[234] Emad Kasaeyan Naeini, Sina Shahhosseini, Ajan Subramanian, Tingjue Yin, Amir M.\nRahmani, and Nikil Dutt. An Edge-Assisted and Smart System for Real-Time Pain\nMonitoring. In Proceedings - 4th IEEE/ACM Conference on Connected Health: Appli-188 BIBLIOGRAPHY\ncations, Systems and Engineering Technologies, CHASE 2019 , pages 47\u201352. Institute\nof Electrical and Electronics Engineers Inc., 2019.\n[235] Patrick Thiam, Peter Bellmann, Hans A. Kestler, and Friedhelm Schwenker. Explor-\ning deep physiological models for nociceptive pain recognition. Sensors , 19(20):4503,\noct 2019.\n[236] Ahmad Al-Qerem. An efficient machine-learning model based on data augmentation\nfor pain intensity recognition. Egyptian Informatics Journal , 21(4):241\u2013257, 2020.\n[237] R Zhi, C Zhou, J Yu, T Li, and G Zamzmi. Multimodal-based stream integrated\nneural networks for pain assessment. IEICE Transactions on Information and Systems ,\nE104D(12):2184\u20132194, 2021.\n[238] Md Sirajus Salekin, Ghada Zamzmi, Dmitry Goldgof, Rangachar Kasturi, Thao Ho,\nand Yu Sun. Multimodal spatio-temporal deep learning approach for neonatal postop-\nerative pain assessment. Computers in Biology and Medicine , 129, 2021.\n[239] Run Wang, Ke Xu, Hui Feng, and Wei Chen. Hybrid RNN-ANN Based Deep Phys-\niological Network for Pain Recognition. In Proceedings of the Annual International\nConference of the IEEE Engineering in Medicine and Biology Society, EMBS , volume\n2020-July, pages 5584\u20135587. Institute of Electrical and Electronics Engineers Inc., jul\n2020.\n[240] Yun Zhao, Franklin Ly, Qinghang Hong, Zhuowei Cheng, Tyler Santander, Henry T.\nYang, Paul K. Hansma, and Linda Petzold. How Much Does It Hurt: A Deep Learning\nFramework for Chronic Pain Score Assessment. In Cuzzocrea A Zaniolo C Wu X Di\nFatta G. Sheng V ., editor, IEEE International Conference on Data Mining Workshops,\nICDMW , volume 2020-Novem, pages 651\u2013660. IEEE Computer Society, 2020.\n[241] Xinhui Yuan and Marwa Mahmoud. ALANet:Autoencoder-LSTM for pain and pro-\ntective behaviour detection. In Proceedings - 2020 15th IEEE International Confer-\nence on Automatic Face and Gesture Recognition, FG 2020 , pages 824\u2013828, 2020.\n[242] Yi Li, Shreya Ghosh, Jyoti Joshi, and Sharon Oviatt. LSTM-DNN based Approach\nfor Pain Intensity and Protective Behaviour Prediction. In Gomez-Fernandez F Struc\nV ., editor, Proceedings - 2020 15th IEEE International Conference on Automatic Face\nand Gesture Recognition, FG 2020 , pages 819\u2013823. Institute of Electrical and Elec-\ntronics Engineers Inc., 2020.\n[243] Patrick Thiam, Hans A. Kestler, and Friedhelm Schwenker. Multimodal deep denois-\ning convolutional autoencoders for pain intensity classification based on physiologicalBIBLIOGRAPHY 189\nsignals. In ICPRAM 2020 - Proceedings of the 9th International Conference on Pat-\ntern Recognition Applications and Methods , pages 289\u2013296. SCITEPRESS - Science\nand Technology Publications, 2020.\n[244] Patrick Thiam, Heinke Hihn, Daniel A Braun, Hans A Kestler, and Friedhelm\nSchwenker. Multi-Modal Pain Intensity Assessment Based on Physiological Signals:\nA Deep Learning Perspective. Frontiers in Physiology , 12, 2021.\n[245] Saranya Devi Subramaniam and Brindha Dass. Automated Nociceptive Pain Assess-\nment Using Physiological Signals and a Hybrid Deep Learning Network. IEEE Sen-\nsors Journal , 21(3):3335\u20133343, 2021.\n[246] Yi Li, Shreya Ghosh, and Jyoti Joshi. PLAAN: Pain Level Assessment with Anomaly-\ndetection based Network. Journal on Multimodal User Interfaces , 2021.\n[247] Tianyang Lin, Yuxin Wang, Xiangyang Liu, and Xipeng Qiu. A survey of transform-\ners.AI Open , 3:111\u2013132, 2022.\n[248] Philipp Werner, Ayoub Al-Hamadi, Kerstin Limbrecht-Ecklundt, Steffen Walter,\nSascha Gruss, and Harald C. Traue. Automatic Pain Assessment with Facial Activity\nDescriptors. IEEE Transactions on Affective Computing , 8(3):286\u2013299, jul 2017.\n[249] Kunz M and Lautenbacher S. The faces of pain: a cluster analysis of individual\ndifferences in facial activity patterns of pain. European journal of pain (London,\nEngland) , 18(6):813\u2013823, 2014.\n[250] Manon Ranger, Celeste Johnston, and Karthika Anand. Current controversies regard-\ning pain assessment in neonates. Seminars in perinatology , 31:283\u20138, 11 2007.\n[251] Mark R. Jones, Ken P. Ehrhardt, Juan G. Ripoll, Bharat Sharma, Ira W. Padnos,\nRachel J. Kaye, and Alan D. Kaye. Pain in the Elderly, feb 2016.\n[252] Laura Pence Forsythe, Beverly Thorn, Melissa Day, and Grace Shelby. Race and Sex\nDifferences in Primary Appraisals, Catastrophizing, and Experimental Pain Outcomes.\nThe Journal of Pain , 12(5):563\u2013572, may 2011.\n[253] Philipp Tschandl, Noel Codella, Beng \u00a8u Nisa Akay, Giuseppe Argenziano, Ralph P\nBraun, Horacio Cabo, David Gutman, Allan Halpern, Brian Helba, Rainer Hofmann-\nWellenhof, Aimilios Lallas, Jan Lapins, Caterina Longo, Josep Malvehy, Michael A\nMarchetti, Ashfaq Marghoob, Scott Menzies, Amanda Oakley, John Paoli, Susana\nPuig, Christoph Rinner, Cliff Rosendahl, Alon Scope, Christoph Sinz, H Peter Soyer,\nLuc Thomas, Iris Zalaudek, and Harald Kittler. Comparison of the accuracy of human190 BIBLIOGRAPHY\nreaders versus machine-learning algorithms for pigmented skin lesion classification:\nan open, web-based, international, diagnostic study. The Lancet Oncology , 20(7):938\u2013\n947, 2019.\n[254] Guang Yang, Qinghao Ye, and Jun Xia. Unbox the black-box for the medical explain-\nable AI via multi-modal and multi-centre data fusion: A mini-review, two showcases\nand beyond. Information Fusion , 77:29\u201352, 2022.\n[255] Karim Lekadir, Richard Osuala, Catherine Gallin, Noussair Lazrak, Kaisar Kushibar,\nGianna Tsakou, Susanna Ausso, Leonor Cerda Alberich, Kostas Marias, Manolis\nTsiknakis, Sara Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, Henry C\nWoodruff, Philippe Lambin, and Luis Marti-Bonmati. Future-ai: Guiding princi-\nples and consensus recommendations for trustworthy artificial intelligence in medical\nimaging, 2021.\n[256] Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris Kotsiantis. Explainable ai:\nA review of machine learning interpretability methods. Entropy , 23(1):1\u201345, 2021.\n[257] W. James Murdoch, Chandan Singh, Karl Kumbier, Reza Abbasi-Asl, and Bin Yu.\nDefinitions, methods, and applications in interpretable machine learning. Proceedings\nof the National Academy of Sciences , 116(44):22071\u201322080, 2019.\n[258] Rachael E. Jack. Culture and facial expressions of emotion. Visual Cognition , 21(9-\n10):1248\u20131286, 2013.\n[259] Erik Cambria, Newton Howard, Jane Hsu, and Amir Hussain. Sentic blending: Scal-\nable multimodal fusion for the continuous interpretation of semantics and sentics. In\n2013 IEEE Symposium on Computational Intelligence for Human-like Intelligence\n(CIHLI) , pages 108\u2013117, 2013.\n[260] Saurabh Hinduja, Shaun Canavan, and Gurmeet Kaur. Multimodal Fusion of Physio-\nlogical Signals and Facial Action Units for Pain Recognition. In Gomez-Fernandez F\nStruc V ., editor, IEEE International Conference on Automatic Face and Gesture\nRecognition , pages 577\u2013581. Institute of Electrical and Electronics Engineers Inc.,\n2020.\n[261] Tobias B. Ricken, Peter Bellmann, Sascha Gruss, Hans A. Kestler, Steffen Walter,\nand Friedhelm Schwenker. Pain recognition differences between female and male\nsubjects: An analysis based on the physiological signals of the x-ite pain database. In\nCompanion Publication of the 25th International Conference on Multimodal Interac-\ntion, ICMI \u201923 Companion, pages 121\u2013130, New York, NY , USA, 2023. Association\nfor Computing Machinery.BIBLIOGRAPHY 191\n[262] A real-time qrs detection algorithm. IEEE Transactions on Biomedical Engineering ,\nBME-32:230\u2013236, 1985.\n[263] M A Z Fariha, R Ikeura, S Hayakawa, and S Tsutsumi. Analysis of Pan-Tompkins\nAlgorithm Performance with Noisy ECG Signals. Journal of Physics: Conference\nSeries , 1532(1):12022, 2020.\n[264] Feifei Liu, Shoushui Wei, Yibin Li, Xinge Jiang, Zhimin Zhang, Ling Zhang, and\nChengyu Liu. The accuracy on the common Pan-Tompkins based QRS detection\nmethods through low-quality electrocardiogram database. Journal of Medical Imag-\ning and Health Informatics , 7(5):1039\u20131043, 2017.\n[265] Kai Zhao, Yongfu Li, Guoxing Wang, Yu Pu, and Yong Lian. A robust QRS detec-\ntion and accurate R-peak identification algorithm for wearable ECG sensors. Science\nChina Information Sciences , 64(8):182401, 2021.\n[266] Daniel Lopez-Martinez and Rosalind Picard. Continuous Pain Intensity Estimation\nfrom Autonomic Signals with Recurrent Neural Networks. Conference proceedings\n: ... Annual International Conference of the IEEE Engineering in Medicine and Biol-\nogy Society. IEEE Engineering in Medicine and Biology Society. Annual Conference ,\n2018:5624\u20135627, jul 2018.\n[267] Philipp Werner, Ayoub Al-Hamadi, Robert Niese, Steffen Walter, Sascha Gruss, and\nHarald C. Traue. Automatic pain recognition from video and biomedical signals. In\nInternational Conference on Pattern Recognition , pages 4582\u20134587. Institute of Elec-\ntrical and Electronics Engineers Inc., 2014.\n[268] Roberto Cipolla, Yarin Gal, and Alex Kendall. Multi-task learning using uncertainty\nto weigh losses for scene geometry and semantics. In 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 7482\u20137491, 2018.\n[269] Dong Huang, Xiaoyi Feng, Haixi Zhang, Zitong Yu, Jinye Peng, Guoying Zhao, and\nZhaoqiang Xia. Spatio-temporal pain estimation network with measuring pseudo\nheart rate gain. IEEE Transactions on Multimedia , 24:3300\u20133313, 2022.\n[270] Daniel Lopez-Martinez and Rosalind Picard. Continuous Pain Intensity Estimation\nfrom Autonomic Signals with Recurrent Neural Networks. In International Confer-\nence of the IEEE Engineering in Medicine and Biology Society. , volume 2018, pages\n5624\u20135627, 2018.\n[271] Philipp Werner, Daniel Lopez-Martinez, Steffen Walter, Ayoub Al-Hamadi, Sascha\nGruss, and Rosalind Picard. Automatic Recognition Methods Supporting Pain As-\nsessment: A Survey. IEEE Transactions on Affective Computing , 2019.192 BIBLIOGRAPHY\n[272] Anderson Faria Claret, Karina Rabello Casali, Tatiana Sousa Cunha, and Matheus Car-\ndoso Moraes. Automatic classification of emotions based on cardiac signals: A\nsystematic literature review. Annals of Biomedical Engineering , 51(11):2393\u20132414,\n2023.\n[273] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Proceed-\nings of the 31st International Conference on Neural Information Processing Systems ,\nNIPS\u201917, pages 5998\u20136008, Red Hook, NY , USA, 2017. Curran Associates Inc.\n[274] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words:\nTransformers for image recognition at scale, 2021.\n[275] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing XU, and Yunhe Wang. Trans-\nformer in transformer. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and\nJ. Wortman Vaughan, editors, Advances in Neural Information Processing Systems ,\nvolume 34, pages 15908\u201315919. Curran Associates, Inc., 2021.\n[276] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek, Seungjin Choi, and Yee Whye\nTeh. Set transformer: A framework for attention-based permutation-invariant neural\nnetworks. In International conference on machine learning , pages 3744\u20133753. PMLR,\n2019.\n[277] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and\nJoao Carreira. Perceiver: General perception with iterative attention. In International\nconference on machine learning , pages 4651\u20134664. PMLR, 2021.\n[278] Kaipeng Zhang, Zhanpeng Zhang, Zhifeng Li, and Yu Qiao. Joint Face Detection\nand Alignment using Multi-task Cascaded Convolutional Networks. IEEE Signal\nProcessing Letters , 23(10):1499\u20131503, apr 2016.\n[279] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d\nface alignment problem? (and a dataset of 230,000 3d facial landmarks). In Proceed-\nings of the IEEE International Conference on Computer Vision (ICCV) , Oct 2017.\n[280] Qiong Cao, Li Shen, Weidi Xie, Omkar M. Parkhi, and Andrew Zisserman. VG-\nGFace2: A dataset for recognising faces across pose and age. In Proceedings -\n13th IEEE International Conference on Automatic Face and Gesture Recognition, FG\n2018 , pages 67\u201374. Institute of Electrical and Electronics Engineers Inc., oct 2018.BIBLIOGRAPHY 193\n[281] Dan Hendrycks, Norman Mu, Ekin D Cubuk, Barret Zoph, Justin Gilmer, and Balaji\nLakshminarayanan. Augmix: A simple data processing method to improve robustness\nand uncertainty. arXiv preprint arXiv:1912.02781 , 2019.\n[282] Samuel G. MG\u2019Oller and Frank Hutter. Trivialaugment: Tuning-free yet state-of-the-\nart data augmentation. In 2021 IEEE/CVF International Conference on Computer\nVision (ICCV) , pages 754\u2013762, 2021.\n[283] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention\nvisualization. In Proceedings of the IEEE/CVF conference on computer vision and\npattern recognition , pages 782\u2013791, 2021.\n[284] Sabrina Patania, Giuseppe Boccignone, Sathya Bur \u02c7si\u00b4c, Alessandro D\u2019Amelio, and\nRaffaella Lanzarotti. Deep graph neural network for video-based facial pain expres-\nsion assessment. SAC \u201922, pages 585\u2013591, New York, NY , USA, 2022. Association\nfor Computing Machinery.\n[285] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, \u0141 ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon,\nU. V on Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Processing Systems , volume 30. Curran\nAssociates, Inc., 2017.\n[286] Tsz-Him Cheung and Dit-Yan Yeung. {MODALS }: Modality-agnostic automated\ndata augmentation in the latent space. In International Conference on Learning Rep-\nresentations , 2021.\n[287] Ali Mollahosseini, Behzad Hasani, and Mohammad H. Mahoor. Affectnet: A\ndatabase for facial expression, valence, and arousal computing in the wild. IEEE\nTransactions on Affective Computing , 10(1):18\u201331, 2019.\n[288] Shichuan Du, Yong Tao, and Aleix M. Martinez. Compound facial expressions of\nemotion. Proceedings of the National Academy of Sciences , 111(15):E1454\u2013E1462,\n2014.\n[289] Shan Li, Weihong Deng, and JunPing Du. Reliable crowdsourcing and deep locality-\npreserving learning for expression recognition in the wild. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition (CVPR) , July 2017.\n[290] Roberto Cipolla, Yarin Gal, and Alex Kendall. Multi-task learning using uncertainty\nto weigh losses for scene geometry and semantics. In 2018 IEEE/CVF Conference on\nComputer Vision and Pattern Recognition , pages 7482\u20137491, 2018.194 BIBLIOGRAPHY\n[291] Mohammad Kachuee, Shayan Fazeli, and Majid Sarrafzadeh. Ecg heartbeat classifi-\ncation: A deep transferable representation. In 2018 IEEE International Conference\non Healthcare Informatics (ICHI) , pages 443\u2013444, 2018.\n[292] G.B. Moody and R.G. Mark. The impact of the mit-bih arrhythmia database. IEEE\nEngineering in Medicine and Biology Magazine , 20(3):45\u201350, 2001.\n[293] R. Bousseljot, D. Kreiseler, and A. Schnabel. Nutzung der ekg-signaldatenbank car-\ndiodat der ptb G\u2019Ober das internet. Biomedical Engineering / Biomedizinische Tech-\nnik, 40(s1):317\u2013318, 1995.\n[294] AL Goldberger, LA Amaral, L Glass, JM Hausdorff, PC Ivanov, RG Mark, JE Mietus,\nGB Moody, CK Peng, and HE Stanley. Physiobank, physiotoolkit, and physionet:\ncomponents of a new research resource for complex physiologic signals. Circulation ,\n101(23):E215\u201320, June 2000.\n[295] Philipp Werner, Ayoub Al-Hamadi, and Steffen Walter. Analysis of facial expressive-\nness during experimentally induced heat pain. In 2017 Seventh International Con-\nference on Affective Computing and Intelligent Interaction Workshops and Demos\n(ACIIW) , pages 176\u2013180, 2017.\n[296] Philipp Werner, Ayoub Al-Hamadi, Kerstin Limbrecht-Ecklundt, Steffen Walter,\nSascha Gruss, and Harald C Traue. Automatic pain assessment with facial activity\ndescriptors. IEEE Transactions on Affective Computing , 8(3):286\u2013299, 2016.\n[297] Markus K \u00a8achele, Philipp Werner, Ayoub Al-Hamadi, G \u00a8unther Palm, Steffen Walter,\nand Friedhelm Schwenker. Bio-visual fusion for person-independent recognition of\npain intensity. In Multiple Classifier Systems , pages 220\u2013230. Springer International\nPublishing, 2015.\n[298] Vasileios C. Pezoulas, Dimitrios I. Zaridis, Eugenia Mylona, Christos Androutsos,\nKosmas Apostolidis, Nikolaos S. Tachos, and Dimitrios I. Fotiadis. Synthetic data\ngeneration methods in healthcare: A review on open-source tools and methods. Com-\nputational and Structural Biotechnology Journal , 23:2892\u20132910, 2024.\n[299] Mustafa MM Al Qudah, Ahmad SA Mohamed, and Syaheerah L Lutfi. Affective\nstate recognition using thermal-based imaging: A survey. Computer Systems Science\n& Engineering , 37(1), 2021.\n[300] Stephanos Ioannou, Vittorio Gallese, and Arcangelo Merla. Thermal infrared imaging\nin psychophysiology: Potentialities and limits. Psychophysiology , 51(10):951\u2013963,\n2014.BIBLIOGRAPHY 195\n[301] Sophie Jarlier, Didier Grandjean, Sylvain Delplanque, Karim N\u2019Diaye, Isabelle\nCayeux, Maria Ines Velazco, David Sander, Patrik Vuilleumier, and Klaus R. Scherer.\nThermal analysis of facial muscles contractions. IEEE Transactions on Affective Com-\nputing , 2(1):2\u20139, 2011.\n[302] A Merla, L Di Donato, PM Rossini, and GL Romani. Emotion detection through func-\ntional infrared imaging: preliminary results. Biomedizinische Technick , 48(2):284\u2013\n286, 2004.\n[303] Youssef Mohamed, Arzu G \u00a8uneysu, S \u00b4everin Lemaignan, and Iolanda Leite. Multi-\nmodal affect detection using thermal and optical imaging in a gamified robotic exer-\ncise. International Journal of Social Robotics , Oct 2023.\n[304] Varlik K Erel and Heval Selman Ozkan. Thermal camera as a pain monitor. Journal\nof Pain Research , 10:2827\u20132832, 2017.\n[305] Mohammad A. Haque, Ruben B. Bautista, Fatemeh Noroozi, Kaustubh Kulkarni,\nChristian B. Laursen, Ramin Irani, Marco Bellantonio, Sergio Escalera, Golamreza\nAnbarjafari, Kamal Nasrollahi, Ole K. Andersen, Erika G. Spaich, and Thomas B.\nMoeslund. Deep multimodal pain recognition: A database and comparison of spatio-\ntemporal visual modalities. In 2018 13th IEEE International Conference on Automatic\nFace & Gesture Recognition (FG 2018) , pages 250\u2013257, 2018.\n[306] Mehdi Mirza and Simon Osindero. Conditional generative adversarial nets. arXiv\npreprint arXiv:1411.1784 , 2014.\n[307] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C\nCourville. Improved training of wasserstein gans. In I. Guyon, U. V on Luxburg,\nS. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances\nin Neural Information Processing Systems , volume 30. Curran Associates, Inc., 2017.\n[308] Ting-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan\nCatanzaro. High-resolution image synthesis and semantic manipulation with condi-\ntional gans. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR) , June 2018.\n[309] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image trans-\nlation with conditional adversarial networks. In Proceedings of the IEEE conference\non computer vision and pattern recognition , pages 1125\u20131134, 2017.\n[310] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai,\nThomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkor-\neit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for196 BIBLIOGRAPHY\nvision. In M. Ranzato, A. Beygelzimer, Y . Dauphin, P.S. Liang, and J. Wortman\nVaughan, editors, Advances in Neural Information Processing Systems , volume 34,\npages 24261\u201324272. Curran Associates, Inc., 2021.\n[311] Yehui Tang, Kai Han, Jianyuan Guo, Chang Xu, Yanxi Li, Chao Xu, and Yunhe Wang.\nAn image patch is a wave: Phase-aware vision mlp. In 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR) , pages 10925\u201310934, 2022.\n[312] Madina Abdrakhmanova, Askat Kuzdeuov, Sheikh Jarju, Yerbolat Khassanov,\nMichael Lewis, and Huseyin Atakan Varol. Speakingfaces: A large-scale multimodal\ndataset of voice commands with visual and thermal video streams. Sensors , 21(10),\n2021.\n[313] Gwangbin Bae, Martin de La Gorce, Tadas Baltru \u02c7saitis, Charlie Hewitt, Dong Chen,\nJulien Valentin, Roberto Cipolla, and Jingjing Shen. Digiface-1m: 1 million digital\nface images for face recognition. In Proceedings of the IEEE/CVF Winter Conference\non Applications of Computer Vision (WACV) , pages 3526\u20133535, January 2023.\n[314] Xu Zheng, Yuanhuiyi Lyu, and Lin Wang. Learning modality-agnostic representation\nfor semantic segmentation from any modalities, 2024.\n[315] Dingkang Yang, Mingcheng Li, Linhao Qu, Kun Yang, Peng Zhai, Song Wang, and\nLihua Zhang. Asynchronous multimodal video sequence fusion via learning modality-\nexclusive and -agnostic representations, 2024.\n[316] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Syd-\nney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brun-\nskill, et al. On the opportunities and risks of foundation models. arXiv preprint\narXiv:2108.07258 , 2021.\n[317] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura\nGustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr\nDollar, and Ross Girshick. Segment anything. In 2023 IEEE/CVF International Con-\nference on Computer Vision (ICCV) , pages 3992\u20134003, 2023.\n[318] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything\nin medical images. Nature Communications , 15(1):654, 2024.\n[319] Junde Wu, Wei Ji, Yuanpei Liu, Huazhu Fu, Min Xu, Yanwu Xu, and Yueming Jin.\nMedical sam adapter: Adapting segment anything model for medical image segmen-\ntation. arXiv preprint arXiv:2304.12620 , 2023.BIBLIOGRAPHY 197\n[320] Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander\nNovikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias\nSpringenberg, et al. A generalist agent. arXiv preprint arXiv:2205.06175 , 2022.\n[321] Muhammad Awais, Muzammal Naseer, Salman Khan, Rao Muhammad Anwer,\nHisham Cholakkal, Mubarak Shah, Ming-Hsuan Yang, and Fahad Shahbaz Khan.\nFoundational models defining a new era in vision: A survey and outlook. arXiv\npreprint arXiv:2307.13721 , 2023.\n[322] Pau Rodriguez, Guillem Cucurull, Jordi Gonzalez, Josep M. Gonfaus, Kamal Nasrol-\nlahi, Thomas B. Moeslund, and F. Xavier Roca. Deep pain: Exploiting long short-term\nmemory networks for facial expression classification. IEEE Transactions on Cyber-\nnetics , 52(5):3314\u20133324, 2022.\n[323] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua\nZhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,\nSylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recog-\nnition at scale. arXiv preprint arXiv:2010.11929 , 2020.\n[324] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan,\nand Zicheng Liu. Mobile-former: Bridging mobilenet and transformer. In 2022\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) , pages\n5260\u20135269, 2022.\n[325] Sitong Wu, Tianyi Wu, Haoru Tan, and Guodong Guo. Pale transformer: A general\nvision transformer backbone with pale-shaped attention. In Proceedings of the AAAI\nConference on Artificial Intelligence , volume 36, pages 2731\u20132739, 2022.\n[326] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin,\nand Baining Guo. Swin transformer: Hierarchical vision transformer using shifted\nwindows. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV) ,\npages 9992\u201310002, 2021.\n[327] Xinyu Liu, Houwen Peng, Ningxin Zheng, Yuqing Yang, Han Hu, and Yixuan Yuan.\nEfficientvit: Memory efficient vision transformer with cascaded group attention. In\n2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) ,\npages 14420\u201314430, 2023.\n[328] Judith M. Ford, Vanessa A. Palzes, Brian J. Roach, and Daniel H. Mathalon. Did I\nDo That? Abnormal Predictive Processes in Schizophrenia When Button Pressing to\nDeliver a Tone. Schizophrenia Bulletin , 40(4):804\u2013812, 07 2013.198 BIBLIOGRAPHY\n[329] David Gaddy and Dan Klein. Digital voicing of silent speech. In Proceedings of the\n2020 Conference on Empirical Methods in Natural Language Processing (EMNLP) ,\npages 5521\u20135530, Online, 2020. Association for Computational Linguistics.\n[330] Ekin D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V . Le. Randaugment: Prac-\ntical automated data augmentation with a reduced search space. In 2020 IEEE/CVF\nConference on Computer Vision and Pattern Recognition Workshops (CVPRW) , pages\n3008\u20133017, 2020.\n[331] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wo-\njna. Rethinking the inception architecture for computer vision. In Proceedings of\nthe IEEE conference on computer vision and pattern recognition , pages 2818\u20132826,\n2016.\n[332] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya Sutskever, and Ruslan R\nSalakhutdinov. Improving neural networks by preventing co-adaptation of feature\ndetectors. arXiv preprint arXiv:1207.0580 , 2012.\n[333] Raul Fernandez Rojas, Niraj Hirachan, Nicholas Brown, Gordon Waddington, Luke\nMurtagh, Ben Seymour, and Roland Goecke. Multimodal physiological sensing for\nthe assessment of acute pain. Frontiers in Pain Research , 4, 2023.\n[334] Raul Fernandez Rojas, Xu Huang, Jesus Hernandez-Juarez, and Keng-Liang Ou.\nPhysiological fluctuations show frequency-specific networks in fnirs signals during\nresting state. In 2017 39th Annual International Conference of the IEEE Engineering\nin Medicine and Biology Society (EMBC) , pages 2550\u20132553, 2017.\n[335] John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima Anandkumar, and\nBryan Catanzaro. Efficient token mixing for transformers via adaptive fourier neural\noperators. In International Conference on Learning Representations , 2021.\n[336] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang\nZhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition ,\npages 10371\u201310381, 2024.\n[337] Fadi Boutros, Marco Huber, Patrick Siebke, Tim Rieber, and Naser Damer. Sface:\nPrivacy-friendly and accurate face recognition using synthetic data. In 2022 IEEE\nInternational Joint Conference on Biometrics (IJCB) , pages 1\u201311, 2022.\n[338] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Fractalnet: Ultra-deep\nneural networks without residuals. arXiv preprint arXiv:1605.07648 , 2016.BIBLIOGRAPHY 199\n[339] Philipp Werner, Ayoub Al-Hamadi, Robert Niese, Steffen Walter, Sascha Gruss, and\nHarald C. Traue. Automatic pain recognition from video and biomedical signals. In\n2014 22nd International Conference on Pattern Recognition , pages 4582\u20134587, 2014.\n[340] Ruijing Yang, Ziyu Guan, Zitong Yu, Xiaoyi Feng, Jinye Peng, and Guoying Zhao.\nNon-contact pain recognition from video sequences with remote physiological mea-\nsurements prediction. In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth Interna-\ntional Joint Conference on Artificial Intelligence, IJCAI-21 , pages 1231\u20131237. Inter-\nnational Joint Conferences on Artificial Intelligence Organization, 8 2021.\n[341] Zhenyuan Lu, Burcu Ozek, and Sagar Kamarthi. Transformer encoder with multi-\nscale deep learning for pain classification using physiological signals. Frontiers in\nPhysiology , 14, 2023.\n[342] Kim Ngan Phan, Ngumimi Karen Iyortsuun, Sudarshan Pant, Hyung-Jeong Yang, and\nSoo-Hyung Kim. Pain recognition with physiological signals using multi-level con-\ntext information. IEEE Access , 11:20114\u201320127, 2023.\n[343] Mingzhe Jiang, Yufei Li, Jiangshan He, Yuqiang Yang, Hui Xie, and Xueli Chen.\nPhysiological time-series fusion with hybrid attention for adaptive recognition of pain.\nIEEE Journal of Biomedical and Health Informatics , pages 1\u20139, 2024.\n[344] Ruicong Zhi and Junwei Yu. Multi-modal fusion based automatic pain assessment.\nInProceedings of 2019 IEEE 8th Joint International Information Technology and Ar-\ntificial Intelligence Conference, ITAIC 2019 , pages 1378\u20131382. Institute of Electrical\nand Electronics Engineers Inc., may 2019.\n[345] Pooja Prajod, Dominik Schiller, Daksitha Withanage Don, and Elisabeth Andre. Faces\nof experimental pain: Transferability of deep learned heat pain features to electrical\npain, 2024.\n[346] Minh-Duc Nguyen, Hyung-Jeong Yang, Soo-Hyung Kim, Ji-Eun Shin, and Seung-\nWon Kim. Transformer with leveraged masked autoencoder for video-based pain\nassessment, 2024.\n[347] Manisha Shantaram Patil and Hitendra Dhansing Patil. Logistic regression based\nmodel for pain intensity level detection from biomedical signal. International Re-\nsearch Journal of Multidisciplinary Scope , 2024.\n[348] Manisha S. Patil and Hitendra D. Patil. Ensemble neural networks for multimodal\nacute pain intensity evaluation using video and physiological signals. Journal of Com-\nputational Analysis and Applications (JoCAAA) , 33(05):779\u2013791, Sep. 2024.200 BIBLIOGRAPHY\n[349] Xinwei Ji, Tianming Zhao, Wei Li, and Albert Zomaya. Automatic pain assess-\nment with ultra-short electrodermal activity signal. In Proceedings of the 38th\nACM/SIGAPP Symposium on Applied Computing , SAC \u201923, pages 618\u2013625, New\nYork, NY , USA, 2023. Association for Computing Machinery.\n[350] Markus K \u00a8achele, Patrick Thiam, Mohammadreza Amirian, Philipp Werner, Steffen\nWalter, Friedhelm Schwenker, and G \u00a8unther Palm. Multimodal data fusion for person-\nindependent, continuous estimation of pain intensity. In Lazaros Iliadis and Chrisina\nJayne, editors, Engineering Applications of Neural Networks , pages 275\u2013285, Cham,\n2015. Springer International Publishing.\n[351] Fatemeh Pouromran, Srinivasan Radhakrishnan, and Sagar Kamarthi. Exploration\nof physiological sensors, features, and machine learning models for pain intensity\nestimation. PLOS ONE , 16(7):1\u201317, 07 2021.\n[352] Patrick Thiam, Heinke Hihn, Daniel A. Braun, Hans A. Kestler, and Friedhelm\nSchwenker. Multi-modal pain intensity assessment based on physiological signals:\nA deep learning perspective. Frontiers in Physiology , 12, 2021.\n[353] Mingzhe Jiang, Riitta Rosio, Sanna Salantera, Amir M. Rahmani, Pasi Liljeberg,\nDaniel S. da Silva, Victor Hugo C. de Albuquerque, and Wanqing Wu. Personal-\nized and adaptive neural networks for pain detection from multi-modal physiological\nfeatures. Expert Systems with Applications , 235:121082, 2024.\n[354] Markus K \u00a8achele, Patrick Thiam, Mohammadreza Amirian, Friedhelm Schwenker,\nand GG\u2019Onther Palm. Methods for person-centered continuous pain intensity assess-\nment from bio-physiological channels. IEEE Journal of Selected Topics in Signal\nProcessing , 10(5):854\u2013864, 2016.\n[355] Peter Bellmann and Friedhelm Schwenker. Automated pain assessment: Is it use-\nful to combine person-specific data samples? In 2020 IEEE Symposium Series on\nComputational Intelligence (SSCI) , pages 1588\u20131593, 2020.Appendix\nSupplementary Metrics\nTable 1: Results utilizing the video modality reported on precision, recall, and F1 score\n(refer to Section 5.3).\nEpochs MetricPretraining stage Pipeline Augmentations Task\n1st2ndFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n500Precision \u2713 - \u2713 -\u2713 - - 72.53 31.24\nRecall \u2713 - \u2713 -\u2713 - - 74.31 29.61\nF1 \u2713 - \u2713 -\u2713 - - 71.95 27.16\n500Precision - \u2713 \u2713 -\u2713 - - 74.21 33.36\nRecall - \u2713 \u2713 -\u2713 - - 76.74 33.41\nF1 - \u2713 \u2713 -\u2713 - - 72.24 28.77\n500Precision - \u2713 - \u2713 \u2713 - - 68.11 31.50\nRecall - \u2713 - \u2713 \u2713 - - 72.15 27.99\nF1 - \u2713 - \u2713 \u2713 - - 65.92 25.14\n500Precision - \u2713 \u2713 \u2713 \u2713 - - 65.14 27.78\nRecall - \u2713 \u2713 \u2713 \u2713 - - 70.36 18.42\nF1 - \u2713 \u2713 \u2713 \u2713 - - 61.93 18.86\n500Precision - \u2713 \u2713 \u2713c\u2713 - - 74.88 33.96\nRecall - \u2713 \u2713 \u2713c\u2713 - - 77.41 34.31\nF1 - \u2713 \u2713 \u2713c\u2713 - - 73.90 29.20\n500Precision - \u2713 \u2713 \u2713c\u2713 \u2713 - 73.09 32.17\nRecall - \u2713 \u2713 \u2713c\u2713 \u2713 - 75.72 28.41\nF1 - \u2713 \u2713 \u2713c\u2713 \u2713 - 71.92 26.02\n500Precision - \u2713 \u2713 \u2713c\u2713 - \u2713 74.87 33.88\nRecall - \u2713 \u2713 \u2713c\u2713 - \u2713 77.80 29.30\nF1 - \u2713 \u2713 \u2713c\u2713 - \u2713 73.59 27.74\n500Precision - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 73.12 32.79\nRecall - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 76.18 28.51\nF1 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 71.91 26.57\n800Precision - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 77.15 35.39\nRecall - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 79.35 35.11\nF1 - \u2713 \u2713 \u2713c\u2713 \u2713 \u2713 76.33 31.70\n201202 Appendix\nTable 2: Results utilizing the heart rate modality reported on precision, recall, and F1 score\n(refer to Section 5.3).\nEpochs MetricHR\nEncoderAugmentations Task\nBasic Mask AugmNet NP vs P 4 MC\n500Precision \u2713 \u2713 - - 61.73 27.66\nRecall \u2713 \u2713 - - 65.04 20.91\nF1 \u2713 \u2713 - - 57.74 19.73\n500Precision - \u2713 - - 61.97 27.71\nRecall - \u2713 - - 66.01 22.13\nF1 - \u2713 - - 57.79 20.61\n500Precision \u2713 \u2713 \u2713 - 61.97 27.80\nRecall \u2713 \u2713 \u2713 - 65.27 20.98\nF1 \u2713 \u2713 \u2713 - 57.38 20.97\n500Precision \u2713 \u2713 - \u2713 62.09 28.00\nRecall \u2713 \u2713 - \u2713 65.73 21.27\nF1 \u2713 \u2713 - \u2713 58.04 21.61\n500Precision \u2713 \u2713 \u2713 \u2713 61.63 27.86\nRecall \u2713 \u2713 \u2713 \u2713 65.08 21.24\nF1 \u2713 \u2713 \u2713 \u2713 56.78 21.17\n800Precision \u2713 \u2713 \u2713 \u2713 65.44 29.73\nRecall \u2713 \u2713 \u2713 \u2713 69.85 27.40\nF1 \u2713 \u2713 \u2713 \u2713 62.07 23.71\n800Precision \u2713 - - \u2713 67.07 31.11\nRecall \u2713 - - \u2713 71.24 29.33\nF1 \u2713 - - \u2713 63.97 25.83\nTable 3: Results utilizing the video & the heart rate modality reported on precision, recall\nand F1 score (refer to Section 5.3).\nEpochs MetricHR\nEncoderPipeline Augmentations Task\nFull frame Tiles Basic Mask AugmNet NP vs P 4 MC\n800Precision \u2713 \u2713 \u2713c- - \u2713 82.69 39.13\nRecall \u2713 \u2713 \u2713c- - \u2713 84.71 37.67\nF1 \u2713 \u2713 \u2713c- - \u2713 81.44 36.31203\nTable 4: Results utilizing the RGB video modality, reported on recall and F1 score (refer to\nSection 6.2).\nEpochsAugmentations\nMetricTask\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 30-50 0.9Recall 71.29 29.61\nF1 68.53 27.22\n200 \u2713 30-50 0.9Recall 71.93 24.43\nF1 69.61 23.78\n300 \u2713 30-50 0.9Recall 71.34 30.64\nF1 69.65 26.12\nTable 5: Results utilizing the synthetic thermal video modality, reported on recall and F1\nscore (refer to Section 6.2).\nEpochsAugmentations\nMetricTask\nBasic Masking P(Aug) NP vs P 4 MC\n200 \u2713 30-50 0.9Recall 72.04 28.80\nF1 69.16 26.45\n200 \u2713 30-50 0.9Recall 72.18 30.89\nF1 69.44 26.45\n300 \u2713 30-50 0.9Recall 72.52 24.96\nF1 70.01 23.43204 Appendix\nTable 6: Results utilizing the fusion of RGB & synthetic thermal video modality, reported\non recall and F1 score (refer to Section 6.2).\nEpochsFusion\nweightsAugmentations\nMetricTask\nBasic Masking P(Aug) NP vs P 4 MC\n100 \u2013 \u2713 30-50 0.9Recall 67.05 21.68\nF1 62.96 18.29\n100 W2 \u2713 30-50 0.9Recall 68.72 21.69\nF1 62.98 19.35\n100 W3 \u2713 30-50 0.9Recall 66.12 23.12\nF1 59.72 19.67\n300 W2 \u2713 30-50 0.9Recall 71.40 26.39\nF1 68.82 26.18\n500 W2 \u2713 10-20 0.7Recall 73.20 29.69\nF1 70.30 27.84\nTable 7: Results of the proposed approaches, reported on macro-averaged precision, recall\nand F1 score (refer to Section 7.2).\nModality ApproachMetrics\nPrecision Recall F1\nVideo Addition 44.91 44.97 44.60\nfNIRS HbO & Addition 44.68 45.08 43.60\nFusion Single Diagram 46.76 47.29 46.70205\nSupplementary Figures\nFigure 1: Attention maps generated by the Spatial-Module .Yellow and red colors signify\nintense focus on specific areas. (1strow) Sequence of original frames. (2ndrow)\nDerived from the Spatial-Module after initial stage pretraining. (3rdrow) Derived\nfrom the Spatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module following training on BioVid (refer to Section 5.3).206 Appendix\nFigure 2: Attention maps generated by the Spatial-Module .Yellow and red colors signify\nintense focus on specific areas. (1strow) Sequence of original frames. (2ndrow)\nDerived from the Spatial-Module after initial stage pretraining. (3rdrow) Derived\nfrom the Spatial-Module post second stage pretraining. (4throw) Derived from the\nSpatial-Module following training on BioVid (refer to Section 5.3).207\nabc\nFigure 3: Additional attention maps from the PainViT\u20132 (refer to Section 7.2).208Acronyms\nAI A rtificial Intelligence\nECG E lectrocardio graphy\nEDA E lectro dermal Activity\nEEG E lectro encephalo graphy\nEMG E lectro myography\nFACS F acial Action Coding System\nFLOPS Fl oating-point Operations perSecond\nfMRI F unctional Magnetic Resonance Imaging\nfNIRS F unctional Near-Infrared Spectroscopy\nGSR G alvanic SkinResponse\nML M achine Learning\nNIPS N eonatal/ Infant PainScale\nNIRS N ear-Infrared Spectroscopy\nPPG P hotoplethysmo graphy\nPSPI P rkachin and Solomon PainIntensity Scale\nRGB R edGreen Blue\nSpO2 S aturation of Peripheral Oxygen\nV AS V isual Analog Scale\nVRS V erbal Rating Scale\n209"}}